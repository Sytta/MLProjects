{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from implementation import *\n",
    "#from proj1_helpers import *\n",
    "#from cross_validation import *\n",
    "from data_preprocessing import *\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "TRAIN_PATH = \"./data/train.csv\"\n",
    "TEST_PATH = \"./data/test.csv\"\n",
    "USE_COLS = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 28, 29, 31)\n",
    "\n",
    "def make_predictions(weight, degree, name):\n",
    "    y_test, x_test, ids_test = load_csv_data(\"./data/test.csv\", sub_sample=False)\n",
    "    x_test, mean_x_test, std_x_test = standardize(x_test)\n",
    "    tx_poly = build_poly(x_test, degree)\n",
    "    y_pred = predict_labels(weight, tx_poly)\n",
    "    create_csv_submission(ids_test, y_pred,  name +\".csv\")\n",
    "\n",
    "\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w, mse=True):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    if mse:\n",
    "        return calculate_mse(e)\n",
    "    else:\n",
    "        return calculate_mae(e)\n",
    "    \n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    a = tx.T.dot(tx) + lambda_ * 2 * len(y) * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y)\n",
    "    w_ridge = np.linalg.solve(a, b)\n",
    "    return w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False, cut_values=True):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    if (cut_values):\n",
    "        print('load_csv_data : dropping uniform distribution values')\n",
    "        # drop Pri_tau_phi(17), Pri_lep_phi(20), Pri_met_phi(22), Pri_jet_leading_Phi(27), Pri_jet_subleading_phi(30)\n",
    "        # because of uniform distribution\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, \\\n",
    "            usecols=USE_COLS)\n",
    "    else:\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "        \n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('Split_data_according_to_jet')\n",
    "    print('Loading files...')\n",
    "    y_tr, x_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "    y_te, x_te, ids_te = load_csv_data(TEST_PATH)\n",
    "    return y_tr, x_tr, ids_tr, y_te, x_te, ids_te\n",
    "\n",
    "def load_headers():\n",
    "    \"\"\"Load all the headers from the training file and drop the unnecessary ones\"\"\"\n",
    "    with open(TRAIN_PATH) as train_file:\n",
    "        reader = csv.reader(train_file)\n",
    "        headers = next(reader)\n",
    "    \n",
    "    # Only use the columns in USE_COLS\n",
    "    headers = [headers[i] for i in USE_COLS]\n",
    "    # drop ID and Predictions cols\n",
    "    headers = headers[2:]\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_data_according_to_jet\n",
      "Loading files...\n",
      "load_csv_data : dropping uniform distribution values\n",
      "load_csv_data : dropping uniform distribution values\n"
     ]
    }
   ],
   "source": [
    "y_tr, x_tr, ids_tr, y_te, x_te, ids_te = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DER_mass_MMC',\n",
       " 'DER_mass_transverse_met_lep',\n",
       " 'DER_mass_vis',\n",
       " 'DER_pt_h',\n",
       " 'DER_deltaeta_jet_jet',\n",
       " 'DER_mass_jet_jet',\n",
       " 'DER_prodeta_jet_jet',\n",
       " 'DER_deltar_tau_lep',\n",
       " 'DER_pt_tot',\n",
       " 'DER_sum_pt',\n",
       " 'DER_pt_ratio_lep_tau',\n",
       " 'DER_met_phi_centrality',\n",
       " 'DER_lep_eta_centrality',\n",
       " 'PRI_tau_pt',\n",
       " 'PRI_tau_eta',\n",
       " 'PRI_lep_pt',\n",
       " 'PRI_lep_eta',\n",
       " 'PRI_met',\n",
       " 'PRI_met_sumet',\n",
       " 'PRI_jet_num',\n",
       " 'PRI_jet_leading_pt',\n",
       " 'PRI_jet_leading_eta',\n",
       " 'PRI_jet_subleading_pt',\n",
       " 'PRI_jet_subleading_eta',\n",
       " 'PRI_jet_all_pt']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = load_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_in_DER_pt_h(y_tr, x_tr, ids_tr, jet):\n",
    "    print(\"Removing outliers in Der_pt_h\")\n",
    "    # Remove the outliers in DER_pt_h (col 3):\n",
    "    #  JET 0: 2834.999 when the max value is 117.707 outside of outlier - threshold to 120 \n",
    "    #  JET 2: 1053.807 when max value is 734 outside of outlier- Threshold to 800\n",
    "    OUTLIERS = [120, 999, 800, 999]\n",
    "    outlier = OUTLIERS[jet]\n",
    "    tr_smaller_than_outlier = (x_tr[:, 3] < outlier)\n",
    "    x_tr = x_tr[tr_smaller_than_outlier]\n",
    "    y_tr = y_tr[tr_smaller_than_outlier]\n",
    "    ids_tr = ids_tr[tr_smaller_than_outlier]\n",
    "    return y_tr, x_tr, ids_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_NAN_columns(x_tr, x_te, headers_jet):\n",
    "    print(\"Removing nan columns\")\n",
    "    nan_cols = []\n",
    "    # Find all columns with -999\n",
    "    for col_idx in range(x_tr.shape[1]):\n",
    "        col = x_tr[:, col_idx]\n",
    "        nb_nan_in_col = len(x_tr[col == -999])\n",
    "        # A column has all NaN if len of col = nb NaN values in col\n",
    "        if (nb_nan_in_col == len(col)):\n",
    "            nan_cols.append(col_idx)\n",
    "    \n",
    "    # Remove all nan columns\n",
    "    x_tr_updated = np.delete(x_tr, nan_cols, axis=1)\n",
    "    x_te_updated = np.delete(x_te, nan_cols, axis=1)\n",
    "    headers_jet_updated = np.delete(headers_jet, nan_cols)\n",
    "    \n",
    "    return x_tr_updated, x_te_updated, headers_jet_updated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_mass(x, y, ids):\n",
    "        # Get all the rows idx with invalid mass (i.e. DER_mass_MMC = -999)\n",
    "        invalid_mass_row_idx = x[:, 0] == -999\n",
    "        valid_mass_row_idx = x[:, 0] > 0\n",
    "        # Process for each data table\n",
    "        x_invalid_mass = x[invalid_mass_row_idx]\n",
    "        x_valid_mass = x[valid_mass_row_idx]\n",
    "        y_invalid_mass = y[invalid_mass_row_idx]\n",
    "        y_valid_mass = y[valid_mass_row_idx]\n",
    "        ids_invalid_mass = ids[invalid_mass_row_idx]\n",
    "        ids_valid_mass = ids[valid_mass_row_idx]\n",
    "        \n",
    "        return x_invalid_mass, x_valid_mass, y_invalid_mass, y_valid_mass, ids_invalid_mass, ids_valid_mass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_csv(x, y, ids, headers, jet, isTrain, isMassValid):\n",
    "    \"\"\"\n",
    "    Write data into new csv file\n",
    "    \"\"\"\n",
    "    # Add 'Id' & 'Prediction' to headers\n",
    "    headers = np.insert(headers, 0, ['Id', 'Prediction'])\n",
    "    \n",
    "    # Remove 'DER_mass_MMC' if mass is not valid\n",
    "    if not isMassValid:\n",
    "        headers = np.delete(headers, np.where(headers =='DER_mass_MMC'))\n",
    "    \n",
    "    # Generate file name\n",
    "    base = './data/train_' if isTrain else './data/test_'\n",
    "    valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "    file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "    \n",
    "    print(\"Outputing {}\".format(file_name))\n",
    "        \n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        data_ = dict.fromkeys(headers)\n",
    "        # Transform -1 and 1 into 's' and 'b'\n",
    "        for id_, y_, x_ in zip(ids, y, x):\n",
    "            data_['Id'] = int(id_)\n",
    "            if (y_ != -1 and y_ !=1):\n",
    "                raise Exception('Prediction not -1 and 1!!!')\n",
    "            data_['Prediction'] = 's' if y_ == 1 else 'b'\n",
    "            \n",
    "            for idx, x_value in enumerate(x_):\n",
    "                data_[headers[idx + 2]] = float(x_value)\n",
    "            writer.writerow(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers): \n",
    "    for jet in range(4):\n",
    "        print(\"\\n\\nSplitting for jet {}\".format(jet))\n",
    "\n",
    "        # PRI_jet_num (24 -> 24 - 3 cols dropped before 24 - 2 cols (id, label) = col 19)\n",
    "        col_jet = 19\n",
    "        \n",
    "        # TRAIN - Get all the rows having Pri_jet_num = jet for TRAINING set and delete PRI_jet_num col\n",
    "        x_tr_jet = x_tr[x_tr[:, col_jet] == jet]\n",
    "        x_tr_jet = np.delete(x_tr_jet, col_jet, axis=1)\n",
    "        # Delete PRI_jet_num in headers\n",
    "        headers_jet = np.delete(headers, col_jet)\n",
    "\n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_tr_jet = y_tr[x_tr[:, col_jet] == jet]\n",
    "        ids_tr_jet = ids_tr[x_tr[:, col_jet] == jet]\n",
    "        \n",
    "        # TEST - Get all the rows having Pri_jet_num = jet for TEST set and delete PRI_jet_num col\n",
    "        x_te_jet = x_te[x_te[:, col_jet] == jet]\n",
    "        x_te_jet = np.delete(x_te_jet, col_jet, axis=1)\n",
    "                \n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_te_jet = y_te[x_te[:, col_jet] == jet]\n",
    "        ids_te_jet = ids_te[x_te[:, col_jet] == jet]\n",
    "            \n",
    "        # Remove outliers\n",
    "        y_tr_jet, x_tr_jet, ids_tr_jet = remove_outlier_in_DER_pt_h(y_tr_jet, x_tr_jet, ids_tr_jet, jet)\n",
    "\n",
    "        # Remove col PRI_jet_all_pt from x because it only contains 0 values\n",
    "        if jet == 0:\n",
    "            print(\"Deleted col Pri_jet_all_pt in set with jet_num = 0\")\n",
    "            x_tr_jet = np.delete(x_tr_jet, -1, axis=1)\n",
    "            x_te_jet = np.delete(x_te_jet, -1, axis=1)\n",
    "            headers_jet = np.delete(headers_jet, -1)\n",
    "        \n",
    "        # Remove all the columns with only NaN values\n",
    "        x_tr_jet, x_te_jet, headers_jet = remove_all_NAN_columns(x_tr_jet, x_te_jet, headers_jet)\n",
    "\n",
    "        # Split the dataset again into valid/invalid values of DER_mass_MMC \n",
    "        # TRAIN\n",
    "        x_tr_jet_invalid_mass, x_tr_jet_valid_mass, y_tr_jet_invalid_mass, y_tr_jet_valid_mass, ids_tr_jet_invalid_mass, ids_tr_jet_valid_mass = split_data_according_to_mass(x_tr_jet, y_tr_jet, ids_tr_jet)\n",
    "        # TEST\n",
    "        x_te_jet_invalid_mass, x_te_jet_valid_mass, y_te_jet_invalid_mass, y_te_jet_valid_mass, ids_te_jet_invalid_mass, ids_te_jet_valid_mass = split_data_according_to_mass(x_te_jet, y_te_jet, ids_te_jet)\n",
    "        \n",
    "        # Remove 'DER_mass_MMC' (col 0) if the mass is not valid\n",
    "        x_tr_jet_invalid_mass = np.delete(x_tr_jet_invalid_mass, 0, axis=1)\n",
    "        x_te_jet_invalid_mass = np.delete(x_te_jet_invalid_mass, 0, axis=1)\n",
    "        \n",
    "        # Save into CSV\n",
    "        #x, y, ids, headers, jet, isTrain, isMassValid\n",
    "        # TRAIN\n",
    "        output_to_csv(x_tr_jet_invalid_mass, y_tr_jet_invalid_mass, ids_tr_jet_invalid_mass, headers_jet, jet, True, False)\n",
    "        output_to_csv(x_tr_jet_valid_mass, y_tr_jet_valid_mass, ids_tr_jet_valid_mass, headers_jet, jet, True, True)\n",
    "\n",
    "        # TEST\n",
    "        output_to_csv(x_te_jet_invalid_mass, y_te_jet_invalid_mass, ids_te_jet_invalid_mass, headers_jet, jet, False, False)\n",
    "        output_to_csv(x_te_jet_valid_mass, y_te_jet_valid_mass, ids_te_jet_valid_mass, headers_jet, jet, False, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_jet_0_valid_mass.csv',\n",
       " './data/train_jet_1_valid_mass.csv',\n",
       " './data/train_jet_2_valid_mass.csv',\n",
       " './data/train_jet_3_valid_mass.csv',\n",
       " './data/train_jet_0_invalid_mass.csv',\n",
       " './data/train_jet_1_invalid_mass.csv',\n",
       " './data/train_jet_2_invalid_mass.csv',\n",
       " './data/train_jet_3_invalid_mass.csv']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the real files\n",
    "def generate_processed_filenames(isTrain):\n",
    "    file_names = []\n",
    "    isMassValids = [True, False]\n",
    "    jets = range(4)\n",
    "    \n",
    "    for isMassValid in isMassValids:\n",
    "        for jet in jets:\n",
    "            # Generate file name\n",
    "            base = './data/train_' if isTrain else './data/test_'\n",
    "            valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "            file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "            file_names.append(file_name)\n",
    "                \n",
    "    return file_names\n",
    "\n",
    "file_names = generate_processed_filenames(True)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(isTrain):\n",
    "    \"\"\"Load all Train/Test processed data\"\"\"\n",
    "    file_names = generate_processed_filenames(isTrain)\n",
    "\n",
    "    ys = []\n",
    "    xs = []\n",
    "    ids = []\n",
    "\n",
    "    for i in range(4):\n",
    "        y, x, id_ = load_csv_data(file_names[i], cut_values = False)\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "        ids.append(id_)\n",
    "        \n",
    "    return ys, xs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train, xs_train, ids_train = load_processed_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean=None, std=None):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    if mean is None or std is None:\n",
    "        mean_x = np.mean(x, axis = 0)\n",
    "        std_x = np.std(x, axis = 0)\n",
    "    else:\n",
    "        mean_x = mean\n",
    "        std_x = std\n",
    "        \n",
    "    x = x - mean_x\n",
    "    x = x / std_x \n",
    "\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standandize xs\n",
    "x_means = []\n",
    "x_stds = []\n",
    "x_standardized = []\n",
    "\n",
    "for x_ in xs_train:\n",
    "    x, mean_x, std_x = standardize(x_)\n",
    "    x_standardized.append(x)\n",
    "    x_means.append(mean_x)\n",
    "    x_stds.append(std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  to the  0.1  :  0.10000000000000000555\n",
      "2  to the  0.1  :  0.20000000000000001110\n",
      "3  to the  0.1  :  0.30000000000000004441\n",
      "4  to the  0.1  :  0.40000000000000002220\n",
      "5  to the  0.1  :  0.50000000000000000000\n",
      "6  to the  0.1  :  0.60000000000000008882\n",
      "7  to the  0.1  :  0.70000000000000006661\n",
      "8  to the  0.1  :  0.80000000000000004441\n",
      "9  to the  0.1  :  0.90000000000000002220\n",
      "1  to the  0.01  :  0.01000000000000000021\n",
      "2  to the  0.01  :  0.02000000000000000042\n",
      "3  to the  0.01  :  0.02999999999999999889\n",
      "4  to the  0.01  :  0.04000000000000000083\n",
      "5  to the  0.01  :  0.05000000000000000278\n",
      "6  to the  0.01  :  0.05999999999999999778\n",
      "7  to the  0.01  :  0.07000000000000000666\n",
      "8  to the  0.01  :  0.08000000000000000167\n",
      "9  to the  0.01  :  0.08999999999999999667\n",
      "1  to the  0.001  :  0.00100000000000000002\n",
      "2  to the  0.001  :  0.00200000000000000004\n",
      "3  to the  0.001  :  0.00300000000000000006\n",
      "4  to the  0.001  :  0.00400000000000000008\n",
      "5  to the  0.001  :  0.00500000000000000010\n",
      "6  to the  0.001  :  0.00600000000000000012\n",
      "7  to the  0.001  :  0.00700000000000000015\n",
      "8  to the  0.001  :  0.00800000000000000017\n",
      "9  to the  0.001  :  0.00900000000000000105\n",
      "1  to the  0.0001  :  0.00010000000000000000\n",
      "2  to the  0.0001  :  0.00020000000000000001\n",
      "3  to the  0.0001  :  0.00030000000000000003\n",
      "4  to the  0.0001  :  0.00040000000000000002\n",
      "5  to the  0.0001  :  0.00050000000000000001\n",
      "6  to the  0.0001  :  0.00060000000000000006\n",
      "7  to the  0.0001  :  0.00069999999999999999\n",
      "8  to the  0.0001  :  0.00080000000000000004\n",
      "9  to the  0.0001  :  0.00090000000000000008\n",
      "1  to the  1e-05  :  0.00001000000000000000\n",
      "2  to the  1e-05  :  0.00002000000000000000\n",
      "3  to the  1e-05  :  0.00003000000000000000\n",
      "4  to the  1e-05  :  0.00004000000000000000\n",
      "5  to the  1e-05  :  0.00005000000000000000\n",
      "6  to the  1e-05  :  0.00006000000000000001\n",
      "7  to the  1e-05  :  0.00007000000000000001\n",
      "8  to the  1e-05  :  0.00008000000000000001\n",
      "9  to the  1e-05  :  0.00009000000000000001\n",
      "1  to the  1e-06  :  0.00000100000000000000\n",
      "2  to the  1e-06  :  0.00000200000000000000\n",
      "3  to the  1e-06  :  0.00000300000000000000\n",
      "4  to the  1e-06  :  0.00000400000000000000\n",
      "5  to the  1e-06  :  0.00000500000000000000\n",
      "6  to the  1e-06  :  0.00000600000000000000\n",
      "7  to the  1e-06  :  0.00000700000000000000\n",
      "8  to the  1e-06  :  0.00000800000000000000\n",
      "9  to the  1e-06  :  0.00000900000000000000\n",
      "1  to the  1e-07  :  0.00000010000000000000\n",
      "2  to the  1e-07  :  0.00000020000000000000\n",
      "3  to the  1e-07  :  0.00000030000000000000\n",
      "4  to the  1e-07  :  0.00000040000000000000\n",
      "5  to the  1e-07  :  0.00000050000000000000\n",
      "6  to the  1e-07  :  0.00000060000000000000\n",
      "7  to the  1e-07  :  0.00000070000000000000\n",
      "8  to the  1e-07  :  0.00000080000000000000\n",
      "9  to the  1e-07  :  0.00000090000000000000\n",
      "1  to the  1e-08  :  0.00000001000000000000\n",
      "2  to the  1e-08  :  0.00000002000000000000\n",
      "3  to the  1e-08  :  0.00000003000000000000\n",
      "4  to the  1e-08  :  0.00000004000000000000\n",
      "5  to the  1e-08  :  0.00000005000000000000\n",
      "6  to the  1e-08  :  0.00000006000000000000\n",
      "7  to the  1e-08  :  0.00000007000000000000\n",
      "8  to the  1e-08  :  0.00000008000000000000\n",
      "9  to the  1e-08  :  0.00000009000000000000\n",
      "1  to the  1e-09  :  0.00000000100000000000\n",
      "2  to the  1e-09  :  0.00000000200000000000\n",
      "3  to the  1e-09  :  0.00000000300000000000\n",
      "4  to the  1e-09  :  0.00000000400000000000\n",
      "5  to the  1e-09  :  0.00000000500000000000\n",
      "6  to the  1e-09  :  0.00000000600000000000\n",
      "7  to the  1e-09  :  0.00000000700000000000\n",
      "8  to the  1e-09  :  0.00000000800000000000\n",
      "9  to the  1e-09  :  0.00000000900000000000\n",
      "1  to the  1e-10  :  0.00000000010000000000\n",
      "2  to the  1e-10  :  0.00000000020000000000\n",
      "3  to the  1e-10  :  0.00000000030000000000\n",
      "4  to the  1e-10  :  0.00000000040000000000\n",
      "5  to the  1e-10  :  0.00000000050000000000\n",
      "6  to the  1e-10  :  0.00000000060000000000\n",
      "7  to the  1e-10  :  0.00000000070000000000\n",
      "8  to the  1e-10  :  0.00000000080000000000\n",
      "9  to the  1e-10  :  0.00000000090000000000\n",
      "1  to the  1e-11  :  0.00000000001000000000\n",
      "2  to the  1e-11  :  0.00000000002000000000\n",
      "3  to the  1e-11  :  0.00000000003000000000\n",
      "4  to the  1e-11  :  0.00000000004000000000\n",
      "5  to the  1e-11  :  0.00000000005000000000\n",
      "6  to the  1e-11  :  0.00000000006000000000\n",
      "7  to the  1e-11  :  0.00000000007000000000\n",
      "8  to the  1e-11  :  0.00000000008000000000\n",
      "9  to the  1e-11  :  0.00000000009000000000\n",
      "1  to the  1e-12  :  0.00000000000100000000\n",
      "2  to the  1e-12  :  0.00000000000200000000\n",
      "3  to the  1e-12  :  0.00000000000300000000\n",
      "4  to the  1e-12  :  0.00000000000400000000\n",
      "5  to the  1e-12  :  0.00000000000500000000\n",
      "6  to the  1e-12  :  0.00000000000600000000\n",
      "7  to the  1e-12  :  0.00000000000700000000\n",
      "8  to the  1e-12  :  0.00000000000800000000\n",
      "9  to the  1e-12  :  0.00000000000900000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.e-01, 2.e-01, 3.e-01, 4.e-01, 5.e-01, 6.e-01, 7.e-01, 8.e-01,\n",
       "       9.e-01, 1.e-02, 2.e-02, 3.e-02, 4.e-02, 5.e-02, 6.e-02, 7.e-02,\n",
       "       8.e-02, 9.e-02, 1.e-03, 2.e-03, 3.e-03, 4.e-03, 5.e-03, 6.e-03,\n",
       "       7.e-03, 8.e-03, 9.e-03, 1.e-04, 2.e-04, 3.e-04, 4.e-04, 5.e-04,\n",
       "       6.e-04, 7.e-04, 8.e-04, 9.e-04, 1.e-05, 2.e-05, 3.e-05, 4.e-05,\n",
       "       5.e-05, 6.e-05, 7.e-05, 8.e-05, 9.e-05, 1.e-06, 2.e-06, 3.e-06,\n",
       "       4.e-06, 5.e-06, 6.e-06, 7.e-06, 8.e-06, 9.e-06, 1.e-07, 2.e-07,\n",
       "       3.e-07, 4.e-07, 5.e-07, 6.e-07, 7.e-07, 8.e-07, 9.e-07, 1.e-08,\n",
       "       2.e-08, 3.e-08, 4.e-08, 5.e-08, 6.e-08, 7.e-08, 8.e-08, 9.e-08,\n",
       "       1.e-09, 2.e-09, 3.e-09, 4.e-09, 5.e-09, 6.e-09, 7.e-09, 8.e-09,\n",
       "       9.e-09, 1.e-10, 2.e-10, 3.e-10, 4.e-10, 5.e-10, 6.e-10, 7.e-10,\n",
       "       8.e-10, 9.e-10, 1.e-11, 2.e-11, 3.e-11, 4.e-11, 5.e-11, 6.e-11,\n",
       "       7.e-11, 8.e-11, 9.e-11, 1.e-12, 2.e-12, 3.e-12, 4.e-12, 5.e-12,\n",
       "       6.e-12, 7.e-12, 8.e-12, 9.e-12])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_lambdas(start, end, deg):\n",
    "    if isinstance(start, float) or isinstance(end, float):\n",
    "        return np.linspace(start, end, deg)\n",
    "    else :\n",
    "        index = 0\n",
    "        lambdas = np.zeros(deg * (end - start + 1))\n",
    "        for deg_ in range(1, deg + 1):\n",
    "            for i in range(start, end + 1):\n",
    "                lambda_ = i * (10 ** int(-deg_))\n",
    "                lambdas[index] = lambda_\n",
    "                index = index + 1\n",
    "                print(i, \" to the \", (10 ** int(-deg_)), \" : \", '{0:.20f}'.format(lambda_))\n",
    "        return lambdas\n",
    "\n",
    "generate_lambdas(1, 9, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.00e-12, 8.05e-12, 8.10e-12, 8.15e-12, 8.20e-12, 8.25e-12,\n",
       "       8.30e-12, 8.35e-12, 8.40e-12, 8.45e-12, 8.50e-12, 8.55e-12,\n",
       "       8.60e-12, 8.65e-12, 8.70e-12, 8.75e-12, 8.80e-12, 8.85e-12,\n",
       "       8.90e-12, 8.95e-12, 9.00e-12])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.00000000000800000000, 0.00000000000900000000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f4a912b4bbf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# build w using ridge regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize a weights dictionary for each jet\n",
    "weights_jet = dict.fromkeys(range(4))\n",
    "degrees_jet = dict.fromkeys(range(4))\n",
    "lambdas_jet = dict.fromkeys(range(4))\n",
    "\n",
    "# build w using ridge regression\n",
    "k_fold = 10\n",
    "degrees = np.arange(5, 13)\n",
    "lambdas = generate_lambdas(13)\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10000000000000000555\n",
      "1 0.20000000000000001110\n",
      "2 0.30000000000000004441\n",
      "3 0.40000000000000002220\n",
      "4 0.50000000000000000000\n",
      "5 0.60000000000000008882\n",
      "6 0.70000000000000006661\n",
      "7 0.80000000000000004441\n",
      "8 0.90000000000000002220\n",
      "9 0.01000000000000000021\n",
      "10 0.02000000000000000042\n",
      "11 0.02999999999999999889\n",
      "12 0.04000000000000000083\n",
      "13 0.05000000000000000278\n",
      "14 0.05999999999999999778\n",
      "15 0.07000000000000000666\n",
      "16 0.08000000000000000167\n",
      "17 0.08999999999999999667\n",
      "18 0.00100000000000000002\n",
      "19 0.00200000000000000004\n",
      "20 0.00300000000000000006\n",
      "21 0.00400000000000000008\n",
      "22 0.00500000000000000010\n",
      "23 0.00600000000000000012\n",
      "24 0.00700000000000000015\n",
      "25 0.00800000000000000017\n",
      "26 0.00900000000000000105\n",
      "27 0.00010000000000000000\n",
      "28 0.00020000000000000001\n",
      "29 0.00030000000000000003\n",
      "30 0.00040000000000000002\n",
      "31 0.00050000000000000001\n",
      "32 0.00060000000000000006\n",
      "33 0.00069999999999999999\n",
      "34 0.00080000000000000004\n",
      "35 0.00090000000000000008\n",
      "36 0.00001000000000000000\n",
      "37 0.00002000000000000000\n",
      "38 0.00003000000000000000\n",
      "39 0.00004000000000000000\n",
      "40 0.00005000000000000000\n",
      "41 0.00006000000000000001\n",
      "42 0.00007000000000000001\n",
      "43 0.00008000000000000001\n",
      "44 0.00009000000000000001\n",
      "45 0.00000100000000000000\n",
      "46 0.00000200000000000000\n",
      "47 0.00000300000000000000\n",
      "48 0.00000400000000000000\n",
      "49 0.00000500000000000000\n",
      "50 0.00000600000000000000\n",
      "51 0.00000700000000000000\n",
      "52 0.00000800000000000000\n",
      "53 0.00000900000000000000\n",
      "54 0.00000010000000000000\n",
      "55 0.00000020000000000000\n",
      "56 0.00000030000000000000\n",
      "57 0.00000040000000000000\n",
      "58 0.00000050000000000000\n",
      "59 0.00000060000000000000\n",
      "60 0.00000070000000000000\n",
      "61 0.00000080000000000000\n",
      "62 0.00000090000000000000\n",
      "63 0.00000001000000000000\n",
      "64 0.00000002000000000000\n",
      "65 0.00000003000000000000\n",
      "66 0.00000004000000000000\n",
      "67 0.00000005000000000000\n",
      "68 0.00000006000000000000\n",
      "69 0.00000007000000000000\n",
      "70 0.00000008000000000000\n",
      "71 0.00000009000000000000\n",
      "72 0.00000000100000000000\n",
      "73 0.00000000200000000000\n",
      "74 0.00000000300000000000\n",
      "75 0.00000000400000000000\n",
      "76 0.00000000500000000000\n",
      "77 0.00000000600000000000\n",
      "78 0.00000000700000000000\n",
      "79 0.00000000800000000000\n",
      "80 0.00000000900000000000\n",
      "81 0.00000000010000000000\n",
      "82 0.00000000020000000000\n",
      "83 0.00000000030000000000\n",
      "84 0.00000000040000000000\n",
      "85 0.00000000050000000000\n",
      "86 0.00000000060000000000\n",
      "87 0.00000000070000000000\n",
      "88 0.00000000080000000000\n",
      "89 0.00000000090000000000\n",
      "90 0.00000000001000000000\n",
      "91 0.00000000002000000000\n",
      "92 0.00000000003000000000\n",
      "93 0.00000000004000000000\n",
      "94 0.00000000005000000000\n",
      "95 0.00000000006000000000\n",
      "96 0.00000000007000000000\n",
      "97 0.00000000008000000000\n",
      "98 0.00000000009000000000\n",
      "99 0.00000000000100000000\n",
      "100 0.00000000000200000000\n",
      "101 0.00000000000300000000\n",
      "102 0.00000000000400000000\n",
      "103 0.00000000000500000000\n",
      "104 0.00000000000600000000\n",
      "105 0.00000000000700000000\n",
      "106 0.00000000000800000000\n",
      "107 0.00000000000900000000\n",
      "108 0.00000000000010000000\n",
      "109 0.00000000000020000000\n",
      "110 0.00000000000030000000\n",
      "111 0.00000000000040000000\n",
      "112 0.00000000000050000000\n",
      "113 0.00000000000060000000\n",
      "114 0.00000000000070000000\n",
      "115 0.00000000000080000000\n",
      "116 0.00000000000090000000\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    print(i, '{0:.20f}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_wrongness(y, x, w):\n",
    "    \"\"\" return the percentage of right prediction\"\"\"\n",
    "    y_pred = np.dot(x, w)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    wrong = np.sum(y_pred != y)\n",
    "    wrongness = float(wrong) / float(y.shape[0])\n",
    "    \n",
    "    return wrongness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    # ridge regression\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    #loss_tr = np.sqrt(2 * compute_loss(y_tr, tx_tr, w))\n",
    "    #loss_te = np.sqrt(2 * compute_loss(y_te, tx_te, w))\n",
    "    loss_tr = percentage_of_wrongness(y_tr, tx_tr, w)\n",
    "    loss_te = percentage_of_wrongness(y_te, tx_te, w)\n",
    "    return loss_tr, loss_te, w\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_best_lambdas(y, tx, k_indices, lambdas, degree):\n",
    "    # for each degree, we compute the best lambdas and the associated error\n",
    "    print(\"New turn of cv for degree {}\".format(degree))\n",
    "    errors = []\n",
    "    for lambda_ in lambdas:\n",
    "        errors_temp = []\n",
    "        w_te_temp = []\n",
    "        for k in range(k_fold):\n",
    "            _, loss_te, w = cross_validation(y, tx, k_indices, k, lambda_, degree)\n",
    "            errors_temp.append(loss_te)\n",
    "        \n",
    "        #print(\"For lambda: {}, loss_te_mean: {}, loss_te_median: {}\".format(lambda_, np.mean(errors_temp), np.median(errors_temp)))\n",
    "        errors.append(np.median(errors_temp))\n",
    "\n",
    "    ind_lambda_opt = np.argmin(errors)\n",
    "    best_lambda = lambdas[ind_lambda_opt]\n",
    "    least_error = errors[ind_lambda_opt]\n",
    "    print(\"For degree {deg}, te_loss={te_loss}, lambda={lambda_}\".format(deg=degree, te_loss=least_error, lambda_=best_lambda))\n",
    "    return ind_lambda_opt, best_lambda, least_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ./data/train_jet_0_valid_mass.csv\n",
      "New turn of cv for degree 8\n",
      "\n",
      "For degree 8, te_loss=0.19198969910544864, lambda=1e-07\n",
      "\n",
      "\n",
      "For digit 0\n",
      "Generated new lambdas with 0.0000090000000000 and 0.0000002000000000\n",
      "\n",
      "\n",
      "New turn of cv for degree 8\n",
      "\n",
      "For degree 8, te_loss=0.19198969910544864, lambda=2e-07\n",
      "\n",
      "\n",
      "For digit 1\n",
      "Generated new lambdas with 0.0020000000000000 and 0.0040000000000000\n",
      "\n",
      "\n",
      "New turn of cv for degree 8\n",
      "\n",
      "For degree 8, te_loss=0.19280292762266196, lambda=0.0021\n",
      "\n",
      "\n",
      "For digit 2\n",
      "Generated new lambdas with 0.1000000000000000 and 0.3000000000000000\n",
      "\n",
      "\n",
      "New turn of cv for degree 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-4f7106f9d3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated new lambdas with {0:.16f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_lamdba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and {0:.16f}\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mind_lambda_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinding_best_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofound_lambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mbest_lambdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-64b8c7e900d4>\u001b[0m in \u001b[0;36mfinding_best_lambdas\u001b[0;34m(y, tx, k_indices, lambdas, degree)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mw_te_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0merrors_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-7aeaf14ab9a0>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_indice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# form data with polynomial degree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# ridge regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-7aeaf14ab9a0>\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train with ridge regression for each tx in x and save the weights\n",
    "for tx, y, f in zip(x_standardized, ys_train, file_names):\n",
    "    print(\"Training for {}\".format(f))\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    best_lambdas = []\n",
    "    best_errors = []\n",
    "    # vary degree\n",
    "    for degree in degrees:\n",
    "        ind_lambda_opt, _, _ = finding_best_lambdas(y, tx, k_indices, lambdas, degree)\n",
    "        profound_lambdas = lambdas.copy()\n",
    "        \n",
    "        for i in range(3):\n",
    "            print(\"For digit\", i)\n",
    "            \n",
    "            if ind_lambda_opt == 0:\n",
    "                previous_lambda = profound_lambdas[ind_lambda_opt]\n",
    "                next_lambda = profound_lambdas[ind_lambda_opt + 1]\n",
    "                profound_lambdas = generate_lambdas(previous_lamdba, next_lambda, 11)\n",
    "\n",
    "            elif ind_lambda_opt == len(profound_lambdas) - 1:\n",
    "                previous_lambda = profound_lambdas[ind_lambda_opt - 1]\n",
    "                next_lambda = profound_lambdas[ind_lambda_opt]\n",
    "                profound_lambdas = generate_lambdas(previous_lamdba, next_lambda, 11)\n",
    "\n",
    "            else:\n",
    "                previous_lamdba = profound_lambdas[ind_lambda_opt - 1]\n",
    "                next_lambda = profound_lambdas[ind_lambda_opt + 1]\n",
    "                profound_lambdas = generate_lambdas(previous_lamdba, next_lambda, 21)\n",
    "\n",
    "            print(\"Generated new lambdas with {0:.16f}\".format(previous_lamdba) + \" and {0:.16f}\\n\".format(next_lambda))\n",
    "\n",
    "            ind_lambda_opt, best_lambda, best_error = finding_best_lambdas(y, tx, k_indices, profound_lambdas, degree)\n",
    "            \n",
    "            best_lambdas.append(best_lambda)\n",
    "            best_errors.append(best_error)\n",
    "    \n",
    "\n",
    "    # find the one having the least test error\n",
    "    ind_best_degree = np.argmin(best_errors)\n",
    "\n",
    "    best_degree = degrees[ind_best_degree]\n",
    "    print(\"\\nBest degree:\", best_degree)\n",
    "    best_lambda = best_lambdas[ind_best_degree]\n",
    "    print(\"Best lambda:\", best_lambda)\n",
    "    \n",
    "    tx_extended = build_poly(tx, best_degree)\n",
    "    w_star = ridge_regression(y, tx, best_lambda)\n",
    "    \n",
    "    print(\"Wrong prediction : {} \\n\\n\", perc_wrong_pred(y, tx, w_star))\n",
    "\n",
    "    # record the weights & degree\n",
    "    degrees_jet[f] = best_degree\n",
    "    lambdas_jet[f] = best_lambda\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    degree = 7\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr = 0\n",
    "        loss_te = 0\n",
    "        for idx in range(k_fold):\n",
    "            e_tr, e_te = cross_validation(y, x, k_indices, idx, lambda_, degree)\n",
    "            loss_tr += e_tr\n",
    "            loss_te += e_te\n",
    "\n",
    "        rmse_tr.append(loss_tr/k_fold)\n",
    "        rmse_te.append(loss_te/k_fold)\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    return rmse_tr, rmse_te\n",
    "\n",
    "    \n",
    "\n",
    "a, b = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

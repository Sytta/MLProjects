the shape of y_train: (125000,), the shape of x_train: (125000, 30), the shape of y_test: (125000,), the shape of x_test: (125000, 30)

Testing least sqaures gd test
Gradient Descent(0/499): loss=0.5
Gradient Descent(1/499): loss=0.453803067403
Gradient Descent(2/499): loss=0.431172559824
Gradient Descent(3/499): loss=0.415520068756
Gradient Descent(4/499): loss=0.404093479865
Gradient Descent(5/499): loss=0.395529639957
Gradient Descent(6/499): loss=0.388963065638
Gradient Descent(7/499): loss=0.383822048681
Gradient Descent(8/499): loss=0.379720754537
Gradient Descent(9/499): loss=0.376393271689
Gradient Descent(10/499): loss=0.373652401312
Gradient Descent(11/499): loss=0.371363577688
Gradient Descent(12/499): loss=0.369428135847
Gradient Descent(13/499): loss=0.36777240716
Gradient Descent(14/499): loss=0.366340487947
Gradient Descent(15/499): loss=0.36508935234
Gradient Descent(16/499): loss=0.363985483241
Gradient Descent(17/499): loss=0.363002502566
Gradient Descent(18/499): loss=0.362119471159
Gradient Descent(19/499): loss=0.361319646129
Gradient Descent(20/499): loss=0.36058955681
Gradient Descent(21/499): loss=0.359918307067
Gradient Descent(22/499): loss=0.359297041418
Gradient Descent(23/499): loss=0.358718531847
Gradient Descent(24/499): loss=0.358176854946
Gradient Descent(25/499): loss=0.357667137617
Gradient Descent(26/499): loss=0.357185355463
Gradient Descent(27/499): loss=0.356728172073
Gradient Descent(28/499): loss=0.356292810326
Gradient Descent(29/499): loss=0.355876948962
Gradient Descent(30/499): loss=0.355478639182
Gradient Descent(31/499): loss=0.355096237228
Gradient Descent(32/499): loss=0.354728349752
Gradient Descent(33/499): loss=0.35437378945
Gradient Descent(34/499): loss=0.354031538976
Gradient Descent(35/499): loss=0.353700721535
Gradient Descent(36/499): loss=0.353380576871
Gradient Descent(37/499): loss=0.353070441644
Gradient Descent(38/499): loss=0.352769733357
Gradient Descent(39/499): loss=0.352477937173
Gradient Descent(40/499): loss=0.352194595086
Gradient Descent(41/499): loss=0.351919297006
Gradient Descent(42/499): loss=0.351651673411
Gradient Descent(43/499): loss=0.351391389279
Gradient Descent(44/499): loss=0.351138139057
Gradient Descent(45/499): loss=0.350891642498
Gradient Descent(46/499): loss=0.350651641189
Gradient Descent(47/499): loss=0.350417895664
Gradient Descent(48/499): loss=0.350190182984
Gradient Descent(49/499): loss=0.349968294714
Gradient Descent(50/499): loss=0.349752035215
Gradient Descent(51/499): loss=0.349541220209
Gradient Descent(52/499): loss=0.349335675557
Gradient Descent(53/499): loss=0.349135236221
Gradient Descent(54/499): loss=0.348939745383
Gradient Descent(55/499): loss=0.348749053684
Gradient Descent(56/499): loss=0.348563018568
Gradient Descent(57/499): loss=0.34838150372
Gradient Descent(58/499): loss=0.348204378571
Gradient Descent(59/499): loss=0.348031517866
Gradient Descent(60/499): loss=0.347862801292
Gradient Descent(61/499): loss=0.347698113138
Gradient Descent(62/499): loss=0.347537342003
Gradient Descent(63/499): loss=0.347380380532
Gradient Descent(64/499): loss=0.347227125176
Gradient Descent(65/499): loss=0.347077475983
Gradient Descent(66/499): loss=0.346931336401
Gradient Descent(67/499): loss=0.346788613108
Gradient Descent(68/499): loss=0.346649215847
Gradient Descent(69/499): loss=0.34651305728
Gradient Descent(70/499): loss=0.346380052856
Gradient Descent(71/499): loss=0.346250120685
Gradient Descent(72/499): loss=0.346123181418
Gradient Descent(73/499): loss=0.345999158146
Gradient Descent(74/499): loss=0.345877976295
Gradient Descent(75/499): loss=0.345759563532
Gradient Descent(76/499): loss=0.34564384968
Gradient Descent(77/499): loss=0.345530766632
Gradient Descent(78/499): loss=0.345420248274
Gradient Descent(79/499): loss=0.34531223041
Gradient Descent(80/499): loss=0.345206650694
Gradient Descent(81/499): loss=0.345103448562
Gradient Descent(82/499): loss=0.345002565172
Gradient Descent(83/499): loss=0.344903943337
Gradient Descent(84/499): loss=0.344807527477
Gradient Descent(85/499): loss=0.34471326356
Gradient Descent(86/499): loss=0.344621099049
Gradient Descent(87/499): loss=0.344530982854
Gradient Descent(88/499): loss=0.344442865287
Gradient Descent(89/499): loss=0.344356698011
Gradient Descent(90/499): loss=0.344272434002
Gradient Descent(91/499): loss=0.344190027504
Gradient Descent(92/499): loss=0.344109433991
Gradient Descent(93/499): loss=0.344030610125
Gradient Descent(94/499): loss=0.343953513724
Gradient Descent(95/499): loss=0.343878103724
Gradient Descent(96/499): loss=0.343804340142
Gradient Descent(97/499): loss=0.343732184049
Gradient Descent(98/499): loss=0.343661597534
Gradient Descent(99/499): loss=0.343592543674
Gradient Descent(100/499): loss=0.343524986507
Gradient Descent(101/499): loss=0.343458891001
Gradient Descent(102/499): loss=0.343394223026
Gradient Descent(103/499): loss=0.343330949331
Gradient Descent(104/499): loss=0.343269037518
Gradient Descent(105/499): loss=0.343208456012
Gradient Descent(106/499): loss=0.343149174044
Gradient Descent(107/499): loss=0.343091161625
Gradient Descent(108/499): loss=0.343034389523
Gradient Descent(109/499): loss=0.342978829244
Gradient Descent(110/499): loss=0.342924453008
Gradient Descent(111/499): loss=0.342871233732
Gradient Descent(112/499): loss=0.342819145009
Gradient Descent(113/499): loss=0.342768161088
Gradient Descent(114/499): loss=0.342718256859
Gradient Descent(115/499): loss=0.342669407832
Gradient Descent(116/499): loss=0.342621590124
Gradient Descent(117/499): loss=0.342574780437
Gradient Descent(118/499): loss=0.342528956046
Gradient Descent(119/499): loss=0.342484094782
Gradient Descent(120/499): loss=0.342440175018
Gradient Descent(121/499): loss=0.342397175654
Gradient Descent(122/499): loss=0.342355076101
Gradient Descent(123/499): loss=0.34231385627
Gradient Descent(124/499): loss=0.342273496557
Gradient Descent(125/499): loss=0.34223397783
Gradient Descent(126/499): loss=0.34219528142
Gradient Descent(127/499): loss=0.342157389101
Gradient Descent(128/499): loss=0.342120283088
Gradient Descent(129/499): loss=0.342083946018
Gradient Descent(130/499): loss=0.342048360942
Gradient Descent(131/499): loss=0.342013511314
Gradient Descent(132/499): loss=0.341979380981
Gradient Descent(133/499): loss=0.34194595417
Gradient Descent(134/499): loss=0.341913215483
Gradient Descent(135/499): loss=0.341881149882
Gradient Descent(136/499): loss=0.341849742685
Gradient Descent(137/499): loss=0.341818979554
Gradient Descent(138/499): loss=0.341788846483
Gradient Descent(139/499): loss=0.341759329797
Gradient Descent(140/499): loss=0.341730416139
Gradient Descent(141/499): loss=0.341702092461
Gradient Descent(142/499): loss=0.341674346018
Gradient Descent(143/499): loss=0.341647164362
Gradient Descent(144/499): loss=0.341620535331
Gradient Descent(145/499): loss=0.341594447043
Gradient Descent(146/499): loss=0.341568887891
Gradient Descent(147/499): loss=0.341543846534
Gradient Descent(148/499): loss=0.341519311891
Gradient Descent(149/499): loss=0.341495273134
Gradient Descent(150/499): loss=0.341471719683
Gradient Descent(151/499): loss=0.3414486412
Gradient Descent(152/499): loss=0.34142602758
Gradient Descent(153/499): loss=0.341403868949
Gradient Descent(154/499): loss=0.341382155656
Gradient Descent(155/499): loss=0.341360878269
Gradient Descent(156/499): loss=0.341340027569
Gradient Descent(157/499): loss=0.341319594545
Gradient Descent(158/499): loss=0.341299570387
Gradient Descent(159/499): loss=0.341279946485
Gradient Descent(160/499): loss=0.341260714421
Gradient Descent(161/499): loss=0.341241865967
Gradient Descent(162/499): loss=0.341223393075
Gradient Descent(163/499): loss=0.34120528788
Gradient Descent(164/499): loss=0.341187542692
Gradient Descent(165/499): loss=0.34117014999
Gradient Descent(166/499): loss=0.341153102421
Gradient Descent(167/499): loss=0.341136392795
Gradient Descent(168/499): loss=0.34112001408
Gradient Descent(169/499): loss=0.341103959401
Gradient Descent(170/499): loss=0.341088222033
Gradient Descent(171/499): loss=0.341072795399
Gradient Descent(172/499): loss=0.341057673067
Gradient Descent(173/499): loss=0.341042848747
Gradient Descent(174/499): loss=0.341028316286
Gradient Descent(175/499): loss=0.341014069663
Gradient Descent(176/499): loss=0.341000102993
Gradient Descent(177/499): loss=0.340986410516
Gradient Descent(178/499): loss=0.340972986599
Gradient Descent(179/499): loss=0.34095982573
Gradient Descent(180/499): loss=0.340946922517
Gradient Descent(181/499): loss=0.340934271686
Gradient Descent(182/499): loss=0.340921868076
Gradient Descent(183/499): loss=0.340909706637
Gradient Descent(184/499): loss=0.340897782429
Gradient Descent(185/499): loss=0.340886090617
Gradient Descent(186/499): loss=0.340874626472
Gradient Descent(187/499): loss=0.340863385364
Gradient Descent(188/499): loss=0.340852362765
Gradient Descent(189/499): loss=0.340841554241
Gradient Descent(190/499): loss=0.340830955455
Gradient Descent(191/499): loss=0.340820562162
Gradient Descent(192/499): loss=0.340810370207
Gradient Descent(193/499): loss=0.340800375523
Gradient Descent(194/499): loss=0.340790574131
Gradient Descent(195/499): loss=0.340780962135
Gradient Descent(196/499): loss=0.340771535722
Gradient Descent(197/499): loss=0.340762291161
Gradient Descent(198/499): loss=0.340753224798
Gradient Descent(199/499): loss=0.340744333056
Gradient Descent(200/499): loss=0.340735612436
Gradient Descent(201/499): loss=0.340727059509
Gradient Descent(202/499): loss=0.340718670922
Gradient Descent(203/499): loss=0.340710443388
Gradient Descent(204/499): loss=0.340702373693
Gradient Descent(205/499): loss=0.340694458688
Gradient Descent(206/499): loss=0.340686695291
Gradient Descent(207/499): loss=0.340679080483
Gradient Descent(208/499): loss=0.34067161131
Gradient Descent(209/499): loss=0.340664284877
Gradient Descent(210/499): loss=0.340657098351
Gradient Descent(211/499): loss=0.340650048958
Gradient Descent(212/499): loss=0.34064313398
Gradient Descent(213/499): loss=0.340636350758
Gradient Descent(214/499): loss=0.340629696685
Gradient Descent(215/499): loss=0.34062316921
Gradient Descent(216/499): loss=0.340616765834
Gradient Descent(217/499): loss=0.34061048411
Gradient Descent(218/499): loss=0.34060432164
Gradient Descent(219/499): loss=0.340598276077
Gradient Descent(220/499): loss=0.340592345122
Gradient Descent(221/499): loss=0.340586526522
Gradient Descent(222/499): loss=0.340580818072
Gradient Descent(223/499): loss=0.340575217611
Gradient Descent(224/499): loss=0.340569723021
Gradient Descent(225/499): loss=0.340564332231
Gradient Descent(226/499): loss=0.340559043208
Gradient Descent(227/499): loss=0.340553853964
Gradient Descent(228/499): loss=0.340548762549
Gradient Descent(229/499): loss=0.340543767055
Gradient Descent(230/499): loss=0.340538865609
Gradient Descent(231/499): loss=0.340534056381
Gradient Descent(232/499): loss=0.340529337574
Gradient Descent(233/499): loss=0.34052470743
Gradient Descent(234/499): loss=0.340520164225
Gradient Descent(235/499): loss=0.340515706271
Gradient Descent(236/499): loss=0.340511331912
Gradient Descent(237/499): loss=0.340507039529
Gradient Descent(238/499): loss=0.340502827532
Gradient Descent(239/499): loss=0.340498694366
Gradient Descent(240/499): loss=0.340494638504
Gradient Descent(241/499): loss=0.340490658454
Gradient Descent(242/499): loss=0.340486752749
Gradient Descent(243/499): loss=0.340482919956
Gradient Descent(244/499): loss=0.340479158667
Gradient Descent(245/499): loss=0.340475467505
Gradient Descent(246/499): loss=0.340471845118
Gradient Descent(247/499): loss=0.340468290184
Gradient Descent(248/499): loss=0.340464801405
Gradient Descent(249/499): loss=0.340461377509
Gradient Descent(250/499): loss=0.340458017251
Gradient Descent(251/499): loss=0.340454719409
Gradient Descent(252/499): loss=0.340451482786
Gradient Descent(253/499): loss=0.340448306209
Gradient Descent(254/499): loss=0.340445188529
Gradient Descent(255/499): loss=0.340442128617
Gradient Descent(256/499): loss=0.34043912537
Gradient Descent(257/499): loss=0.340436177703
Gradient Descent(258/499): loss=0.340433284557
Gradient Descent(259/499): loss=0.34043044489
Gradient Descent(260/499): loss=0.340427657681
Gradient Descent(261/499): loss=0.340424921933
Gradient Descent(262/499): loss=0.340422236663
Gradient Descent(263/499): loss=0.340419600911
Gradient Descent(264/499): loss=0.340417013736
Gradient Descent(265/499): loss=0.340414474213
Gradient Descent(266/499): loss=0.340411981438
Gradient Descent(267/499): loss=0.340409534522
Gradient Descent(268/499): loss=0.340407132596
Gradient Descent(269/499): loss=0.340404774807
Gradient Descent(270/499): loss=0.340402460317
Gradient Descent(271/499): loss=0.340400188307
Gradient Descent(272/499): loss=0.340397957974
Gradient Descent(273/499): loss=0.340395768528
Gradient Descent(274/499): loss=0.340393619196
Gradient Descent(275/499): loss=0.340391509221
Gradient Descent(276/499): loss=0.34038943786
Gradient Descent(277/499): loss=0.340387404384
Gradient Descent(278/499): loss=0.340385408078
Gradient Descent(279/499): loss=0.340383448242
Gradient Descent(280/499): loss=0.340381524188
Gradient Descent(281/499): loss=0.340379635244
Gradient Descent(282/499): loss=0.340377780748
Gradient Descent(283/499): loss=0.340375960053
Gradient Descent(284/499): loss=0.340374172523
Gradient Descent(285/499): loss=0.340372417536
Gradient Descent(286/499): loss=0.34037069448
Gradient Descent(287/499): loss=0.340369002756
Gradient Descent(288/499): loss=0.340367341778
Gradient Descent(289/499): loss=0.340365710968
Gradient Descent(290/499): loss=0.340364109761
Gradient Descent(291/499): loss=0.340362537604
Gradient Descent(292/499): loss=0.340360993953
Gradient Descent(293/499): loss=0.340359478274
Gradient Descent(294/499): loss=0.340357990046
Gradient Descent(295/499): loss=0.340356528754
Gradient Descent(296/499): loss=0.340355093896
Gradient Descent(297/499): loss=0.340353684979
Gradient Descent(298/499): loss=0.34035230152
Gradient Descent(299/499): loss=0.340350943042
Gradient Descent(300/499): loss=0.340349609082
Gradient Descent(301/499): loss=0.340348299182
Gradient Descent(302/499): loss=0.340347012894
Gradient Descent(303/499): loss=0.34034574978
Gradient Descent(304/499): loss=0.340344509408
Gradient Descent(305/499): loss=0.340343291357
Gradient Descent(306/499): loss=0.34034209521
Gradient Descent(307/499): loss=0.340340920562
Gradient Descent(308/499): loss=0.340339767013
Gradient Descent(309/499): loss=0.340338634173
Gradient Descent(310/499): loss=0.340337521657
Gradient Descent(311/499): loss=0.340336429088
Gradient Descent(312/499): loss=0.340335356099
Gradient Descent(313/499): loss=0.340334302325
Gradient Descent(314/499): loss=0.340333267412
Gradient Descent(315/499): loss=0.34033225101
Gradient Descent(316/499): loss=0.340331252778
Gradient Descent(317/499): loss=0.34033027238
Gradient Descent(318/499): loss=0.340329309486
Gradient Descent(319/499): loss=0.340328363774
Gradient Descent(320/499): loss=0.340327434926
Gradient Descent(321/499): loss=0.340326522632
Gradient Descent(322/499): loss=0.340325626586
Gradient Descent(323/499): loss=0.34032474649
Gradient Descent(324/499): loss=0.340323882049
Gradient Descent(325/499): loss=0.340323032975
Gradient Descent(326/499): loss=0.340322198986
Gradient Descent(327/499): loss=0.340321379804
Gradient Descent(328/499): loss=0.340320575158
Gradient Descent(329/499): loss=0.34031978478
Gradient Descent(330/499): loss=0.340319008407
Gradient Descent(331/499): loss=0.340318245784
Gradient Descent(332/499): loss=0.340317496658
Gradient Descent(333/499): loss=0.340316760781
Gradient Descent(334/499): loss=0.340316037911
Gradient Descent(335/499): loss=0.340315327809
Gradient Descent(336/499): loss=0.340314630242
Gradient Descent(337/499): loss=0.34031394498
Gradient Descent(338/499): loss=0.340313271797
Gradient Descent(339/499): loss=0.340312610474
Gradient Descent(340/499): loss=0.340311960793
Gradient Descent(341/499): loss=0.340311322542
Gradient Descent(342/499): loss=0.340310695512
Gradient Descent(343/499): loss=0.340310079497
Gradient Descent(344/499): loss=0.340309474298
Gradient Descent(345/499): loss=0.340308879717
Gradient Descent(346/499): loss=0.34030829556
Gradient Descent(347/499): loss=0.340307721637
Gradient Descent(348/499): loss=0.340307157763
Gradient Descent(349/499): loss=0.340306603753
Gradient Descent(350/499): loss=0.340306059429
Gradient Descent(351/499): loss=0.340305524615
Gradient Descent(352/499): loss=0.340304999137
Gradient Descent(353/499): loss=0.340304482826
Gradient Descent(354/499): loss=0.340303975515
Gradient Descent(355/499): loss=0.340303477041
Gradient Descent(356/499): loss=0.340302987244
Gradient Descent(357/499): loss=0.340302505966
Gradient Descent(358/499): loss=0.340302033053
Gradient Descent(359/499): loss=0.340301568353
Gradient Descent(360/499): loss=0.340301111717
Gradient Descent(361/499): loss=0.340300663
Gradient Descent(362/499): loss=0.340300222058
Gradient Descent(363/499): loss=0.34029978875
Gradient Descent(364/499): loss=0.340299362939
Gradient Descent(365/499): loss=0.340298944488
Gradient Descent(366/499): loss=0.340298533265
Gradient Descent(367/499): loss=0.34029812914
Gradient Descent(368/499): loss=0.340297731984
Gradient Descent(369/499): loss=0.340297341671
Gradient Descent(370/499): loss=0.340296958079
Gradient Descent(371/499): loss=0.340296581085
Gradient Descent(372/499): loss=0.340296210571
Gradient Descent(373/499): loss=0.340295846421
Gradient Descent(374/499): loss=0.34029548852
Gradient Descent(375/499): loss=0.340295136755
Gradient Descent(376/499): loss=0.340294791015
Gradient Descent(377/499): loss=0.340294451194
Gradient Descent(378/499): loss=0.340294117184
Gradient Descent(379/499): loss=0.340293788881
Gradient Descent(380/499): loss=0.340293466182
Gradient Descent(381/499): loss=0.340293148987
Gradient Descent(382/499): loss=0.340292837198
Gradient Descent(383/499): loss=0.340292530716
Gradient Descent(384/499): loss=0.340292229448
Gradient Descent(385/499): loss=0.3402919333
Gradient Descent(386/499): loss=0.34029164218
Gradient Descent(387/499): loss=0.340291355998
Gradient Descent(388/499): loss=0.340291074666
Gradient Descent(389/499): loss=0.340290798097
Gradient Descent(390/499): loss=0.340290526206
Gradient Descent(391/499): loss=0.34029025891
Gradient Descent(392/499): loss=0.340289996126
Gradient Descent(393/499): loss=0.340289737774
Gradient Descent(394/499): loss=0.340289483775
Gradient Descent(395/499): loss=0.340289234051
Gradient Descent(396/499): loss=0.340288988526
Gradient Descent(397/499): loss=0.340288747126
Gradient Descent(398/499): loss=0.340288509776
Gradient Descent(399/499): loss=0.340288276406
Gradient Descent(400/499): loss=0.340288046943
Gradient Descent(401/499): loss=0.340287821319
Gradient Descent(402/499): loss=0.340287599466
Gradient Descent(403/499): loss=0.340287381315
Gradient Descent(404/499): loss=0.340287166803
Gradient Descent(405/499): loss=0.340286955864
Gradient Descent(406/499): loss=0.340286748435
Gradient Descent(407/499): loss=0.340286544453
Gradient Descent(408/499): loss=0.340286343858
Gradient Descent(409/499): loss=0.34028614659
Gradient Descent(410/499): loss=0.340285952589
Gradient Descent(411/499): loss=0.340285761798
Gradient Descent(412/499): loss=0.340285574161
Gradient Descent(413/499): loss=0.34028538962
Gradient Descent(414/499): loss=0.340285208123
Gradient Descent(415/499): loss=0.340285029615
Gradient Descent(416/499): loss=0.340284854043
Gradient Descent(417/499): loss=0.340284681356
Gradient Descent(418/499): loss=0.340284511503
Gradient Descent(419/499): loss=0.340284344433
Gradient Descent(420/499): loss=0.340284180099
Gradient Descent(421/499): loss=0.340284018451
Gradient Descent(422/499): loss=0.340283859442
Gradient Descent(423/499): loss=0.340283703027
Gradient Descent(424/499): loss=0.340283549159
Gradient Descent(425/499): loss=0.340283397794
Gradient Descent(426/499): loss=0.340283248888
Gradient Descent(427/499): loss=0.340283102398
Gradient Descent(428/499): loss=0.340282958282
Gradient Descent(429/499): loss=0.340282816497
Gradient Descent(430/499): loss=0.340282677003
Gradient Descent(431/499): loss=0.340282539761
Gradient Descent(432/499): loss=0.34028240473
Gradient Descent(433/499): loss=0.340282271872
Gradient Descent(434/499): loss=0.340282141149
Gradient Descent(435/499): loss=0.340282012524
Gradient Descent(436/499): loss=0.34028188596
Gradient Descent(437/499): loss=0.340281761421
Gradient Descent(438/499): loss=0.340281638873
Gradient Descent(439/499): loss=0.340281518279
Gradient Descent(440/499): loss=0.340281399607
Gradient Descent(441/499): loss=0.340281282823
Gradient Descent(442/499): loss=0.340281167894
Gradient Descent(443/499): loss=0.340281054788
Gradient Descent(444/499): loss=0.340280943473
Gradient Descent(445/499): loss=0.340280833918
Gradient Descent(446/499): loss=0.340280726092
Gradient Descent(447/499): loss=0.340280619967
Gradient Descent(448/499): loss=0.340280515511
Gradient Descent(449/499): loss=0.340280412697
Gradient Descent(450/499): loss=0.340280311496
Gradient Descent(451/499): loss=0.34028021188
Gradient Descent(452/499): loss=0.340280113821
Gradient Descent(453/499): loss=0.340280017293
Gradient Descent(454/499): loss=0.34027992227
Gradient Descent(455/499): loss=0.340279828725
Gradient Descent(456/499): loss=0.340279736633
Gradient Descent(457/499): loss=0.34027964597
Gradient Descent(458/499): loss=0.340279556709
Gradient Descent(459/499): loss=0.340279468828
Gradient Descent(460/499): loss=0.340279382303
Gradient Descent(461/499): loss=0.340279297109
Gradient Descent(462/499): loss=0.340279213226
Gradient Descent(463/499): loss=0.340279130629
Gradient Descent(464/499): loss=0.340279049298
Gradient Descent(465/499): loss=0.340278969209
Gradient Descent(466/499): loss=0.340278890343
Gradient Descent(467/499): loss=0.340278812678
Gradient Descent(468/499): loss=0.340278736194
Gradient Descent(469/499): loss=0.34027866087
Gradient Descent(470/499): loss=0.340278586687
Gradient Descent(471/499): loss=0.340278513625
Gradient Descent(472/499): loss=0.340278441666
Gradient Descent(473/499): loss=0.34027837079
Gradient Descent(474/499): loss=0.340278300979
Gradient Descent(475/499): loss=0.340278232215
Gradient Descent(476/499): loss=0.34027816448
Gradient Descent(477/499): loss=0.340278097757
Gradient Descent(478/499): loss=0.340278032029
Gradient Descent(479/499): loss=0.340277967278
Gradient Descent(480/499): loss=0.340277903489
Gradient Descent(481/499): loss=0.340277840644
Gradient Descent(482/499): loss=0.340277778729
Gradient Descent(483/499): loss=0.340277717726
Gradient Descent(484/499): loss=0.340277657622
Gradient Descent(485/499): loss=0.3402775984
Gradient Descent(486/499): loss=0.340277540047
Gradient Descent(487/499): loss=0.340277482547
Gradient Descent(488/499): loss=0.340277425885
Gradient Descent(489/499): loss=0.340277370049
Gradient Descent(490/499): loss=0.340277315023
Gradient Descent(491/499): loss=0.340277260796
Gradient Descent(492/499): loss=0.340277207352
Gradient Descent(493/499): loss=0.340277154679
Gradient Descent(494/499): loss=0.340277102764
Gradient Descent(495/499): loss=0.340277051595
Gradient Descent(496/499): loss=0.340277001159
Gradient Descent(497/499): loss=0.340276951443
Gradient Descent(498/499): loss=0.340276902437
Gradient Descent(499/499): loss=0.340276854127
Train loss: 0.3403, test_loss: 0.3398, accuracy of GD is:  0.7446

Testing least sqaures sgd test
SGD(0/999): loss=0.5
SGD(1/999): loss=0.482783188968
SGD(2/999): loss=0.466015881336
SGD(3/999): loss=0.464877423764
SGD(4/999): loss=0.464669347695
SGD(5/999): loss=0.457551741521
SGD(6/999): loss=0.439067501374
SGD(7/999): loss=0.436168355778
SGD(8/999): loss=0.431647718742
SGD(9/999): loss=0.432307806145
SGD(10/999): loss=0.433233670563
SGD(11/999): loss=0.441264802843
SGD(12/999): loss=0.429999190564
SGD(13/999): loss=0.419502427931
SGD(14/999): loss=0.493208475533
SGD(15/999): loss=0.459651487789
SGD(16/999): loss=0.452577067922
SGD(17/999): loss=0.426380820824
SGD(18/999): loss=0.41147263741
SGD(19/999): loss=0.411569155284
SGD(20/999): loss=0.416120023623
SGD(21/999): loss=0.415303475832
SGD(22/999): loss=0.428867735152
SGD(23/999): loss=0.448980427677
SGD(24/999): loss=0.419861044085
SGD(25/999): loss=0.405289306905
SGD(26/999): loss=0.414256648198
SGD(27/999): loss=0.406279444322
SGD(28/999): loss=0.398168298264
SGD(29/999): loss=0.425087450744
SGD(30/999): loss=0.460417869771
SGD(31/999): loss=0.448215310024
SGD(32/999): loss=0.422864348629
SGD(33/999): loss=0.405492178774
SGD(34/999): loss=0.406044204626
SGD(35/999): loss=0.409023177857
SGD(36/999): loss=0.412244530561
SGD(37/999): loss=0.410273522009
SGD(38/999): loss=0.413443724913
SGD(39/999): loss=0.41868880372
SGD(40/999): loss=0.436866047527
SGD(41/999): loss=0.423292982478
SGD(42/999): loss=0.419483091516
SGD(43/999): loss=0.423973209012
SGD(44/999): loss=0.425938199678
SGD(45/999): loss=0.427495839472
SGD(46/999): loss=0.43189259513
SGD(47/999): loss=0.439903470378
SGD(48/999): loss=0.579511542947
SGD(49/999): loss=0.652304552055
SGD(50/999): loss=0.565406604736
SGD(51/999): loss=0.615050659867
SGD(52/999): loss=0.658609424441
SGD(53/999): loss=0.791289478138
SGD(54/999): loss=0.75048043963
SGD(55/999): loss=0.49655355474
SGD(56/999): loss=0.491732015999
SGD(57/999): loss=0.484284868504
SGD(58/999): loss=0.454355646934
SGD(59/999): loss=0.422373600389
SGD(60/999): loss=0.431350235389
SGD(61/999): loss=0.421418566835
SGD(62/999): loss=0.40964339025
SGD(63/999): loss=0.415211346096
SGD(64/999): loss=0.424075603678
SGD(65/999): loss=0.450242033936
SGD(66/999): loss=0.420686023213
SGD(67/999): loss=0.417784494396
SGD(68/999): loss=0.428887035479
SGD(69/999): loss=0.412692032913
SGD(70/999): loss=0.424938356179
SGD(71/999): loss=0.431765522018
SGD(72/999): loss=0.447241117014
SGD(73/999): loss=0.418564424113
SGD(74/999): loss=0.442413609831
SGD(75/999): loss=0.443726597523
SGD(76/999): loss=0.447334050904
SGD(77/999): loss=0.427137931564
SGD(78/999): loss=0.427573750254
SGD(79/999): loss=0.44595505598
SGD(80/999): loss=0.459563701832
SGD(81/999): loss=0.470608032742
SGD(82/999): loss=0.480842485905
SGD(83/999): loss=0.50257293599
SGD(84/999): loss=0.516544361224
SGD(85/999): loss=0.515082471029
SGD(86/999): loss=0.530942114726
SGD(87/999): loss=0.537607740549
SGD(88/999): loss=0.557245756327
SGD(89/999): loss=0.610728514784
SGD(90/999): loss=0.625888314992
SGD(91/999): loss=0.492113846828
SGD(92/999): loss=0.430545113661
SGD(93/999): loss=0.427047268166
SGD(94/999): loss=0.44530349257
SGD(95/999): loss=0.428911954126
SGD(96/999): loss=0.429244275309
SGD(97/999): loss=0.435847812486
SGD(98/999): loss=0.441249375793
SGD(99/999): loss=0.466973672704
SGD(100/999): loss=0.434550820024
SGD(101/999): loss=0.437503859218
SGD(102/999): loss=0.433263256818
SGD(103/999): loss=0.442822142432
SGD(104/999): loss=0.435814208224
SGD(105/999): loss=0.435898367163
SGD(106/999): loss=0.474173023055
SGD(107/999): loss=0.49841941764
SGD(108/999): loss=0.520669152065
SGD(109/999): loss=0.513734778239
SGD(110/999): loss=0.498974314442
SGD(111/999): loss=0.467733761746
SGD(112/999): loss=0.500741871661
SGD(113/999): loss=0.471209709304
SGD(114/999): loss=0.450520365921
SGD(115/999): loss=0.438468237565
SGD(116/999): loss=0.431805142052
SGD(117/999): loss=0.431451669709
SGD(118/999): loss=0.456061952649
SGD(119/999): loss=0.459501591256
SGD(120/999): loss=0.446244677823
SGD(121/999): loss=0.458088471612
SGD(122/999): loss=0.460475741426
SGD(123/999): loss=0.478225282045
SGD(124/999): loss=0.470060695825
SGD(125/999): loss=0.467994927391
SGD(126/999): loss=0.471386255951
SGD(127/999): loss=0.467575132252
SGD(128/999): loss=0.456470368838
SGD(129/999): loss=0.414813285449
SGD(130/999): loss=0.419541366394
SGD(131/999): loss=0.419196586083
SGD(132/999): loss=0.416810098994
SGD(133/999): loss=0.41676179076
SGD(134/999): loss=0.405943373092
SGD(135/999): loss=0.449971287296
SGD(136/999): loss=0.505170906368
SGD(137/999): loss=0.422669078571
SGD(138/999): loss=0.420962305231
SGD(139/999): loss=0.414467819829
SGD(140/999): loss=0.414722426834
SGD(141/999): loss=0.418936167709
SGD(142/999): loss=0.447663018143
SGD(143/999): loss=0.479382287429
SGD(144/999): loss=0.412195975275
SGD(145/999): loss=0.408327181062
SGD(146/999): loss=0.406365461019
SGD(147/999): loss=0.405133708282
SGD(148/999): loss=0.411614345681
SGD(149/999): loss=0.411641809729
SGD(150/999): loss=0.412564487624
SGD(151/999): loss=0.418495533349
SGD(152/999): loss=0.434333977853
SGD(153/999): loss=0.433365738916
SGD(154/999): loss=0.437909875593
SGD(155/999): loss=0.431788409002
SGD(156/999): loss=0.454607924337
SGD(157/999): loss=0.452919024297
SGD(158/999): loss=0.490601708588
SGD(159/999): loss=0.455111116138
SGD(160/999): loss=0.467734438922
SGD(161/999): loss=0.487822777101
SGD(162/999): loss=0.490516510861
SGD(163/999): loss=0.493028561994
SGD(164/999): loss=0.410186417776
SGD(165/999): loss=0.407531748691
SGD(166/999): loss=0.41825334452
SGD(167/999): loss=0.468629830036
SGD(168/999): loss=0.425724476921
SGD(169/999): loss=0.386406510522
SGD(170/999): loss=0.389948651109
SGD(171/999): loss=0.390359704667
SGD(172/999): loss=0.391161931558
SGD(173/999): loss=0.404196246836
SGD(174/999): loss=0.408640019085
SGD(175/999): loss=0.423158658072
SGD(176/999): loss=0.427061366978
SGD(177/999): loss=0.389391385659
SGD(178/999): loss=0.404039108758
SGD(179/999): loss=0.42260755808
SGD(180/999): loss=0.43270902701
SGD(181/999): loss=0.446709961277
SGD(182/999): loss=0.456342351172
SGD(183/999): loss=0.403636193791
SGD(184/999): loss=0.457831547108
SGD(185/999): loss=0.437067327987
SGD(186/999): loss=0.417767294417
SGD(187/999): loss=0.404038698537
SGD(188/999): loss=0.421507719015
SGD(189/999): loss=0.399343865934
SGD(190/999): loss=0.402884410383
SGD(191/999): loss=0.406756536363
SGD(192/999): loss=0.490718013315
SGD(193/999): loss=0.413133923856
SGD(194/999): loss=0.47103690384
SGD(195/999): loss=0.483826763764
SGD(196/999): loss=0.480830646091
SGD(197/999): loss=0.467433059963
SGD(198/999): loss=0.490801497965
SGD(199/999): loss=0.524199629075
SGD(200/999): loss=0.521313297918
SGD(201/999): loss=0.506380349969
SGD(202/999): loss=0.517512138351
SGD(203/999): loss=0.490021423686
SGD(204/999): loss=0.488856992858
SGD(205/999): loss=0.566675600827
SGD(206/999): loss=0.588952607786
SGD(207/999): loss=0.574028836981
SGD(208/999): loss=0.583650523538
SGD(209/999): loss=0.579516024368
SGD(210/999): loss=0.523216632964
SGD(211/999): loss=0.524599937075
SGD(212/999): loss=0.434450107768
SGD(213/999): loss=0.457536938366
SGD(214/999): loss=0.463347277866
SGD(215/999): loss=0.463414514535
SGD(216/999): loss=0.450736830104
SGD(217/999): loss=0.409364034733
SGD(218/999): loss=0.426694442127
SGD(219/999): loss=0.387473934327
SGD(220/999): loss=0.388021078771
SGD(221/999): loss=0.405853962764
SGD(222/999): loss=0.395311315487
SGD(223/999): loss=0.434381696137
SGD(224/999): loss=0.42848600181
SGD(225/999): loss=0.418448390032
SGD(226/999): loss=0.416640638603
SGD(227/999): loss=0.407882873048
SGD(228/999): loss=0.394868351297
SGD(229/999): loss=0.397841181462
SGD(230/999): loss=0.41348301319
SGD(231/999): loss=0.407942712332
SGD(232/999): loss=0.418089103183
SGD(233/999): loss=0.401336362033
SGD(234/999): loss=0.387749685755
SGD(235/999): loss=0.392712897898
SGD(236/999): loss=0.394246679145
SGD(237/999): loss=0.399434156841
SGD(238/999): loss=0.387445578409
SGD(239/999): loss=0.395513904778
SGD(240/999): loss=0.401103578435
SGD(241/999): loss=0.396581531552
SGD(242/999): loss=0.396880786564
SGD(243/999): loss=0.401780369505
SGD(244/999): loss=0.39641105282
SGD(245/999): loss=0.397758204242
SGD(246/999): loss=0.400122484413
SGD(247/999): loss=0.399189814333
SGD(248/999): loss=0.392691108446
SGD(249/999): loss=0.399418345043
SGD(250/999): loss=0.424737058345
SGD(251/999): loss=0.486804778896
SGD(252/999): loss=0.556656907045
SGD(253/999): loss=0.560500154122
SGD(254/999): loss=0.546148369533
SGD(255/999): loss=0.517792363765
SGD(256/999): loss=0.422476504719
SGD(257/999): loss=0.43422222702
SGD(258/999): loss=0.428277735639
SGD(259/999): loss=0.399959530087
SGD(260/999): loss=0.38735660847
SGD(261/999): loss=0.402599284938
SGD(262/999): loss=0.417391195156
SGD(263/999): loss=0.39206748682
SGD(264/999): loss=0.389718435094
SGD(265/999): loss=0.391668253393
SGD(266/999): loss=0.408424205013
SGD(267/999): loss=0.390601700717
SGD(268/999): loss=0.385581144334
SGD(269/999): loss=0.386157919019
SGD(270/999): loss=0.388455075577
SGD(271/999): loss=0.389142332206
SGD(272/999): loss=0.401992093785
SGD(273/999): loss=0.401645281618
SGD(274/999): loss=0.410819212092
SGD(275/999): loss=0.419931991707
SGD(276/999): loss=0.420411891779
SGD(277/999): loss=0.427119444067
SGD(278/999): loss=0.435390619898
SGD(279/999): loss=0.42214775486
SGD(280/999): loss=0.445097710758
SGD(281/999): loss=0.427533537761
SGD(282/999): loss=0.409153160952
SGD(283/999): loss=0.408917099025
SGD(284/999): loss=0.400145162458
SGD(285/999): loss=0.398663423964
SGD(286/999): loss=0.397553673846
SGD(287/999): loss=0.397186865092
SGD(288/999): loss=0.397954629346
SGD(289/999): loss=0.398793126698
SGD(290/999): loss=0.398339884747
SGD(291/999): loss=0.413167625444
SGD(292/999): loss=0.403578166763
SGD(293/999): loss=0.396511273496
SGD(294/999): loss=0.397560345996
SGD(295/999): loss=0.398577131001
SGD(296/999): loss=0.398358763683
SGD(297/999): loss=0.394771741935
SGD(298/999): loss=0.395051757265
SGD(299/999): loss=0.394273913886
SGD(300/999): loss=0.392498229507
SGD(301/999): loss=0.392554249337
SGD(302/999): loss=0.391672827507
SGD(303/999): loss=0.391605154144
SGD(304/999): loss=0.392422356977
SGD(305/999): loss=0.398704129223
SGD(306/999): loss=0.400432143326
SGD(307/999): loss=0.390661773435
SGD(308/999): loss=0.404710384865
SGD(309/999): loss=0.394777965734
SGD(310/999): loss=0.40197903616
SGD(311/999): loss=0.423431727866
SGD(312/999): loss=0.429016051951
SGD(313/999): loss=0.420677884101
SGD(314/999): loss=0.410725934414
SGD(315/999): loss=0.404880769384
SGD(316/999): loss=0.406747987611
SGD(317/999): loss=0.394192214809
SGD(318/999): loss=0.396032942316
SGD(319/999): loss=0.375175780336
SGD(320/999): loss=0.374739824163
SGD(321/999): loss=0.376301992145
SGD(322/999): loss=0.377767480774
SGD(323/999): loss=0.38004966337
SGD(324/999): loss=0.383696807107
SGD(325/999): loss=0.378244207717
SGD(326/999): loss=0.396528217118
SGD(327/999): loss=0.382206602743
SGD(328/999): loss=0.387176181138
SGD(329/999): loss=0.377961518331
SGD(330/999): loss=0.38119770073
SGD(331/999): loss=0.389604865751
SGD(332/999): loss=0.387588824311
SGD(333/999): loss=0.413990805326
SGD(334/999): loss=0.410649123427
SGD(335/999): loss=0.39035107742
SGD(336/999): loss=0.391864366504
SGD(337/999): loss=0.405784524028
SGD(338/999): loss=0.418628212644
SGD(339/999): loss=0.420690093432
SGD(340/999): loss=0.420012188248
SGD(341/999): loss=0.427081590419
SGD(342/999): loss=0.405512501211
SGD(343/999): loss=0.420864125678
SGD(344/999): loss=0.40851228367
SGD(345/999): loss=0.391980041268
SGD(346/999): loss=0.404927679481
SGD(347/999): loss=0.423538532913
SGD(348/999): loss=0.413660592284
SGD(349/999): loss=0.417401184233
SGD(350/999): loss=0.410354388837
SGD(351/999): loss=0.415625135484
SGD(352/999): loss=0.418576121709
SGD(353/999): loss=0.397238143531
SGD(354/999): loss=0.402047808756
SGD(355/999): loss=0.382553914047
SGD(356/999): loss=0.382756364594
SGD(357/999): loss=0.386204530304
SGD(358/999): loss=0.387616998915
SGD(359/999): loss=0.390840849613
SGD(360/999): loss=0.384773439642
SGD(361/999): loss=0.406228047129
SGD(362/999): loss=0.456961658724
SGD(363/999): loss=0.477077959864
SGD(364/999): loss=0.40380734031
SGD(365/999): loss=0.382004292707
SGD(366/999): loss=0.381088473905
SGD(367/999): loss=0.385079575708
SGD(368/999): loss=0.373013737956
SGD(369/999): loss=0.374277349605
SGD(370/999): loss=0.381971860212
SGD(371/999): loss=0.377173118689
SGD(372/999): loss=0.377362430897
SGD(373/999): loss=0.381655418503
SGD(374/999): loss=0.387381255665
SGD(375/999): loss=0.40035142335
SGD(376/999): loss=0.405180429458
SGD(377/999): loss=0.403471969455
SGD(378/999): loss=0.406609663839
SGD(379/999): loss=0.407114985187
SGD(380/999): loss=0.407340203964
SGD(381/999): loss=0.431142579975
SGD(382/999): loss=0.431887875317
SGD(383/999): loss=0.431954343797
SGD(384/999): loss=0.454014627897
SGD(385/999): loss=0.425942109711
SGD(386/999): loss=0.428679781506
SGD(387/999): loss=0.429463331608
SGD(388/999): loss=0.426479949831
SGD(389/999): loss=0.435734553089
SGD(390/999): loss=0.412645296526
SGD(391/999): loss=0.421698841473
SGD(392/999): loss=0.437645622313
SGD(393/999): loss=0.396344033176
SGD(394/999): loss=0.401617374893
SGD(395/999): loss=0.400484482923
SGD(396/999): loss=0.39418799315
SGD(397/999): loss=0.40212798979
SGD(398/999): loss=0.406667747242
SGD(399/999): loss=0.393042722348
SGD(400/999): loss=0.391388980248
SGD(401/999): loss=0.412002918384
SGD(402/999): loss=0.41634014918
SGD(403/999): loss=0.424707804367
SGD(404/999): loss=0.426671314801
SGD(405/999): loss=0.431221462523
SGD(406/999): loss=0.391956179416
SGD(407/999): loss=0.392811931053
SGD(408/999): loss=0.400649337803
SGD(409/999): loss=0.395175279432
SGD(410/999): loss=0.401135832401
SGD(411/999): loss=0.402245651047
SGD(412/999): loss=0.407521066769
SGD(413/999): loss=0.402141486791
SGD(414/999): loss=0.401686294729
SGD(415/999): loss=0.402356776498
SGD(416/999): loss=0.42184042066
SGD(417/999): loss=0.428510926312
SGD(418/999): loss=0.464551170425
SGD(419/999): loss=0.475465089434
SGD(420/999): loss=0.441969592817
SGD(421/999): loss=0.42188909643
SGD(422/999): loss=0.411245684301
SGD(423/999): loss=0.42914385685
SGD(424/999): loss=0.439354110648
SGD(425/999): loss=0.429190034059
SGD(426/999): loss=0.4300733747
SGD(427/999): loss=0.458411727858
SGD(428/999): loss=0.44770270933
SGD(429/999): loss=0.433061710845
SGD(430/999): loss=0.417174703332
SGD(431/999): loss=0.415244834362
SGD(432/999): loss=0.419662921573
SGD(433/999): loss=0.440593146418
SGD(434/999): loss=0.49234589045
SGD(435/999): loss=0.501790505355
SGD(436/999): loss=0.485355863757
SGD(437/999): loss=0.463992416352
SGD(438/999): loss=0.417463480196
SGD(439/999): loss=0.425357710208
SGD(440/999): loss=0.4523719805
SGD(441/999): loss=0.42297311313
SGD(442/999): loss=0.390822800447
SGD(443/999): loss=0.390400239797
SGD(444/999): loss=0.398418838194
SGD(445/999): loss=0.398762300778
SGD(446/999): loss=0.397595665375
SGD(447/999): loss=0.416908139178
SGD(448/999): loss=0.417162560454
SGD(449/999): loss=0.413920972552
SGD(450/999): loss=0.400490808569
SGD(451/999): loss=0.404003165367
SGD(452/999): loss=0.397474905876
SGD(453/999): loss=0.405517533083
SGD(454/999): loss=0.401800309398
SGD(455/999): loss=0.400723029756
SGD(456/999): loss=0.405872475388
SGD(457/999): loss=0.408307511835
SGD(458/999): loss=0.417711474918
SGD(459/999): loss=0.409421986321
SGD(460/999): loss=0.455834624449
SGD(461/999): loss=0.440042852583
SGD(462/999): loss=0.441627380249
SGD(463/999): loss=0.431793453378
SGD(464/999): loss=0.411042576428
SGD(465/999): loss=0.41138991522
SGD(466/999): loss=0.421903212163
SGD(467/999): loss=0.412168450228
SGD(468/999): loss=0.410551552108
SGD(469/999): loss=0.462079211292
SGD(470/999): loss=0.461312470835
SGD(471/999): loss=0.484514187308
SGD(472/999): loss=0.481932790373
SGD(473/999): loss=0.413524003158
SGD(474/999): loss=0.413429895038
SGD(475/999): loss=0.421330186738
SGD(476/999): loss=0.405331595198
SGD(477/999): loss=0.410415567539
SGD(478/999): loss=0.398938006834
SGD(479/999): loss=0.397535522387
SGD(480/999): loss=0.399980203654
SGD(481/999): loss=0.424264543244
SGD(482/999): loss=0.437747232329
SGD(483/999): loss=0.406693212466
SGD(484/999): loss=0.409923178443
SGD(485/999): loss=0.412869448639
SGD(486/999): loss=0.412851178709
SGD(487/999): loss=0.418908338057
SGD(488/999): loss=0.417053079632
SGD(489/999): loss=0.42205564246
SGD(490/999): loss=0.416439464022
SGD(491/999): loss=0.43127829818
SGD(492/999): loss=0.427100277669
SGD(493/999): loss=0.425182832068
SGD(494/999): loss=0.427858276275
SGD(495/999): loss=0.407468306165
SGD(496/999): loss=0.398475428619
SGD(497/999): loss=0.393288227572
SGD(498/999): loss=0.393578001532
SGD(499/999): loss=0.42491137991
SGD(500/999): loss=0.447079121283
SGD(501/999): loss=0.485926923579
SGD(502/999): loss=0.456321592691
SGD(503/999): loss=0.437441302943
SGD(504/999): loss=0.491783711702
SGD(505/999): loss=0.499860838571
SGD(506/999): loss=0.447674451527
SGD(507/999): loss=0.435587754563
SGD(508/999): loss=0.393843029852
SGD(509/999): loss=0.388883009708
SGD(510/999): loss=0.403926512738
SGD(511/999): loss=0.422759548493
SGD(512/999): loss=0.420206539008
SGD(513/999): loss=0.395287261366
SGD(514/999): loss=0.399719821742
SGD(515/999): loss=0.386459640175
SGD(516/999): loss=0.378058839601
SGD(517/999): loss=0.376467409535
SGD(518/999): loss=0.379567574248
SGD(519/999): loss=0.401174175925
SGD(520/999): loss=0.395048759556
SGD(521/999): loss=0.414187047489
SGD(522/999): loss=0.392309377534
SGD(523/999): loss=0.392102292197
SGD(524/999): loss=0.384244954926
SGD(525/999): loss=0.408563721736
SGD(526/999): loss=0.398434102266
SGD(527/999): loss=0.398026273244
SGD(528/999): loss=0.392808513143
SGD(529/999): loss=0.384975630889
SGD(530/999): loss=0.384085901158
SGD(531/999): loss=0.414205295871
SGD(532/999): loss=0.460882563518
SGD(533/999): loss=0.478718092383
SGD(534/999): loss=0.479863185997
SGD(535/999): loss=0.474287927479
SGD(536/999): loss=0.459302312751
SGD(537/999): loss=0.456632990774
SGD(538/999): loss=0.453902543506
SGD(539/999): loss=0.458130325121
SGD(540/999): loss=0.451424863222
SGD(541/999): loss=0.42543823126
SGD(542/999): loss=0.412426618967
SGD(543/999): loss=0.403020820348
SGD(544/999): loss=0.402984314352
SGD(545/999): loss=0.400151724241
SGD(546/999): loss=0.393313965013
SGD(547/999): loss=0.382994761349
SGD(548/999): loss=0.397154795883
SGD(549/999): loss=0.404254026208
SGD(550/999): loss=0.398067271178
SGD(551/999): loss=0.393885327098
SGD(552/999): loss=0.401079290997
SGD(553/999): loss=0.384185954499
SGD(554/999): loss=0.392599697105
SGD(555/999): loss=0.385819581934
SGD(556/999): loss=0.382900374923
SGD(557/999): loss=0.412355753905
SGD(558/999): loss=0.46216748513
SGD(559/999): loss=0.463602903542
SGD(560/999): loss=0.429378071635
SGD(561/999): loss=0.405928238479
SGD(562/999): loss=0.386147207327
SGD(563/999): loss=0.423370823778
SGD(564/999): loss=0.423280268978
SGD(565/999): loss=0.406759466261
SGD(566/999): loss=0.383495132947
SGD(567/999): loss=0.383812950673
SGD(568/999): loss=0.403599286018
SGD(569/999): loss=0.401203245164
SGD(570/999): loss=0.401153356666
SGD(571/999): loss=0.401356701785
SGD(572/999): loss=0.432767161383
SGD(573/999): loss=0.442078712239
SGD(574/999): loss=0.440111399652
SGD(575/999): loss=0.437075573539
SGD(576/999): loss=0.444904762919
SGD(577/999): loss=0.405959649769
SGD(578/999): loss=0.425250142675
SGD(579/999): loss=0.41263737267
SGD(580/999): loss=0.411898734016
SGD(581/999): loss=0.406930424225
SGD(582/999): loss=0.430669340576
SGD(583/999): loss=0.442860619301
SGD(584/999): loss=0.494751875514
SGD(585/999): loss=0.494539979525
SGD(586/999): loss=0.498977147413
SGD(587/999): loss=0.507943026733
SGD(588/999): loss=0.419569340015
SGD(589/999): loss=0.414340649187
SGD(590/999): loss=0.433523021705
SGD(591/999): loss=0.390789375941
SGD(592/999): loss=0.478189976303
SGD(593/999): loss=0.476055966302
SGD(594/999): loss=0.479798000995
SGD(595/999): loss=0.494274090836
SGD(596/999): loss=0.489578973398
SGD(597/999): loss=0.407428090068
SGD(598/999): loss=0.454695892873
SGD(599/999): loss=0.448540738568
SGD(600/999): loss=0.454287866222
SGD(601/999): loss=0.467231363883
SGD(602/999): loss=0.473623208572
SGD(603/999): loss=0.497605224507
SGD(604/999): loss=0.503116846667
SGD(605/999): loss=0.65658013559
SGD(606/999): loss=0.662963337241
SGD(607/999): loss=0.578376608366
SGD(608/999): loss=0.542216574708
SGD(609/999): loss=0.613950640317
SGD(610/999): loss=0.668541783
SGD(611/999): loss=0.607143928799
SGD(612/999): loss=0.559679243661
SGD(613/999): loss=0.424598003365
SGD(614/999): loss=0.418321231808
SGD(615/999): loss=0.444382533614
SGD(616/999): loss=0.484689914328
SGD(617/999): loss=0.484193649931
SGD(618/999): loss=0.438163272355
SGD(619/999): loss=0.433833449133
SGD(620/999): loss=0.452806518317
SGD(621/999): loss=0.477448423235
SGD(622/999): loss=0.475950581657
SGD(623/999): loss=0.43663464523
SGD(624/999): loss=0.423105369532
SGD(625/999): loss=0.422817809221
SGD(626/999): loss=0.422170075954
SGD(627/999): loss=0.418872927819
SGD(628/999): loss=0.419006878391
SGD(629/999): loss=0.419708203375
SGD(630/999): loss=0.546738062757
SGD(631/999): loss=0.544988021681
SGD(632/999): loss=0.557111107517
SGD(633/999): loss=0.562799369429
SGD(634/999): loss=0.472621587011
SGD(635/999): loss=0.480629566492
SGD(636/999): loss=0.493526325781
SGD(637/999): loss=0.442900422179
SGD(638/999): loss=0.469910691308
SGD(639/999): loss=0.447465373872
SGD(640/999): loss=0.438046505859
SGD(641/999): loss=0.438125419333
SGD(642/999): loss=0.420992302669
SGD(643/999): loss=0.423985792925
SGD(644/999): loss=0.442791380513
SGD(645/999): loss=0.439350617256
SGD(646/999): loss=0.498497950453
SGD(647/999): loss=0.468705972854
SGD(648/999): loss=0.461600259808
SGD(649/999): loss=0.473293608996
SGD(650/999): loss=0.467434236346
SGD(651/999): loss=0.461253456132
SGD(652/999): loss=0.484558682767
SGD(653/999): loss=0.514575623114
SGD(654/999): loss=0.479499620221
SGD(655/999): loss=0.480781562735
SGD(656/999): loss=0.465677121882
SGD(657/999): loss=0.472075080468
SGD(658/999): loss=0.437114270823
SGD(659/999): loss=0.442067669719
SGD(660/999): loss=0.437903969725
SGD(661/999): loss=0.438825819439
SGD(662/999): loss=0.450010647467
SGD(663/999): loss=0.432016509777
SGD(664/999): loss=0.435502791703
SGD(665/999): loss=0.436552385659
SGD(666/999): loss=0.439849298654
SGD(667/999): loss=0.45764012637
SGD(668/999): loss=0.471449564947
SGD(669/999): loss=0.463996502072
SGD(670/999): loss=0.431590366249
SGD(671/999): loss=0.432555510777
SGD(672/999): loss=0.437379956416
SGD(673/999): loss=0.434794347037
SGD(674/999): loss=0.422559213251
SGD(675/999): loss=0.44871278277
SGD(676/999): loss=0.44565822169
SGD(677/999): loss=0.419146882853
SGD(678/999): loss=0.41142746277
SGD(679/999): loss=0.40829849541
SGD(680/999): loss=0.412665391163
SGD(681/999): loss=0.412517380087
SGD(682/999): loss=0.410507906938
SGD(683/999): loss=0.426722607038
SGD(684/999): loss=0.432477170398
SGD(685/999): loss=0.43280314636
SGD(686/999): loss=0.403268753213
SGD(687/999): loss=0.403735735505
SGD(688/999): loss=0.41812726502
SGD(689/999): loss=0.423224904732
SGD(690/999): loss=0.421311241213
SGD(691/999): loss=0.410326644584
SGD(692/999): loss=0.405628778569
SGD(693/999): loss=0.404283204695
SGD(694/999): loss=0.40754595469
SGD(695/999): loss=0.405558138067
SGD(696/999): loss=0.397859959568
SGD(697/999): loss=0.397292595273
SGD(698/999): loss=0.42602244421
SGD(699/999): loss=0.433311489429
SGD(700/999): loss=0.457492361782
SGD(701/999): loss=0.476148731579
SGD(702/999): loss=0.461593162397
SGD(703/999): loss=0.463514625484
SGD(704/999): loss=0.4579833577
SGD(705/999): loss=0.394156050531
SGD(706/999): loss=0.397505996679
SGD(707/999): loss=0.419869487399
SGD(708/999): loss=0.386106369762
SGD(709/999): loss=0.389222908393
SGD(710/999): loss=0.382922177199
SGD(711/999): loss=0.386663300748
SGD(712/999): loss=0.387990176879
SGD(713/999): loss=0.389590834302
SGD(714/999): loss=0.387058064324
SGD(715/999): loss=0.398874147378
SGD(716/999): loss=0.398837554175
SGD(717/999): loss=0.402456440957
SGD(718/999): loss=0.382312746873
SGD(719/999): loss=0.393624852236
SGD(720/999): loss=0.379760122948
SGD(721/999): loss=0.3855017803
SGD(722/999): loss=0.388104373602
SGD(723/999): loss=0.379443502519
SGD(724/999): loss=0.375858020228
SGD(725/999): loss=0.376210004697
SGD(726/999): loss=0.378515120963
SGD(727/999): loss=0.397999997423
SGD(728/999): loss=0.380238859161
SGD(729/999): loss=0.383041178004
SGD(730/999): loss=0.380207518056
SGD(731/999): loss=0.392219336922
SGD(732/999): loss=0.458415215237
SGD(733/999): loss=0.484697646453
SGD(734/999): loss=0.517458662323
SGD(735/999): loss=0.551060446184
SGD(736/999): loss=0.549767969844
SGD(737/999): loss=0.585034837629
SGD(738/999): loss=0.497885795741
SGD(739/999): loss=0.501231758032
SGD(740/999): loss=0.448024400064
SGD(741/999): loss=0.611590615535
SGD(742/999): loss=0.543638524946
SGD(743/999): loss=0.553424061036
SGD(744/999): loss=0.445068052926
SGD(745/999): loss=0.424371417557
SGD(746/999): loss=0.429245387644
SGD(747/999): loss=0.431135048831
SGD(748/999): loss=0.420844976474
SGD(749/999): loss=0.415734007103
SGD(750/999): loss=0.398144671324
SGD(751/999): loss=0.395853359434
SGD(752/999): loss=0.395396568357
SGD(753/999): loss=0.389026009501
SGD(754/999): loss=0.391215264812
SGD(755/999): loss=0.387117762599
SGD(756/999): loss=0.388777678982
SGD(757/999): loss=0.391050325875
SGD(758/999): loss=0.419492257704
SGD(759/999): loss=0.391943419113
SGD(760/999): loss=0.397198136247
SGD(761/999): loss=0.405357755973
SGD(762/999): loss=0.438229157052
SGD(763/999): loss=0.422954265523
SGD(764/999): loss=0.41352544312
SGD(765/999): loss=0.413477365034
SGD(766/999): loss=0.416923254622
SGD(767/999): loss=0.399543444835
SGD(768/999): loss=0.400942323673
SGD(769/999): loss=0.41078457188
SGD(770/999): loss=0.402409199443
SGD(771/999): loss=0.383259405738
SGD(772/999): loss=0.384241762798
SGD(773/999): loss=0.386863063274
SGD(774/999): loss=0.38172616681
SGD(775/999): loss=0.382363185716
SGD(776/999): loss=0.402073392016
SGD(777/999): loss=0.399841131852
SGD(778/999): loss=0.402099945748
SGD(779/999): loss=0.422069453517
SGD(780/999): loss=0.512171363353
SGD(781/999): loss=0.50888115634
SGD(782/999): loss=0.489982353752
SGD(783/999): loss=0.491994571307
SGD(784/999): loss=0.487064183508
SGD(785/999): loss=0.491179107829
SGD(786/999): loss=0.396468902204
SGD(787/999): loss=0.391946538397
SGD(788/999): loss=0.379034540349
SGD(789/999): loss=0.379938430508
SGD(790/999): loss=0.382478357136
SGD(791/999): loss=0.390375990901
SGD(792/999): loss=0.37990819696
SGD(793/999): loss=0.394269370513
SGD(794/999): loss=0.400813069509
SGD(795/999): loss=0.384622304899
SGD(796/999): loss=0.384661252594
SGD(797/999): loss=0.381047020115
SGD(798/999): loss=0.379412639127
SGD(799/999): loss=0.389855777291
SGD(800/999): loss=0.389195666592
SGD(801/999): loss=0.399625159215
SGD(802/999): loss=0.397406819308
SGD(803/999): loss=0.385795427575
SGD(804/999): loss=0.401206141458
SGD(805/999): loss=0.399048394635
SGD(806/999): loss=0.412577463492
SGD(807/999): loss=0.409692826695
SGD(808/999): loss=0.403763948732
SGD(809/999): loss=0.396610295653
SGD(810/999): loss=0.397411481032
SGD(811/999): loss=0.39773165233
SGD(812/999): loss=0.391193223956
SGD(813/999): loss=0.394394704792
SGD(814/999): loss=0.393626816202
SGD(815/999): loss=0.381174525656
SGD(816/999): loss=0.419350404033
SGD(817/999): loss=0.438093792959
SGD(818/999): loss=0.451253986042
SGD(819/999): loss=0.395046983961
SGD(820/999): loss=0.378662090632
SGD(821/999): loss=0.377931702551
SGD(822/999): loss=0.376151320468
SGD(823/999): loss=0.399692849389
SGD(824/999): loss=0.409036389476
SGD(825/999): loss=0.381240360992
SGD(826/999): loss=0.381189356619
SGD(827/999): loss=0.394968255405
SGD(828/999): loss=0.403497183159
SGD(829/999): loss=0.394952346557
SGD(830/999): loss=0.395505646031
SGD(831/999): loss=0.391425336712
SGD(832/999): loss=0.391938494505
SGD(833/999): loss=0.398322508634
SGD(834/999): loss=0.404278116171
SGD(835/999): loss=0.393483411254
SGD(836/999): loss=0.393800784662
SGD(837/999): loss=0.387748535287
SGD(838/999): loss=0.387627040877
SGD(839/999): loss=0.385712399785
SGD(840/999): loss=0.387189384162
SGD(841/999): loss=0.390822167413
SGD(842/999): loss=0.388155857773
SGD(843/999): loss=0.392649555486
SGD(844/999): loss=0.412720989881
SGD(845/999): loss=0.436836205251
SGD(846/999): loss=0.462696528507
SGD(847/999): loss=0.420602154866
SGD(848/999): loss=0.396959505529
SGD(849/999): loss=0.393843413581
SGD(850/999): loss=0.413186833108
SGD(851/999): loss=0.394358196009
SGD(852/999): loss=0.394854006194
SGD(853/999): loss=0.397863990387
SGD(854/999): loss=0.398794496617
SGD(855/999): loss=0.396425705731
SGD(856/999): loss=0.397071046733
SGD(857/999): loss=0.39861289412
SGD(858/999): loss=0.39327616322
SGD(859/999): loss=0.397893021673
SGD(860/999): loss=0.395196949613
SGD(861/999): loss=0.401609251756
SGD(862/999): loss=0.402956159045
SGD(863/999): loss=0.387807897291
SGD(864/999): loss=0.387567373105
SGD(865/999): loss=0.387832531212
SGD(866/999): loss=0.390656919005
SGD(867/999): loss=0.390034807581
SGD(868/999): loss=0.393443314469
SGD(869/999): loss=0.40228924925
SGD(870/999): loss=0.473612520207
SGD(871/999): loss=0.40206614634
SGD(872/999): loss=0.403429569939
SGD(873/999): loss=0.412057298971
SGD(874/999): loss=0.415937381842
SGD(875/999): loss=0.419695679234
SGD(876/999): loss=0.417027513075
SGD(877/999): loss=0.405853551549
SGD(878/999): loss=0.405618800342
SGD(879/999): loss=0.402689380117
SGD(880/999): loss=0.41968622061
SGD(881/999): loss=0.414658140489
SGD(882/999): loss=0.401949097982
SGD(883/999): loss=0.385978331327
SGD(884/999): loss=0.38900718995
SGD(885/999): loss=0.38664134986
SGD(886/999): loss=0.397260784356
SGD(887/999): loss=0.395148873823
SGD(888/999): loss=0.401923012605
SGD(889/999): loss=0.395979806362
SGD(890/999): loss=0.394067231513
SGD(891/999): loss=0.398805641689
SGD(892/999): loss=0.39792458411
SGD(893/999): loss=0.410432334108
SGD(894/999): loss=0.396941111029
SGD(895/999): loss=0.398350930988
SGD(896/999): loss=0.39004379725
SGD(897/999): loss=0.391454288765
SGD(898/999): loss=0.398981284533
SGD(899/999): loss=0.40322896858
SGD(900/999): loss=0.397834400492
SGD(901/999): loss=0.401953888653
SGD(902/999): loss=0.396646633989
SGD(903/999): loss=0.397568889491
SGD(904/999): loss=0.398409106598
SGD(905/999): loss=0.390401736956
SGD(906/999): loss=0.440217881034
SGD(907/999): loss=0.463677126611
SGD(908/999): loss=0.395743875205
SGD(909/999): loss=0.393461212392
SGD(910/999): loss=0.389671298976
SGD(911/999): loss=0.394583620557
SGD(912/999): loss=0.404185803226
SGD(913/999): loss=0.408773506338
SGD(914/999): loss=0.465032317072
SGD(915/999): loss=0.400483962025
SGD(916/999): loss=0.391248266842
SGD(917/999): loss=0.392981174703
SGD(918/999): loss=0.393961449795
SGD(919/999): loss=0.410380924611
SGD(920/999): loss=0.411628155629
SGD(921/999): loss=0.425005454967
SGD(922/999): loss=0.422558064436
SGD(923/999): loss=0.424341406752
SGD(924/999): loss=0.468366779781
SGD(925/999): loss=0.494506728191
SGD(926/999): loss=0.395755477465
SGD(927/999): loss=0.407146441269
SGD(928/999): loss=0.402467150787
SGD(929/999): loss=0.422420549376
SGD(930/999): loss=0.417156857616
SGD(931/999): loss=0.406491579834
SGD(932/999): loss=0.402244786005
SGD(933/999): loss=0.426006322154
SGD(934/999): loss=0.417292864368
SGD(935/999): loss=0.389162769164
SGD(936/999): loss=0.388217081516
SGD(937/999): loss=0.379237922722
SGD(938/999): loss=0.379853728456
SGD(939/999): loss=0.385803804353
SGD(940/999): loss=0.394633016165
SGD(941/999): loss=0.389736633876
SGD(942/999): loss=0.414557768022
SGD(943/999): loss=0.425731234999
SGD(944/999): loss=0.378640696726
SGD(945/999): loss=0.38046660129
SGD(946/999): loss=0.390671379703
SGD(947/999): loss=0.393291004653
SGD(948/999): loss=0.384047815665
SGD(949/999): loss=0.379844647619
SGD(950/999): loss=0.384037267395
SGD(951/999): loss=0.386268483608
SGD(952/999): loss=0.383792574307
SGD(953/999): loss=0.383556421286
SGD(954/999): loss=0.381946838703
SGD(955/999): loss=0.382289934312
SGD(956/999): loss=0.38409966396
SGD(957/999): loss=0.385989639388
SGD(958/999): loss=0.383829546253
SGD(959/999): loss=0.385076547865
SGD(960/999): loss=0.38458233134
SGD(961/999): loss=0.381670514063
SGD(962/999): loss=0.385011439852
SGD(963/999): loss=0.381122693795
SGD(964/999): loss=0.378110030831
SGD(965/999): loss=0.378733485712
SGD(966/999): loss=0.377966314018
SGD(967/999): loss=0.378525568681
SGD(968/999): loss=0.394231870304
SGD(969/999): loss=0.395272823239
SGD(970/999): loss=0.401458758555
SGD(971/999): loss=0.414841421631
SGD(972/999): loss=0.407124750651
SGD(973/999): loss=0.400810306933
SGD(974/999): loss=0.408071785236
SGD(975/999): loss=0.381199154588
SGD(976/999): loss=0.382185540227
SGD(977/999): loss=0.384864520289
SGD(978/999): loss=0.38490881289
SGD(979/999): loss=0.393429359102
SGD(980/999): loss=0.393556112479
SGD(981/999): loss=0.386574147557
SGD(982/999): loss=0.383954123924
SGD(983/999): loss=0.384068138936
SGD(984/999): loss=0.385805806649
SGD(985/999): loss=0.385702433257
SGD(986/999): loss=0.404106138729
SGD(987/999): loss=0.437635830426
SGD(988/999): loss=0.437677465991
SGD(989/999): loss=0.444155440279
SGD(990/999): loss=0.415641257561
SGD(991/999): loss=0.415084991679
SGD(992/999): loss=0.433757614372
SGD(993/999): loss=0.437584648407
SGD(994/999): loss=0.432501355722
SGD(995/999): loss=0.446554243739
SGD(996/999): loss=0.43029086077
SGD(997/999): loss=0.440801277971
SGD(998/999): loss=0.407993248484
SGD(999/999): loss=0.405145509887
Train loss: 0.4051, test_loss: 0.3911, accuracy of SGD is:  0.7270

Testing least sqaures test
Train loss: 0.3396, test_loss: 0.3393, accuracy of Least Squares is:  0.7448

Testing ridge regression test
Degree: 2, k-th train: 0, train loss: 0.8036, test_loss: 0.8002, lambda: 1e-10, accuracy: 0.767456
Degree: 2, k-th train: 1, train loss: 0.8016, test_loss: 0.8063, lambda: 1e-10, accuracy: 0.763136
Degree: 2, k-th train: 2, train loss: 0.8019, test_loss: 0.8095, lambda: 1e-10, accuracy: 0.764192
Degree: 2, k-th train: 3, train loss: 0.8027, test_loss: 0.8028, lambda: 1e-10, accuracy: 0.763104
Degree: 2, k-th train: 0, train loss: 0.8040, test_loss: 0.8005, lambda: 1.80472176683e-10, accuracy: 0.767104
Degree: 2, k-th train: 1, train loss: 0.8019, test_loss: 0.8066, lambda: 1.80472176683e-10, accuracy: 0.762784
Degree: 2, k-th train: 2, train loss: 0.8022, test_loss: 0.8099, lambda: 1.80472176683e-10, accuracy: 0.764256
Degree: 2, k-th train: 3, train loss: 0.8031, test_loss: 0.8032, lambda: 1.80472176683e-10, accuracy: 0.76224
Degree: 2, k-th train: 0, train loss: 0.8042, test_loss: 0.8007, lambda: 3.25702065566e-10, accuracy: 0.76688
Degree: 2, k-th train: 1, train loss: 0.8022, test_loss: 0.8068, lambda: 3.25702065566e-10, accuracy: 0.7624
Degree: 2, k-th train: 2, train loss: 0.8025, test_loss: 0.8101, lambda: 3.25702065566e-10, accuracy: 0.764416
Degree: 2, k-th train: 3, train loss: 0.8033, test_loss: 0.8034, lambda: 3.25702065566e-10, accuracy: 0.761728
Degree: 2, k-th train: 0, train loss: 0.8044, test_loss: 0.8009, lambda: 5.87801607227e-10, accuracy: 0.766784
Degree: 2, k-th train: 1, train loss: 0.8024, test_loss: 0.8070, lambda: 5.87801607227e-10, accuracy: 0.762016
Degree: 2, k-th train: 2, train loss: 0.8026, test_loss: 0.8103, lambda: 5.87801607227e-10, accuracy: 0.764352
Degree: 2, k-th train: 3, train loss: 0.8035, test_loss: 0.8036, lambda: 5.87801607227e-10, accuracy: 0.761344
Degree: 2, k-th train: 0, train loss: 0.8045, test_loss: 0.8009, lambda: 1.06081835514e-09, accuracy: 0.766656
Degree: 2, k-th train: 1, train loss: 0.8025, test_loss: 0.8071, lambda: 1.06081835514e-09, accuracy: 0.76208
Degree: 2, k-th train: 2, train loss: 0.8027, test_loss: 0.8104, lambda: 1.06081835514e-09, accuracy: 0.764384
Degree: 2, k-th train: 3, train loss: 0.8036, test_loss: 0.8037, lambda: 1.06081835514e-09, accuracy: 0.76144
Degree: 2, k-th train: 0, train loss: 0.8046, test_loss: 0.8010, lambda: 1.91448197617e-09, accuracy: 0.766496
Degree: 2, k-th train: 1, train loss: 0.8025, test_loss: 0.8072, lambda: 1.91448197617e-09, accuracy: 0.762144
Degree: 2, k-th train: 2, train loss: 0.8028, test_loss: 0.8105, lambda: 1.91448197617e-09, accuracy: 0.764384
Degree: 2, k-th train: 3, train loss: 0.8037, test_loss: 0.8038, lambda: 1.91448197617e-09, accuracy: 0.761312
Degree: 2, k-th train: 0, train loss: 0.8046, test_loss: 0.8010, lambda: 3.45510729459e-09, accuracy: 0.766496
Degree: 2, k-th train: 1, train loss: 0.8026, test_loss: 0.8072, lambda: 3.45510729459e-09, accuracy: 0.76208
Degree: 2, k-th train: 2, train loss: 0.8028, test_loss: 0.8106, lambda: 3.45510729459e-09, accuracy: 0.764288
Degree: 2, k-th train: 3, train loss: 0.8037, test_loss: 0.8038, lambda: 3.45510729459e-09, accuracy: 0.76144
Degree: 2, k-th train: 0, train loss: 0.8047, test_loss: 0.8011, lambda: 6.23550734127e-09, accuracy: 0.766464
Degree: 2, k-th train: 1, train loss: 0.8026, test_loss: 0.8072, lambda: 6.23550734127e-09, accuracy: 0.761888
Degree: 2, k-th train: 2, train loss: 0.8028, test_loss: 0.8106, lambda: 6.23550734127e-09, accuracy: 0.764224
Degree: 2, k-th train: 3, train loss: 0.8037, test_loss: 0.8038, lambda: 6.23550734127e-09, accuracy: 0.761568
Degree: 2, k-th train: 0, train loss: 0.8047, test_loss: 0.8011, lambda: 1.1253355826e-08, accuracy: 0.766688
Degree: 2, k-th train: 1, train loss: 0.8026, test_loss: 0.8073, lambda: 1.1253355826e-08, accuracy: 0.761984
Degree: 2, k-th train: 2, train loss: 0.8029, test_loss: 0.8106, lambda: 1.1253355826e-08, accuracy: 0.76432
Degree: 2, k-th train: 3, train loss: 0.8038, test_loss: 0.8038, lambda: 1.1253355826e-08, accuracy: 0.76128
Degree: 2, k-th train: 0, train loss: 0.8047, test_loss: 0.8011, lambda: 2.0309176209e-08, accuracy: 0.766688
Degree: 2, k-th train: 1, train loss: 0.8026, test_loss: 0.8073, lambda: 2.0309176209e-08, accuracy: 0.761856
Degree: 2, k-th train: 2, train loss: 0.8029, test_loss: 0.8106, lambda: 2.0309176209e-08, accuracy: 0.764064
Degree: 2, k-th train: 3, train loss: 0.8038, test_loss: 0.8038, lambda: 2.0309176209e-08, accuracy: 0.761568
Degree: 2, k-th train: 0, train loss: 0.8047, test_loss: 0.8011, lambda: 3.66524123708e-08, accuracy: 0.766496
Degree: 2, k-th train: 1, train loss: 0.8027, test_loss: 0.8074, lambda: 3.66524123708e-08, accuracy: 0.76192
Degree: 2, k-th train: 2, train loss: 0.8029, test_loss: 0.8107, lambda: 3.66524123708e-08, accuracy: 0.764192
Degree: 2, k-th train: 3, train loss: 0.8038, test_loss: 0.8038, lambda: 3.66524123708e-08, accuracy: 0.76144
Degree: 2, k-th train: 0, train loss: 0.8048, test_loss: 0.8012, lambda: 6.61474064123e-08, accuracy: 0.766528
Degree: 2, k-th train: 1, train loss: 0.8027, test_loss: 0.8074, lambda: 6.61474064123e-08, accuracy: 0.761536
Degree: 2, k-th train: 2, train loss: 0.8030, test_loss: 0.8108, lambda: 6.61474064123e-08, accuracy: 0.764128
Degree: 2, k-th train: 3, train loss: 0.8039, test_loss: 0.8038, lambda: 6.61474064123e-08, accuracy: 0.761664
Degree: 2, k-th train: 0, train loss: 0.8049, test_loss: 0.8013, lambda: 1.19377664171e-07, accuracy: 0.766208
Degree: 2, k-th train: 1, train loss: 0.8028, test_loss: 0.8076, lambda: 1.19377664171e-07, accuracy: 0.7616
Degree: 2, k-th train: 2, train loss: 0.8031, test_loss: 0.8109, lambda: 1.19377664171e-07, accuracy: 0.763296
Degree: 2, k-th train: 3, train loss: 0.8040, test_loss: 0.8039, lambda: 1.19377664171e-07, accuracy: 0.762176
Degree: 2, k-th train: 0, train loss: 0.8051, test_loss: 0.8015, lambda: 2.15443469003e-07, accuracy: 0.766176
Degree: 2, k-th train: 1, train loss: 0.8030, test_loss: 0.8078, lambda: 2.15443469003e-07, accuracy: 0.761408
Degree: 2, k-th train: 2, train loss: 0.8032, test_loss: 0.8111, lambda: 2.15443469003e-07, accuracy: 0.762528
Degree: 2, k-th train: 3, train loss: 0.8042, test_loss: 0.8040, lambda: 2.15443469003e-07, accuracy: 0.76192
Degree: 2, k-th train: 0, train loss: 0.8053, test_loss: 0.8017, lambda: 3.88815518031e-07, accuracy: 0.766336
Degree: 2, k-th train: 1, train loss: 0.8032, test_loss: 0.8081, lambda: 3.88815518031e-07, accuracy: 0.760992
Degree: 2, k-th train: 2, train loss: 0.8035, test_loss: 0.8114, lambda: 3.88815518031e-07, accuracy: 0.762784
Degree: 2, k-th train: 3, train loss: 0.8045, test_loss: 0.8042, lambda: 3.88815518031e-07, accuracy: 0.762112
Degree: 2, k-th train: 0, train loss: 0.8056, test_loss: 0.8019, lambda: 7.0170382867e-07, accuracy: 0.76672
Degree: 2, k-th train: 1, train loss: 0.8034, test_loss: 0.8084, lambda: 7.0170382867e-07, accuracy: 0.761056
Degree: 2, k-th train: 2, train loss: 0.8037, test_loss: 0.8117, lambda: 7.0170382867e-07, accuracy: 0.762656
Degree: 2, k-th train: 3, train loss: 0.8047, test_loss: 0.8044, lambda: 7.0170382867e-07, accuracy: 0.76208
Degree: 2, k-th train: 0, train loss: 0.8058, test_loss: 0.8021, lambda: 1.26638017347e-06, accuracy: 0.766688
Degree: 2, k-th train: 1, train loss: 0.8036, test_loss: 0.8086, lambda: 1.26638017347e-06, accuracy: 0.760928
Degree: 2, k-th train: 2, train loss: 0.8039, test_loss: 0.8119, lambda: 1.26638017347e-06, accuracy: 0.762464
Degree: 2, k-th train: 3, train loss: 0.8049, test_loss: 0.8045, lambda: 1.26638017347e-06, accuracy: 0.761536
Degree: 2, k-th train: 0, train loss: 0.8059, test_loss: 0.8023, lambda: 2.28546386413e-06, accuracy: 0.766208
Degree: 2, k-th train: 1, train loss: 0.8038, test_loss: 0.8088, lambda: 2.28546386413e-06, accuracy: 0.760896
Degree: 2, k-th train: 2, train loss: 0.8041, test_loss: 0.8120, lambda: 2.28546386413e-06, accuracy: 0.761856
Degree: 2, k-th train: 3, train loss: 0.8051, test_loss: 0.8047, lambda: 2.28546386413e-06, accuracy: 0.761376
Degree: 2, k-th train: 0, train loss: 0.8060, test_loss: 0.8024, lambda: 4.1246263829e-06, accuracy: 0.766144
Degree: 2, k-th train: 1, train loss: 0.8039, test_loss: 0.8089, lambda: 4.1246263829e-06, accuracy: 0.760384
Degree: 2, k-th train: 2, train loss: 0.8042, test_loss: 0.8121, lambda: 4.1246263829e-06, accuracy: 0.762048
Degree: 2, k-th train: 3, train loss: 0.8052, test_loss: 0.8048, lambda: 4.1246263829e-06, accuracy: 0.760928
Degree: 2, k-th train: 0, train loss: 0.8061, test_loss: 0.8025, lambda: 7.44380301325e-06, accuracy: 0.765664
Degree: 2, k-th train: 1, train loss: 0.8040, test_loss: 0.8090, lambda: 7.44380301325e-06, accuracy: 0.760096
Degree: 2, k-th train: 2, train loss: 0.8043, test_loss: 0.8122, lambda: 7.44380301325e-06, accuracy: 0.76176
Degree: 2, k-th train: 3, train loss: 0.8053, test_loss: 0.8049, lambda: 7.44380301325e-06, accuracy: 0.760672
Degree: 2, k-th train: 0, train loss: 0.8062, test_loss: 0.8025, lambda: 1.3433993326e-05, accuracy: 0.76528
Degree: 2, k-th train: 1, train loss: 0.8040, test_loss: 0.8090, lambda: 1.3433993326e-05, accuracy: 0.759936
Degree: 2, k-th train: 2, train loss: 0.8043, test_loss: 0.8123, lambda: 1.3433993326e-05, accuracy: 0.761664
Degree: 2, k-th train: 3, train loss: 0.8054, test_loss: 0.8050, lambda: 1.3433993326e-05, accuracy: 0.7608
Degree: 2, k-th train: 0, train loss: 0.8063, test_loss: 0.8026, lambda: 2.42446201708e-05, accuracy: 0.765152
Degree: 2, k-th train: 1, train loss: 0.8041, test_loss: 0.8091, lambda: 2.42446201708e-05, accuracy: 0.760096
Degree: 2, k-th train: 2, train loss: 0.8044, test_loss: 0.8123, lambda: 2.42446201708e-05, accuracy: 0.761696
Degree: 2, k-th train: 3, train loss: 0.8055, test_loss: 0.8051, lambda: 2.42446201708e-05, accuracy: 0.760512
Degree: 2, k-th train: 0, train loss: 0.8063, test_loss: 0.8026, lambda: 4.37547937507e-05, accuracy: 0.765152
Degree: 2, k-th train: 1, train loss: 0.8041, test_loss: 0.8091, lambda: 4.37547937507e-05, accuracy: 0.759936
Degree: 2, k-th train: 2, train loss: 0.8045, test_loss: 0.8124, lambda: 4.37547937507e-05, accuracy: 0.761792
Degree: 2, k-th train: 3, train loss: 0.8055, test_loss: 0.8051, lambda: 4.37547937507e-05, accuracy: 0.760448
Degree: 2, k-th train: 0, train loss: 0.8063, test_loss: 0.8027, lambda: 7.8965228685e-05, accuracy: 0.764896
Degree: 2, k-th train: 1, train loss: 0.8042, test_loss: 0.8092, lambda: 7.8965228685e-05, accuracy: 0.760032
Degree: 2, k-th train: 2, train loss: 0.8045, test_loss: 0.8124, lambda: 7.8965228685e-05, accuracy: 0.761792
Degree: 2, k-th train: 3, train loss: 0.8055, test_loss: 0.8051, lambda: 7.8965228685e-05, accuracy: 0.760384
Degree: 2, k-th train: 0, train loss: 0.8064, test_loss: 0.8027, lambda: 0.00014251026703, accuracy: 0.764864
Degree: 2, k-th train: 1, train loss: 0.8042, test_loss: 0.8092, lambda: 0.00014251026703, accuracy: 0.760224
Degree: 2, k-th train: 2, train loss: 0.8045, test_loss: 0.8124, lambda: 0.00014251026703, accuracy: 0.761664
Degree: 2, k-th train: 3, train loss: 0.8055, test_loss: 0.8052, lambda: 0.00014251026703, accuracy: 0.760416
Degree: 2, k-th train: 0, train loss: 0.8064, test_loss: 0.8027, lambda: 0.000257191380906, accuracy: 0.764992
Degree: 2, k-th train: 1, train loss: 0.8042, test_loss: 0.8092, lambda: 0.000257191380906, accuracy: 0.760224
Degree: 2, k-th train: 2, train loss: 0.8045, test_loss: 0.8124, lambda: 0.000257191380906, accuracy: 0.761504
Degree: 2, k-th train: 3, train loss: 0.8056, test_loss: 0.8052, lambda: 0.000257191380906, accuracy: 0.760448
Degree: 2, k-th train: 0, train loss: 0.8064, test_loss: 0.8028, lambda: 0.000464158883361, accuracy: 0.76496
Degree: 2, k-th train: 1, train loss: 0.8043, test_loss: 0.8092, lambda: 0.000464158883361, accuracy: 0.760576
Degree: 2, k-th train: 2, train loss: 0.8046, test_loss: 0.8125, lambda: 0.000464158883361, accuracy: 0.761504
Degree: 2, k-th train: 3, train loss: 0.8056, test_loss: 0.8053, lambda: 0.000464158883361, accuracy: 0.760288
Degree: 2, k-th train: 0, train loss: 0.8065, test_loss: 0.8028, lambda: 0.000837677640068, accuracy: 0.76512
Degree: 2, k-th train: 1, train loss: 0.8043, test_loss: 0.8092, lambda: 0.000837677640068, accuracy: 0.76064
Degree: 2, k-th train: 2, train loss: 0.8047, test_loss: 0.8126, lambda: 0.000837677640068, accuracy: 0.761856
Degree: 2, k-th train: 3, train loss: 0.8057, test_loss: 0.8054, lambda: 0.000837677640068, accuracy: 0.76016
Degree: 2, k-th train: 0, train loss: 0.8067, test_loss: 0.8030, lambda: 0.00151177507062, accuracy: 0.765152
Degree: 2, k-th train: 1, train loss: 0.8045, test_loss: 0.8093, lambda: 0.00151177507062, accuracy: 0.76032
Degree: 2, k-th train: 2, train loss: 0.8048, test_loss: 0.8128, lambda: 0.00151177507062, accuracy: 0.761472
Degree: 2, k-th train: 3, train loss: 0.8058, test_loss: 0.8056, lambda: 0.00151177507062, accuracy: 0.760352
Degree: 2, k-th train: 0, train loss: 0.8070, test_loss: 0.8034, lambda: 0.00272833337649, accuracy: 0.764832
Degree: 2, k-th train: 1, train loss: 0.8049, test_loss: 0.8096, lambda: 0.00272833337649, accuracy: 0.75984
Degree: 2, k-th train: 2, train loss: 0.8052, test_loss: 0.8132, lambda: 0.00272833337649, accuracy: 0.761184
Degree: 2, k-th train: 3, train loss: 0.8061, test_loss: 0.8060, lambda: 0.00272833337649, accuracy: 0.76016
Degree: 2, k-th train: 0, train loss: 0.8075, test_loss: 0.8039, lambda: 0.00492388263171, accuracy: 0.764544
Degree: 2, k-th train: 1, train loss: 0.8054, test_loss: 0.8100, lambda: 0.00492388263171, accuracy: 0.759008
Degree: 2, k-th train: 2, train loss: 0.8057, test_loss: 0.8138, lambda: 0.00492388263171, accuracy: 0.76144
Degree: 2, k-th train: 3, train loss: 0.8067, test_loss: 0.8066, lambda: 0.00492388263171, accuracy: 0.759488
Degree: 2, k-th train: 0, train loss: 0.8083, test_loss: 0.8047, lambda: 0.00888623816274, accuracy: 0.765152
Degree: 2, k-th train: 1, train loss: 0.8062, test_loss: 0.8106, lambda: 0.00888623816274, accuracy: 0.75824
Degree: 2, k-th train: 2, train loss: 0.8065, test_loss: 0.8147, lambda: 0.00888623816274, accuracy: 0.761152
Degree: 2, k-th train: 3, train loss: 0.8074, test_loss: 0.8074, lambda: 0.00888623816274, accuracy: 0.75904
Degree: 2, k-th train: 0, train loss: 0.8094, test_loss: 0.8059, lambda: 0.0160371874375, accuracy: 0.763776
Degree: 2, k-th train: 1, train loss: 0.8073, test_loss: 0.8115, lambda: 0.0160371874375, accuracy: 0.756992
Degree: 2, k-th train: 2, train loss: 0.8076, test_loss: 0.8158, lambda: 0.0160371874375, accuracy: 0.761888
Degree: 2, k-th train: 3, train loss: 0.8085, test_loss: 0.8086, lambda: 0.0160371874375, accuracy: 0.7576
Degree: 2, k-th train: 0, train loss: 0.8109, test_loss: 0.8075, lambda: 0.0289426612472, accuracy: 0.7632
Degree: 2, k-th train: 1, train loss: 0.8090, test_loss: 0.8128, lambda: 0.0289426612472, accuracy: 0.75536
Degree: 2, k-th train: 2, train loss: 0.8091, test_loss: 0.8172, lambda: 0.0289426612472, accuracy: 0.76144
Degree: 2, k-th train: 3, train loss: 0.8101, test_loss: 0.8104, lambda: 0.0289426612472, accuracy: 0.756256
Degree: 2, k-th train: 0, train loss: 0.8131, test_loss: 0.8097, lambda: 0.0522334507427, accuracy: 0.76112
Degree: 2, k-th train: 1, train loss: 0.8113, test_loss: 0.8147, lambda: 0.0522334507427, accuracy: 0.753952
Degree: 2, k-th train: 2, train loss: 0.8112, test_loss: 0.8189, lambda: 0.0522334507427, accuracy: 0.760288
Degree: 2, k-th train: 3, train loss: 0.8123, test_loss: 0.8129, lambda: 0.0522334507427, accuracy: 0.753856
Degree: 2, k-th train: 0, train loss: 0.8162, test_loss: 0.8128, lambda: 0.0942668455118, accuracy: 0.75904
Degree: 2, k-th train: 1, train loss: 0.8144, test_loss: 0.8176, lambda: 0.0942668455118, accuracy: 0.750752
Degree: 2, k-th train: 2, train loss: 0.8143, test_loss: 0.8210, lambda: 0.0942668455118, accuracy: 0.758016
Degree: 2, k-th train: 3, train loss: 0.8153, test_loss: 0.8164, lambda: 0.0942668455118, accuracy: 0.750912
Degree: 2, k-th train: 0, train loss: 0.8207, test_loss: 0.8172, lambda: 0.170125427985, accuracy: 0.754752
Degree: 2, k-th train: 1, train loss: 0.8191, test_loss: 0.8220, lambda: 0.170125427985, accuracy: 0.746528
Degree: 2, k-th train: 2, train loss: 0.8189, test_loss: 0.8239, lambda: 0.170125427985, accuracy: 0.754944
Degree: 2, k-th train: 3, train loss: 0.8198, test_loss: 0.8215, lambda: 0.170125427985, accuracy: 0.746368
Degree: 2, k-th train: 0, train loss: 0.8277, test_loss: 0.8241, lambda: 0.307029062976, accuracy: 0.750272
Degree: 2, k-th train: 1, train loss: 0.8261, test_loss: 0.8288, lambda: 0.307029062976, accuracy: 0.739936
Degree: 2, k-th train: 2, train loss: 0.8260, test_loss: 0.8290, lambda: 0.307029062976, accuracy: 0.749216
Degree: 2, k-th train: 3, train loss: 0.8266, test_loss: 0.8291, lambda: 0.307029062976, accuracy: 0.741824
Degree: 2, k-th train: 0, train loss: 0.8379, test_loss: 0.8343, lambda: 0.554102033001, accuracy: 0.7408
Degree: 2, k-th train: 1, train loss: 0.8363, test_loss: 0.8388, lambda: 0.554102033001, accuracy: 0.73072
Degree: 2, k-th train: 2, train loss: 0.8366, test_loss: 0.8376, lambda: 0.554102033001, accuracy: 0.741824
Degree: 2, k-th train: 3, train loss: 0.8366, test_loss: 0.8399, lambda: 0.554102033001, accuracy: 0.73344
Degree: 2, k-th train: 0, train loss: 0.8516, test_loss: 0.8481, lambda: 1.0, accuracy: 0.731296
Degree: 2, k-th train: 1, train loss: 0.8500, test_loss: 0.8523, lambda: 1.0, accuracy: 0.72224
Degree: 2, k-th train: 2, train loss: 0.8506, test_loss: 0.8501, lambda: 1.0, accuracy: 0.73184
Degree: 2, k-th train: 3, train loss: 0.8500, test_loss: 0.8540, lambda: 1.0, accuracy: 0.725888
-------------Degree: 2, test_loss: 0.8047, lambda: 1e-10, best_accuracy: 0.764472-----------------
Degree: 3, k-th train: 0, train loss: 0.7924, test_loss: 0.7901, lambda: 1e-10, accuracy: 0.776576
Degree: 3, k-th train: 1, train loss: 0.7905, test_loss: 0.7962, lambda: 1e-10, accuracy: 0.77184
Degree: 3, k-th train: 2, train loss: 0.7910, test_loss: 0.8181, lambda: 1e-10, accuracy: 0.773536
Degree: 3, k-th train: 3, train loss: 0.7917, test_loss: 0.7952, lambda: 1e-10, accuracy: 0.772128
Degree: 3, k-th train: 0, train loss: 0.7933, test_loss: 0.7908, lambda: 1.80472176683e-10, accuracy: 0.775808
Degree: 3, k-th train: 1, train loss: 0.7913, test_loss: 0.7969, lambda: 1.80472176683e-10, accuracy: 0.770944
Degree: 3, k-th train: 2, train loss: 0.7918, test_loss: 0.8191, lambda: 1.80472176683e-10, accuracy: 0.772384
Degree: 3, k-th train: 3, train loss: 0.7924, test_loss: 0.7960, lambda: 1.80472176683e-10, accuracy: 0.771104
Degree: 3, k-th train: 0, train loss: 0.7940, test_loss: 0.7914, lambda: 3.25702065566e-10, accuracy: 0.775456
Degree: 3, k-th train: 1, train loss: 0.7920, test_loss: 0.7976, lambda: 3.25702065566e-10, accuracy: 0.76992
Degree: 3, k-th train: 2, train loss: 0.7925, test_loss: 0.8199, lambda: 3.25702065566e-10, accuracy: 0.77184
Degree: 3, k-th train: 3, train loss: 0.7931, test_loss: 0.7967, lambda: 3.25702065566e-10, accuracy: 0.77072
Degree: 3, k-th train: 0, train loss: 0.7946, test_loss: 0.7918, lambda: 5.87801607227e-10, accuracy: 0.774912
Degree: 3, k-th train: 1, train loss: 0.7925, test_loss: 0.7981, lambda: 5.87801607227e-10, accuracy: 0.76912
Degree: 3, k-th train: 2, train loss: 0.7930, test_loss: 0.8205, lambda: 5.87801607227e-10, accuracy: 0.771712
Degree: 3, k-th train: 3, train loss: 0.7936, test_loss: 0.7972, lambda: 5.87801607227e-10, accuracy: 0.769696
Degree: 3, k-th train: 0, train loss: 0.7950, test_loss: 0.7922, lambda: 1.06081835514e-09, accuracy: 0.774368
Degree: 3, k-th train: 1, train loss: 0.7929, test_loss: 0.7985, lambda: 1.06081835514e-09, accuracy: 0.7688
Degree: 3, k-th train: 2, train loss: 0.7934, test_loss: 0.8210, lambda: 1.06081835514e-09, accuracy: 0.771136
Degree: 3, k-th train: 3, train loss: 0.7940, test_loss: 0.7976, lambda: 1.06081835514e-09, accuracy: 0.769344
Degree: 3, k-th train: 0, train loss: 0.7953, test_loss: 0.7924, lambda: 1.91448197617e-09, accuracy: 0.774112
Degree: 3, k-th train: 1, train loss: 0.7932, test_loss: 0.7988, lambda: 1.91448197617e-09, accuracy: 0.768256
Degree: 3, k-th train: 2, train loss: 0.7936, test_loss: 0.8213, lambda: 1.91448197617e-09, accuracy: 0.771008
Degree: 3, k-th train: 3, train loss: 0.7943, test_loss: 0.7979, lambda: 1.91448197617e-09, accuracy: 0.768704
Degree: 3, k-th train: 0, train loss: 0.7955, test_loss: 0.7926, lambda: 3.45510729459e-09, accuracy: 0.77424
Degree: 3, k-th train: 1, train loss: 0.7934, test_loss: 0.7990, lambda: 3.45510729459e-09, accuracy: 0.768416
Degree: 3, k-th train: 2, train loss: 0.7938, test_loss: 0.8216, lambda: 3.45510729459e-09, accuracy: 0.770784
Degree: 3, k-th train: 3, train loss: 0.7945, test_loss: 0.7981, lambda: 3.45510729459e-09, accuracy: 0.768672
Degree: 3, k-th train: 0, train loss: 0.7957, test_loss: 0.7927, lambda: 6.23550734127e-09, accuracy: 0.77392
Degree: 3, k-th train: 1, train loss: 0.7936, test_loss: 0.7991, lambda: 6.23550734127e-09, accuracy: 0.768096
Degree: 3, k-th train: 2, train loss: 0.7939, test_loss: 0.8217, lambda: 6.23550734127e-09, accuracy: 0.770784
Degree: 3, k-th train: 3, train loss: 0.7946, test_loss: 0.7981, lambda: 6.23550734127e-09, accuracy: 0.768416
Degree: 3, k-th train: 0, train loss: 0.7958, test_loss: 0.7928, lambda: 1.1253355826e-08, accuracy: 0.773664
Degree: 3, k-th train: 1, train loss: 0.7937, test_loss: 0.7992, lambda: 1.1253355826e-08, accuracy: 0.767744
Degree: 3, k-th train: 2, train loss: 0.7940, test_loss: 0.8218, lambda: 1.1253355826e-08, accuracy: 0.770816
Degree: 3, k-th train: 3, train loss: 0.7947, test_loss: 0.7981, lambda: 1.1253355826e-08, accuracy: 0.768576
Degree: 3, k-th train: 0, train loss: 0.7958, test_loss: 0.7928, lambda: 2.0309176209e-08, accuracy: 0.773504
Degree: 3, k-th train: 1, train loss: 0.7937, test_loss: 0.7993, lambda: 2.0309176209e-08, accuracy: 0.767776
Degree: 3, k-th train: 2, train loss: 0.7941, test_loss: 0.8218, lambda: 2.0309176209e-08, accuracy: 0.770688
Degree: 3, k-th train: 3, train loss: 0.7948, test_loss: 0.7981, lambda: 2.0309176209e-08, accuracy: 0.768672
Degree: 3, k-th train: 0, train loss: 0.7959, test_loss: 0.7928, lambda: 3.66524123708e-08, accuracy: 0.773536
Degree: 3, k-th train: 1, train loss: 0.7938, test_loss: 0.7993, lambda: 3.66524123708e-08, accuracy: 0.767904
Degree: 3, k-th train: 2, train loss: 0.7941, test_loss: 0.8218, lambda: 3.66524123708e-08, accuracy: 0.770624
Degree: 3, k-th train: 3, train loss: 0.7948, test_loss: 0.7979, lambda: 3.66524123708e-08, accuracy: 0.76864
Degree: 3, k-th train: 0, train loss: 0.7959, test_loss: 0.7929, lambda: 6.61474064123e-08, accuracy: 0.773664
Degree: 3, k-th train: 1, train loss: 0.7938, test_loss: 0.7993, lambda: 6.61474064123e-08, accuracy: 0.76784
Degree: 3, k-th train: 2, train loss: 0.7941, test_loss: 0.8218, lambda: 6.61474064123e-08, accuracy: 0.770496
Degree: 3, k-th train: 3, train loss: 0.7948, test_loss: 0.7977, lambda: 6.61474064123e-08, accuracy: 0.76848
Degree: 3, k-th train: 0, train loss: 0.7960, test_loss: 0.7929, lambda: 1.19377664171e-07, accuracy: 0.773632
Degree: 3, k-th train: 1, train loss: 0.7938, test_loss: 0.7994, lambda: 1.19377664171e-07, accuracy: 0.767712
Degree: 3, k-th train: 2, train loss: 0.7942, test_loss: 0.8218, lambda: 1.19377664171e-07, accuracy: 0.770304
Degree: 3, k-th train: 3, train loss: 0.7949, test_loss: 0.7973, lambda: 1.19377664171e-07, accuracy: 0.768224
Degree: 3, k-th train: 0, train loss: 0.7960, test_loss: 0.7929, lambda: 2.15443469003e-07, accuracy: 0.7736
Degree: 3, k-th train: 1, train loss: 0.7939, test_loss: 0.7995, lambda: 2.15443469003e-07, accuracy: 0.76736
Degree: 3, k-th train: 2, train loss: 0.7942, test_loss: 0.8218, lambda: 2.15443469003e-07, accuracy: 0.769952
Degree: 3, k-th train: 3, train loss: 0.7949, test_loss: 0.7969, lambda: 2.15443469003e-07, accuracy: 0.768288
Degree: 3, k-th train: 0, train loss: 0.7961, test_loss: 0.7930, lambda: 3.88815518031e-07, accuracy: 0.773344
Degree: 3, k-th train: 1, train loss: 0.7939, test_loss: 0.7996, lambda: 3.88815518031e-07, accuracy: 0.76768
Degree: 3, k-th train: 2, train loss: 0.7943, test_loss: 0.8218, lambda: 3.88815518031e-07, accuracy: 0.769792
Degree: 3, k-th train: 3, train loss: 0.7950, test_loss: 0.7966, lambda: 3.88815518031e-07, accuracy: 0.768
Degree: 3, k-th train: 0, train loss: 0.7962, test_loss: 0.7931, lambda: 7.0170382867e-07, accuracy: 0.773568
Degree: 3, k-th train: 1, train loss: 0.7941, test_loss: 0.7998, lambda: 7.0170382867e-07, accuracy: 0.768
Degree: 3, k-th train: 2, train loss: 0.7944, test_loss: 0.8219, lambda: 7.0170382867e-07, accuracy: 0.769344
Degree: 3, k-th train: 3, train loss: 0.7952, test_loss: 0.7965, lambda: 7.0170382867e-07, accuracy: 0.768192
Degree: 3, k-th train: 0, train loss: 0.7964, test_loss: 0.7933, lambda: 1.26638017347e-06, accuracy: 0.773568
Degree: 3, k-th train: 1, train loss: 0.7943, test_loss: 0.8001, lambda: 1.26638017347e-06, accuracy: 0.767392
Degree: 3, k-th train: 2, train loss: 0.7947, test_loss: 0.8220, lambda: 1.26638017347e-06, accuracy: 0.768448
Degree: 3, k-th train: 3, train loss: 0.7954, test_loss: 0.7965, lambda: 1.26638017347e-06, accuracy: 0.768064
Degree: 3, k-th train: 0, train loss: 0.7967, test_loss: 0.7936, lambda: 2.28546386413e-06, accuracy: 0.773248
Degree: 3, k-th train: 1, train loss: 0.7945, test_loss: 0.8004, lambda: 2.28546386413e-06, accuracy: 0.767392
Degree: 3, k-th train: 2, train loss: 0.7949, test_loss: 0.8223, lambda: 2.28546386413e-06, accuracy: 0.768
Degree: 3, k-th train: 3, train loss: 0.7957, test_loss: 0.7967, lambda: 2.28546386413e-06, accuracy: 0.767872
Degree: 3, k-th train: 0, train loss: 0.7970, test_loss: 0.7938, lambda: 4.1246263829e-06, accuracy: 0.773216
Degree: 3, k-th train: 1, train loss: 0.7948, test_loss: 0.8006, lambda: 4.1246263829e-06, accuracy: 0.767136
Degree: 3, k-th train: 2, train loss: 0.7952, test_loss: 0.8225, lambda: 4.1246263829e-06, accuracy: 0.767392
Degree: 3, k-th train: 3, train loss: 0.7960, test_loss: 0.7969, lambda: 4.1246263829e-06, accuracy: 0.76736
Degree: 3, k-th train: 0, train loss: 0.7972, test_loss: 0.7941, lambda: 7.44380301325e-06, accuracy: 0.772928
Degree: 3, k-th train: 1, train loss: 0.7950, test_loss: 0.8009, lambda: 7.44380301325e-06, accuracy: 0.766656
Degree: 3, k-th train: 2, train loss: 0.7954, test_loss: 0.8227, lambda: 7.44380301325e-06, accuracy: 0.767584
Degree: 3, k-th train: 3, train loss: 0.7963, test_loss: 0.7970, lambda: 7.44380301325e-06, accuracy: 0.767104
Degree: 3, k-th train: 0, train loss: 0.7974, test_loss: 0.7942, lambda: 1.3433993326e-05, accuracy: 0.772608
Degree: 3, k-th train: 1, train loss: 0.7951, test_loss: 0.8010, lambda: 1.3433993326e-05, accuracy: 0.766656
Degree: 3, k-th train: 2, train loss: 0.7955, test_loss: 0.8229, lambda: 1.3433993326e-05, accuracy: 0.767552
Degree: 3, k-th train: 3, train loss: 0.7964, test_loss: 0.7971, lambda: 1.3433993326e-05, accuracy: 0.766944
Degree: 3, k-th train: 0, train loss: 0.7975, test_loss: 0.7943, lambda: 2.42446201708e-05, accuracy: 0.772352
Degree: 3, k-th train: 1, train loss: 0.7952, test_loss: 0.8011, lambda: 2.42446201708e-05, accuracy: 0.766528
Degree: 3, k-th train: 2, train loss: 0.7957, test_loss: 0.8230, lambda: 2.42446201708e-05, accuracy: 0.767328
Degree: 3, k-th train: 3, train loss: 0.7966, test_loss: 0.7972, lambda: 2.42446201708e-05, accuracy: 0.767104
Degree: 3, k-th train: 0, train loss: 0.7976, test_loss: 0.7944, lambda: 4.37547937507e-05, accuracy: 0.77248
Degree: 3, k-th train: 1, train loss: 0.7953, test_loss: 0.8012, lambda: 4.37547937507e-05, accuracy: 0.76656
Degree: 3, k-th train: 2, train loss: 0.7957, test_loss: 0.8231, lambda: 4.37547937507e-05, accuracy: 0.767296
Degree: 3, k-th train: 3, train loss: 0.7966, test_loss: 0.7973, lambda: 4.37547937507e-05, accuracy: 0.767168
Degree: 3, k-th train: 0, train loss: 0.7976, test_loss: 0.7944, lambda: 7.8965228685e-05, accuracy: 0.772544
Degree: 3, k-th train: 1, train loss: 0.7954, test_loss: 0.8012, lambda: 7.8965228685e-05, accuracy: 0.766624
Degree: 3, k-th train: 2, train loss: 0.7958, test_loss: 0.8231, lambda: 7.8965228685e-05, accuracy: 0.767104
Degree: 3, k-th train: 3, train loss: 0.7967, test_loss: 0.7973, lambda: 7.8965228685e-05, accuracy: 0.766976
Degree: 3, k-th train: 0, train loss: 0.7977, test_loss: 0.7944, lambda: 0.00014251026703, accuracy: 0.772544
Degree: 3, k-th train: 1, train loss: 0.7954, test_loss: 0.8013, lambda: 0.00014251026703, accuracy: 0.766432
Degree: 3, k-th train: 2, train loss: 0.7958, test_loss: 0.8231, lambda: 0.00014251026703, accuracy: 0.767296
Degree: 3, k-th train: 3, train loss: 0.7968, test_loss: 0.7974, lambda: 0.00014251026703, accuracy: 0.766752
Degree: 3, k-th train: 0, train loss: 0.7978, test_loss: 0.7944, lambda: 0.000257191380906, accuracy: 0.77216
Degree: 3, k-th train: 1, train loss: 0.7955, test_loss: 0.8013, lambda: 0.000257191380906, accuracy: 0.766304
Degree: 3, k-th train: 2, train loss: 0.7959, test_loss: 0.8230, lambda: 0.000257191380906, accuracy: 0.76736
Degree: 3, k-th train: 3, train loss: 0.7968, test_loss: 0.7974, lambda: 0.000257191380906, accuracy: 0.766528
Degree: 3, k-th train: 0, train loss: 0.7979, test_loss: 0.7944, lambda: 0.000464158883361, accuracy: 0.772
Degree: 3, k-th train: 1, train loss: 0.7956, test_loss: 0.8014, lambda: 0.000464158883361, accuracy: 0.766496
Degree: 3, k-th train: 2, train loss: 0.7960, test_loss: 0.8229, lambda: 0.000464158883361, accuracy: 0.7672
Degree: 3, k-th train: 3, train loss: 0.7969, test_loss: 0.7975, lambda: 0.000464158883361, accuracy: 0.766496
Degree: 3, k-th train: 0, train loss: 0.7981, test_loss: 0.7945, lambda: 0.000837677640068, accuracy: 0.771424
Degree: 3, k-th train: 1, train loss: 0.7958, test_loss: 0.8015, lambda: 0.000837677640068, accuracy: 0.765952
Degree: 3, k-th train: 2, train loss: 0.7961, test_loss: 0.8228, lambda: 0.000837677640068, accuracy: 0.766944
Degree: 3, k-th train: 3, train loss: 0.7971, test_loss: 0.7977, lambda: 0.000837677640068, accuracy: 0.767104
Degree: 3, k-th train: 0, train loss: 0.7983, test_loss: 0.7946, lambda: 0.00151177507062, accuracy: 0.771456
Degree: 3, k-th train: 1, train loss: 0.7960, test_loss: 0.8017, lambda: 0.00151177507062, accuracy: 0.765696
Degree: 3, k-th train: 2, train loss: 0.7964, test_loss: 0.8226, lambda: 0.00151177507062, accuracy: 0.766976
Degree: 3, k-th train: 3, train loss: 0.7973, test_loss: 0.7980, lambda: 0.00151177507062, accuracy: 0.76688
Degree: 3, k-th train: 0, train loss: 0.7987, test_loss: 0.7950, lambda: 0.00272833337649, accuracy: 0.77136
Degree: 3, k-th train: 1, train loss: 0.7964, test_loss: 0.8019, lambda: 0.00272833337649, accuracy: 0.765376
Degree: 3, k-th train: 2, train loss: 0.7967, test_loss: 0.8222, lambda: 0.00272833337649, accuracy: 0.76672
Degree: 3, k-th train: 3, train loss: 0.7977, test_loss: 0.7984, lambda: 0.00272833337649, accuracy: 0.7664
Degree: 3, k-th train: 0, train loss: 0.7992, test_loss: 0.7954, lambda: 0.00492388263171, accuracy: 0.770464
Degree: 3, k-th train: 1, train loss: 0.7968, test_loss: 0.8023, lambda: 0.00492388263171, accuracy: 0.76528
Degree: 3, k-th train: 2, train loss: 0.7972, test_loss: 0.8217, lambda: 0.00492388263171, accuracy: 0.766688
Degree: 3, k-th train: 3, train loss: 0.7981, test_loss: 0.7990, lambda: 0.00492388263171, accuracy: 0.76512
Degree: 3, k-th train: 0, train loss: 0.7998, test_loss: 0.7961, lambda: 0.00888623816274, accuracy: 0.769856
Degree: 3, k-th train: 1, train loss: 0.7975, test_loss: 0.8027, lambda: 0.00888623816274, accuracy: 0.764256
Degree: 3, k-th train: 2, train loss: 0.7979, test_loss: 0.8208, lambda: 0.00888623816274, accuracy: 0.766304
Degree: 3, k-th train: 3, train loss: 0.7988, test_loss: 0.7998, lambda: 0.00888623816274, accuracy: 0.76528
Degree: 3, k-th train: 0, train loss: 0.8008, test_loss: 0.7972, lambda: 0.0160371874375, accuracy: 0.768544
Degree: 3, k-th train: 1, train loss: 0.7985, test_loss: 0.8034, lambda: 0.0160371874375, accuracy: 0.764032
Degree: 3, k-th train: 2, train loss: 0.7989, test_loss: 0.8196, lambda: 0.0160371874375, accuracy: 0.766304
Degree: 3, k-th train: 3, train loss: 0.7998, test_loss: 0.8012, lambda: 0.0160371874375, accuracy: 0.765152
Degree: 3, k-th train: 0, train loss: 0.8023, test_loss: 0.7988, lambda: 0.0289426612472, accuracy: 0.76736
Degree: 3, k-th train: 1, train loss: 0.8001, test_loss: 0.8046, lambda: 0.0289426612472, accuracy: 0.762528
Degree: 3, k-th train: 2, train loss: 0.8005, test_loss: 0.8184, lambda: 0.0289426612472, accuracy: 0.765664
Degree: 3, k-th train: 3, train loss: 0.8012, test_loss: 0.8033, lambda: 0.0289426612472, accuracy: 0.76352
Degree: 3, k-th train: 0, train loss: 0.8045, test_loss: 0.8010, lambda: 0.0522334507427, accuracy: 0.7672
Degree: 3, k-th train: 1, train loss: 0.8024, test_loss: 0.8066, lambda: 0.0522334507427, accuracy: 0.76096
Degree: 3, k-th train: 2, train loss: 0.8028, test_loss: 0.8175, lambda: 0.0522334507427, accuracy: 0.765344
Degree: 3, k-th train: 3, train loss: 0.8034, test_loss: 0.8062, lambda: 0.0522334507427, accuracy: 0.76192
Degree: 3, k-th train: 0, train loss: 0.8078, test_loss: 0.8044, lambda: 0.0942668455118, accuracy: 0.765152
Degree: 3, k-th train: 1, train loss: 0.8059, test_loss: 0.8096, lambda: 0.0942668455118, accuracy: 0.757856
Degree: 3, k-th train: 2, train loss: 0.8063, test_loss: 0.8174, lambda: 0.0942668455118, accuracy: 0.763296
Degree: 3, k-th train: 3, train loss: 0.8066, test_loss: 0.8104, lambda: 0.0942668455118, accuracy: 0.759552
Degree: 3, k-th train: 0, train loss: 0.8129, test_loss: 0.8094, lambda: 0.170125427985, accuracy: 0.762208
Degree: 3, k-th train: 1, train loss: 0.8111, test_loss: 0.8145, lambda: 0.170125427985, accuracy: 0.75552
Degree: 3, k-th train: 2, train loss: 0.8116, test_loss: 0.8191, lambda: 0.170125427985, accuracy: 0.759712
Degree: 3, k-th train: 3, train loss: 0.8115, test_loss: 0.8164, lambda: 0.170125427985, accuracy: 0.75568
Degree: 3, k-th train: 0, train loss: 0.8206, test_loss: 0.8171, lambda: 0.307029062976, accuracy: 0.757312
Degree: 3, k-th train: 1, train loss: 0.8189, test_loss: 0.8220, lambda: 0.307029062976, accuracy: 0.748416
Degree: 3, k-th train: 2, train loss: 0.8194, test_loss: 0.8239, lambda: 0.307029062976, accuracy: 0.75504
Degree: 3, k-th train: 3, train loss: 0.8190, test_loss: 0.8246, lambda: 0.307029062976, accuracy: 0.75024
Degree: 3, k-th train: 0, train loss: 0.8312, test_loss: 0.8277, lambda: 0.554102033001, accuracy: 0.75136
Degree: 3, k-th train: 1, train loss: 0.8295, test_loss: 0.8323, lambda: 0.554102033001, accuracy: 0.74
Degree: 3, k-th train: 2, train loss: 0.8301, test_loss: 0.8325, lambda: 0.554102033001, accuracy: 0.748256
Degree: 3, k-th train: 3, train loss: 0.8293, test_loss: 0.8350, lambda: 0.554102033001, accuracy: 0.742464
Degree: 3, k-th train: 0, train loss: 0.8441, test_loss: 0.8408, lambda: 1.0, accuracy: 0.741696
Degree: 3, k-th train: 1, train loss: 0.8424, test_loss: 0.8450, lambda: 1.0, accuracy: 0.733504
Degree: 3, k-th train: 2, train loss: 0.8431, test_loss: 0.8443, lambda: 1.0, accuracy: 0.741184
Degree: 3, k-th train: 3, train loss: 0.8421, test_loss: 0.8471, lambda: 1.0, accuracy: 0.737152
-------------Degree: 3, test_loss: 0.7999, lambda: 1e-10, best_accuracy: 0.77352-----------------
Degree: 4, k-th train: 0, train loss: 0.7801, test_loss: 0.7853, lambda: 1e-10, accuracy: 0.789088
Degree: 4, k-th train: 1, train loss: 0.7793, test_loss: 0.7846, lambda: 1e-10, accuracy: 0.782656
Degree: 4, k-th train: 2, train loss: 0.7805, test_loss: 0.8119, lambda: 1e-10, accuracy: 0.783904
Degree: 4, k-th train: 3, train loss: 0.7803, test_loss: 0.7879, lambda: 1e-10, accuracy: 0.784864
Degree: 4, k-th train: 0, train loss: 0.7809, test_loss: 0.7860, lambda: 1.80472176683e-10, accuracy: 0.788096
Degree: 4, k-th train: 1, train loss: 0.7801, test_loss: 0.7853, lambda: 1.80472176683e-10, accuracy: 0.782464
Degree: 4, k-th train: 2, train loss: 0.7812, test_loss: 0.8128, lambda: 1.80472176683e-10, accuracy: 0.783936
Degree: 4, k-th train: 3, train loss: 0.7810, test_loss: 0.7887, lambda: 1.80472176683e-10, accuracy: 0.784224
Degree: 4, k-th train: 0, train loss: 0.7817, test_loss: 0.7869, lambda: 3.25702065566e-10, accuracy: 0.786944
Degree: 4, k-th train: 1, train loss: 0.7811, test_loss: 0.7861, lambda: 3.25702065566e-10, accuracy: 0.782208
Degree: 4, k-th train: 2, train loss: 0.7821, test_loss: 0.8138, lambda: 3.25702065566e-10, accuracy: 0.783264
Degree: 4, k-th train: 3, train loss: 0.7819, test_loss: 0.7895, lambda: 3.25702065566e-10, accuracy: 0.78352
Degree: 4, k-th train: 0, train loss: 0.7826, test_loss: 0.7876, lambda: 5.87801607227e-10, accuracy: 0.786144
Degree: 4, k-th train: 1, train loss: 0.7819, test_loss: 0.7868, lambda: 5.87801607227e-10, accuracy: 0.780448
Degree: 4, k-th train: 2, train loss: 0.7829, test_loss: 0.8147, lambda: 5.87801607227e-10, accuracy: 0.782816
Degree: 4, k-th train: 3, train loss: 0.7827, test_loss: 0.7904, lambda: 5.87801607227e-10, accuracy: 0.782688
Degree: 4, k-th train: 0, train loss: 0.7832, test_loss: 0.7882, lambda: 1.06081835514e-09, accuracy: 0.78528
Degree: 4, k-th train: 1, train loss: 0.7826, test_loss: 0.7874, lambda: 1.06081835514e-09, accuracy: 0.780128
Degree: 4, k-th train: 2, train loss: 0.7835, test_loss: 0.8155, lambda: 1.06081835514e-09, accuracy: 0.78224
Degree: 4, k-th train: 3, train loss: 0.7833, test_loss: 0.7914, lambda: 1.06081835514e-09, accuracy: 0.7816
Degree: 4, k-th train: 0, train loss: 0.7837, test_loss: 0.7887, lambda: 1.91448197617e-09, accuracy: 0.785088
Degree: 4, k-th train: 1, train loss: 0.7831, test_loss: 0.7879, lambda: 1.91448197617e-09, accuracy: 0.779552
Degree: 4, k-th train: 2, train loss: 0.7840, test_loss: 0.8159, lambda: 1.91448197617e-09, accuracy: 0.781984
Degree: 4, k-th train: 3, train loss: 0.7837, test_loss: 0.7924, lambda: 1.91448197617e-09, accuracy: 0.780256
Degree: 4, k-th train: 0, train loss: 0.7841, test_loss: 0.7889, lambda: 3.45510729459e-09, accuracy: 0.784608
Degree: 4, k-th train: 1, train loss: 0.7834, test_loss: 0.7882, lambda: 3.45510729459e-09, accuracy: 0.77936
Degree: 4, k-th train: 2, train loss: 0.7843, test_loss: 0.8162, lambda: 3.45510729459e-09, accuracy: 0.781632
Degree: 4, k-th train: 3, train loss: 0.7840, test_loss: 0.7937, lambda: 3.45510729459e-09, accuracy: 0.77952
Degree: 4, k-th train: 0, train loss: 0.7843, test_loss: 0.7891, lambda: 6.23550734127e-09, accuracy: 0.784672
Degree: 4, k-th train: 1, train loss: 0.7836, test_loss: 0.7884, lambda: 6.23550734127e-09, accuracy: 0.778976
Degree: 4, k-th train: 2, train loss: 0.7845, test_loss: 0.8164, lambda: 6.23550734127e-09, accuracy: 0.78128
Degree: 4, k-th train: 3, train loss: 0.7843, test_loss: 0.7957, lambda: 6.23550734127e-09, accuracy: 0.778944
Degree: 4, k-th train: 0, train loss: 0.7845, test_loss: 0.7892, lambda: 1.1253355826e-08, accuracy: 0.784608
Degree: 4, k-th train: 1, train loss: 0.7838, test_loss: 0.7886, lambda: 1.1253355826e-08, accuracy: 0.778144
Degree: 4, k-th train: 2, train loss: 0.7846, test_loss: 0.8164, lambda: 1.1253355826e-08, accuracy: 0.78096
Degree: 4, k-th train: 3, train loss: 0.7844, test_loss: 0.7984, lambda: 1.1253355826e-08, accuracy: 0.779072
Degree: 4, k-th train: 0, train loss: 0.7847, test_loss: 0.7893, lambda: 2.0309176209e-08, accuracy: 0.784608
Degree: 4, k-th train: 1, train loss: 0.7840, test_loss: 0.7887, lambda: 2.0309176209e-08, accuracy: 0.778272
Degree: 4, k-th train: 2, train loss: 0.7848, test_loss: 0.8162, lambda: 2.0309176209e-08, accuracy: 0.781056
Degree: 4, k-th train: 3, train loss: 0.7846, test_loss: 0.8016, lambda: 2.0309176209e-08, accuracy: 0.778848
Degree: 4, k-th train: 0, train loss: 0.7848, test_loss: 0.7894, lambda: 3.66524123708e-08, accuracy: 0.78464
Degree: 4, k-th train: 1, train loss: 0.7841, test_loss: 0.7889, lambda: 3.66524123708e-08, accuracy: 0.778176
Degree: 4, k-th train: 2, train loss: 0.7849, test_loss: 0.8161, lambda: 3.66524123708e-08, accuracy: 0.78112
Degree: 4, k-th train: 3, train loss: 0.7847, test_loss: 0.8047, lambda: 3.66524123708e-08, accuracy: 0.77904
Degree: 4, k-th train: 0, train loss: 0.7849, test_loss: 0.7894, lambda: 6.61474064123e-08, accuracy: 0.78448
Degree: 4, k-th train: 1, train loss: 0.7842, test_loss: 0.7891, lambda: 6.61474064123e-08, accuracy: 0.778144
Degree: 4, k-th train: 2, train loss: 0.7850, test_loss: 0.8158, lambda: 6.61474064123e-08, accuracy: 0.780832
Degree: 4, k-th train: 3, train loss: 0.7847, test_loss: 0.8073, lambda: 6.61474064123e-08, accuracy: 0.77888
Degree: 4, k-th train: 0, train loss: 0.7850, test_loss: 0.7895, lambda: 1.19377664171e-07, accuracy: 0.784064
Degree: 4, k-th train: 1, train loss: 0.7843, test_loss: 0.7892, lambda: 1.19377664171e-07, accuracy: 0.77824
Degree: 4, k-th train: 2, train loss: 0.7851, test_loss: 0.8156, lambda: 1.19377664171e-07, accuracy: 0.780864
Degree: 4, k-th train: 3, train loss: 0.7848, test_loss: 0.8090, lambda: 1.19377664171e-07, accuracy: 0.778912
Degree: 4, k-th train: 0, train loss: 0.7851, test_loss: 0.7895, lambda: 2.15443469003e-07, accuracy: 0.783904
Degree: 4, k-th train: 1, train loss: 0.7843, test_loss: 0.7892, lambda: 2.15443469003e-07, accuracy: 0.778048
Degree: 4, k-th train: 2, train loss: 0.7852, test_loss: 0.8155, lambda: 2.15443469003e-07, accuracy: 0.780832
Degree: 4, k-th train: 3, train loss: 0.7848, test_loss: 0.8100, lambda: 2.15443469003e-07, accuracy: 0.778848
Degree: 4, k-th train: 0, train loss: 0.7851, test_loss: 0.7895, lambda: 3.88815518031e-07, accuracy: 0.783744
Degree: 4, k-th train: 1, train loss: 0.7844, test_loss: 0.7893, lambda: 3.88815518031e-07, accuracy: 0.77792
Degree: 4, k-th train: 2, train loss: 0.7852, test_loss: 0.8153, lambda: 3.88815518031e-07, accuracy: 0.780576
Degree: 4, k-th train: 3, train loss: 0.7848, test_loss: 0.8103, lambda: 3.88815518031e-07, accuracy: 0.778816
Degree: 4, k-th train: 0, train loss: 0.7852, test_loss: 0.7895, lambda: 7.0170382867e-07, accuracy: 0.783808
Degree: 4, k-th train: 1, train loss: 0.7844, test_loss: 0.7894, lambda: 7.0170382867e-07, accuracy: 0.77776
Degree: 4, k-th train: 2, train loss: 0.7853, test_loss: 0.8152, lambda: 7.0170382867e-07, accuracy: 0.78064
Degree: 4, k-th train: 3, train loss: 0.7849, test_loss: 0.8101, lambda: 7.0170382867e-07, accuracy: 0.778912
Degree: 4, k-th train: 0, train loss: 0.7853, test_loss: 0.7895, lambda: 1.26638017347e-06, accuracy: 0.784064
Degree: 4, k-th train: 1, train loss: 0.7845, test_loss: 0.7895, lambda: 1.26638017347e-06, accuracy: 0.777856
Degree: 4, k-th train: 2, train loss: 0.7854, test_loss: 0.8151, lambda: 1.26638017347e-06, accuracy: 0.780544
Degree: 4, k-th train: 3, train loss: 0.7849, test_loss: 0.8095, lambda: 1.26638017347e-06, accuracy: 0.778944
Degree: 4, k-th train: 0, train loss: 0.7854, test_loss: 0.7895, lambda: 2.28546386413e-06, accuracy: 0.784096
Degree: 4, k-th train: 1, train loss: 0.7846, test_loss: 0.7897, lambda: 2.28546386413e-06, accuracy: 0.777856
Degree: 4, k-th train: 2, train loss: 0.7855, test_loss: 0.8152, lambda: 2.28546386413e-06, accuracy: 0.780064
Degree: 4, k-th train: 3, train loss: 0.7851, test_loss: 0.8089, lambda: 2.28546386413e-06, accuracy: 0.77888
Degree: 4, k-th train: 0, train loss: 0.7856, test_loss: 0.7895, lambda: 4.1246263829e-06, accuracy: 0.783904
Degree: 4, k-th train: 1, train loss: 0.7847, test_loss: 0.7899, lambda: 4.1246263829e-06, accuracy: 0.777856
Degree: 4, k-th train: 2, train loss: 0.7856, test_loss: 0.8153, lambda: 4.1246263829e-06, accuracy: 0.78048
Degree: 4, k-th train: 3, train loss: 0.7852, test_loss: 0.8086, lambda: 4.1246263829e-06, accuracy: 0.779104
Degree: 4, k-th train: 0, train loss: 0.7858, test_loss: 0.7894, lambda: 7.44380301325e-06, accuracy: 0.783328
Degree: 4, k-th train: 1, train loss: 0.7849, test_loss: 0.7902, lambda: 7.44380301325e-06, accuracy: 0.777472
Degree: 4, k-th train: 2, train loss: 0.7859, test_loss: 0.8155, lambda: 7.44380301325e-06, accuracy: 0.779872
Degree: 4, k-th train: 3, train loss: 0.7855, test_loss: 0.8085, lambda: 7.44380301325e-06, accuracy: 0.778368
Degree: 4, k-th train: 0, train loss: 0.7861, test_loss: 0.7892, lambda: 1.3433993326e-05, accuracy: 0.78288
Degree: 4, k-th train: 1, train loss: 0.7852, test_loss: 0.7905, lambda: 1.3433993326e-05, accuracy: 0.777024
Degree: 4, k-th train: 2, train loss: 0.7861, test_loss: 0.8158, lambda: 1.3433993326e-05, accuracy: 0.77904
Degree: 4, k-th train: 3, train loss: 0.7858, test_loss: 0.8087, lambda: 1.3433993326e-05, accuracy: 0.778592
Degree: 4, k-th train: 0, train loss: 0.7864, test_loss: 0.7888, lambda: 2.42446201708e-05, accuracy: 0.78256
Degree: 4, k-th train: 1, train loss: 0.7854, test_loss: 0.7908, lambda: 2.42446201708e-05, accuracy: 0.776608
Degree: 4, k-th train: 2, train loss: 0.7864, test_loss: 0.8161, lambda: 2.42446201708e-05, accuracy: 0.778496
Degree: 4, k-th train: 3, train loss: 0.7860, test_loss: 0.8088, lambda: 2.42446201708e-05, accuracy: 0.778464
Degree: 4, k-th train: 0, train loss: 0.7868, test_loss: 0.7882, lambda: 4.37547937507e-05, accuracy: 0.7824
Degree: 4, k-th train: 1, train loss: 0.7857, test_loss: 0.7912, lambda: 4.37547937507e-05, accuracy: 0.77664
Degree: 4, k-th train: 2, train loss: 0.7867, test_loss: 0.8165, lambda: 4.37547937507e-05, accuracy: 0.778048
Degree: 4, k-th train: 3, train loss: 0.7863, test_loss: 0.8087, lambda: 4.37547937507e-05, accuracy: 0.778144
Degree: 4, k-th train: 0, train loss: 0.7873, test_loss: 0.7876, lambda: 7.8965228685e-05, accuracy: 0.781888
Degree: 4, k-th train: 1, train loss: 0.7861, test_loss: 0.7917, lambda: 7.8965228685e-05, accuracy: 0.776064
Degree: 4, k-th train: 2, train loss: 0.7871, test_loss: 0.8170, lambda: 7.8965228685e-05, accuracy: 0.777184
Degree: 4, k-th train: 3, train loss: 0.7867, test_loss: 0.8085, lambda: 7.8965228685e-05, accuracy: 0.777216
Degree: 4, k-th train: 0, train loss: 0.7881, test_loss: 0.7873, lambda: 0.00014251026703, accuracy: 0.7808
Degree: 4, k-th train: 1, train loss: 0.7866, test_loss: 0.7924, lambda: 0.00014251026703, accuracy: 0.775648
Degree: 4, k-th train: 2, train loss: 0.7876, test_loss: 0.8176, lambda: 0.00014251026703, accuracy: 0.776192
Degree: 4, k-th train: 3, train loss: 0.7873, test_loss: 0.8083, lambda: 0.00014251026703, accuracy: 0.776416
Degree: 4, k-th train: 0, train loss: 0.7891, test_loss: 0.7875, lambda: 0.000257191380906, accuracy: 0.780352
Degree: 4, k-th train: 1, train loss: 0.7874, test_loss: 0.7932, lambda: 0.000257191380906, accuracy: 0.775328
Degree: 4, k-th train: 2, train loss: 0.7884, test_loss: 0.8184, lambda: 0.000257191380906, accuracy: 0.775808
Degree: 4, k-th train: 3, train loss: 0.7881, test_loss: 0.8082, lambda: 0.000257191380906, accuracy: 0.775584
Degree: 4, k-th train: 0, train loss: 0.7900, test_loss: 0.7881, lambda: 0.000464158883361, accuracy: 0.779808
Degree: 4, k-th train: 1, train loss: 0.7881, test_loss: 0.7941, lambda: 0.000464158883361, accuracy: 0.774464
Degree: 4, k-th train: 2, train loss: 0.7892, test_loss: 0.8190, lambda: 0.000464158883361, accuracy: 0.77456
Degree: 4, k-th train: 3, train loss: 0.7889, test_loss: 0.8084, lambda: 0.000464158883361, accuracy: 0.775008
Degree: 4, k-th train: 0, train loss: 0.7908, test_loss: 0.7887, lambda: 0.000837677640068, accuracy: 0.779296
Degree: 4, k-th train: 1, train loss: 0.7889, test_loss: 0.7948, lambda: 0.000837677640068, accuracy: 0.77408
Degree: 4, k-th train: 2, train loss: 0.7899, test_loss: 0.8191, lambda: 0.000837677640068, accuracy: 0.774112
Degree: 4, k-th train: 3, train loss: 0.7897, test_loss: 0.8088, lambda: 0.000837677640068, accuracy: 0.774592
Degree: 4, k-th train: 0, train loss: 0.7915, test_loss: 0.7893, lambda: 0.00151177507062, accuracy: 0.77888
Degree: 4, k-th train: 1, train loss: 0.7894, test_loss: 0.7953, lambda: 0.00151177507062, accuracy: 0.773888
Degree: 4, k-th train: 2, train loss: 0.7905, test_loss: 0.8187, lambda: 0.00151177507062, accuracy: 0.774144
Degree: 4, k-th train: 3, train loss: 0.7903, test_loss: 0.8095, lambda: 0.00151177507062, accuracy: 0.7744
Degree: 4, k-th train: 0, train loss: 0.7920, test_loss: 0.7899, lambda: 0.00272833337649, accuracy: 0.778432
Degree: 4, k-th train: 1, train loss: 0.7900, test_loss: 0.7957, lambda: 0.00272833337649, accuracy: 0.773024
Degree: 4, k-th train: 2, train loss: 0.7911, test_loss: 0.8175, lambda: 0.00272833337649, accuracy: 0.773888
Degree: 4, k-th train: 3, train loss: 0.7909, test_loss: 0.8108, lambda: 0.00272833337649, accuracy: 0.774496
Degree: 4, k-th train: 0, train loss: 0.7926, test_loss: 0.7906, lambda: 0.00492388263171, accuracy: 0.778144
Degree: 4, k-th train: 1, train loss: 0.7905, test_loss: 0.7962, lambda: 0.00492388263171, accuracy: 0.772544
Degree: 4, k-th train: 2, train loss: 0.7917, test_loss: 0.8156, lambda: 0.00492388263171, accuracy: 0.773056
Degree: 4, k-th train: 3, train loss: 0.7914, test_loss: 0.8127, lambda: 0.00492388263171, accuracy: 0.773664
Degree: 4, k-th train: 0, train loss: 0.7933, test_loss: 0.7915, lambda: 0.00888623816274, accuracy: 0.7784
Degree: 4, k-th train: 1, train loss: 0.7913, test_loss: 0.7967, lambda: 0.00888623816274, accuracy: 0.77184
Degree: 4, k-th train: 2, train loss: 0.7925, test_loss: 0.8133, lambda: 0.00888623816274, accuracy: 0.77232
Degree: 4, k-th train: 3, train loss: 0.7922, test_loss: 0.8155, lambda: 0.00888623816274, accuracy: 0.773248
Degree: 4, k-th train: 0, train loss: 0.7943, test_loss: 0.7928, lambda: 0.0160371874375, accuracy: 0.777184
Degree: 4, k-th train: 1, train loss: 0.7925, test_loss: 0.7976, lambda: 0.0160371874375, accuracy: 0.77184
Degree: 4, k-th train: 2, train loss: 0.7936, test_loss: 0.8110, lambda: 0.0160371874375, accuracy: 0.771648
Degree: 4, k-th train: 3, train loss: 0.7932, test_loss: 0.8188, lambda: 0.0160371874375, accuracy: 0.77312
Degree: 4, k-th train: 0, train loss: 0.7958, test_loss: 0.7944, lambda: 0.0289426612472, accuracy: 0.776512
Degree: 4, k-th train: 1, train loss: 0.7940, test_loss: 0.7989, lambda: 0.0289426612472, accuracy: 0.771488
Degree: 4, k-th train: 2, train loss: 0.7951, test_loss: 0.8092, lambda: 0.0289426612472, accuracy: 0.7712
Degree: 4, k-th train: 3, train loss: 0.7946, test_loss: 0.8220, lambda: 0.0289426612472, accuracy: 0.77136
Degree: 4, k-th train: 0, train loss: 0.7978, test_loss: 0.7965, lambda: 0.0522334507427, accuracy: 0.77536
Degree: 4, k-th train: 1, train loss: 0.7961, test_loss: 0.8007, lambda: 0.0522334507427, accuracy: 0.770176
Degree: 4, k-th train: 2, train loss: 0.7972, test_loss: 0.8081, lambda: 0.0522334507427, accuracy: 0.770944
Degree: 4, k-th train: 3, train loss: 0.7965, test_loss: 0.8244, lambda: 0.0522334507427, accuracy: 0.76976
Degree: 4, k-th train: 0, train loss: 0.8008, test_loss: 0.7995, lambda: 0.0942668455118, accuracy: 0.7736
Degree: 4, k-th train: 1, train loss: 0.7992, test_loss: 0.8035, lambda: 0.0942668455118, accuracy: 0.767936
Degree: 4, k-th train: 2, train loss: 0.8003, test_loss: 0.8082, lambda: 0.0942668455118, accuracy: 0.770176
Degree: 4, k-th train: 3, train loss: 0.7995, test_loss: 0.8256, lambda: 0.0942668455118, accuracy: 0.766528
Degree: 4, k-th train: 0, train loss: 0.8057, test_loss: 0.8042, lambda: 0.170125427985, accuracy: 0.769664
Degree: 4, k-th train: 1, train loss: 0.8042, test_loss: 0.8081, lambda: 0.170125427985, accuracy: 0.763744
Degree: 4, k-th train: 2, train loss: 0.8052, test_loss: 0.8107, lambda: 0.170125427985, accuracy: 0.766624
Degree: 4, k-th train: 3, train loss: 0.8044, test_loss: 0.8259, lambda: 0.170125427985, accuracy: 0.763232
Degree: 4, k-th train: 0, train loss: 0.8132, test_loss: 0.8115, lambda: 0.307029062976, accuracy: 0.763488
Degree: 4, k-th train: 1, train loss: 0.8118, test_loss: 0.8154, lambda: 0.307029062976, accuracy: 0.756608
Degree: 4, k-th train: 2, train loss: 0.8127, test_loss: 0.8165, lambda: 0.307029062976, accuracy: 0.760352
Degree: 4, k-th train: 3, train loss: 0.8119, test_loss: 0.8269, lambda: 0.307029062976, accuracy: 0.756
Degree: 4, k-th train: 0, train loss: 0.8235, test_loss: 0.8216, lambda: 0.554102033001, accuracy: 0.754656
Degree: 4, k-th train: 1, train loss: 0.8221, test_loss: 0.8253, lambda: 0.554102033001, accuracy: 0.746624
Degree: 4, k-th train: 2, train loss: 0.8230, test_loss: 0.8256, lambda: 0.554102033001, accuracy: 0.750624
Degree: 4, k-th train: 3, train loss: 0.8222, test_loss: 0.8309, lambda: 0.554102033001, accuracy: 0.746752
Degree: 4, k-th train: 0, train loss: 0.8357, test_loss: 0.8337, lambda: 1.0, accuracy: 0.745408
Degree: 4, k-th train: 1, train loss: 0.8343, test_loss: 0.8371, lambda: 1.0, accuracy: 0.73712
Degree: 4, k-th train: 2, train loss: 0.8352, test_loss: 0.8373, lambda: 1.0, accuracy: 0.743872
Degree: 4, k-th train: 3, train loss: 0.8344, test_loss: 0.8394, lambda: 1.0, accuracy: 0.738944
-------------Degree: 4, test_loss: 0.7924, lambda: 1e-10, best_accuracy: 0.785128-----------------
Degree: 5, k-th train: 0, train loss: 0.7696, test_loss: 0.7951, lambda: 1e-10, accuracy: 0.798336
Degree: 5, k-th train: 1, train loss: 0.7690, test_loss: 0.7790, lambda: 1e-10, accuracy: 0.793248
Degree: 5, k-th train: 2, train loss: 0.7707, test_loss: 0.8779, lambda: 1e-10, accuracy: 0.794496
Degree: 5, k-th train: 3, train loss: 0.7705, test_loss: 0.8073, lambda: 1e-10, accuracy: 0.795008
Degree: 5, k-th train: 0, train loss: 0.7700, test_loss: 0.7955, lambda: 1.80472176683e-10, accuracy: 0.79792
Degree: 5, k-th train: 1, train loss: 0.7695, test_loss: 0.7793, lambda: 1.80472176683e-10, accuracy: 0.792448
Degree: 5, k-th train: 2, train loss: 0.7710, test_loss: 0.8793, lambda: 1.80472176683e-10, accuracy: 0.794592
Degree: 5, k-th train: 3, train loss: 0.7709, test_loss: 0.8205, lambda: 1.80472176683e-10, accuracy: 0.794208
Degree: 5, k-th train: 0, train loss: 0.7707, test_loss: 0.7961, lambda: 3.25702065566e-10, accuracy: 0.797152
Degree: 5, k-th train: 1, train loss: 0.7702, test_loss: 0.7798, lambda: 3.25702065566e-10, accuracy: 0.792256
Degree: 5, k-th train: 2, train loss: 0.7717, test_loss: 0.8809, lambda: 3.25702065566e-10, accuracy: 0.793664
Degree: 5, k-th train: 3, train loss: 0.7716, test_loss: 0.8392, lambda: 3.25702065566e-10, accuracy: 0.793344
Degree: 5, k-th train: 0, train loss: 0.7715, test_loss: 0.7970, lambda: 5.87801607227e-10, accuracy: 0.796704
Degree: 5, k-th train: 1, train loss: 0.7711, test_loss: 0.7806, lambda: 5.87801607227e-10, accuracy: 0.79152
Degree: 5, k-th train: 2, train loss: 0.7725, test_loss: 0.8824, lambda: 5.87801607227e-10, accuracy: 0.7928
Degree: 5, k-th train: 3, train loss: 0.7724, test_loss: 0.8604, lambda: 5.87801607227e-10, accuracy: 0.792064
Degree: 5, k-th train: 0, train loss: 0.7724, test_loss: 0.7978, lambda: 1.06081835514e-09, accuracy: 0.795616
Degree: 5, k-th train: 1, train loss: 0.7720, test_loss: 0.7816, lambda: 1.06081835514e-09, accuracy: 0.79088
Degree: 5, k-th train: 2, train loss: 0.7733, test_loss: 0.8836, lambda: 1.06081835514e-09, accuracy: 0.791968
Degree: 5, k-th train: 3, train loss: 0.7732, test_loss: 0.8797, lambda: 1.06081835514e-09, accuracy: 0.79152
Degree: 5, k-th train: 0, train loss: 0.7732, test_loss: 0.7985, lambda: 1.91448197617e-09, accuracy: 0.794848
Degree: 5, k-th train: 1, train loss: 0.7728, test_loss: 0.7824, lambda: 1.91448197617e-09, accuracy: 0.79024
Degree: 5, k-th train: 2, train loss: 0.7741, test_loss: 0.8842, lambda: 1.91448197617e-09, accuracy: 0.791744
Degree: 5, k-th train: 3, train loss: 0.7739, test_loss: 0.8945, lambda: 1.91448197617e-09, accuracy: 0.790624
Degree: 5, k-th train: 0, train loss: 0.7738, test_loss: 0.7990, lambda: 3.45510729459e-09, accuracy: 0.794528
Degree: 5, k-th train: 1, train loss: 0.7734, test_loss: 0.7831, lambda: 3.45510729459e-09, accuracy: 0.79008
Degree: 5, k-th train: 2, train loss: 0.7746, test_loss: 0.8844, lambda: 3.45510729459e-09, accuracy: 0.791296
Degree: 5, k-th train: 3, train loss: 0.7745, test_loss: 0.9045, lambda: 3.45510729459e-09, accuracy: 0.790528
Degree: 5, k-th train: 0, train loss: 0.7742, test_loss: 0.7994, lambda: 6.23550734127e-09, accuracy: 0.794464
Degree: 5, k-th train: 1, train loss: 0.7738, test_loss: 0.7835, lambda: 6.23550734127e-09, accuracy: 0.789568
Degree: 5, k-th train: 2, train loss: 0.7750, test_loss: 0.8843, lambda: 6.23550734127e-09, accuracy: 0.791168
Degree: 5, k-th train: 3, train loss: 0.7748, test_loss: 0.9106, lambda: 6.23550734127e-09, accuracy: 0.789856
Degree: 5, k-th train: 0, train loss: 0.7745, test_loss: 0.7996, lambda: 1.1253355826e-08, accuracy: 0.794208
Degree: 5, k-th train: 1, train loss: 0.7741, test_loss: 0.7837, lambda: 1.1253355826e-08, accuracy: 0.789376
Degree: 5, k-th train: 2, train loss: 0.7752, test_loss: 0.8840, lambda: 1.1253355826e-08, accuracy: 0.790656
Degree: 5, k-th train: 3, train loss: 0.7751, test_loss: 0.9138, lambda: 1.1253355826e-08, accuracy: 0.789856
Degree: 5, k-th train: 0, train loss: 0.7746, test_loss: 0.7997, lambda: 2.0309176209e-08, accuracy: 0.794112
Degree: 5, k-th train: 1, train loss: 0.7742, test_loss: 0.7839, lambda: 2.0309176209e-08, accuracy: 0.789248
Degree: 5, k-th train: 2, train loss: 0.7754, test_loss: 0.8837, lambda: 2.0309176209e-08, accuracy: 0.790592
Degree: 5, k-th train: 3, train loss: 0.7752, test_loss: 0.9147, lambda: 2.0309176209e-08, accuracy: 0.789888
Degree: 5, k-th train: 0, train loss: 0.7747, test_loss: 0.7997, lambda: 3.66524123708e-08, accuracy: 0.794144
Degree: 5, k-th train: 1, train loss: 0.7743, test_loss: 0.7840, lambda: 3.66524123708e-08, accuracy: 0.789056
Degree: 5, k-th train: 2, train loss: 0.7755, test_loss: 0.8834, lambda: 3.66524123708e-08, accuracy: 0.790592
Degree: 5, k-th train: 3, train loss: 0.7753, test_loss: 0.9137, lambda: 3.66524123708e-08, accuracy: 0.790048
Degree: 5, k-th train: 0, train loss: 0.7748, test_loss: 0.7996, lambda: 6.61474064123e-08, accuracy: 0.794112
Degree: 5, k-th train: 1, train loss: 0.7744, test_loss: 0.7841, lambda: 6.61474064123e-08, accuracy: 0.78912
Degree: 5, k-th train: 2, train loss: 0.7756, test_loss: 0.8832, lambda: 6.61474064123e-08, accuracy: 0.790624
Degree: 5, k-th train: 3, train loss: 0.7754, test_loss: 0.9112, lambda: 6.61474064123e-08, accuracy: 0.790048
Degree: 5, k-th train: 0, train loss: 0.7749, test_loss: 0.7993, lambda: 1.19377664171e-07, accuracy: 0.79424
Degree: 5, k-th train: 1, train loss: 0.7745, test_loss: 0.7841, lambda: 1.19377664171e-07, accuracy: 0.788992
Degree: 5, k-th train: 2, train loss: 0.7756, test_loss: 0.8831, lambda: 1.19377664171e-07, accuracy: 0.790656
Degree: 5, k-th train: 3, train loss: 0.7755, test_loss: 0.9077, lambda: 1.19377664171e-07, accuracy: 0.790016
Degree: 5, k-th train: 0, train loss: 0.7750, test_loss: 0.7989, lambda: 2.15443469003e-07, accuracy: 0.79408
Degree: 5, k-th train: 1, train loss: 0.7745, test_loss: 0.7841, lambda: 2.15443469003e-07, accuracy: 0.788832
Degree: 5, k-th train: 2, train loss: 0.7757, test_loss: 0.8832, lambda: 2.15443469003e-07, accuracy: 0.790624
Degree: 5, k-th train: 3, train loss: 0.7755, test_loss: 0.9041, lambda: 2.15443469003e-07, accuracy: 0.789792
Degree: 5, k-th train: 0, train loss: 0.7750, test_loss: 0.7980, lambda: 3.88815518031e-07, accuracy: 0.79408
Degree: 5, k-th train: 1, train loss: 0.7746, test_loss: 0.7841, lambda: 3.88815518031e-07, accuracy: 0.788608
Degree: 5, k-th train: 2, train loss: 0.7757, test_loss: 0.8835, lambda: 3.88815518031e-07, accuracy: 0.790304
Degree: 5, k-th train: 3, train loss: 0.7755, test_loss: 0.9012, lambda: 3.88815518031e-07, accuracy: 0.789728
Degree: 5, k-th train: 0, train loss: 0.7751, test_loss: 0.7966, lambda: 7.0170382867e-07, accuracy: 0.79408
Degree: 5, k-th train: 1, train loss: 0.7746, test_loss: 0.7842, lambda: 7.0170382867e-07, accuracy: 0.788384
Degree: 5, k-th train: 2, train loss: 0.7757, test_loss: 0.8841, lambda: 7.0170382867e-07, accuracy: 0.789824
Degree: 5, k-th train: 3, train loss: 0.7756, test_loss: 0.8997, lambda: 7.0170382867e-07, accuracy: 0.789568
Degree: 5, k-th train: 0, train loss: 0.7751, test_loss: 0.7942, lambda: 1.26638017347e-06, accuracy: 0.793824
Degree: 5, k-th train: 1, train loss: 0.7747, test_loss: 0.7844, lambda: 1.26638017347e-06, accuracy: 0.788448
Degree: 5, k-th train: 2, train loss: 0.7758, test_loss: 0.8852, lambda: 1.26638017347e-06, accuracy: 0.789888
Degree: 5, k-th train: 3, train loss: 0.7756, test_loss: 0.8997, lambda: 1.26638017347e-06, accuracy: 0.789696
Degree: 5, k-th train: 0, train loss: 0.7753, test_loss: 0.7908, lambda: 2.28546386413e-06, accuracy: 0.793504
Degree: 5, k-th train: 1, train loss: 0.7748, test_loss: 0.7846, lambda: 2.28546386413e-06, accuracy: 0.788576
Degree: 5, k-th train: 2, train loss: 0.7759, test_loss: 0.8871, lambda: 2.28546386413e-06, accuracy: 0.789696
Degree: 5, k-th train: 3, train loss: 0.7757, test_loss: 0.9014, lambda: 2.28546386413e-06, accuracy: 0.789152
Degree: 5, k-th train: 0, train loss: 0.7757, test_loss: 0.7863, lambda: 4.1246263829e-06, accuracy: 0.792768
Degree: 5, k-th train: 1, train loss: 0.7750, test_loss: 0.7851, lambda: 4.1246263829e-06, accuracy: 0.788384
Degree: 5, k-th train: 2, train loss: 0.7761, test_loss: 0.8901, lambda: 4.1246263829e-06, accuracy: 0.789632
Degree: 5, k-th train: 3, train loss: 0.7759, test_loss: 0.9046, lambda: 4.1246263829e-06, accuracy: 0.7888
Degree: 5, k-th train: 0, train loss: 0.7764, test_loss: 0.7816, lambda: 7.44380301325e-06, accuracy: 0.79216
Degree: 5, k-th train: 1, train loss: 0.7754, test_loss: 0.7859, lambda: 7.44380301325e-06, accuracy: 0.787904
Degree: 5, k-th train: 2, train loss: 0.7765, test_loss: 0.8948, lambda: 7.44380301325e-06, accuracy: 0.788928
Degree: 5, k-th train: 3, train loss: 0.7763, test_loss: 0.9090, lambda: 7.44380301325e-06, accuracy: 0.788352
Degree: 5, k-th train: 0, train loss: 0.7775, test_loss: 0.7784, lambda: 1.3433993326e-05, accuracy: 0.790976
Degree: 5, k-th train: 1, train loss: 0.7760, test_loss: 0.7871, lambda: 1.3433993326e-05, accuracy: 0.787072
Degree: 5, k-th train: 2, train loss: 0.7772, test_loss: 0.9011, lambda: 1.3433993326e-05, accuracy: 0.787552
Degree: 5, k-th train: 3, train loss: 0.7770, test_loss: 0.9144, lambda: 1.3433993326e-05, accuracy: 0.78688
Degree: 5, k-th train: 0, train loss: 0.7790, test_loss: 0.7775, lambda: 2.42446201708e-05, accuracy: 0.789472
Degree: 5, k-th train: 1, train loss: 0.7770, test_loss: 0.7886, lambda: 2.42446201708e-05, accuracy: 0.78608
Degree: 5, k-th train: 2, train loss: 0.7782, test_loss: 0.9086, lambda: 2.42446201708e-05, accuracy: 0.786624
Degree: 5, k-th train: 3, train loss: 0.7780, test_loss: 0.9204, lambda: 2.42446201708e-05, accuracy: 0.785824
Degree: 5, k-th train: 0, train loss: 0.7806, test_loss: 0.7786, lambda: 4.37547937507e-05, accuracy: 0.788672
Degree: 5, k-th train: 1, train loss: 0.7782, test_loss: 0.7904, lambda: 4.37547937507e-05, accuracy: 0.784736
Degree: 5, k-th train: 2, train loss: 0.7795, test_loss: 0.9159, lambda: 4.37547937507e-05, accuracy: 0.785664
Degree: 5, k-th train: 3, train loss: 0.7792, test_loss: 0.9265, lambda: 4.37547937507e-05, accuracy: 0.78512
Degree: 5, k-th train: 0, train loss: 0.7820, test_loss: 0.7802, lambda: 7.8965228685e-05, accuracy: 0.787552
Degree: 5, k-th train: 1, train loss: 0.7795, test_loss: 0.7920, lambda: 7.8965228685e-05, accuracy: 0.783072
Degree: 5, k-th train: 2, train loss: 0.7809, test_loss: 0.9213, lambda: 7.8965228685e-05, accuracy: 0.783712
Degree: 5, k-th train: 3, train loss: 0.7805, test_loss: 0.9320, lambda: 7.8965228685e-05, accuracy: 0.784064
Degree: 5, k-th train: 0, train loss: 0.7831, test_loss: 0.7814, lambda: 0.00014251026703, accuracy: 0.786496
Degree: 5, k-th train: 1, train loss: 0.7806, test_loss: 0.7932, lambda: 0.00014251026703, accuracy: 0.782368
Degree: 5, k-th train: 2, train loss: 0.7821, test_loss: 0.9237, lambda: 0.00014251026703, accuracy: 0.782912
Degree: 5, k-th train: 3, train loss: 0.7816, test_loss: 0.9360, lambda: 0.00014251026703, accuracy: 0.78336
Degree: 5, k-th train: 0, train loss: 0.7840, test_loss: 0.7821, lambda: 0.000257191380906, accuracy: 0.786176
Degree: 5, k-th train: 1, train loss: 0.7815, test_loss: 0.7940, lambda: 0.000257191380906, accuracy: 0.781504
Degree: 5, k-th train: 2, train loss: 0.7830, test_loss: 0.9228, lambda: 0.000257191380906, accuracy: 0.781632
Degree: 5, k-th train: 3, train loss: 0.7826, test_loss: 0.9387, lambda: 0.000257191380906, accuracy: 0.782688
Degree: 5, k-th train: 0, train loss: 0.7848, test_loss: 0.7826, lambda: 0.000464158883361, accuracy: 0.785248
Degree: 5, k-th train: 1, train loss: 0.7823, test_loss: 0.7945, lambda: 0.000464158883361, accuracy: 0.780672
Degree: 5, k-th train: 2, train loss: 0.7839, test_loss: 0.9188, lambda: 0.000464158883361, accuracy: 0.781184
Degree: 5, k-th train: 3, train loss: 0.7835, test_loss: 0.9407, lambda: 0.000464158883361, accuracy: 0.781856
Degree: 5, k-th train: 0, train loss: 0.7858, test_loss: 0.7833, lambda: 0.000837677640068, accuracy: 0.784672
Degree: 5, k-th train: 1, train loss: 0.7832, test_loss: 0.7948, lambda: 0.000837677640068, accuracy: 0.780032
Degree: 5, k-th train: 2, train loss: 0.7847, test_loss: 0.9125, lambda: 0.000837677640068, accuracy: 0.780032
Degree: 5, k-th train: 3, train loss: 0.7844, test_loss: 0.9429, lambda: 0.000837677640068, accuracy: 0.78096
Degree: 5, k-th train: 0, train loss: 0.7867, test_loss: 0.7841, lambda: 0.00151177507062, accuracy: 0.783808
Degree: 5, k-th train: 1, train loss: 0.7841, test_loss: 0.7952, lambda: 0.00151177507062, accuracy: 0.779584
Degree: 5, k-th train: 2, train loss: 0.7856, test_loss: 0.9047, lambda: 0.00151177507062, accuracy: 0.779584
Degree: 5, k-th train: 3, train loss: 0.7853, test_loss: 0.9458, lambda: 0.00151177507062, accuracy: 0.780608
Degree: 5, k-th train: 0, train loss: 0.7875, test_loss: 0.7851, lambda: 0.00272833337649, accuracy: 0.783072
Degree: 5, k-th train: 1, train loss: 0.7850, test_loss: 0.7955, lambda: 0.00272833337649, accuracy: 0.779008
Degree: 5, k-th train: 2, train loss: 0.7864, test_loss: 0.8958, lambda: 0.00272833337649, accuracy: 0.778656
Degree: 5, k-th train: 3, train loss: 0.7863, test_loss: 0.9485, lambda: 0.00272833337649, accuracy: 0.780224
Degree: 5, k-th train: 0, train loss: 0.7883, test_loss: 0.7859, lambda: 0.00492388263171, accuracy: 0.782816
Degree: 5, k-th train: 1, train loss: 0.7858, test_loss: 0.7957, lambda: 0.00492388263171, accuracy: 0.778912
Degree: 5, k-th train: 2, train loss: 0.7872, test_loss: 0.8858, lambda: 0.00492388263171, accuracy: 0.778464
Degree: 5, k-th train: 3, train loss: 0.7871, test_loss: 0.9495, lambda: 0.00492388263171, accuracy: 0.779552
Degree: 5, k-th train: 0, train loss: 0.7891, test_loss: 0.7867, lambda: 0.00888623816274, accuracy: 0.782912
Degree: 5, k-th train: 1, train loss: 0.7867, test_loss: 0.7959, lambda: 0.00888623816274, accuracy: 0.777888
Degree: 5, k-th train: 2, train loss: 0.7880, test_loss: 0.8751, lambda: 0.00888623816274, accuracy: 0.77808
Degree: 5, k-th train: 3, train loss: 0.7880, test_loss: 0.9462, lambda: 0.00888623816274, accuracy: 0.77856
Degree: 5, k-th train: 0, train loss: 0.7900, test_loss: 0.7876, lambda: 0.0160371874375, accuracy: 0.781728
Degree: 5, k-th train: 1, train loss: 0.7877, test_loss: 0.7961, lambda: 0.0160371874375, accuracy: 0.777472
Degree: 5, k-th train: 2, train loss: 0.7889, test_loss: 0.8642, lambda: 0.0160371874375, accuracy: 0.777216
Degree: 5, k-th train: 3, train loss: 0.7889, test_loss: 0.9355, lambda: 0.0160371874375, accuracy: 0.77776
Degree: 5, k-th train: 0, train loss: 0.7912, test_loss: 0.7887, lambda: 0.0289426612472, accuracy: 0.779968
Degree: 5, k-th train: 1, train loss: 0.7889, test_loss: 0.7966, lambda: 0.0289426612472, accuracy: 0.776512
Degree: 5, k-th train: 2, train loss: 0.7901, test_loss: 0.8537, lambda: 0.0289426612472, accuracy: 0.7752
Degree: 5, k-th train: 3, train loss: 0.7900, test_loss: 0.9149, lambda: 0.0289426612472, accuracy: 0.776
Degree: 5, k-th train: 0, train loss: 0.7931, test_loss: 0.7905, lambda: 0.0522334507427, accuracy: 0.778272
Degree: 5, k-th train: 1, train loss: 0.7909, test_loss: 0.7977, lambda: 0.0522334507427, accuracy: 0.773664
Degree: 5, k-th train: 2, train loss: 0.7921, test_loss: 0.8446, lambda: 0.0522334507427, accuracy: 0.773696
Degree: 5, k-th train: 3, train loss: 0.7919, test_loss: 0.8846, lambda: 0.0522334507427, accuracy: 0.772736
Degree: 5, k-th train: 0, train loss: 0.7964, test_loss: 0.7937, lambda: 0.0942668455118, accuracy: 0.77568
Degree: 5, k-th train: 1, train loss: 0.7943, test_loss: 0.8002, lambda: 0.0942668455118, accuracy: 0.770368
Degree: 5, k-th train: 2, train loss: 0.7955, test_loss: 0.8390, lambda: 0.0942668455118, accuracy: 0.771872
Degree: 5, k-th train: 3, train loss: 0.7953, test_loss: 0.8505, lambda: 0.0942668455118, accuracy: 0.769088
Degree: 5, k-th train: 0, train loss: 0.8021, test_loss: 0.7993, lambda: 0.170125427985, accuracy: 0.771392
Degree: 5, k-th train: 1, train loss: 0.8001, test_loss: 0.8052, lambda: 0.170125427985, accuracy: 0.76528
Degree: 5, k-th train: 2, train loss: 0.8012, test_loss: 0.8403, lambda: 0.170125427985, accuracy: 0.767424
Degree: 5, k-th train: 3, train loss: 0.8010, test_loss: 0.8235, lambda: 0.170125427985, accuracy: 0.764416
Degree: 5, k-th train: 0, train loss: 0.8108, test_loss: 0.8081, lambda: 0.307029062976, accuracy: 0.765344
Degree: 5, k-th train: 1, train loss: 0.8089, test_loss: 0.8135, lambda: 0.307029062976, accuracy: 0.757408
Degree: 5, k-th train: 2, train loss: 0.8099, test_loss: 0.8516, lambda: 0.307029062976, accuracy: 0.761184
Degree: 5, k-th train: 3, train loss: 0.8098, test_loss: 0.8142, lambda: 0.307029062976, accuracy: 0.756032
Degree: 5, k-th train: 0, train loss: 0.8222, test_loss: 0.8198, lambda: 0.554102033001, accuracy: 0.756736
Degree: 5, k-th train: 1, train loss: 0.8204, test_loss: 0.8247, lambda: 0.554102033001, accuracy: 0.747872
Degree: 5, k-th train: 2, train loss: 0.8212, test_loss: 0.8722, lambda: 0.554102033001, accuracy: 0.751008
Degree: 5, k-th train: 3, train loss: 0.8211, test_loss: 0.8239, lambda: 0.554102033001, accuracy: 0.746624
Degree: 5, k-th train: 0, train loss: 0.8345, test_loss: 0.8326, lambda: 1.0, accuracy: 0.748544
Degree: 5, k-th train: 1, train loss: 0.8329, test_loss: 0.8373, lambda: 1.0, accuracy: 0.738912
Degree: 5, k-th train: 2, train loss: 0.8335, test_loss: 0.8960, lambda: 1.0, accuracy: 0.744928
Degree: 5, k-th train: 3, train loss: 0.8334, test_loss: 0.8442, lambda: 1.0, accuracy: 0.740448
-------------Degree: 5, test_loss: 0.8148, lambda: 1e-10, best_accuracy: 0.795272-----------------
*********Best Degree: 5, best_test_loss: 0.8148, Best_lambda: 1e-10, best accuracy: 0.795272***********
Train loss: 0.7706, test_loss: 1934.6180, accuracy of Ridge Regression is:  0.7957

 Testing logicistic regression test
Train loss: 62249.8776, test_loss: 62171.1574, accuracy of Logistic Regression is:  0.7506

 Testing reg logicistic regression test
Train loss: 62250.2159, test_loss: 62175.0789, accuracy of Regularized Logistic Regression is:  0.7506

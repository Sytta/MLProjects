{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from implementation import *\n",
    "#from proj1_helpers import *\n",
    "#from cross_validation import *\n",
    "from data_preprocessing import *\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "TRAIN_PATH = \"./data/train.csv\"\n",
    "TEST_PATH = \"./data/test.csv\"\n",
    "USE_COLS = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 28, 29, 31)\n",
    "\n",
    "def make_predictions(weight, degree, name):\n",
    "    y_test, x_test, ids_test = load_csv_data(\"./data/test.csv\", sub_sample=False)\n",
    "    x_test, mean_x_test, std_x_test = standardize(x_test)\n",
    "    tx_poly = build_poly(x_test, degree)\n",
    "    y_pred = predict_labels(weight, tx_poly)\n",
    "    create_csv_submission(ids_test, y_pred,  name +\".csv\")\n",
    "\n",
    "\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w, mse=True):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    if mse:\n",
    "        return calculate_mse(e)\n",
    "    else:\n",
    "        return calculate_mae(e)\n",
    "    \n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    a = tx.T.dot(tx) + lambda_ * 2 * len(y) * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y)\n",
    "    w_ridge = np.linalg.solve(a, b)\n",
    "    return w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False, cut_values=True):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    if (cut_values):\n",
    "        print('load_csv_data : dropping uniform distribution values')\n",
    "        # drop Pri_tau_phi(17), Pri_lep_phi(20), Pri_met_phi(22), Pri_jet_leading_Phi(27), Pri_jet_subleading_phi(30)\n",
    "        # because of uniform distribution\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, \\\n",
    "            usecols=USE_COLS)\n",
    "    else:\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "        \n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('Split_data_according_to_jet')\n",
    "    print('Loading files...')\n",
    "    y_tr, x_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "    y_te, x_te, ids_te = load_csv_data(TEST_PATH)\n",
    "    return y_tr, x_tr, ids_tr, y_te, x_te, ids_te\n",
    "\n",
    "def load_headers():\n",
    "    \"\"\"Load all the headers from the training file and drop the unnecessary ones\"\"\"\n",
    "    with open(TRAIN_PATH) as train_file:\n",
    "        reader = csv.reader(train_file)\n",
    "        headers = next(reader)\n",
    "    \n",
    "    # Only use the columns in USE_COLS\n",
    "    headers = [headers[i] for i in USE_COLS]\n",
    "    # drop ID and Predictions cols\n",
    "    headers = headers[2:]\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_data_according_to_jet\n",
      "Loading files...\n",
      "load_csv_data : dropping uniform distribution values\n",
      "load_csv_data : dropping uniform distribution values\n"
     ]
    }
   ],
   "source": [
    "y_tr, x_tr, ids_tr, y_te, x_te, ids_te = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DER_mass_MMC',\n",
       " 'DER_mass_transverse_met_lep',\n",
       " 'DER_mass_vis',\n",
       " 'DER_pt_h',\n",
       " 'DER_deltaeta_jet_jet',\n",
       " 'DER_mass_jet_jet',\n",
       " 'DER_prodeta_jet_jet',\n",
       " 'DER_deltar_tau_lep',\n",
       " 'DER_pt_tot',\n",
       " 'DER_sum_pt',\n",
       " 'DER_pt_ratio_lep_tau',\n",
       " 'DER_met_phi_centrality',\n",
       " 'DER_lep_eta_centrality',\n",
       " 'PRI_tau_pt',\n",
       " 'PRI_tau_eta',\n",
       " 'PRI_lep_pt',\n",
       " 'PRI_lep_eta',\n",
       " 'PRI_met',\n",
       " 'PRI_met_sumet',\n",
       " 'PRI_jet_num',\n",
       " 'PRI_jet_leading_pt',\n",
       " 'PRI_jet_leading_eta',\n",
       " 'PRI_jet_subleading_pt',\n",
       " 'PRI_jet_subleading_eta',\n",
       " 'PRI_jet_all_pt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = load_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_in_DER_pt_h(y_tr, x_tr, ids_tr, jet):\n",
    "    print(\"Removing outliers in Der_pt_h\")\n",
    "    # Remove the outliers in DER_pt_h (col 3):\n",
    "    #  JET 0: 2834.999 when the max value is 117.707 outside of outlier - threshold to 120 \n",
    "    #  JET 2: 1053.807 when max value is 734 outside of outlier- Threshold to 800\n",
    "    OUTLIERS = [120, 999, 800, 999]\n",
    "    outlier = OUTLIERS[jet]\n",
    "    tr_smaller_than_outlier = (x_tr[:, 3] < outlier)\n",
    "    x_tr = x_tr[tr_smaller_than_outlier]\n",
    "    y_tr = y_tr[tr_smaller_than_outlier]\n",
    "    ids_tr = ids_tr[tr_smaller_than_outlier]\n",
    "    return y_tr, x_tr, ids_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_NAN_columns(x_tr, x_te, headers_jet):\n",
    "    print(\"Removing nan columns\")\n",
    "    nan_cols = []\n",
    "    # Find all columns with -999\n",
    "    for col_idx in range(x_tr.shape[1]):\n",
    "        col = x_tr[:, col_idx]\n",
    "        nb_nan_in_col = len(x_tr[col == -999])\n",
    "        # A column has all NaN if len of col = nb NaN values in col\n",
    "        if (nb_nan_in_col == len(col)):\n",
    "            nan_cols.append(col_idx)\n",
    "    \n",
    "    # Remove all nan columns\n",
    "    x_tr_updated = np.delete(x_tr, nan_cols, axis=1)\n",
    "    x_te_updated = np.delete(x_te, nan_cols, axis=1)\n",
    "    headers_jet_updated = np.delete(headers_jet, nan_cols)\n",
    "    \n",
    "    return x_tr_updated, x_te_updated, headers_jet_updated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_mass(x, y, ids):\n",
    "        # Get all the rows idx with invalid mass (i.e. DER_mass_MMC = -999)\n",
    "        invalid_mass_row_idx = x[:, 0] == -999\n",
    "        valid_mass_row_idx = x[:, 0] > 0\n",
    "        # Process for each data table\n",
    "        x_invalid_mass = x[invalid_mass_row_idx]\n",
    "        x_valid_mass = x[valid_mass_row_idx]\n",
    "        y_invalid_mass = y[invalid_mass_row_idx]\n",
    "        y_valid_mass = y[valid_mass_row_idx]\n",
    "        ids_invalid_mass = ids[invalid_mass_row_idx]\n",
    "        ids_valid_mass = ids[valid_mass_row_idx]\n",
    "        \n",
    "        return x_invalid_mass, x_valid_mass, y_invalid_mass, y_valid_mass, ids_invalid_mass, ids_valid_mass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_csv(x, y, ids, headers, jet, isTrain, isMassValid):\n",
    "    \"\"\"\n",
    "    Write data into new csv file\n",
    "    \"\"\"\n",
    "    # Add 'Id' & 'Prediction' to headers\n",
    "    headers = np.insert(headers, 0, ['Id', 'Prediction'])\n",
    "    \n",
    "    # Remove 'DER_mass_MMC' if mass is not valid\n",
    "    if not isMassValid:\n",
    "        headers = np.delete(headers, np.where(headers =='DER_mass_MMC'))\n",
    "    \n",
    "    # Generate file name\n",
    "    base = './data/train_' if isTrain else './data/test_'\n",
    "    valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "    file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "    \n",
    "    print(\"Outputing {}\".format(file_name))\n",
    "        \n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        data_ = dict.fromkeys(headers)\n",
    "        # Transform -1 and 1 into 's' and 'b'\n",
    "        for id_, y_, x_ in zip(ids, y, x):\n",
    "            data_['Id'] = int(id_)\n",
    "            if (y_ != -1 and y_ !=1):\n",
    "                raise Exception('Prediction not -1 and 1!!!')\n",
    "            data_['Prediction'] = 's' if y_ == 1 else 'b'\n",
    "            \n",
    "            for idx, x_value in enumerate(x_):\n",
    "                data_[headers[idx + 2]] = float(x_value)\n",
    "            writer.writerow(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers): \n",
    "    for jet in range(4):\n",
    "        print(\"\\n\\nSplitting for jet {}\".format(jet))\n",
    "        print(x_te.shape)\n",
    "\n",
    "        # PRI_jet_num (24 -> 24 - 3 cols dropped before 24 - 2 cols (id, label) = col 19)\n",
    "        col_jet = 19\n",
    "        \n",
    "        # TRAIN - Get all the rows having Pri_jet_num = jet for TRAINING set and delete PRI_jet_num col\n",
    "        x_tr_jet = x_tr[x_tr[:, col_jet] == jet]\n",
    "        x_tr_jet = np.delete(x_tr_jet, col_jet, axis=1)\n",
    "        # Delete PRI_jet_num in headers\n",
    "        headers_jet = np.delete(headers, col_jet)\n",
    "\n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_tr_jet = y_tr[x_tr[:, col_jet] == jet]\n",
    "        ids_tr_jet = ids_tr[x_tr[:, col_jet] == jet]\n",
    "        \n",
    "        # TEST - Get all the rows having Pri_jet_num = jet for TEST set and delete PRI_jet_num col\n",
    "        x_te_jet = x_te[x_te[:, col_jet] == jet]\n",
    "        x_te_jet = np.delete(x_te_jet, col_jet, axis=1)\n",
    "        print(x_te_jet.shape)\n",
    "                \n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_te_jet = y_te[x_te[:, col_jet] == jet]\n",
    "        ids_te_jet = ids_te[x_te[:, col_jet] == jet]\n",
    "            \n",
    "        # Remove outliers\n",
    "        y_tr_jet, x_tr_jet, ids_tr_jet = remove_outlier_in_DER_pt_h(y_tr_jet, x_tr_jet, ids_tr_jet, jet)\n",
    "\n",
    "        # Remove col PRI_jet_all_pt from x because it only contains 0 values\n",
    "        if jet == 0:\n",
    "            print(\"Deleted col Pri_jet_all_pt in set with jet_num = 0\")\n",
    "            x_tr_jet = np.delete(x_tr_jet, -1, axis=1)\n",
    "            x_te_jet = np.delete(x_te_jet, -1, axis=1)\n",
    "            print(x_te_jet.shape)\n",
    "            headers_jet = np.delete(headers_jet, -1)\n",
    "        \n",
    "        # Remove all the columns with only NaN values\n",
    "        x_tr_jet, x_te_jet, headers_jet = remove_all_NAN_columns(x_tr_jet, x_te_jet, headers_jet)\n",
    "\n",
    "        # Split the dataset again into valid/invalid values of DER_mass_MMC \n",
    "        # TRAIN\n",
    "        x_tr_jet_invalid_mass, x_tr_jet_valid_mass, y_tr_jet_invalid_mass, y_tr_jet_valid_mass, ids_tr_jet_invalid_mass, ids_tr_jet_valid_mass = split_data_according_to_mass(x_tr_jet, y_tr_jet, ids_tr_jet)\n",
    "        # TEST\n",
    "        x_te_jet_invalid_mass, x_te_jet_valid_mass, y_te_jet_invalid_mass, y_te_jet_valid_mass, ids_te_jet_invalid_mass, ids_te_jet_valid_mass = split_data_according_to_mass(x_te_jet, y_te_jet, ids_te_jet)\n",
    "        print(\"valid, invalid\", x_te_jet_invalid_mass.shape, x_te_jet_valid_mass.shape)\n",
    "        \n",
    "        # Remove 'DER_mass_MMC' (col 0) if the mass is not valid\n",
    "        x_tr_jet_invalid_mass = np.delete(x_tr_jet_invalid_mass, 0, axis=1)\n",
    "        x_te_jet_invalid_mass = np.delete(x_te_jet_invalid_mass, 0, axis=1)\n",
    "        \n",
    "        print(\"remove dermassmmc\", x_te_jet_invalid_mass.shape)\n",
    "\n",
    "        \n",
    "        # Save into CSV\n",
    "        #x, y, ids, headers, jet, isTrain, isMassValid\n",
    "        # TRAIN\n",
    "        output_to_csv(x_tr_jet_invalid_mass, y_tr_jet_invalid_mass, ids_tr_jet_invalid_mass, headers_jet, jet, True, False)\n",
    "        output_to_csv(x_tr_jet_valid_mass, y_tr_jet_valid_mass, ids_tr_jet_valid_mass, headers_jet, jet, True, True)\n",
    "\n",
    "        # TEST\n",
    "        output_to_csv(x_te_jet_invalid_mass, y_te_jet_invalid_mass, ids_te_jet_invalid_mass, headers_jet, jet, False, False)\n",
    "        output_to_csv(x_te_jet_valid_mass, y_te_jet_valid_mass, ids_te_jet_valid_mass, headers_jet, jet, False, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Splitting for jet 0\n",
      "(568238, 25)\n",
      "(227458, 24)\n",
      "Removing outliers in Der_pt_h\n",
      "Deleted col Pri_jet_all_pt in set with jet_num = 0\n",
      "(227458, 23)\n",
      "Removing nan columns\n",
      "valid, invalid (59263, 15) (168195, 15)\n",
      "remove dermassmmc (59263, 14)\n",
      "Outputing ./data/train_jet_0_invalid_mass.csv\n",
      "Outputing ./data/train_jet_0_valid_mass.csv\n",
      "Outputing ./data/test_jet_0_invalid_mass.csv\n",
      "Outputing ./data/test_jet_0_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 1\n",
      "(568238, 25)\n",
      "(175338, 24)\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "valid, invalid (17243, 18) (158095, 18)\n",
      "remove dermassmmc (17243, 17)\n",
      "Outputing ./data/train_jet_1_invalid_mass.csv\n",
      "Outputing ./data/train_jet_1_valid_mass.csv\n",
      "Outputing ./data/test_jet_1_invalid_mass.csv\n",
      "Outputing ./data/test_jet_1_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 2\n",
      "(568238, 25)\n",
      "(114648, 24)\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "valid, invalid (6743, 24) (107905, 24)\n",
      "remove dermassmmc (6743, 23)\n",
      "Outputing ./data/train_jet_2_invalid_mass.csv\n",
      "Outputing ./data/train_jet_2_valid_mass.csv\n",
      "Outputing ./data/test_jet_2_invalid_mass.csv\n",
      "Outputing ./data/test_jet_2_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 3\n",
      "(568238, 25)\n",
      "(50794, 24)\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "valid, invalid (3239, 24) (47555, 24)\n",
      "remove dermassmmc (3239, 23)\n",
      "Outputing ./data/train_jet_3_invalid_mass.csv\n",
      "Outputing ./data/train_jet_3_valid_mass.csv\n",
      "Outputing ./data/test_jet_3_invalid_mass.csv\n",
      "Outputing ./data/test_jet_3_valid_mass.csv\n"
     ]
    }
   ],
   "source": [
    "split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_jet_0_valid_mass.csv',\n",
       " './data/train_jet_1_valid_mass.csv',\n",
       " './data/train_jet_2_valid_mass.csv',\n",
       " './data/train_jet_3_valid_mass.csv',\n",
       " './data/train_jet_0_invalid_mass.csv',\n",
       " './data/train_jet_1_invalid_mass.csv',\n",
       " './data/train_jet_2_invalid_mass.csv',\n",
       " './data/train_jet_3_invalid_mass.csv']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the real files\n",
    "def generate_processed_filenames(isTrain):\n",
    "    file_names = []\n",
    "    isMassValids = [True, False]\n",
    "    jets = range(4)\n",
    "    \n",
    "    for isMassValid in isMassValids:\n",
    "        for jet in jets:\n",
    "            # Generate file name\n",
    "            base = './data/train_' if isTrain else './data/test_'\n",
    "            valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "            file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "            file_names.append(file_name)\n",
    "                \n",
    "    return file_names\n",
    "\n",
    "file_names = generate_processed_filenames(True)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73789, 17)\n",
      "(69982, 20)\n",
      "(47426, 26)\n",
      "(20687, 26)\n",
      "(26123, 16)\n",
      "(7562, 19)\n",
      "(2952, 25)\n",
      "(1477, 25)\n",
      "249998\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for f in file_names:\n",
    "    df = pd.read_csv(f)\n",
    "    print(df.shape)\n",
    "    sum += df.shape[0]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(isTrain):\n",
    "    \"\"\"Load all Train/Test processed data\"\"\"\n",
    "    file_names = generate_processed_filenames(isTrain)\n",
    "    print(file_names)\n",
    "    ys = []\n",
    "    xs = []\n",
    "    ids = []\n",
    "\n",
    "    for i in range(len(file_names)):\n",
    "        y, x, id_ = load_csv_data(file_names[i], cut_values = False)\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "        ids.append(id_)\n",
    "        \n",
    "    return ys, xs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/train_jet_0_valid_mass.csv', './data/train_jet_1_valid_mass.csv', './data/train_jet_2_valid_mass.csv', './data/train_jet_3_valid_mass.csv', './data/train_jet_0_invalid_mass.csv', './data/train_jet_1_invalid_mass.csv', './data/train_jet_2_invalid_mass.csv', './data/train_jet_3_invalid_mass.csv']\n"
     ]
    }
   ],
   "source": [
    "ys_train, xs_train, ids_train = load_processed_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean=None, std=None):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    if mean is None or std is None:\n",
    "        mean_x = np.mean(x, axis = 0)\n",
    "        std_x = np.std(x, axis = 0)\n",
    "    else:\n",
    "        mean_x = mean\n",
    "        std_x = std\n",
    "        \n",
    "    x = x - mean_x\n",
    "    x = x / std_x \n",
    "\n",
    "    return x, mean_x, std_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standandize xs\n",
    "x_means = []\n",
    "x_stds = []\n",
    "x_standardized = []\n",
    "\n",
    "for x_ in xs_train:\n",
    "    x, mean_x, std_x = standardize(x_)\n",
    "    x_standardized.append(x)\n",
    "    x_means.append(mean_x)\n",
    "    x_stds.append(std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a weights dictionary for each jet\n",
    "weights_jet = dict.fromkeys(file_names)\n",
    "degrees_jet = dict.fromkeys(file_names)\n",
    "lambdas_jet = dict.fromkeys(file_names)\n",
    "\n",
    "# build w using ridge regression\n",
    "k_fold = 10\n",
    "degrees = np.arange(5, 15)\n",
    "lambdas = np.logspace(-20, -3, 100)\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_jet_0_valid_mass.csv',\n",
       " './data/train_jet_1_valid_mass.csv',\n",
       " './data/train_jet_2_valid_mass.csv',\n",
       " './data/train_jet_3_valid_mass.csv',\n",
       " './data/train_jet_0_invalid_mass.csv',\n",
       " './data/train_jet_1_invalid_mass.csv',\n",
       " './data/train_jet_2_invalid_mass.csv',\n",
       " './data/train_jet_3_invalid_mass.csv']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-20, 1.48496826e-20, 2.20513074e-20, 3.27454916e-20,\n",
       "       4.86260158e-20, 7.22080902e-20, 1.07226722e-19, 1.59228279e-19,\n",
       "       2.36448941e-19, 3.51119173e-19, 5.21400829e-19, 7.74263683e-19,\n",
       "       1.14975700e-18, 1.70735265e-18, 2.53536449e-18, 3.76493581e-18,\n",
       "       5.59081018e-18, 8.30217568e-18, 1.23284674e-17, 1.83073828e-17,\n",
       "       2.71858824e-17, 4.03701726e-17, 5.99484250e-17, 8.90215085e-17,\n",
       "       1.32194115e-16, 1.96304065e-16, 2.91505306e-16, 4.32876128e-16,\n",
       "       6.42807312e-16, 9.54548457e-16, 1.41747416e-15, 2.10490414e-15,\n",
       "       3.12571585e-15, 4.64158883e-15, 6.89261210e-15, 1.02353102e-14,\n",
       "       1.51991108e-14, 2.25701972e-14, 3.35160265e-14, 4.97702356e-14,\n",
       "       7.39072203e-14, 1.09749877e-13, 1.62975083e-13, 2.42012826e-13,\n",
       "       3.59381366e-13, 5.33669923e-13, 7.92482898e-13, 1.17681195e-12,\n",
       "       1.74752840e-12, 2.59502421e-12, 3.85352859e-12, 5.72236766e-12,\n",
       "       8.49753436e-12, 1.26185688e-11, 1.87381742e-11, 2.78255940e-11,\n",
       "       4.13201240e-11, 6.13590727e-11, 9.11162756e-11, 1.35304777e-10,\n",
       "       2.00923300e-10, 2.98364724e-10, 4.43062146e-10, 6.57933225e-10,\n",
       "       9.77009957e-10, 1.45082878e-09, 2.15443469e-09, 3.19926714e-09,\n",
       "       4.75081016e-09, 7.05480231e-09, 1.04761575e-08, 1.55567614e-08,\n",
       "       2.31012970e-08, 3.43046929e-08, 5.09413801e-08, 7.56463328e-08,\n",
       "       1.12332403e-07, 1.66810054e-07, 2.47707636e-07, 3.67837977e-07,\n",
       "       5.46227722e-07, 8.11130831e-07, 1.20450354e-06, 1.78864953e-06,\n",
       "       2.65608778e-06, 3.94420606e-06, 5.85702082e-06, 8.69749003e-06,\n",
       "       1.29154967e-05, 1.91791026e-05, 2.84803587e-05, 4.22924287e-05,\n",
       "       6.28029144e-05, 9.32603347e-05, 1.38488637e-04, 2.05651231e-04,\n",
       "       3.05385551e-04, 4.53487851e-04, 6.73415066e-04, 1.00000000e-03])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    # ridge regression\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "#     loss_tr = np.sqrt(2 * compute_loss(y_tr, tx_tr, w))\n",
    "#     loss_te = np.sqrt(2 * compute_loss(y_te, tx_te, w))\n",
    "    loss_tr = perc_wrong_pred(y_tr, tx_tr, w)\n",
    "    loss_te = perc_wrong_pred(y_te, tx_te, w)\n",
    "    return loss_tr, loss_te, w\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_wrong_pred(y, tx, w_star):\n",
    "    \"\"\"\n",
    "        Return the percentage of wrong predictions (between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    pred = np.dot(tx, w_star)\n",
    "\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred <= 0] = -1\n",
    "\n",
    "    right = np.sum(pred == y)\n",
    "    wrong = len(pred) - right\n",
    "\n",
    "    return float(wrong) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ./data/train_jet_0_valid_mass.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train with ridge regression for each tx in x and save the weights\n",
    "for tx, y, f in zip(x_standardized, ys_train, file_names):\n",
    "    print(\"Training for {}\".format(f))\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # for each degree, we compute the best lambdas and the associated rmse\n",
    "    best_lambdas = []\n",
    "    best_rmses = []\n",
    "    best_w = []\n",
    "    # vary degree\n",
    "    for degree in degrees:\n",
    "        # cross validation\n",
    "        rmse_te = []\n",
    "        w_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            rmse_te_tmp = []\n",
    "            w_te_temp = []\n",
    "            for k in range(k_fold):\n",
    "                _, loss_te, w = cross_validation(y, tx, k_indices, k, lambda_, degree)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "                w_te_temp.append(w)\n",
    "\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "            # least error w\n",
    "            w_te.append(w_te_temp[np.argmin(rmse_te_tmp)])\n",
    "            #w_te.append(np.mean(w_te_temp, axis=0))\n",
    "\n",
    "        ind_lambda_opt = np.argmin(rmse_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_rmses.append(rmse_te[ind_lambda_opt])\n",
    "        best_w.append(w_te[ind_lambda_opt])\n",
    "        print(\"For degree {deg}, te_loss={te_loss}, lambda={lambda_}\".format(deg=degree, te_loss=rmse_te[ind_lambda_opt], lambda_=lambdas[ind_lambda_opt]))\n",
    "\n",
    "    # find the one having the least test error\n",
    "    ind_best_degree = np.argmin(best_rmses)\n",
    "\n",
    "    best_weights = best_w[ind_best_degree]\n",
    "    #print(\"\\nBest weights shape:\", best_weights.shape)\n",
    "    best_degree = degrees[ind_best_degree]\n",
    "    print(\"\\nBest degree:\", best_degree)\n",
    "    best_lambda = best_lambdas[ind_best_degree]\n",
    "    print(\"Best lambda:\", best_lambda)\n",
    "    \n",
    "    tx_extended = build_poly(tx, best_degree)\n",
    "    w = ridge_regression(y, tx_extended, best_lambda)\n",
    "    \n",
    "    perc_error_1 = perc_wrong_pred(y, tx_extended, w)\n",
    "    perc_error_2 = perc_wrong_pred(y, tx_extended, best_weights)\n",
    "    \n",
    "    print(\"Wrong prediction : {},  the lambda best weights: {}\\n\\n\", perc_error_1, perc_error_2)\n",
    "    \n",
    "    if (perc_error_2 < perc_error_1) :\n",
    "        w_star = best_weights\n",
    "\n",
    "    \n",
    "    \n",
    "    # record the weights & degree\n",
    "    weights_jet[f] = w_star\n",
    "    degrees_jet[f] = best_degree\n",
    "    lambdas_jet[f] = best_lambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_jet, lambdas_jet\n",
    "\n",
    "degrees_final = {k: v for k, v in degrees_jet.items() if v is not None}\n",
    "lambdas_final = {k: v for k, v in lambdas_jet.items() if v is not None}\n",
    "degrees_final, lambdas_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys_test, xs_test, ids_test = load_processed_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xs_test:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the weights\n",
    "weights = []\n",
    "for x, y, f in zip(x_standardized, ys_train, file_names):\n",
    "    x_poly = build_poly(x, degrees_jet[f])\n",
    "    w = ridge_regression(y, x_poly, lambdas_jet[f])\n",
    "    print(len(w), degrees_jet[f])\n",
    "    weights.append(w)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tx_poly = build_poly(x_test, degree)\n",
    "    y_pred = predict_labels(weight, tx_poly)\n",
    "    create_csv_submission(ids_test, y_pred,  name +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id': int(r1), 'Prediction': int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xs_test, ids_test, x_means, x_stds, degrees, weights):\n",
    "    _, xs_test, ids_test = load_processed_data(False)\n",
    "    idx = 0\n",
    "    ids = []\n",
    "    y_preds = []\n",
    "    for x, id_, mean, std, degree, weight in zip(xs_test, ids_test, x_means, x_stds, degrees, weights):\n",
    "        x_std, _, _ = standardize(x, mean, std)\n",
    "        x_expanded = build_poly(x_std, degree)\n",
    "        y_pred = predict_labels(weight, x_expanded)\n",
    "        ids = np.append(ids, id_)\n",
    "        y_preds = np.append(y_preds, y_pred)\n",
    "        idx = idx + 1\n",
    "    return ids, y_preds\n",
    "\n",
    "degrees = list(degrees_final.values())\n",
    "\n",
    "idds, yys = predict(xs_test, ids_test, x_means, x_stds, degrees, weights)\n",
    "\n",
    "create_csv_submission(idds, yys, \"Hope.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    degree = 7\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr = 0\n",
    "        loss_te = 0\n",
    "        for idx in range(k_fold):\n",
    "            e_tr, e_te = cross_validation(y, x, k_indices, idx, lambda_, degree)\n",
    "            loss_tr += e_tr\n",
    "            loss_te += e_te\n",
    "\n",
    "        rmse_tr.append(loss_tr/k_fold)\n",
    "        rmse_te.append(loss_te/k_fold)\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    return rmse_tr, rmse_te\n",
    "\n",
    "    \n",
    "\n",
    "a, b = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

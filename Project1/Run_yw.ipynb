{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from implementation import *\n",
    "#from proj1_helpers import *\n",
    "#from cross_validation import *\n",
    "from data_preprocessing import *\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "TRAIN_PATH = \"./data/train.csv\"\n",
    "TEST_PATH = \"./data/test.csv\"\n",
    "USE_COLS = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 28, 29, 31)\n",
    "\n",
    "def make_predictions(weight, degree, name):\n",
    "    y_test, x_test, ids_test = load_csv_data(\"./data/test.csv\", sub_sample=False)\n",
    "    x_test, mean_x_test, std_x_test = standardize(x_test)\n",
    "    tx_poly = build_poly(x_test, degree)\n",
    "    y_pred = predict_labels(weight, tx_poly)\n",
    "    create_csv_submission(ids_test, y_pred,  name +\".csv\")\n",
    "\n",
    "\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w, mse=True):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    if mse:\n",
    "        return calculate_mse(e)\n",
    "    else:\n",
    "        return calculate_mae(e)\n",
    "    \n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    a = tx.T.dot(tx) + lambda_ * 2 * len(y) * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y)\n",
    "    w_ridge = np.linalg.solve(a, b)\n",
    "    return w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False, cut_values=True):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    if (cut_values):\n",
    "        print('load_csv_data : dropping uniform distribution values')\n",
    "        # drop Pri_tau_phi(17), Pri_lep_phi(20), Pri_met_phi(22), Pri_jet_leading_Phi(27), Pri_jet_subleading_phi(30)\n",
    "        # because of uniform distribution\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, \\\n",
    "            usecols=USE_COLS)\n",
    "    else:\n",
    "        x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "        \n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('Split_data_according_to_jet')\n",
    "    print('Loading files...')\n",
    "    y_tr, x_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "    y_te, x_te, ids_te = load_csv_data(TEST_PATH)\n",
    "    return y_tr, x_tr, ids_tr, y_te, x_te, ids_te\n",
    "\n",
    "def load_headers():\n",
    "    \"\"\"Load all the headers from the training file and drop the unnecessary ones\"\"\"\n",
    "    with open(TRAIN_PATH) as train_file:\n",
    "        reader = csv.reader(train_file)\n",
    "        headers = next(reader)\n",
    "    \n",
    "    # Only use the columns in USE_COLS\n",
    "    headers = [headers[i] for i in USE_COLS]\n",
    "    # drop ID and Predictions cols\n",
    "    headers = headers[2:]\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_data_according_to_jet\n",
      "Loading files...\n",
      "load_csv_data : dropping uniform distribution values\n",
      "load_csv_data : dropping uniform distribution values\n"
     ]
    }
   ],
   "source": [
    "y_tr, x_tr, ids_tr, y_te, x_te, ids_te = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DER_mass_MMC',\n",
       " 'DER_mass_transverse_met_lep',\n",
       " 'DER_mass_vis',\n",
       " 'DER_pt_h',\n",
       " 'DER_deltaeta_jet_jet',\n",
       " 'DER_mass_jet_jet',\n",
       " 'DER_prodeta_jet_jet',\n",
       " 'DER_deltar_tau_lep',\n",
       " 'DER_pt_tot',\n",
       " 'DER_sum_pt',\n",
       " 'DER_pt_ratio_lep_tau',\n",
       " 'DER_met_phi_centrality',\n",
       " 'DER_lep_eta_centrality',\n",
       " 'PRI_tau_pt',\n",
       " 'PRI_tau_eta',\n",
       " 'PRI_lep_pt',\n",
       " 'PRI_lep_eta',\n",
       " 'PRI_met',\n",
       " 'PRI_met_sumet',\n",
       " 'PRI_jet_num',\n",
       " 'PRI_jet_leading_pt',\n",
       " 'PRI_jet_leading_eta',\n",
       " 'PRI_jet_subleading_pt',\n",
       " 'PRI_jet_subleading_eta',\n",
       " 'PRI_jet_all_pt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = load_headers()\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_in_DER_pt_h(y_tr, x_tr, ids_tr, jet):\n",
    "    print(\"Removing outliers in Der_pt_h\")\n",
    "    # Remove the outliers in DER_pt_h (col 3):\n",
    "    #  JET 0: 2834.999 when the max value is 117.707 outside of outlier - threshold to 120 \n",
    "    #  JET 2: 1053.807 when max value is 734 outside of outlier- Threshold to 800\n",
    "    OUTLIERS = [120, 999, 800, 999]\n",
    "    outlier = OUTLIERS[jet]\n",
    "    tr_smaller_than_outlier = (x_tr[:, 3] < outlier)\n",
    "    x_tr = x_tr[tr_smaller_than_outlier]\n",
    "    y_tr = y_tr[tr_smaller_than_outlier]\n",
    "    ids_tr = ids_tr[tr_smaller_than_outlier]\n",
    "    return y_tr, x_tr, ids_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_NAN_columns(x_tr, x_te, headers_jet):\n",
    "    print(\"Removing nan columns\")\n",
    "    nan_cols = []\n",
    "    # Find all columns with -999\n",
    "    for col_idx in range(x_tr.shape[1]):\n",
    "        col = x_tr[:, col_idx]\n",
    "        nb_nan_in_col = len(x_tr[col == -999])\n",
    "        # A column has all NaN if len of col = nb NaN values in col\n",
    "        if (nb_nan_in_col == len(col)):\n",
    "            nan_cols.append(col_idx)\n",
    "    \n",
    "    # Remove all nan columns\n",
    "    x_tr_updated = np.delete(x_tr, nan_cols, axis=1)\n",
    "    x_te_updated = np.delete(x_te, nan_cols, axis=1)\n",
    "    headers_jet_updated = np.delete(headers_jet, nan_cols)\n",
    "    \n",
    "    return x_tr_updated, x_te_updated, headers_jet_updated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_mass(x, y, ids):\n",
    "        # Get all the rows idx with invalid mass (i.e. DER_mass_MMC = -999)\n",
    "        invalid_mass_row_idx = x[:, 0] == -999\n",
    "        valid_mass_row_idx = x[:, 0] > 0\n",
    "        # Process for each data table\n",
    "        x_invalid_mass = x[invalid_mass_row_idx]\n",
    "        x_valid_mass = x[valid_mass_row_idx]\n",
    "        y_invalid_mass = y[invalid_mass_row_idx]\n",
    "        y_valid_mass = y[valid_mass_row_idx]\n",
    "        ids_invalid_mass = ids[invalid_mass_row_idx]\n",
    "        ids_valid_mass = ids[valid_mass_row_idx]\n",
    "        \n",
    "        return x_invalid_mass, x_valid_mass, y_invalid_mass, y_valid_mass, ids_invalid_mass, ids_valid_mass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_csv(x, y, ids, headers, jet, isTrain, isMassValid):\n",
    "    \"\"\"\n",
    "    Write data into new csv file\n",
    "    \"\"\"\n",
    "    # Add 'Id' & 'Prediction' to headers\n",
    "    headers = np.insert(headers, 0, ['Id', 'Prediction'])\n",
    "    \n",
    "    # Remove 'DER_mass_MMC' if mass is not valid\n",
    "    if not isMassValid:\n",
    "        headers = np.delete(headers, np.where(headers =='DER_mass_MMC'))\n",
    "    \n",
    "    # Generate file name\n",
    "    base = './data/train_' if isTrain else './data/test_'\n",
    "    valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "    file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "    \n",
    "    print(\"Outputing {}\".format(file_name))\n",
    "        \n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        data_ = dict.fromkeys(headers)\n",
    "        # Transform -1 and 1 into 's' and 'b'\n",
    "        for id_, y_, x_ in zip(ids, y, x):\n",
    "            data_['Id'] = int(id_)\n",
    "            if (y_ != -1 and y_ !=1):\n",
    "                raise Exception('Prediction not -1 and 1!!!')\n",
    "            data_['Prediction'] = 's' if y_ == 1 else 'b'\n",
    "            \n",
    "            for idx, x_value in enumerate(x_):\n",
    "                data_[headers[idx + 2]] = float(x_value)\n",
    "            writer.writerow(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers): \n",
    "    for jet in range(4):\n",
    "        print(\"\\n\\nSplitting for jet {}\".format(jet))\n",
    "\n",
    "        # PRI_jet_num (24 -> 24 - 3 cols dropped before 24 - 2 cols (id, label) = col 19)\n",
    "        col_jet = 19\n",
    "        \n",
    "        # TRAIN - Get all the rows having Pri_jet_num = jet for TRAINING set and delete PRI_jet_num col\n",
    "        x_tr_jet = x_tr[x_tr[:, col_jet] == jet]\n",
    "        x_tr_jet = np.delete(x_tr_jet, col_jet, axis=1)\n",
    "        # Delete PRI_jet_num in headers\n",
    "        headers_jet = np.delete(headers, col_jet)\n",
    "\n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_tr_jet = y_tr[x_tr[:, col_jet] == jet]\n",
    "        ids_tr_jet = ids_tr[x_tr[:, col_jet] == jet]\n",
    "        \n",
    "        # TEST - Get all the rows having Pri_jet_num = jet for TEST set and delete PRI_jet_num col\n",
    "        x_te_jet = x_te[x_te[:, col_jet] == jet]\n",
    "        x_te_jet = np.delete(x_te_jet, col_jet, axis=1)\n",
    "                \n",
    "        # Using the row found in x_tr to select the rows in y and ids\n",
    "        y_te_jet = y_te[x_te[:, col_jet] == jet]\n",
    "        ids_te_jet = ids_te[x_te[:, col_jet] == jet]\n",
    "            \n",
    "        # Remove outliers\n",
    "        y_tr_jet, x_tr_jet, ids_tr_jet = remove_outlier_in_DER_pt_h(y_tr_jet, x_tr_jet, ids_tr_jet, jet)\n",
    "\n",
    "        # Remove col PRI_jet_all_pt from x because it only contains 0 values\n",
    "        if jet == 0:\n",
    "            print(\"Deleted col Pri_jet_all_pt in set with jet_num = 0\")\n",
    "            x_tr_jet = np.delete(x_tr_jet, -1, axis=1)\n",
    "            x_te_jet = np.delete(x_te_jet, -1, axis=1)\n",
    "            headers_jet = np.delete(headers_jet, -1)\n",
    "        \n",
    "        # Remove all the columns with only NaN values\n",
    "        x_tr_jet, x_te_jet, headers_jet = remove_all_NAN_columns(x_tr_jet, x_te_jet, headers_jet)\n",
    "\n",
    "        # Split the dataset again into valid/invalid values of DER_mass_MMC \n",
    "        # TRAIN\n",
    "        x_tr_jet_invalid_mass, x_tr_jet_valid_mass, y_tr_jet_invalid_mass, y_tr_jet_valid_mass, ids_tr_jet_invalid_mass, ids_tr_jet_valid_mass = split_data_according_to_mass(x_tr_jet, y_tr_jet, ids_tr_jet)\n",
    "        # TEST\n",
    "        x_te_jet_invalid_mass, x_te_jet_valid_mass, y_te_jet_invalid_mass, y_te_jet_valid_mass, ids_te_jet_invalid_mass, ids_te_jet_valid_mass = split_data_according_to_mass(x_te_jet, y_te_jet, ids_te_jet)\n",
    "        \n",
    "        # Remove 'DER_mass_MMC' (col 0) if the mass is not valid\n",
    "        x_tr_jet_invalid_mass = np.delete(x_tr_jet_invalid_mass, 0, axis=1)\n",
    "        x_te_jet_invalid_mass = np.delete(x_te_jet_invalid_mass, 0, axis=1)\n",
    "        \n",
    "        # Save into CSV\n",
    "        #x, y, ids, headers, jet, isTrain, isMassValid\n",
    "        # TRAIN\n",
    "        output_to_csv(x_tr_jet_invalid_mass, y_tr_jet_invalid_mass, ids_tr_jet_invalid_mass, headers_jet, jet, True, False)\n",
    "        output_to_csv(x_tr_jet_valid_mass, y_tr_jet_valid_mass, ids_tr_jet_valid_mass, headers_jet, jet, True, True)\n",
    "\n",
    "        # TEST\n",
    "        output_to_csv(x_te_jet_invalid_mass, y_te_jet_invalid_mass, ids_te_jet_invalid_mass, headers_jet, jet, False, False)\n",
    "        output_to_csv(x_te_jet_valid_mass, y_te_jet_valid_mass, ids_te_jet_valid_mass, headers_jet, jet, False, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Splitting for jet 0\n",
      "Removing outliers in Der_pt_h\n",
      "Deleted col Pri_jet_all_pt in set with jet_num = 0\n",
      "Removing nan columns\n",
      "Outputing ./data/train_jet_0_invalid_mass.csv\n",
      "Outputing ./data/train_jet_0_valid_mass.csv\n",
      "Outputing ./data/test_jet_0_invalid_mass.csv\n",
      "Outputing ./data/test_jet_0_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 1\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "Outputing ./data/train_jet_1_invalid_mass.csv\n",
      "Outputing ./data/train_jet_1_valid_mass.csv\n",
      "Outputing ./data/test_jet_1_invalid_mass.csv\n",
      "Outputing ./data/test_jet_1_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 2\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "Outputing ./data/train_jet_2_invalid_mass.csv\n",
      "Outputing ./data/train_jet_2_valid_mass.csv\n",
      "Outputing ./data/test_jet_2_invalid_mass.csv\n",
      "Outputing ./data/test_jet_2_valid_mass.csv\n",
      "\n",
      "\n",
      "Splitting for jet 3\n",
      "Removing outliers in Der_pt_h\n",
      "Removing nan columns\n",
      "Outputing ./data/train_jet_3_invalid_mass.csv\n",
      "Outputing ./data/train_jet_3_valid_mass.csv\n",
      "Outputing ./data/test_jet_3_invalid_mass.csv\n",
      "Outputing ./data/test_jet_3_valid_mass.csv\n"
     ]
    }
   ],
   "source": [
    "split_data_according_to_jet_and_mass(y_tr, x_tr, ids_tr, y_te, x_te, ids_te, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_jet_0_valid_mass.csv',\n",
       " './data/train_jet_1_valid_mass.csv',\n",
       " './data/train_jet_2_valid_mass.csv',\n",
       " './data/train_jet_3_valid_mass.csv',\n",
       " './data/train_jet_0_invalid_mass.csv',\n",
       " './data/train_jet_1_invalid_mass.csv',\n",
       " './data/train_jet_2_invalid_mass.csv',\n",
       " './data/train_jet_3_invalid_mass.csv']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the real files\n",
    "def generate_processed_filenames(isTrain):\n",
    "    file_names = []\n",
    "    isMassValids = [True, False]\n",
    "    jets = range(4)\n",
    "    \n",
    "    for isMassValid in isMassValids:\n",
    "        for jet in jets:\n",
    "            # Generate file name\n",
    "            base = './data/train_' if isTrain else './data/test_'\n",
    "            valid = '_valid_mass' if isMassValid else '_invalid_mass'\n",
    "            file_name = base + 'jet_' + str(jet) + valid + '.csv'\n",
    "            file_names.append(file_name)\n",
    "                \n",
    "    return file_names\n",
    "\n",
    "file_names = generate_processed_filenames(True)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(isTrain):\n",
    "    \"\"\"Load all Train/Test processed data\"\"\"\n",
    "    file_names = generate_processed_filenames(isTrain)\n",
    "\n",
    "    ys = []\n",
    "    xs = []\n",
    "    ids = []\n",
    "\n",
    "    for i in range(4):\n",
    "        y, x, id_ = load_csv_data(file_names[i], cut_values = False)\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "        ids.append(id_)\n",
    "        \n",
    "    return ys, xs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train, xs_train, ids_train = load_processed_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean=None, std=None):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    if mean is None or std is None:\n",
    "        mean_x = np.mean(x, axis = 0)\n",
    "        std_x = np.std(x, axis = 0)\n",
    "    else:\n",
    "        mean_x = mean\n",
    "        std_x = std\n",
    "        \n",
    "    x = x - mean_x\n",
    "    x = x / std_x \n",
    "\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "# def standardize(x, mean_x=None, std_x=None):\n",
    "#     \"\"\"\n",
    "#         Standardize the original data set.\n",
    "#     \"\"\"\n",
    "#     if mean_x is None:\n",
    "#         mean_x = np.mean(x, axis=0)\n",
    "#     x = x - mean_x\n",
    "#     if std_x is None:\n",
    "#         std_x = np.std(x, axis=0)\n",
    "#     x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "#     tx = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "#     return tx, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standandize xs\n",
    "x_means = []\n",
    "x_stds = []\n",
    "x_standardized = []\n",
    "\n",
    "for x_ in xs_train:\n",
    "    x, mean_x, std_x = standardize(x_)\n",
    "    x_standardized.append(x)\n",
    "    x_means.append(mean_x)\n",
    "    x_stds.append(std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a weights dictionary for each jet\n",
    "weights_jet = dict.fromkeys(range(4))\n",
    "degrees_jet = dict.fromkeys(range(4))\n",
    "lambdas_jet = dict.fromkeys(range(4))\n",
    "\n",
    "# build w using ridge regression\n",
    "k_fold = 10\n",
    "degrees = np.arange(3, 7)\n",
    "lambdas = np.logspace(-10, -1, 60)\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_jet_0_valid_mass.csv',\n",
       " './data/train_jet_1_valid_mass.csv',\n",
       " './data/train_jet_2_valid_mass.csv',\n",
       " './data/train_jet_3_valid_mass.csv',\n",
       " './data/train_jet_0_invalid_mass.csv',\n",
       " './data/train_jet_1_invalid_mass.csv',\n",
       " './data/train_jet_2_invalid_mass.csv',\n",
       " './data/train_jet_3_invalid_mass.csv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    # ridge regression\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_loss(y_tr, tx_tr, w))\n",
    "    loss_te = np.sqrt(2 * compute_loss(y_te, tx_te, w))\n",
    "    return loss_tr, loss_te, w\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_wrong_pred(y, tx, w_star):\n",
    "    \"\"\"\n",
    "        Return the percentage of wrong predictions (between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    pred = np.dot(tx, w_star)\n",
    "\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred <= 0] = -1\n",
    "\n",
    "    right = np.sum(pred == y)\n",
    "    wrong = len(pred) - right\n",
    "\n",
    "    return float(wrong) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ./data/train_jet_0_valid_mass.csv\n",
      "For degree 3, te_loss=0.7635206423901957, lambda=0.0014773776525985128\n",
      "For degree 4, te_loss=0.7572822576260646, lambda=0.001039798418481492\n",
      "For degree 5, te_loss=0.7539459171712302, lambda=1.5361749466718298e-05\n",
      "For degree 6, te_loss=0.778244079494439, lambda=0.049535352089591804\n",
      "\n",
      "\n",
      " Best degree: 5\n",
      "Best lambda: 1.5361749466718298e-05 \n",
      "\n",
      "\n",
      "Wrong prediction : {} 0.272859098239575\n",
      "Training for ./data/train_jet_1_valid_mass.csv\n",
      "For degree 3, te_loss=0.8343966242615828, lambda=0.0014773776525985128\n",
      "For degree 4, te_loss=0.830890556297893, lambda=0.002982471286216894\n",
      "For degree 5, te_loss=0.8259011729532861, lambda=0.002982471286216894\n",
      "For degree 6, te_loss=0.8345118021935469, lambda=0.00017957144943716409\n",
      "\n",
      "\n",
      " Best degree: 5\n",
      "Best lambda: 0.002982471286216894 \n",
      "\n",
      "\n",
      "Wrong prediction : {} 0.33327141264896687\n",
      "Training for ./data/train_jet_2_valid_mass.csv\n",
      "For degree 3, te_loss=0.7881063782221676, lambda=0.004237587160604063\n",
      "For degree 4, te_loss=0.7772331986624467, lambda=0.008554672535565685\n",
      "For degree 5, te_loss=0.7810055088410166, lambda=0.049535352089591804\n",
      "For degree 6, te_loss=0.7700394202982683, lambda=0.006020894493336138\n",
      "\n",
      "\n",
      " Best degree: 6\n",
      "Best lambda: 0.006020894493336138 \n",
      "\n",
      "\n",
      "Wrong prediction : {} 0.27267743431872815\n",
      "Training for ./data/train_jet_3_valid_mass.csv\n",
      "For degree 3, te_loss=0.8175293624349557, lambda=1.1689518164985776e-09\n",
      "For degree 4, te_loss=0.7952727074579806, lambda=1.3664483492953244e-08\n",
      "For degree 5, te_loss=1.2391125171116721, lambda=0.006020894493336138\n",
      "For degree 6, te_loss=2.1206610554889065, lambda=0.1\n",
      "\n",
      "\n",
      " Best degree: 4\n",
      "Best lambda: 1.3664483492953244e-08 \n",
      "\n",
      "\n",
      "Wrong prediction : {} 0.350848358872722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train with ridge regression for each tx in x and save the weights\n",
    "for tx, y, f in zip(x_standardized, ys_train, file_names):\n",
    "    print(\"Training for {}\".format(f))\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # for each degree, we compute the best lambdas and the associated rmse\n",
    "    best_lambdas = []\n",
    "    best_rmses = []\n",
    "    best_w = []\n",
    "    # vary degree\n",
    "    for degree in degrees:\n",
    "        # cross validation\n",
    "        rmse_te = []\n",
    "        w_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            rmse_te_tmp = []\n",
    "            w_te_temp = []\n",
    "            for k in range(k_fold):\n",
    "                _, loss_te, w = cross_validation(y, tx, k_indices, k, lambda_, degree)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "                w_te_temp.append(w)\n",
    "\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "            # mean of all the ws ?????\n",
    "            #w_te.append(np.mean(w_te_temp, axis=0))\n",
    "\n",
    "        ind_lambda_opt = np.argmin(rmse_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_rmses.append(rmse_te[ind_lambda_opt])\n",
    "        #best_w.append(w_te[ind_lambda_opt])\n",
    "        print(\"For degree {deg}, te_loss={te_loss}, lambda={lambda_}\".format(deg=degree, te_loss=rmse_te[ind_lambda_opt], lambda_=lambdas[ind_lambda_opt]))\n",
    "\n",
    "    # find the one having the least test error\n",
    "    ind_best_degree = np.argmin(best_rmses)\n",
    "\n",
    "    #best_weights = best_w[ind_best_degree]\n",
    "    #print(\"\\n\\n Best weights shape:\", best_weights.shape)\n",
    "    best_degree = degrees[ind_best_degree]\n",
    "    print(\"\\n\\n Best degree:\", best_degree)\n",
    "    best_lambda = best_lambdas[ind_best_degree]\n",
    "    print(\"Best lambda:\", best_lambda, \"\\n\\n\")\n",
    "    \n",
    "    tx_extended = build_poly(tx, best_degree)\n",
    "    w_star = ridge_regression(y, tx, best_lambda)\n",
    "    \n",
    "    print(\"Wrong prediction : {}\", perc_wrong_pred(y, tx, w_star))\n",
    "\n",
    "\n",
    "\n",
    "    # record the weights & degree\n",
    "    #weights_jet[jet] = best_weights\n",
    "    degrees_jet[f] = best_degree\n",
    "    lambdas_jet[f] = best_lambda\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'./data/train_jet_0_valid_mass.csv': 5,\n",
       "  './data/train_jet_1_valid_mass.csv': 5,\n",
       "  './data/train_jet_2_valid_mass.csv': 6,\n",
       "  './data/train_jet_3_valid_mass.csv': 4},\n",
       " {'./data/train_jet_0_valid_mass.csv': 1.5361749466718298e-05,\n",
       "  './data/train_jet_1_valid_mass.csv': 0.002982471286216894,\n",
       "  './data/train_jet_2_valid_mass.csv': 0.006020894493336138,\n",
       "  './data/train_jet_3_valid_mass.csv': 1.3664483492953244e-08})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degrees_jet, lambdas_jet\n",
    "\n",
    "degrees_final = {k: v for k, v in degrees_jet.items() if v is not None}\n",
    "lambdas_final = {k: v for k, v in lambdas_jet.items() if v is not None}\n",
    "degrees_final, lambdas_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test, xs_test, ids_test = load_processed_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 5\n",
      "91 5\n",
      "145 6\n",
      "97 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.39732676e-01, -1.21615933e-02, -2.46224727e-01, -4.84678679e-01,\n",
       "        -4.31864266e-02,  3.97199825e-01, -4.30686437e-02,  2.61800306e-01,\n",
       "         6.78592734e-01, -2.40400803e-02,  1.13537588e+00,  2.59783796e-03,\n",
       "        -7.42486433e-01,  1.56782507e-02, -2.78967573e-02,  3.30244083e-02,\n",
       "        -1.07905700e-01,  4.11505045e-02, -3.52251116e-02, -1.01748410e-02,\n",
       "         2.43856576e-02, -9.43890797e-03, -6.86501610e-03, -1.28483014e-01,\n",
       "        -1.95410753e-01, -2.42422815e-01, -7.75303129e-03, -1.63729861e-02,\n",
       "        -5.30673120e-02,  2.52148245e-02, -8.64859576e-03,  3.47634881e-02,\n",
       "         4.03537833e-02,  2.78796424e-02,  3.03169887e-02, -2.19862556e-02,\n",
       "         3.29721565e-02, -3.74319679e-03,  6.53059279e-02,  2.53969800e-01,\n",
       "         2.40587681e-02, -1.29284390e-03,  4.13501428e-02, -9.77236550e-03,\n",
       "        -1.25078663e-02,  2.32038042e-03, -3.74891876e-03, -1.00276556e-02,\n",
       "        -3.16488512e-03, -1.60644645e-02,  8.36052376e-04, -2.78843553e-03,\n",
       "         8.49562836e-04, -1.59117708e-02, -1.16243887e-01, -1.16848584e-03,\n",
       "        -6.65024865e-03, -6.72919377e-03, -1.19520885e-03,  1.54777300e-03,\n",
       "        -2.58422875e-04,  1.30589734e-04,  6.49557171e-04,  1.02692961e-04,\n",
       "        -2.95118382e-02,  5.90867775e-04,  3.10735953e-02, -3.56758581e-05,\n",
       "         1.20807385e-03,  1.72901728e-02,  2.24471172e-05,  3.02609773e-04,\n",
       "         2.98493794e-04,  1.80897588e-03, -5.67235080e-05,  6.67246390e-06]),\n",
       " array([ 8.11243431e-02,  3.78934672e-01, -2.25875105e-01, -2.37456531e-01,\n",
       "        -5.96788990e-02,  3.95320401e-02,  1.53879573e-02,  1.14916427e-01,\n",
       "        -1.16938315e-02,  5.16585357e-02,  2.18464812e-01, -1.27683446e-02,\n",
       "         4.86958532e-02, -3.82711634e-03,  7.71916409e-02, -1.57528386e-02,\n",
       "         4.35803161e-02, -7.39871618e-03,  4.35808215e-02, -1.97422133e-01,\n",
       "        -7.04157962e-02, -8.93936282e-02,  4.80955736e-02, -1.22017679e-01,\n",
       "        -4.18712423e-02, -1.65468785e-02,  6.13796774e-02,  1.52224439e-01,\n",
       "        -8.66779286e-02, -2.77976109e-02, -7.55931485e-02, -7.49766199e-02,\n",
       "        -1.67837146e-02, -2.39823918e-02, -4.20673609e-02,  1.90154391e-01,\n",
       "        -4.20662826e-02,  2.23438589e-02,  3.60282818e-02,  3.40684060e-02,\n",
       "        -1.21945550e-02,  1.63862544e-02,  1.69474672e-02,  1.19083527e-02,\n",
       "        -2.19821938e-02,  4.40646154e-02,  1.19209512e-02,  1.35466500e-02,\n",
       "         2.41099263e-02,  5.09780549e-03,  3.95512643e-03,  2.22958502e-04,\n",
       "         4.73935256e-03,  3.54916394e-03,  4.75339984e-03, -4.11704665e-04,\n",
       "        -3.71788451e-03, -3.10584210e-03,  1.43214639e-03,  2.52225192e-02,\n",
       "        -2.18544815e-03, -1.23055140e-03,  2.45302056e-03, -7.31775526e-02,\n",
       "        -7.10666521e-04, -3.70559027e-03, -2.61298342e-03,  1.89648558e-03,\n",
       "        -2.71411810e-04,  5.87602986e-04, -5.91229800e-04, -1.29812075e-02,\n",
       "        -4.15628888e-04, -2.58344391e-05,  1.06968281e-04,  8.49866800e-05,\n",
       "        -5.85902111e-05, -4.72114832e-03,  6.95781732e-05,  4.28883734e-05,\n",
       "        -8.26476417e-05, -2.92687631e-02,  1.43364397e-05, -2.48159906e-03,\n",
       "         8.72046078e-05, -2.12147885e-03,  6.17959675e-06, -4.67505072e-05,\n",
       "        -9.52329035e-04, -4.21069667e-04,  9.95188889e-04]),\n",
       " array([ 9.59316937e-02,  4.19889604e-01, -6.62513802e-02, -2.30844252e-02,\n",
       "         6.27590519e-02,  1.33234639e-01, -9.63322111e-03, -1.08967108e-01,\n",
       "         4.32152741e-02, -9.86189470e-02,  5.06579427e-02, -5.71595988e-02,\n",
       "         9.22503489e-02,  1.59914874e-01,  1.07266719e-01,  1.16403406e-02,\n",
       "         5.85712452e-02, -1.90562263e-02,  1.03256728e-01, -7.60392200e-02,\n",
       "        -2.32899424e-02,  1.69955323e-03,  7.94355417e-02,  5.14883705e-03,\n",
       "         7.46436209e-03, -2.26135461e-01, -6.83852456e-02, -1.22769694e-01,\n",
       "         8.20534672e-02,  3.73517623e-02,  2.05527238e-01, -2.79360762e-02,\n",
       "        -1.76491435e-01,  8.71320687e-03, -2.00015801e-02,  2.87728579e-02,\n",
       "         8.89370016e-02,  1.41868469e-02, -2.25972784e-02,  2.19195923e-03,\n",
       "        -3.52193609e-02, -2.11825569e-02, -1.72906516e-03,  2.14107051e-03,\n",
       "        -6.14480705e-02,  1.03739750e-01, -3.17903127e-02,  1.45359507e-01,\n",
       "         2.18149624e-02, -3.37667300e-02, -1.59623868e-02,  3.71137129e-02,\n",
       "        -6.26594026e-03, -1.36123179e-01, -1.43547939e-01,  1.18124566e-03,\n",
       "         4.17935147e-02,  1.78872985e-02,  4.03589982e-02, -1.29949147e-02,\n",
       "         1.22243502e-02,  2.79843153e-02, -1.33134352e-03, -9.32757453e-03,\n",
       "         1.62367379e-02,  7.39963602e-03, -5.58434875e-06,  6.91875681e-04,\n",
       "        -7.30038080e-03, -6.44038417e-04,  1.20940220e-03, -5.06374163e-03,\n",
       "         2.56254548e-03,  1.80336958e-02,  1.28164829e-02, -2.76388757e-03,\n",
       "        -1.43004028e-02,  9.46700193e-03,  3.63829706e-02,  6.48669247e-04,\n",
       "         2.94990784e-02, -4.80614956e-03, -1.49433337e-02,  2.17125815e-03,\n",
       "        -1.54099480e-02,  9.56782829e-03,  7.72123972e-04, -2.47574839e-02,\n",
       "        -3.15136479e-03, -1.64292031e-02,  2.78172456e-04, -5.65237393e-04,\n",
       "         8.41984775e-03,  3.10056175e-03,  7.05829211e-04,  2.50066280e-03,\n",
       "        -3.77225572e-03, -1.86883032e-03, -1.73188592e-03, -1.21386299e-05,\n",
       "         3.53442979e-03,  2.32569110e-02, -3.91361563e-03,  9.20565917e-04,\n",
       "        -9.51289362e-03,  4.36498153e-04,  2.20126905e-03, -1.52467457e-04,\n",
       "         1.19930275e-03, -2.70336676e-02, -6.76172547e-05,  1.76604171e-03,\n",
       "         2.73004520e-04, -1.13591043e-03, -4.87694242e-05,  4.49828913e-05,\n",
       "        -1.46305567e-03,  1.20304397e-03, -9.14344373e-05,  1.86831269e-03,\n",
       "         6.22310523e-04,  5.84531869e-05,  6.73255510e-05,  4.54042272e-06,\n",
       "        -2.28712020e-04, -2.77564755e-03,  1.49882665e-04, -1.66317736e-04,\n",
       "         6.79289338e-04, -1.25689844e-05, -1.14604396e-04,  3.77629850e-06,\n",
       "         2.07140501e-03, -7.59998875e-04,  1.77261467e-06,  3.73918179e-03,\n",
       "        -8.38298466e-06,  3.02355219e-03,  2.42547849e-06,  6.12245764e-06,\n",
       "         7.69559888e-05, -1.30498418e-03,  2.96479789e-06, -3.76592005e-03,\n",
       "        -2.75507580e-05]),\n",
       " array([-8.70962445e+00,  8.00859354e-01, -8.56213351e-02, -4.00799321e-01,\n",
       "         1.05583126e-01, -1.40710577e+01, -1.92217074e-01, -1.40786684e+01,\n",
       "        -1.17499089e-01, -2.39603310e-02, -6.32957461e-02,  1.39873777e-01,\n",
       "         1.75315682e-03,  1.01881695e-01,  2.78796202e-01, -1.22925897e-02,\n",
       "        -6.80420004e-03,  1.21988992e-02, -4.21829201e-02, -2.90419217e-02,\n",
       "         4.60938503e-02, -1.29783681e-01,  8.88388780e-02, -4.22717779e-02,\n",
       "        -7.00450011e-02, -4.39075815e-01, -4.44313856e-02,  5.42245647e-02,\n",
       "         9.89224034e-02, -5.22266523e+00,  1.16216268e-01, -4.48279840e-03,\n",
       "        -1.36886858e-01,  1.75273987e-03,  6.54046596e-03, -6.29951433e-02,\n",
       "         4.51512491e-02,  7.72280050e-02, -7.13551583e-02, -8.56484518e-02,\n",
       "        -4.02170586e-03, -6.80675647e-02, -3.35372484e-02,  1.25026286e-02,\n",
       "        -5.40834095e-02,  6.11245298e+00, -3.24986242e-02,  7.87350484e+00,\n",
       "         5.23538175e-02,  6.94474131e-02,  1.16395907e-02, -1.43994375e-03,\n",
       "        -2.02148980e-02, -3.66558539e-02, -2.12625093e-02,  1.43282420e-03,\n",
       "         7.65515439e-02,  1.73152424e-03,  7.90265763e-04,  1.11937324e-02,\n",
       "         3.00554779e-02, -3.71680075e-02,  7.57988359e-03,  1.57217452e-03,\n",
       "         1.21434593e-04, -7.92055221e-03,  7.00427378e-03, -6.77553593e-03,\n",
       "         1.26122863e-02,  3.61907089e-03,  3.35467165e-03, -5.72225891e-03,\n",
       "        -6.07518772e-03, -3.26541416e-03, -5.30934963e-04, -3.18888153e-05,\n",
       "         1.49973293e-03,  2.99764326e-03,  1.08784496e-03,  1.24643025e-03,\n",
       "        -6.55624908e-03, -1.39427592e-04,  2.61649403e-05, -6.74040839e-04,\n",
       "         2.49331507e-03,  3.18622149e-03, -2.85627381e-04,  1.36902637e-02,\n",
       "         6.70294309e-05,  7.15346033e-03, -4.05440696e-04,  3.00756734e-04,\n",
       "        -8.90416995e-04, -1.03135456e-02, -1.16357881e-04, -1.59695911e-02,\n",
       "         2.75711048e-04])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the weights\n",
    "weights = []\n",
    "for x, y, f in zip(x_standardized, ys_train, file_names):\n",
    "    x_poly = build_poly(x, degrees_jet[f])\n",
    "    w = ridge_regression(y, x_poly, lambdas_jet[f])\n",
    "    print(len(w), degrees_jet[f])\n",
    "    weights.append(w)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tx_poly = build_poly(x_test, degree)\n",
    "    y_pred = predict_labels(weight, tx_poly)\n",
    "    create_csv_submission(ids_test, y_pred,  name +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id': int(r1), 'Prediction': int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xs_test, ids_test, x_means, x_stds, degrees, weights):\n",
    "    #_, xs_test, ids_test = load_processed_data(False)\n",
    "    idx = 0\n",
    "    ids = []\n",
    "    y_preds = []\n",
    "    for x, id_, mean, std, degree, weight in zip(xs_test, ids_test, x_means, x_stds, degrees, weights):\n",
    "        x_std, _, _ = standardize(x, mean, std)\n",
    "        x_expanded = build_poly(x_std, degree)\n",
    "        y_pred = predict_labels(weight, x_expanded)\n",
    "        ids = np.append(ids, id_)\n",
    "        y_preds = np.append(y_preds, y_pred)\n",
    "        idx = idx + 1\n",
    "    return ids, y_preds\n",
    "\n",
    "degrees = list(degrees_final.values())\n",
    "\n",
    "idds, yys = predict(xs_test, ids_test, x_means, x_stds, degrees, weights)\n",
    "\n",
    "create_csv_submission(idds, yys, \"Desperation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481750"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    degree = 7\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr = 0\n",
    "        loss_te = 0\n",
    "        for idx in range(k_fold):\n",
    "            e_tr, e_te = cross_validation(y, x, k_indices, idx, lambda_, degree)\n",
    "            loss_tr += e_tr\n",
    "            loss_te += e_te\n",
    "\n",
    "        rmse_tr.append(loss_tr/k_fold)\n",
    "        rmse_te.append(loss_te/k_fold)\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    return rmse_tr, rmse_te\n",
    "\n",
    "    \n",
    "\n",
    "a, b = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

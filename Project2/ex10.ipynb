{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers_lab import load_data, preprocess_data\n",
    "\n",
    "#path_dataset = \"movielens100k.csv\"\n",
    "train_dataset = \"./datas/data_train.csv\"\n",
    "test_dataset = \"./datas/sampleSubmission.csv\"\n",
    "ratings = load_data(train_dataset)\n",
    "test_file = load_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 943)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0] # tuple ([0],)\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data and return train and test data. TODO\n",
    "    # NOTE: we only consider users and movies that have more\n",
    "    # than 10 ratings\n",
    "    # ***************************************************\n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    # Row-based linked list sparse matrix\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "    \n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # Split data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "    \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (9990, 999)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1065253\n",
      "Total number of nonzero elements in test data:111620\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the global mean to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of baseline using the global mean: [[1.12152228]].\n"
     ]
    }
   ],
   "source": [
    "from helpers_lab import calculate_mse\n",
    "\n",
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"baseline method: use the global mean.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    # find the non zero ratings in the train\n",
    "    nonzero_train = train[train.nonzero()]\n",
    "    global_mean_train = nonzero_train.mean()\n",
    "    \n",
    "    # find the non zero ratings in test\n",
    "    nonzero_test = test[test.nonzero()].toarray()\n",
    "    \n",
    "    # predict the ratings as global mean\n",
    "    mse = calculate_mse(nonzero_test, global_mean_train)\n",
    "    rmse = np.sqrt(1.0 * mse / nonzero_test.shape[1])\n",
    "    print(\"test RMSE of baseline using the global mean: {v}.\".format(v=rmse))\n",
    "\n",
    "    return nonzero_test, global_mean_train\n",
    "\n",
    "a, b = baseline_global_mean(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8581107023810133\n"
     ]
    }
   ],
   "source": [
    "print(b) # global_mean is one number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the user means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the user mean: [[1.03317038]].\n"
     ]
    }
   ],
   "source": [
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"baseline method: use the user means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    for user_index in range(num_users):\n",
    "        # find the non-zero ratings for each user in the training dataset\n",
    "        train_ratings = train[:, user_index]\n",
    "        nonzeros_train_ratings = train_ratings[train_ratings.nonzero()]\n",
    "        \n",
    "        # calculate the mean if the number of elements is not 0\n",
    "        if nonzeros_train_ratings.shape[0] != 0:\n",
    "            user_train_mean = nonzeros_train_ratings.mean()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # find the non-zero ratings for each user in the test dataset\n",
    "        test_ratings = test[:, user_index]\n",
    "        nonzeros_test_ratings = test_ratings[test_ratings.nonzero()].todense()\n",
    "        \n",
    "        # calculate the test error \n",
    "        mse += calculate_mse(nonzeros_test_ratings, user_train_mean)\n",
    "        \n",
    "    rmse = np.sqrt(1.0 * mse / test.nnz)\n",
    "    print(\"test RMSE of the baseline using the user mean: {v}.\".format(v=rmse))\n",
    "\n",
    "baseline_user_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the item means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the item mean: [[1.09633198]].\n"
     ]
    }
   ],
   "source": [
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"baseline method: use item means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    for item_index in range(num_items):\n",
    "        train_ratings = train[item_index, :]\n",
    "        nonzeros_train_ratings = train_ratings[train_ratings.nonzero()]\n",
    "        \n",
    "        # calculate the mean if the number of elements is not 0\n",
    "        if nonzeros_train_ratings.shape[0] != 0:\n",
    "            item_train_mean = nonzeros_train_ratings.mean()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # find the non-zero ratings for each user in the test dataset\n",
    "        test_ratings = test[item_index, :]\n",
    "        nonzeros_test_ratings = test_ratings[test_ratings.nonzero()].todense()\n",
    "        \n",
    "        # calculate the test error \n",
    "        mse += calculate_mse(nonzeros_test_ratings, item_train_mean)\n",
    "        \n",
    "    rmse = np.sqrt(1.0 * mse / test.nnz)\n",
    "    print(\"test RMSE of the baseline using the item mean: {v}.\".format(v=rmse))\n",
    "    \n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 999) (20, 9990)\n"
     ]
    }
   ],
   "source": [
    "def init_MF(train, num_features):\n",
    "    # num_features = k\n",
    "    \"\"\"init the parameter for matrix factorization.\n",
    "       Split X = W @ K \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # you should return:\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "    # ***************************************************\n",
    "    num_item, num_user = train.get_shape()\n",
    "    \n",
    "    # random init\n",
    "    user_features = np.random.rand(num_features, num_user) # k x n --> user.T => n x k \n",
    "    item_features = np.random.rand(num_features, num_item) # k x d\n",
    "    \n",
    "    # start by item features.\n",
    "    item_nnz = train.getnnz(axis=1) # Get the count of explicitly-stored values (nonzeros) - 1152\n",
    "    item_sum = train.sum(axis=1) # array of sum per item 1 (i.e. sum[0] = sum over all ratings of 943 reviewers for item 0)\n",
    "    \n",
    "    for ind in range(num_item):\n",
    "        item_features[0, ind] = item_sum[ind, 0] / item_nnz[ind] # W\n",
    "    \n",
    "    return user_features, item_features # W, Z\n",
    "\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "num_epochs = 20     # number of full passes through the train set\n",
    "errors = [0]\n",
    "\n",
    "# set seed\n",
    "np.random.seed(988)\n",
    "\n",
    "# init matrix\n",
    "user, item = init_MF(train, num_features)\n",
    "\n",
    "print(user.shape, item.shape) # W = 1152 * 20 Z = 20 * 943"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    # ***************************************************\n",
    "    mse = 0\n",
    "    for row, col in nz:\n",
    "        user_feature = user_features[:, col]\n",
    "        item_feature = item_features[:, row]\n",
    "        pred = user_feature.T.dot(item_feature) # same as item_feature.T (1152 x 20) * user_feature (20 x 934)\n",
    "        ground_truth = data[row, col]\n",
    "        mse += (ground_truth - pred)**2\n",
    "    return np.sqrt(1.0 * mse / len(nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f264295a7b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnz_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnz_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (item, user) of non-zero ratings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcompute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'user_features' is not defined"
     ]
    }
   ],
   "source": [
    "nz_row, nz_col = train.nonzero()\n",
    "nz_train = list(zip(nz_row, nz_col)) # (item, user) of non-zero ratings\n",
    "compute_error(train, user_features, item_features, nz_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1188016390962718.\n",
      "iter: 1, RMSE on training set: 1.0557733846363557.\n",
      "iter: 2, RMSE on training set: 1.0352090447321285.\n",
      "iter: 3, RMSE on training set: 1.0305003556868906.\n",
      "iter: 4, RMSE on training set: 1.0289338540768067.\n",
      "iter: 5, RMSE on training set: 1.027541155288929.\n",
      "iter: 6, RMSE on training set: 1.0267280525334728.\n",
      "iter: 7, RMSE on training set: 1.0257331157481897.\n",
      "iter: 8, RMSE on training set: 1.024212863517716.\n",
      "iter: 9, RMSE on training set: 1.0250155714728297.\n",
      "iter: 10, RMSE on training set: 1.0250110976515387.\n",
      "iter: 11, RMSE on training set: 1.0247649821945568.\n",
      "iter: 12, RMSE on training set: 1.024444922433812.\n",
      "iter: 13, RMSE on training set: 1.0243942405531554.\n",
      "iter: 14, RMSE on training set: 1.0244300096163954.\n",
      "iter: 15, RMSE on training set: 1.0244421403898756.\n",
      "iter: 16, RMSE on training set: 1.0243492826217602.\n",
      "iter: 17, RMSE on training set: 1.0241854571274716.\n",
      "iter: 18, RMSE on training set: 1.0243454281607476.\n",
      "iter: 19, RMSE on training set: 1.0243365237565685.\n",
      "RMSE on test data: 1.034557674453581.\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorization_SGD(train, test):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO\n",
    "        # do matrix factorization.\n",
    "        # ***************************************************\n",
    "            # update W_d (item_features[:, d]) and Z_n (user_features[:, n])\n",
    "            item_info = item_features[:, d]\n",
    "            user_info = user_features[:, n]\n",
    "            err = train[d, n] - user_info.T.dot(item_info) # loss function\n",
    "\n",
    "    #         calculate the gradient and update\n",
    "            item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
    "            user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
    "\n",
    "        # WRONG! \n",
    "    #         item_features[:, d] =  item_features[:, d] - gamma * err * item_features[d, n]\n",
    "    #         user_features[:, n] =  user_features[:, n] - gamma * err * user_features[d, n]\n",
    "\n",
    "        rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "\n",
    "        errors.append(rmse)\n",
    "    # ***************************************************\n",
    "    # TODO\n",
    "    # evaluate the test error.\n",
    "    # ***************************************************\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    \n",
    "\n",
    "matrix_factorization_SGD(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_user[user] * lambda_user\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return user feature.\n",
    "    # ***************************************************\n",
    "    num_user = nnz_items_per_user.shape[0] # (943 users, 1 item)\n",
    "    num_feature = item_features.shape[0] # item_features = k * 1152 (D)\n",
    "    \n",
    "    lambda_I = lambda_user * sp.eye(num_feature) #sp.eye => Identity matrix - lambda * I_k\n",
    "    updated_user_features = np.zeros((num_feature, num_user))\n",
    "    \n",
    "    for user, items in nz_user_itemindices: # indices of all the items rated by user g\n",
    "        # extract the columns corresponding to the prediction for given item\n",
    "        M = item_features[:, items] # shape :  (20 (k), nb of items rated by user g)\n",
    "        # update column row of user features\n",
    "        V = M @ train[items, user] # train[items, user] shape: (nb of items rated by user g, 1) - W.T @ X_n\n",
    "        A = M @ M.T + nnz_items_per_user[user] * lambda_I # W.T @ W + Z (nnz_items_per_user - (943, 1)) * lambda * I_k\n",
    "        X = np.linalg.solve(A, V)\n",
    "        updated_user_features[:, user] = np.copy(X.T) # for one user, go through all items\n",
    "    return updated_user_features\n",
    "    \n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_item[item] * lambda_item\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return item feature.\n",
    "    # ***************************************************\n",
    "    num_item = nnz_users_per_item.shape[0] # (1152 (items), for 1 user)\n",
    "    num_feature = user_features.shape[0]\n",
    "    \n",
    "    lambda_I = lambda_item * sp.eye(num_feature)\n",
    "    updated_item_features = np.zeros((num_feature, num_item)) # inversed from the derive per user above\n",
    "    \n",
    "    for item, users in nz_item_userindices: # indices of all the users who rated item g\n",
    "        # extract the rows corresponding to the prediction of given user\n",
    "        M = user_features[:, users] # shape: (k (20), nb of users)\n",
    "        # update row column of item features\n",
    "        V = M @ train[item, users].T \n",
    "        A = M @ M.T + nnz_users_per_item[item] * lambda_I\n",
    "        X = np.linalg.solve(A, V)\n",
    "        updated_item_features[:, item] = np.copy(X.T) # ????? for one item, go through all users\n",
    "    return updated_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'build_index_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c90c2d8a81fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_index_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'build_index_groups'"
     ]
    }
   ],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS(train, test):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # start you ALS-WR algorithm.\n",
    "    # ***************************************************\n",
    "    # get the number of non-zero ratings for each user and item\n",
    "    nnz_items_per_user, nnz_users_per_item = train.getnnz(axis=0), train.getnnz(axis=1)\n",
    "    \n",
    "#     print(nnz_items_per_user)\n",
    "    \n",
    "    # group the indices by row or column index\n",
    "    nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "\n",
    "    # run ALS\n",
    "    print(\"\\nstart the ALS algorithm...\")\n",
    "    while change > stop_criterion: # continue until no changes\n",
    "        # update user feature & item feature\n",
    "        user_features = update_user_feature(\n",
    "            train, item_features, lambda_user,\n",
    "            nnz_items_per_user, nz_user_itemindices)\n",
    "        item_features = update_item_feature(\n",
    "            train, user_features, lambda_item,\n",
    "            nnz_users_per_item, nz_item_userindices)\n",
    "\n",
    "        error = compute_error(train, user_features, item_features, nz_train)\n",
    "        print(\"RMSE on training set: {}.\".format(error))\n",
    "        error_list.append(error)\n",
    "        change = np.fabs(error_list[-1] - error_list[-2]) \n",
    "        # Compute the absolute values element-wise. - Last with the previous last i.e. this error vs last error\n",
    "\n",
    "    # evaluate the test error\n",
    "    nnz_row, nnz_col = test.nonzero()\n",
    "    nnz_test = list(zip(nnz_row, nnz_col))\n",
    "    rmse = compute_error(test, user_features, item_features, nnz_test)\n",
    "    print(\"test RMSE after running ALS: {v}.\".format(v=rmse))\n",
    "\n",
    "\n",
    "ALS(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features, item_features = init_MF(train, num_features)\n",
    "print(item_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

Epoch 0: loss 16.129313530385204
Epoch 1: loss 16.12932110403205
Epoch 2: loss 16.12931576003323
Epoch 3: loss 16.129317657269493
Epoch 4: loss 16.12931714102002
Epoch 5: loss 16.129317716128675
Epoch 6: loss 16.129318353726593
Epoch 7: loss 16.129320456581052
Epoch 8: loss 16.12932576790914
Epoch 9: loss 16.129315548192036
Epoch 10: loss 16.129312986650824
Epoch 11: loss 16.12931913963928
Epoch 12: loss 16.129320014488965
Epoch 13: loss 16.129319441973223
Epoch 14: loss 16.12931240039263
Epoch 15: loss 16.12931752451222
Epoch 16: loss 16.12931622494298
Epoch 17: loss 16.129315871269306
Epoch 18: loss 16.12931950679611
Epoch 19: loss 16.129321931949967
-----------Time: 0:01:19.491304, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 20, rmse: 4.017201900482178-------------


Epoch 0: loss 16.129207040123994
Epoch 1: loss 16.129214894065008
Epoch 2: loss 16.12921647652133
Epoch 3: loss 16.129213706509713
Epoch 4: loss 16.129210767440007
Epoch 5: loss 16.129217583696242
Epoch 6: loss 16.129208419555034
Epoch 7: loss 16.12921608991763
Epoch 8: loss 16.129217014810585
Epoch 9: loss 16.129217067446767
Epoch 10: loss 16.129221560450727
Epoch 11: loss 16.129210899419405
Epoch 12: loss 16.129215198473286
Epoch 13: loss 16.12921618248471
Epoch 14: loss 16.129211192159566
Epoch 15: loss 16.129217946963703
Epoch 16: loss 16.12921549406565
Epoch 17: loss 16.12921747842387
Epoch 18: loss 16.12921059008459
Epoch 19: loss 16.12921475093607
-----------Time: 0:01:32.673628, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 50, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129169027723652
Epoch 1: loss 16.129172674918575
Epoch 2: loss 16.129172895057103
Epoch 3: loss 16.129180873198766
Epoch 4: loss 16.129175900764736
Epoch 5: loss 16.129172053396733
Epoch 6: loss 16.129174131618498
Epoch 7: loss 16.129177587456258
Epoch 8: loss 16.129169592201354
Epoch 9: loss 16.129177606643836
Epoch 10: loss 16.12917536766131
Epoch 11: loss 16.129171067051683
Epoch 12: loss 16.129169581311107
Epoch 13: loss 16.129171291598166
Epoch 14: loss 16.12917048234924
Epoch 15: loss 16.12917198753668
Epoch 16: loss 16.129173006293176
Epoch 17: loss 16.129165795654497
Epoch 18: loss 16.129177389357515
Epoch 19: loss 16.129166160736997
-----------Time: 0:02:01.638252, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129172173448723
Epoch 1: loss 16.129175513901743
Epoch 2: loss 16.129170582176485
Epoch 3: loss 16.129172410441196
Epoch 4: loss 16.129166816226032
Epoch 5: loss 16.129178525832376
Epoch 6: loss 16.129177576566015
Epoch 7: loss 16.129168346824045
Epoch 8: loss 16.129171161693098
Epoch 9: loss 16.129170687189564
Epoch 10: loss 16.12917051709431
Epoch 11: loss 16.129161862460993
Epoch 12: loss 16.129174208887378
Epoch 13: loss 16.129171775954777
Epoch 14: loss 16.12917319816892
Epoch 15: loss 16.12917791260786
Epoch 16: loss 16.129173785723573
Epoch 17: loss 16.12917061744014
Epoch 18: loss 16.129174976131072
Epoch 19: loss 16.129175150893577
-----------Time: 0:02:30.206764, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129175885725825
Epoch 1: loss 16.129176469909684
Epoch 2: loss 16.12917477336508
Epoch 3: loss 16.129178900508663
Epoch 4: loss 16.129176921595562
Epoch 5: loss 16.12917264198855
Epoch 6: loss 16.12917033662739
Epoch 7: loss 16.1291763436347
Epoch 8: loss 16.129169485632527
Epoch 9: loss 16.129172061953355
Epoch 10: loss 16.12916941069727
Epoch 11: loss 16.12917285331116
Epoch 12: loss 16.12917261372577
Epoch 13: loss 16.129179649083365
Epoch 14: loss 16.129171437320014
Epoch 15: loss 16.12917426074569
Epoch 16: loss 16.129176259364947
Epoch 17: loss 16.129167241982756
Epoch 18: loss 16.12916945399896
Epoch 19: loss 16.129171141209067
-----------Time: 0:03:00.073002, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129479018548963
Epoch 1: loss 16.12947837032009
Epoch 2: loss 16.129478785705153
Epoch 3: loss 16.12947994577554
Epoch 4: loss 16.129476843870744
Epoch 5: loss 16.12947650497669
Epoch 6: loss 16.12947509598641
Epoch 7: loss 16.12947965951767
Epoch 8: loss 16.129476239980725
Epoch 9: loss 16.12947972822993
Epoch 10: loss 16.129484356324784
Epoch 11: loss 16.12947878026003
Epoch 12: loss 16.12948525243638
Epoch 13: loss 16.129485369117575
Epoch 14: loss 16.12947840247224
Epoch 15: loss 16.129477403940488
Epoch 16: loss 16.129480671532587
Epoch 17: loss 16.129481165482986
Epoch 18: loss 16.129478263491972
Epoch 19: loss 16.129480878965825
-----------Time: 0:01:39.172236, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 20, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129181792646598
Epoch 1: loss 16.129180936206613
Epoch 2: loss 16.129182091869044
Epoch 3: loss 16.129183237937692

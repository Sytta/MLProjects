Epoch 0: loss 16.129381063387665
Epoch 1: loss 16.129384764515233
Epoch 2: loss 16.129386544292423
Epoch 3: loss 16.129383436423918
Epoch 4: loss 16.129383807470127
Epoch 5: loss 16.129387414993445
Epoch 6: loss 16.12938087125263
Epoch 7: loss 16.12937812587371
Epoch 8: loss 16.129385018880242
Epoch 9: loss 16.129381997355825
Epoch 10: loss 16.1293793964023
Epoch 11: loss 16.129384604273056
Epoch 12: loss 16.129381157251206
Epoch 13: loss 16.129378521034027
Epoch 14: loss 16.12938766883987
Epoch 15: loss 16.12939497334209
Epoch 16: loss 16.129383180762453
Epoch 17: loss 16.129393291317815
Epoch 18: loss 16.12938620902845
Epoch 19: loss 16.129386562702123
-----------Time: 0:01:20.849174, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 20, rmse: 4.017219066619873-------------


Epoch 0: loss 16.129205321021026
Epoch 1: loss 16.129205983510932
Epoch 2: loss 16.129210258191407
Epoch 3: loss 16.129213603311676
Epoch 4: loss 16.12920957573605
Epoch 5: loss 16.129204989387134
Epoch 6: loss 16.12921046432819
Epoch 7: loss 16.129209705900408
Epoch 8: loss 16.129205875127067
Epoch 9: loss 16.12920335948046
Epoch 10: loss 16.12920754522393
Epoch 11: loss 16.12920530675999
Epoch 12: loss 16.129210634423444
Epoch 13: loss 16.12920069111113
Epoch 14: loss 16.129199662760847
Epoch 15: loss 16.129204937528826
Epoch 16: loss 16.12920369967097
Epoch 17: loss 16.129208910653226
Epoch 18: loss 16.12921203667214
Epoch 19: loss 16.129203931736907
-----------Time: 0:01:32.940739, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12916137188138
Epoch 1: loss 16.129168088569664
Epoch 2: loss 16.12917376368379
Epoch 3: loss 16.12916735840466
Epoch 4: loss 16.129172683215906
Epoch 5: loss 16.129168800324962
Epoch 6: loss 16.12917095788994
Epoch 7: loss 16.129163895306736
Epoch 8: loss 16.129167121152893
Epoch 9: loss 16.129166371541025
Epoch 10: loss 16.129175102665346
Epoch 11: loss 16.12916931683373
Epoch 12: loss 16.129168307411728
Epoch 13: loss 16.129170600845477
Epoch 14: loss 16.12916716367671
Epoch 15: loss 16.12917304518691
Epoch 16: loss 16.12916667024489
Epoch 17: loss 16.129166149068876
Epoch 18: loss 16.12916491924906
Epoch 19: loss 16.12916588511008
-----------Time: 0:02:01.652500, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129176518915788
Epoch 1: loss 16.12917303222233
Epoch 2: loss 16.129171047345526
Epoch 3: loss 16.12917160897102
Epoch 4: loss 16.129175354956026
Epoch 5: loss 16.12917257846212
Epoch 6: loss 16.129167675518225
Epoch 7: loss 16.12917382228368
Epoch 8: loss 16.129170927812122
Epoch 9: loss 16.129183333875563
Epoch 10: loss 16.129178395668017
Epoch 11: loss 16.129171035677405
Epoch 12: loss 16.129170409229022
Epoch 13: loss 16.12917602029814
Epoch 14: loss 16.12916983749116
Epoch 15: loss 16.129163602307283
Epoch 16: loss 16.1291714285041
Epoch 17: loss 16.129169981138677
Epoch 18: loss 16.129180063949843
Epoch 19: loss 16.129167994706123
-----------Time: 0:02:30.039175, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 150, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129172187191173
Epoch 1: loss 16.129171873189108
Epoch 2: loss 16.1291704437148
Epoch 3: loss 16.12917105823577
Epoch 4: loss 16.12917390629414
Epoch 5: loss 16.129179989273876
Epoch 6: loss 16.129169624872087
Epoch 7: loss 16.12917118502934
Epoch 8: loss 16.129170501536816
Epoch 9: loss 16.129178938105937
Epoch 10: loss 16.129171301710535
Epoch 11: loss 16.129176225397753
Epoch 12: loss 16.12917284319879
Epoch 13: loss 16.12917237388109
Epoch 14: loss 16.12917372893872
Epoch 15: loss 16.129175984515904
Epoch 16: loss 16.12917463749631
Epoch 17: loss 16.129179368270616
Epoch 18: loss 16.12917019116483
Epoch 19: loss 16.129175411740874
-----------Time: 0:02:59.909700, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129421554615213
Epoch 1: loss 16.129432274246422
Epoch 2: loss 16.129435465606807
Epoch 3: loss 16.12942957372494
Epoch 4: loss 16.129432459121297
Epoch 5: loss 16.129431362058753
Epoch 6: loss 16.129431903200217
Epoch 7: loss 16.129430009594035
Epoch 8: loss 16.129425010193685
Epoch 9: loss 16.129426304317803
Epoch 10: loss 16.12942880285117
Epoch 11: loss 16.12942842843417
Epoch 12: loss 16.12942853707733
Epoch 13: loss 16.129424164384652
Epoch 14: loss 16.129427548657947
Epoch 15: loss 16.129432472085874
Epoch 16: loss 16.129430056266514
Epoch 17: loss 16.129430741055494
Epoch 18: loss 16.129431573381368
Epoch 19: loss 16.12943057925757
-----------Time: 0:01:17.018840, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 20, rmse: 4.017233371734619-------------


Epoch 0: loss 16.129178489272267
Epoch 1: loss 16.129175375440056
Epoch 2: loss 16.129185830334595
Epoch 3: loss 16.129184894292106
Epoch 4: loss 16.12917692937431
Epoch 5: loss 16.12918288452331
Epoch 6: loss 16.129185489366208
Epoch 7: loss 16.12918294130816
Epoch 8: loss 16.129185332754112
Epoch 9: loss 16.129177268786947
Epoch 10: loss 16.12918864546294
Epoch 11: loss 16.1291754783788
Epoch 12: loss 16.129178853576892
Epoch 13: loss 16.129178431709544
Epoch 14: loss 16.12917807077571
Epoch 15: loss 16.129181266544045
Epoch 16: loss 16.129180423327927
Epoch 17: loss 16.129180146923137
Epoch 18: loss 16.129178231017885
Epoch 19: loss 16.12918456395467
-----------Time: 0:01:32.775145, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 50, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129167578024603
Epoch 1: loss 16.129172619430186
Epoch 2: loss 16.129174767142082
Epoch 3: loss 16.129170334553056
Epoch 4: loss 16.129173916925094
Epoch 5: loss 16.129163435582818
Epoch 6: loss 16.12917093740591
Epoch 7: loss 16.129175767229587
Epoch 8: loss 16.12916736696128
Epoch 9: loss 16.12916830455952
Epoch 10: loss 16.129168050713094
Epoch 11: loss 16.12917447232759
Epoch 12: loss 16.129167252095126
Epoch 13: loss 16.12917263395051
Epoch 14: loss 16.129169862123856
Epoch 15: loss 16.129172406292533
Epoch 16: loss 16.12917315927519
Epoch 17: loss 16.129168880964635
Epoch 18: loss 16.129177896531786
Epoch 19: loss 16.12917088736264
-----------Time: 0:02:01.608649, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129178343550414
Epoch 1: loss 16.12917941053514
Epoch 2: loss 16.129176382787723
Epoch 3: loss 16.12917886809722
Epoch 4: loss 16.129181330329768
Epoch 5: loss 16.129176468613228
Epoch 6: loss 16.129178680110847
Epoch 7: loss 16.129177278380734
Epoch 8: loss 16.12917843974758
Epoch 9: loss 16.129172657546043
Epoch 10: loss 16.129181490312654
Epoch 11: loss 16.129178630067578
Epoch 12: loss 16.129180812524545
Epoch 13: loss 16.12918298020189
Epoch 14: loss 16.12917412824771
Epoch 15: loss 16.129183015724834
Epoch 16: loss 16.129180365246622
Epoch 17: loss 16.129187267069067
Epoch 18: loss 16.129184290142796
Epoch 19: loss 16.129180993250753
-----------Time: 0:02:29.759421, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129175059622952
Epoch 1: loss 16.129176459797314
Epoch 2: loss 16.129182038714276
Epoch 3: loss 16.129170665927656
Epoch 4: loss 16.12917241225624
Epoch 5: loss 16.12917401623376
Epoch 6: loss 16.129174670426337
Epoch 7: loss 16.129172537234766
Epoch 8: loss 16.129172641210676
Epoch 9: loss 16.129170499981065
Epoch 10: loss 16.129174359276476
Epoch 11: loss 16.12917235106343
Epoch 12: loss 16.129174720469607
Epoch 13: loss 16.129171671200993
Epoch 14: loss 16.129180881236802
Epoch 15: loss 16.129175073883985
Epoch 16: loss 16.12916700887965
Epoch 17: loss 16.12917591658152
Epoch 18: loss 16.129174159103403
Epoch 19: loss 16.12917984303344
-----------Time: 0:02:59.613333, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129292427423923
Epoch 1: loss 16.12929278161618
Epoch 2: loss 16.129294199163073
Epoch 3: loss 16.129290487923136
Epoch 4: loss 16.12929846762055
Epoch 5: loss 16.129290159141455
Epoch 6: loss 16.12929464721887
Epoch 7: loss 16.129290639608694
Epoch 8: loss 16.12929374644003
Epoch 9: loss 16.129298707983818
Epoch 10: loss 16.1292935755669
Epoch 11: loss 16.129296372026253
Epoch 12: loss 16.129299186636015
Epoch 13: loss 16.129302095887194
Epoch 14: loss 16.129297110488583
Epoch 15: loss 16.12929423001877
Epoch 16: loss 16.129298581968126
Epoch 17: loss 16.129292052488342
Epoch 18: loss 16.129293592939433
Epoch 19: loss 16.129304744550364
-----------Time: 0:01:16.183336, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 20, rmse: 4.017242431640625-------------


Epoch 0: loss 16.12923736738212
Epoch 1: loss 16.12922844023339
Epoch 2: loss 16.129236342661923
Epoch 3: loss 16.129231129864625
Epoch 4: loss 16.129224503409805
Epoch 5: loss 16.129230684142453
Epoch 6: loss 16.12922918180722
Epoch 7: loss 16.129231364264186
Epoch 8: loss 16.129228513612897
Epoch 9: loss 16.129226691830475
Epoch 10: loss 16.129233830386106
Epoch 11: loss 16.129232323383622
Epoch 12: loss 16.12923272450765
Epoch 13: loss 16.129227675841904
Epoch 14: loss 16.129224782926094
Epoch 15: loss 16.12922600600433
Epoch 16: loss 16.1292323516464
Epoch 17: loss 16.129233957438963
Epoch 18: loss 16.129227237898476
Epoch 19: loss 16.129232559598226
-----------Time: 0:01:32.726763, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 50, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129174276821765
Epoch 1: loss 16.12917240758899
Epoch 2: loss 16.129180863345688
Epoch 3: loss 16.129186112962383
Epoch 4: loss 16.12917784830356
Epoch 5: loss 16.129183613651144
Epoch 6: loss 16.129182290486373
Epoch 7: loss 16.129187374156476
Epoch 8: loss 16.12917446688247
Epoch 9: loss 16.129180544417082
Epoch 10: loss 16.12918447475838
Epoch 11: loss 16.129185014862674
Epoch 12: loss 16.12918276265628
Epoch 13: loss 16.129176144758084
Epoch 14: loss 16.129177381060188
Epoch 15: loss 16.12918108503996
Epoch 16: loss 16.129177869565464
Epoch 17: loss 16.129177188925148
Epoch 18: loss 16.12917507621761
Epoch 19: loss 16.129182184176837
-----------Time: 0:02:01.405518, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129177386505308
Epoch 1: loss 16.129178420819297
Epoch 2: loss 16.12918301702129
Epoch 3: loss 16.12917304518691
Epoch 4: loss 16.129178617621584
Epoch 5: loss 16.129178647958696
Epoch 6: loss 16.12917562228561
Epoch 7: loss 16.12917954173666
Epoch 8: loss 16.129178598174718
Epoch 9: loss 16.129173590477034
Epoch 10: loss 16.1291752053448
Epoch 11: loss 16.12917884527956
Epoch 12: loss 16.129172927987128
Epoch 13: loss 16.12917625340124
Epoch 14: loss 16.12917240447749
Epoch 15: loss 16.12917997682788
Epoch 16: loss 16.12918140630219
Epoch 17: loss 16.12918021382036
Epoch 18: loss 16.129179260146042
Epoch 19: loss 16.129173187019386
-----------Time: 0:02:29.839327, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129171728504424
Epoch 1: loss 16.1291787275612
Epoch 2: loss 16.12917971416554
Epoch 3: loss 16.12917831554693
Epoch 4: loss 16.12917592306381
Epoch 5: loss 16.129172289352045
Epoch 6: loss 16.12917713343676
Epoch 7: loss 16.129168494101645
Epoch 8: loss 16.129174847522464
Epoch 9: loss 16.129178025658977
Epoch 10: loss 16.129175083477772
Epoch 11: loss 16.129176503099004
Epoch 12: loss 16.129183125145868
Epoch 13: loss 16.129164362809398
Epoch 14: loss 16.12917788227075
Epoch 15: loss 16.129182688498897
Epoch 16: loss 16.129170883473265
Epoch 17: loss 16.129176339486037
Epoch 18: loss 16.129172797044895
Epoch 19: loss 16.129175624878528
-----------Time: 0:02:59.754609, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 200, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129226286298493
Epoch 1: loss 16.129219309022208
Epoch 2: loss 16.12921928801959
Epoch 3: loss 16.129222268575944
Epoch 4: loss 16.129219422851197
Epoch 5: loss 16.129214165974336
Epoch 6: loss 16.129215717834256
Epoch 7: loss 16.129221447658903
Epoch 8: loss 16.129222661921226
Epoch 9: loss 16.129215795881013
Epoch 10: loss 16.129217004957503
Epoch 11: loss 16.129215325266852
Epoch 12: loss 16.129209711086236
Epoch 13: loss 16.12922039752813
Epoch 14: loss 16.129225596842264
Epoch 15: loss 16.129217743160545
Epoch 16: loss 16.129222171600905
Epoch 17: loss 16.129224091914118
Epoch 18: loss 16.129220318184917
Epoch 19: loss 16.129216891128515
-----------Time: 0:01:16.143403, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 20, rmse: 4.0172200202941895-------------


Epoch 0: loss 16.12915656643111
Epoch 1: loss 16.12916297041378
Epoch 2: loss 16.129164029360467
Epoch 3: loss 16.129159554247625
Epoch 4: loss 16.12915878026235
Epoch 5: loss 16.129166576122056
Epoch 6: loss 16.129159568508662
Epoch 7: loss 16.129160503513987
Epoch 8: loss 16.1291610062803
Epoch 9: loss 16.129155442142952
Epoch 10: loss 16.12915689936146
Epoch 11: loss 16.12915995200086
Epoch 12: loss 16.129157483804608
Epoch 13: loss 16.129162413714827
Epoch 14: loss 16.12915473686994
Epoch 15: loss 16.129167894878876
Epoch 16: loss 16.12915728492799
Epoch 17: loss 16.1291603803505
Epoch 18: loss 16.12915677075285
Epoch 19: loss 16.129159528577762
-----------Time: 0:01:32.723585, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 50, rmse: 4.017195224761963-------------


Epoch 0: loss 16.129169145701308
Epoch 1: loss 16.129177001716652
Epoch 2: loss 16.129175244757118
Epoch 3: loss 16.129183700254522
Epoch 4: loss 16.129177633091572
Epoch 5: loss 16.129181172680504
Epoch 6: loss 16.12917824372317
Epoch 7: loss 16.12917848667935
Epoch 8: loss 16.12917520871559
Epoch 9: loss 16.12918004528085
Epoch 10: loss 16.129168963678637
Epoch 11: loss 16.12917315953448
Epoch 12: loss 16.12917332029524
Epoch 13: loss 16.129175147004204
Epoch 14: loss 16.129171206550534
Epoch 15: loss 16.129178524535916
Epoch 16: loss 16.129175763080923
Epoch 17: loss 16.12917180629189
Epoch 18: loss 16.129180854789066
Epoch 19: loss 16.129178035771346
-----------Time: 0:02:01.660720, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 100, rmse: 4.017192363739014-------------


Epoch 0: loss 16.12918041477131
Epoch 1: loss 16.12917043826968
Epoch 2: loss 16.12917032651502
Epoch 3: loss 16.129171618046225
Epoch 4: loss 16.129166581048597
Epoch 5: loss 16.1291685537387
Epoch 6: loss 16.129172139222238
Epoch 7: loss 16.129176237325165
Epoch 8: loss 16.12917043049093
Epoch 9: loss 16.12917160067369
Epoch 10: loss 16.129168608449216
Epoch 11: loss 16.129174930236466
Epoch 12: loss 16.129170394449407
Epoch 13: loss 16.129168017005195
Epoch 14: loss 16.12917133904852
Epoch 15: loss 16.129172135851448
Epoch 16: loss 16.129171562557833
Epoch 17: loss 16.12917190923063
Epoch 18: loss 16.129177911052114
Epoch 19: loss 16.12917042349006
-----------Time: 0:02:29.776084, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917310456467
Epoch 1: loss 16.129181370260664
Epoch 2: loss 16.1291755398309
Epoch 3: loss 16.129180240008804
Epoch 4: loss 16.129180196188532
Epoch 5: loss 16.129175458672645
Epoch 6: loss 16.129176908890276
Epoch 7: loss 16.12917979921317
Epoch 8: loss 16.12917257820283
Epoch 9: loss 16.12917343179061
Epoch 10: loss 16.129171689092107
Epoch 11: loss 16.12917228027684
Epoch 12: loss 16.12917686273638
Epoch 13: loss 16.129178186160445
Epoch 14: loss 16.129177454439695
Epoch 15: loss 16.129179498694267
Epoch 16: loss 16.12917573948539
Epoch 17: loss 16.12918381071272
Epoch 18: loss 16.129173601626572
Epoch 19: loss 16.12916954008375
-----------Time: 0:02:59.340042, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129296340651976
Epoch 1: loss 16.12930338975202
Epoch 2: loss 16.12929136614361
Epoch 3: loss 16.1292999357293
Epoch 4: loss 16.129288370807643
Epoch 5: loss 16.129292777467512
Epoch 6: loss 16.12929656545775
Epoch 7: loss 16.129292747389695
Epoch 8: loss 16.129301117061598
Epoch 9: loss 16.12929044799224
Epoch 10: loss 16.129294527426175
Epoch 11: loss 16.12929684704837
Epoch 12: loss 16.129297705044106
Epoch 13: loss 16.129291411260343
Epoch 14: loss 16.12930283694244
Epoch 15: loss 16.12928977175988
Epoch 16: loss 16.129297889400398
Epoch 17: loss 16.12929209916082
Epoch 18: loss 16.129300226135836
Epoch 19: loss 16.129291779713633
-----------Time: 0:01:16.040786, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 20, rmse: 4.017212867736816-------------


Epoch 0: loss 16.129170027551865
Epoch 1: loss 16.12917838244415
Epoch 2: loss 16.129170077595134
Epoch 3: loss 16.129168400238104
Epoch 4: loss 16.129168313634725
Epoch 5: loss 16.1291713395671
Epoch 6: loss 16.129179219177978
Epoch 7: loss 16.12916704829197
Epoch 8: loss 16.129170503611146
Epoch 9: loss 16.12916624137667
Epoch 10: loss 16.129176083305985
Epoch 11: loss 16.129172274572426
Epoch 12: loss 16.129167269986244
Epoch 13: loss 16.129173287365216
Epoch 14: loss 16.129178224276302
Epoch 15: loss 16.12916839453369
Epoch 16: loss 16.129181061963013
Epoch 17: loss 16.129171512773855
Epoch 18: loss 16.129166715880203
Epoch 19: loss 16.12917150862519
-----------Time: 0:01:32.722475, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 50, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129160940420245
Epoch 1: loss 16.12916930568419
Epoch 2: loss 16.12916164465609
Epoch 3: loss 16.129165869811878
Epoch 4: loss 16.129160208180913
Epoch 5: loss 16.129166483295684
Epoch 6: loss 16.12916105347136
Epoch 7: loss 16.129167116744938
Epoch 8: loss 16.12916663964849
Epoch 9: loss 16.12916412218684
Epoch 10: loss 16.129166123917596
Epoch 11: loss 16.129156796682004
Epoch 12: loss 16.12916146237413
Epoch 13: loss 16.129163458141182
Epoch 14: loss 16.129164188046893
Epoch 15: loss 16.12916412685409
Epoch 16: loss 16.129168511733468
Epoch 17: loss 16.129166445439118
Epoch 18: loss 16.12916628467836
Epoch 19: loss 16.129162308183165
-----------Time: 0:02:01.698605, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129175289355263
Epoch 1: loss 16.12918092790928
Epoch 2: loss 16.129184601551945
Epoch 3: loss 16.129181371297832
Epoch 4: loss 16.12917899878016
Epoch 5: loss 16.129186426705157
Epoch 6: loss 16.129181381410202
Epoch 7: loss 16.129177563342147
Epoch 8: loss 16.12917745184678
Epoch 9: loss 16.12917616057487
Epoch 10: loss 16.12917499920802
Epoch 11: loss 16.129179608893175
Epoch 12: loss 16.129177115804932
Epoch 13: loss 16.129180206560193
Epoch 14: loss 16.129179655824945
Epoch 15: loss 16.129168493064476
Epoch 16: loss 16.129175917877976
Epoch 17: loss 16.12917653317682
Epoch 18: loss 16.129176995752946
Epoch 19: loss 16.129181428601264
-----------Time: 0:02:29.822707, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129183852717954
Epoch 1: loss 16.129174059276156
Epoch 2: loss 16.129176808285155
Epoch 3: loss 16.12917547448943
Epoch 4: loss 16.12917228572196
Epoch 5: loss 16.12918391935588
Epoch 6: loss 16.129172952101243
Epoch 7: loss 16.12917620776593
Epoch 8: loss 16.129177863083175
Epoch 9: loss 16.129168603004096
Epoch 10: loss 16.12917435772073
Epoch 11: loss 16.12917259998332
Epoch 12: loss 16.129174900936523
Epoch 13: loss 16.129178309842516
Epoch 14: loss 16.129167812942747
Epoch 15: loss 16.12917916965329
Epoch 16: loss 16.12917767924547
Epoch 17: loss 16.129175373365726
Epoch 18: loss 16.129174991169982
Epoch 19: loss 16.12917384432346
-----------Time: 0:02:59.367522, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129266802936613
Epoch 1: loss 16.12926721909955
Epoch 2: loss 16.1292666714758
Epoch 3: loss 16.129267946153053
Epoch 4: loss 16.12926772264374
Epoch 5: loss 16.12926837942923
Epoch 6: loss 16.129260897830882
Epoch 7: loss 16.12926911918802
Epoch 8: loss 16.129269287208942
Epoch 9: loss 16.129261333181393
Epoch 10: loss 16.12927503544329
Epoch 11: loss 16.129267465945105
Epoch 12: loss 16.12927165350362
Epoch 13: loss 16.129265224369664
Epoch 14: loss 16.129263253494603
Epoch 15: loss 16.129269470268778
Epoch 16: loss 16.129270936303193
Epoch 17: loss 16.12926878418334
Epoch 18: loss 16.129266919099226
Epoch 19: loss 16.129275119972334
-----------Time: 0:01:16.309318, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 20, rmse: 4.017226219177246-------------


Epoch 0: loss 16.12917988322363
Epoch 1: loss 16.129178186160445
Epoch 2: loss 16.12917660318554
Epoch 3: loss 16.129179864036058
Epoch 4: loss 16.12917996127039
Epoch 5: loss 16.129176612260746
Epoch 6: loss 16.129177110100517
Epoch 7: loss 16.12917264873013
Epoch 8: loss 16.12917234769264
Epoch 9: loss 16.129170062556224
Epoch 10: loss 16.129171651494833
Epoch 11: loss 16.12918604710233
Epoch 12: loss 16.12918264804942
Epoch 13: loss 16.12917724596929
Epoch 14: loss 16.129176724274693
Epoch 15: loss 16.129183638802424
Epoch 16: loss 16.12917223982736
Epoch 17: loss 16.12917215452044
Epoch 18: loss 16.129174291860675
Epoch 19: loss 16.129179910449245
-----------Time: 0:01:32.605532, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 50, rmse: 4.017176151275635-------------


Epoch 0: loss 16.129160389944285
Epoch 1: loss 16.129163421581076
Epoch 2: loss 16.12916337205639
Epoch 3: loss 16.129162368338804
Epoch 4: loss 16.12916341224658
Epoch 5: loss 16.129169413030894
Epoch 6: loss 16.129171620120555
Epoch 7: loss 16.12916585658801
Epoch 8: loss 16.129160024343204
Epoch 9: loss 16.129165646561855
Epoch 10: loss 16.129167727376533
Epoch 11: loss 16.129162033852705
Epoch 12: loss 16.129163925384553
Epoch 13: loss 16.12916831285685
Epoch 14: loss 16.129162225209868
Epoch 15: loss 16.129160473176874
Epoch 16: loss 16.129161658657836
Epoch 17: loss 16.12916042105927
Epoch 18: loss 16.129168275000286
Epoch 19: loss 16.1291706646312
-----------Time: 0:02:01.265418, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129178815461035
Epoch 1: loss 16.12917158407903
Epoch 2: loss 16.129178044068677
Epoch 3: loss 16.12917728797452
Epoch 4: loss 16.129174417617076
Epoch 5: loss 16.129169160999506
Epoch 6: loss 16.12917993119257
Epoch 7: loss 16.129169160480924
Epoch 8: loss 16.12916625667487
Epoch 9: loss 16.129174114245963
Epoch 10: loss 16.129179235513345
Epoch 11: loss 16.129169383730947
Epoch 12: loss 16.129170301882322
Epoch 13: loss 16.12917235054485
Epoch 14: loss 16.129180647096536
Epoch 15: loss 16.1291750212478
Epoch 16: loss 16.12917318416718
Epoch 17: loss 16.129174923494887
Epoch 18: loss 16.12917676679851
Epoch 19: loss 16.1291713395671
-----------Time: 0:02:30.031478, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129177990654618
Epoch 1: loss 16.129166561861023
Epoch 2: loss 16.129180294200737
Epoch 3: loss 16.129178233351507
Epoch 4: loss 16.129177019089184
Epoch 5: loss 16.12917991926516
Epoch 6: loss 16.129181800684634
Epoch 7: loss 16.12917896014572
Epoch 8: loss 16.1291753311012
Epoch 9: loss 16.129172365324468
Epoch 10: loss 16.12917741710171
Epoch 11: loss 16.129180092471913
Epoch 12: loss 16.12917771476841
Epoch 13: loss 16.129175872501957
Epoch 14: loss 16.129170524873054
Epoch 15: loss 16.129175542683107
Epoch 16: loss 16.129177414768087
Epoch 17: loss 16.129173110269086
Epoch 18: loss 16.129173555731967
Epoch 19: loss 16.129174635421975
-----------Time: 0:02:59.372147, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129400185880097
Epoch 1: loss 16.12939830342345
Epoch 2: loss 16.129396930993284
Epoch 3: loss 16.129406616829087
Epoch 4: loss 16.1293989462072
Epoch 5: loss 16.129398694694398
Epoch 6: loss 16.129410217610825
Epoch 7: loss 16.129394160463086
Epoch 8: loss 16.129397180691047
Epoch 9: loss 16.129400004894595
Epoch 10: loss 16.129407829794953
Epoch 11: loss 16.129402954595253
Epoch 12: loss 16.12940169651266
Epoch 13: loss 16.12940597767542
Epoch 14: loss 16.129396630215087
Epoch 15: loss 16.129400621230605
Epoch 16: loss 16.12940082788597
Epoch 17: loss 16.129403574042765
Epoch 18: loss 16.1294018456053
Epoch 19: loss 16.129400285966632
-----------Time: 0:01:16.441219, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 20, rmse: 4.017195224761963-------------


Epoch 0: loss 16.12918634191682
Epoch 1: loss 16.12918276706424
Epoch 2: loss 16.12918531045504
Epoch 3: loss 16.12918021589469
Epoch 4: loss 16.129187174761277
Epoch 5: loss 16.12918661443224
Epoch 6: loss 16.129183259199596
Epoch 7: loss 16.129187847622845
Epoch 8: loss 16.12918062583463
Epoch 9: loss 16.129186876575996
Epoch 10: loss 16.129188250561914
Epoch 11: loss 16.12918713871975
Epoch 12: loss 16.129184635259847
Epoch 13: loss 16.129183444852348
Epoch 14: loss 16.129190119276103
Epoch 15: loss 16.12918378971011
Epoch 16: loss 16.129185046496243
Epoch 17: loss 16.129185778216993
Epoch 18: loss 16.129188326275045
Epoch 19: loss 16.129185061794445
-----------Time: 0:01:32.477754, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 50, rmse: 4.017178535461426-------------


Epoch 0: loss 16.12917158356045
Epoch 1: loss 16.12917114198694
Epoch 2: loss 16.12917516515461
Epoch 3: loss 16.129177408804384
Epoch 4: loss 16.129169783817808
Epoch 5: loss 16.1291739044791
Epoch 6: loss 16.129171887709436
Epoch 7: loss 16.129172016318044
Epoch 8: loss 16.129166398248056
Epoch 9: loss 16.129171986499514
Epoch 10: loss 16.129174782180993
Epoch 11: loss 16.129172305946703
Epoch 12: loss 16.129177289789563
Epoch 13: loss 16.129177329201877
Epoch 14: loss 16.129169068951008
Epoch 15: loss 16.12917556524147
Epoch 16: loss 16.129170647517956
Epoch 17: loss 16.129175941214218
Epoch 18: loss 16.129170653481662
Epoch 19: loss 16.12917101960133
-----------Time: 0:02:01.281320, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.12917394933654
Epoch 1: loss 16.129173557287718
Epoch 2: loss 16.129177254007327
Epoch 3: loss 16.129173599292947
Epoch 4: loss 16.12917946628282
Epoch 5: loss 16.12917196394115
Epoch 6: loss 16.129164482861384
Epoch 7: loss 16.129168861258478
Epoch 8: loss 16.12916699332216
Epoch 9: loss 16.129170684337357
Epoch 10: loss 16.129176135164297
Epoch 11: loss 16.12917850794126
Epoch 12: loss 16.129174036717792
Epoch 13: loss 16.1291793166716
Epoch 14: loss 16.12917863395695
Epoch 15: loss 16.129169807672632
Epoch 16: loss 16.129172905688055
Epoch 17: loss 16.129169132736727
Epoch 18: loss 16.129172293241417
Epoch 19: loss 16.129176515285707
-----------Time: 0:02:29.837566, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129171747692
Epoch 1: loss 16.129168590298807
Epoch 2: loss 16.129170352962756
Epoch 3: loss 16.129182411834815
Epoch 4: loss 16.129171376905084
Epoch 5: loss 16.12917029695578
Epoch 6: loss 16.1291717998096
Epoch 7: loss 16.129171975868562
Epoch 8: loss 16.129170894622803
Epoch 9: loss 16.12917399626831
Epoch 10: loss 16.12916652141154
Epoch 11: loss 16.129168245700342
Epoch 12: loss 16.129173292810336
Epoch 13: loss 16.129168873704472
Epoch 14: loss 16.129171450543883
Epoch 15: loss 16.129172040172865
Epoch 16: loss 16.129174338533154
Epoch 17: loss 16.129171936974828
Epoch 18: loss 16.129179706646088
Epoch 19: loss 16.12917375538646
-----------Time: 0:02:59.238162, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12929299319808
Epoch 1: loss 16.12928934833678
Epoch 2: loss 16.129298691389156
Epoch 3: loss 16.129295204954992
Epoch 4: loss 16.129291434855872
Epoch 5: loss 16.129296777298944
Epoch 6: loss 16.12929704592499
Epoch 7: loss 16.129298264595267
Epoch 8: loss 16.129298272114724
Epoch 9: loss 16.129293956984768
Epoch 10: loss 16.12929292578228
Epoch 11: loss 16.129293870640684
Epoch 12: loss 16.129299341951654
Epoch 13: loss 16.129295958974815
Epoch 14: loss 16.129303256735458
Epoch 15: loss 16.129300383007223
Epoch 16: loss 16.12929392042466
Epoch 17: loss 16.129299138148497
Epoch 18: loss 16.129293511003304
Epoch 19: loss 16.1292954839527
-----------Time: 0:01:16.332804, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 20, rmse: 4.017221450805664-------------


Epoch 0: loss 16.12918768115767
Epoch 1: loss 16.129186928175013
Epoch 2: loss 16.12918899213574
Epoch 3: loss 16.129185754362172
Epoch 4: loss 16.129193278225042
Epoch 5: loss 16.129184490575163
Epoch 6: loss 16.129186968105913
Epoch 7: loss 16.12918912904168
Epoch 8: loss 16.129190157910546
Epoch 9: loss 16.129186490750172
Epoch 10: loss 16.129187438460782
Epoch 11: loss 16.129186379254804
Epoch 12: loss 16.129191860418853
Epoch 13: loss 16.129184556175925
Epoch 14: loss 16.12918671866744
Epoch 15: loss 16.12919021884406
Epoch 16: loss 16.12918338340025
Epoch 17: loss 16.129189372775734
Epoch 18: loss 16.129188881677543
Epoch 19: loss 16.12919146448066
-----------Time: 0:01:32.567117, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 50, rmse: 4.017193794250488-------------


Epoch 0: loss 16.129188056093252
Epoch 1: loss 16.12918535375673
Epoch 2: loss 16.129186859981335
Epoch 3: loss 16.129188994210075
Epoch 4: loss 16.129187176576316
Epoch 5: loss 16.129187984788075
Epoch 6: loss 16.129187682454127
Epoch 7: loss 16.129185729729475
Epoch 8: loss 16.129186257647067
Epoch 9: loss 16.129189479863143
Epoch 10: loss 16.129189578653225
Epoch 11: loss 16.12918959006205
Epoch 12: loss 16.129181095411624
Epoch 13: loss 16.129185185735807
Epoch 14: loss 16.12918125746884
Epoch 15: loss 16.129186255572737
Epoch 16: loss 16.129181878990682
Epoch 17: loss 16.129191308646437
Epoch 18: loss 16.129189853502265
Epoch 19: loss 16.129187908815652
-----------Time: 0:02:01.694727, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12917689981507
Epoch 1: loss 16.129174478291297
Epoch 2: loss 16.12917728097365
Epoch 3: loss 16.129179885557257
Epoch 4: loss 16.129175385293134
Epoch 5: loss 16.129169870161892
Epoch 6: loss 16.12917839592731
Epoch 7: loss 16.12916939591765
Epoch 8: loss 16.129174367055224
Epoch 9: loss 16.12917982254941
Epoch 10: loss 16.129177022459974
Epoch 11: loss 16.129176366452356
Epoch 12: loss 16.129177752884267
Epoch 13: loss 16.12917248122779
Epoch 14: loss 16.129178122634016
Epoch 15: loss 16.129177054093542
Epoch 16: loss 16.129181054962142
Epoch 17: loss 16.129180201892947
Epoch 18: loss 16.129178375443278
Epoch 19: loss 16.129178060922627
-----------Time: 0:02:29.961983, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129175187712974
Epoch 1: loss 16.129173607330987
Epoch 2: loss 16.129172860571327
Epoch 3: loss 16.129175073624694
Epoch 4: loss 16.12918232082348
Epoch 5: loss 16.129180655653155
Epoch 6: loss 16.12917826057712
Epoch 7: loss 16.129172069732103
Epoch 8: loss 16.129176192467728
Epoch 9: loss 16.1291713579768
Epoch 10: loss 16.129168194879195
Epoch 11: loss 16.12917824553821
Epoch 12: loss 16.12917570448103
Epoch 13: loss 16.129176413384126
Epoch 14: loss 16.129178337068126
Epoch 15: loss 16.129179375271487
Epoch 16: loss 16.12917691303894
Epoch 17: loss 16.129175311135754
Epoch 18: loss 16.12917634856124
Epoch 19: loss 16.129173355818185
-----------Time: 0:02:59.760943, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129154059859708
Epoch 1: loss 16.129154784320296
Epoch 2: loss 16.12916028026396
Epoch 3: loss 16.129163880527116
Epoch 4: loss 16.12915935796392
Epoch 5: loss 16.129156394520813
Epoch 6: loss 16.129162727976183
Epoch 7: loss 16.129159263322506
Epoch 8: loss 16.129155123732932
Epoch 9: loss 16.129165028670094
Epoch 10: loss 16.12915704041606
Epoch 11: loss 16.129159668335905
Epoch 12: loss 16.129154364008695
Epoch 13: loss 16.129161208527705
Epoch 14: loss 16.129157152430007
Epoch 15: loss 16.129159721749964
Epoch 16: loss 16.12916689479137
Epoch 17: loss 16.129161198674627
Epoch 18: loss 16.129155662022185
Epoch 19: loss 16.129158613797177
-----------Time: 0:01:16.634105, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 20, rmse: 4.017202377319336-------------


Epoch 0: loss 16.129224637722828
Epoch 1: loss 16.129220978081907
Epoch 2: loss 16.12922824472756
Epoch 3: loss 16.129220934520927
Epoch 4: loss 16.129230696069865
Epoch 5: loss 16.129221293899015
Epoch 6: loss 16.129223108680563
Epoch 7: loss 16.12922858984461
Epoch 8: loss 16.12922402372044
Epoch 9: loss 16.129219777821326
Epoch 10: loss 16.12922773236746
Epoch 11: loss 16.129224605829965
Epoch 12: loss 16.129223310927973
Epoch 13: loss 16.129224354835745
Epoch 14: loss 16.129219190007387
Epoch 15: loss 16.129224709028
Epoch 16: loss 16.12922014497816
Epoch 17: loss 16.12921766044654
Epoch 18: loss 16.1292209015909
Epoch 19: loss 16.12922051524649
-----------Time: 0:01:32.970384, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 50, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12916424794324
Epoch 1: loss 16.129165631782236
Epoch 2: loss 16.12916090619376
Epoch 3: loss 16.129160099537753
Epoch 4: loss 16.129168922710573
Epoch 5: loss 16.12916206289336
Epoch 6: loss 16.129169347430132
Epoch 7: loss 16.129159843616993
Epoch 8: loss 16.129168752096735
Epoch 9: loss 16.129167537834412
Epoch 10: loss 16.129165018557725
Epoch 11: loss 16.129160060903313
Epoch 12: loss 16.12916183212388
Epoch 13: loss 16.129164534719695
Epoch 14: loss 16.129166400581678
Epoch 15: loss 16.129170057888974
Epoch 16: loss 16.12916616644141
Epoch 17: loss 16.129162871105116
Epoch 18: loss 16.129161491414788
Epoch 19: loss 16.129164649067267
-----------Time: 0:02:01.550560, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917439791092
Epoch 1: loss 16.129171598858647
Epoch 2: loss 16.12916837456824
Epoch 3: loss 16.129174195404218
Epoch 4: loss 16.129182811921677
Epoch 5: loss 16.129179602410886
Epoch 6: loss 16.129174994540772
Epoch 7: loss 16.12917585616659
Epoch 8: loss 16.12917497898328
Epoch 9: loss 16.129175504307955
Epoch 10: loss 16.12916957275449
Epoch 11: loss 16.129170784942477
Epoch 12: loss 16.129175009838974
Epoch 13: loss 16.129176106901518
Epoch 14: loss 16.129174801887153
Epoch 15: loss 16.129175023062842
Epoch 16: loss 16.129175788232203
Epoch 17: loss 16.129172646137214
Epoch 18: loss 16.12917719903752
Epoch 19: loss 16.12917278589536
-----------Time: 0:03:21.186384, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129173779500576
Epoch 1: loss 16.12917974398407
Epoch 2: loss 16.12918125746884
Epoch 3: loss 16.12917639367797
Epoch 4: loss 16.129177012088313
Epoch 5: loss 16.129170824095503
Epoch 6: loss 16.12917776973822
Epoch 7: loss 16.129177410360132
Epoch 8: loss 16.1291751195193
Epoch 9: loss 16.12917900707749
Epoch 10: loss 16.129177178294196
Epoch 11: loss 16.12917747855381
Epoch 12: loss 16.12917539903559
Epoch 13: loss 16.129177893160996
Epoch 14: loss 16.12917669523404
Epoch 15: loss 16.129178562911065
Epoch 16: loss 16.12918581399923
Epoch 17: loss 16.12917526005532
Epoch 18: loss 16.1291778623053
Epoch 19: loss 16.129171214847865
-----------Time: 0:04:16.822792, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129201274257824
Epoch 1: loss 16.129200613323665
Epoch 2: loss 16.12920458333657
Epoch 3: loss 16.129203494312065
Epoch 4: loss 16.12920302940232
Epoch 5: loss 16.129206359224387
Epoch 6: loss 16.129203491200567
Epoch 7: loss 16.12920539077045
Epoch 8: loss 16.12919935316674
Epoch 9: loss 16.129207205552003
Epoch 10: loss 16.129205962249024
Epoch 11: loss 16.12920139897706
Epoch 12: loss 16.129204544442835
Epoch 13: loss 16.129207092500888
Epoch 14: loss 16.129198296294387
Epoch 15: loss 16.129197589724917
Epoch 16: loss 16.1292065889567
Epoch 17: loss 16.129206853174786
Epoch 18: loss 16.1292017842843
Epoch 19: loss 16.129206889994187
-----------Time: 0:02:25.783844, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 20, rmse: 4.017210483551025-------------


Epoch 0: loss 16.12914167842896
Epoch 1: loss 16.129140478427672
Epoch 2: loss 16.129139237458322
Epoch 3: loss 16.129140614296446
Epoch 4: loss 16.12914376391089
Epoch 5: loss 16.129138881710315
Epoch 6: loss 16.129140708159987
Epoch 7: loss 16.12913281013941
Epoch 8: loss 16.129139550423222
Epoch 9: loss 16.12914555068895
Epoch 10: loss 16.129144935908688
Epoch 11: loss 16.12913903806312
Epoch 12: loss 16.129137884475018
Epoch 13: loss 16.12913771697268
Epoch 14: loss 16.129141042646083
Epoch 15: loss 16.12914590254758
Epoch 16: loss 16.129146569445446
Epoch 17: loss 16.129137345926473
Epoch 18: loss 16.129138677907157
Epoch 19: loss 16.129148341962473
-----------Time: 0:03:04.134181, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 50, rmse: 4.01719331741333-------------


Epoch 0: loss 16.12917712565801
Epoch 1: loss 16.12917472461827
Epoch 2: loss 16.12917706524308
Epoch 3: loss 16.12917837570257
Epoch 4: loss 16.12917493049576
Epoch 5: loss 16.129179808288374
Epoch 6: loss 16.129176893073492
Epoch 7: loss 16.129178918659072
Epoch 8: loss 16.12918002194461
Epoch 9: loss 16.12918143638001
Epoch 10: loss 16.129182379682664
Epoch 11: loss 16.129179895410335
Epoch 12: loss 16.129179570518026
Epoch 13: loss 16.129179819956494
Epoch 14: loss 16.129174013640842
Epoch 15: loss 16.1291798290317
Epoch 16: loss 16.129174579933586
Epoch 17: loss 16.129173735421013
Epoch 18: loss 16.129178327215048
Epoch 19: loss 16.129174054868198
-----------Time: 0:03:43.085888, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12917301847988
Epoch 1: loss 16.12917263706201
Epoch 2: loss 16.129177513298877
Epoch 3: loss 16.12917725245158
Epoch 4: loss 16.129174423580782
Epoch 5: loss 16.129172480709208
Epoch 6: loss 16.129176893332783
Epoch 7: loss 16.129180931539363
Epoch 8: loss 16.129174565153967
Epoch 9: loss 16.129176623669572
Epoch 10: loss 16.12916994146707
Epoch 11: loss 16.12917486515429
Epoch 12: loss 16.129184462053093
Epoch 13: loss 16.129168847256732
Epoch 14: loss 16.12917745884765
Epoch 15: loss 16.129170710785093
Epoch 16: loss 16.129171492289824
Epoch 17: loss 16.12917475288105
Epoch 18: loss 16.129178075183663
Epoch 19: loss 16.129173173276932
-----------Time: 0:04:32.026361, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129176694196875
Epoch 1: loss 16.12917791260786
Epoch 2: loss 16.12917790431053
Epoch 3: loss 16.12917783533898
Epoch 4: loss 16.129173731272346
Epoch 5: loss 16.129174697133365
Epoch 6: loss 16.129175768266755
Epoch 7: loss 16.129180438107547
Epoch 8: loss 16.129181144417725
Epoch 9: loss 16.129176793764827
Epoch 10: loss 16.129177690135712
Epoch 11: loss 16.129173257546686
Epoch 12: loss 16.12917399600902
Epoch 13: loss 16.12918305332211
Epoch 14: loss 16.129176154870454
Epoch 15: loss 16.129179080197705
Epoch 16: loss 16.12917700819894
Epoch 17: loss 16.12917605270958
Epoch 18: loss 16.129176937153055
Epoch 19: loss 16.129178553057987
-----------Time: 0:05:02.740599, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129276265781687
Epoch 1: loss 16.12927693060522
Epoch 2: loss 16.129267300776387
Epoch 3: loss 16.129271644946996
Epoch 4: loss 16.129271590755064
Epoch 5: loss 16.129268327311628
Epoch 6: loss 16.129271080210003
Epoch 7: loss 16.129275999229975
Epoch 8: loss 16.12927148496411
Epoch 9: loss 16.129268149437628
Epoch 10: loss 16.12927404650532
Epoch 11: loss 16.129271668542525
Epoch 12: loss 16.12926808461474
Epoch 13: loss 16.12926936707074
Epoch 14: loss 16.129272067592222
Epoch 15: loss 16.12927018539487
Epoch 16: loss 16.129263560236506
Epoch 17: loss 16.129276490328166
Epoch 18: loss 16.129273004671877
Epoch 19: loss 16.129264887809235
-----------Time: 0:03:14.352330, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 20, rmse: 4.017205238342285-------------


Epoch 0: loss 16.12920538065808
Epoch 1: loss 16.129205927244666
Epoch 2: loss 16.129215191990998
Epoch 3: loss 16.12920539725274
Epoch 4: loss 16.12921121497722
Epoch 5: loss 16.129210378502684
Epoch 6: loss 16.129210447214945
Epoch 7: loss 16.12921718698017
Epoch 8: loss 16.129207766658915
Epoch 9: loss 16.129209598294413
Epoch 10: loss 16.12920848774871
Epoch 11: loss 16.129210390689387
Epoch 12: loss 16.129210065537784
Epoch 13: loss 16.129203554467704
Epoch 14: loss 16.129203532946505
Epoch 15: loss 16.12920470546289
Epoch 16: loss 16.129209967006997
Epoch 17: loss 16.129209879885035
Epoch 18: loss 16.12920868688462
Epoch 19: loss 16.129205311167947
-----------Time: 0:03:55.620661, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 50, rmse: 4.017194747924805-------------


Epoch 0: loss 16.129197114702798
Epoch 1: loss 16.12919048876656
Epoch 2: loss 16.12918749368988
Epoch 3: loss 16.129185743212634
Epoch 4: loss 16.12919552135623
Epoch 5: loss 16.129191364653412
Epoch 6: loss 16.129193340714306
Epoch 7: loss 16.129198051782456
Epoch 8: loss 16.129189743821943
Epoch 9: loss 16.129187977268618
Epoch 10: loss 16.12919118185287
Epoch 11: loss 16.129188277528232
Epoch 12: loss 16.129192061110512
Epoch 13: loss 16.12919280761088
Epoch 14: loss 16.129190869925136
Epoch 15: loss 16.12919010164428
Epoch 16: loss 16.12919500043951
Epoch 17: loss 16.12918757251451
Epoch 18: loss 16.12919611383742
Epoch 19: loss 16.12919393786274
-----------Time: 0:04:34.281365, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129185510109533
Epoch 1: loss 16.12917657803426
Epoch 2: loss 16.12918040362177
Epoch 3: loss 16.129168382606277
Epoch 4: loss 16.12917692289202
Epoch 5: loss 16.129180987546338
Epoch 6: loss 16.129176828250603
Epoch 7: loss 16.129175301023384
Epoch 8: loss 16.12917693637518
Epoch 9: loss 16.12917965037982
Epoch 10: loss 16.12918294649399
Epoch 11: loss 16.12918424061811
Epoch 12: loss 16.12918285600124
Epoch 13: loss 16.129178582876516
Epoch 14: loss 16.129177343203622
Epoch 15: loss 16.129178476826272
Epoch 16: loss 16.12918996629409
Epoch 17: loss 16.129175624619236
Epoch 18: loss 16.129172430406648
Epoch 19: loss 16.129175008801806
-----------Time: 0:05:25.145294, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.12917331303508
Epoch 1: loss 16.129179185210784
Epoch 2: loss 16.129176429978784
Epoch 3: loss 16.129180423068636
Epoch 4: loss 16.12917882764774
Epoch 5: loss 16.12918536568414
Epoch 6: loss 16.129174107763674
Epoch 7: loss 16.129178461009488
Epoch 8: loss 16.12917426126427
Epoch 9: loss 16.129177045277633
Epoch 10: loss 16.129178298174395
Epoch 11: loss 16.12918498400698
Epoch 12: loss 16.12917774692056
Epoch 13: loss 16.129179712350503
Epoch 14: loss 16.12917626169857
Epoch 15: loss 16.12917762894291
Epoch 16: loss 16.12918235893934
Epoch 17: loss 16.12918070803005
Epoch 18: loss 16.129171232738983
Epoch 19: loss 16.12917927699999
-----------Time: 0:06:00.468471, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129451234163053
Epoch 1: loss 16.1294552796298
Epoch 2: loss 16.129458421465493
Epoch 3: loss 16.12945600797976
Epoch 4: loss 16.12944942041867
Epoch 5: loss 16.129454254391014
Epoch 6: loss 16.129450444879577
Epoch 7: loss 16.129449973487542
Epoch 8: loss 16.12945315369839
Epoch 9: loss 16.129449447125698
Epoch 10: loss 16.1294493550772
Epoch 11: loss 16.12945155205449
Epoch 12: loss 16.129451137706596
Epoch 13: loss 16.12945024963304
Epoch 14: loss 16.129447596561914
Epoch 15: loss 16.129452471761617
Epoch 16: loss 16.12945006838825
Epoch 17: loss 16.12944951091142
Epoch 18: loss 16.129452565625154
Epoch 19: loss 16.12945094271935
-----------Time: 0:03:43.844231, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 20, rmse: 4.017208576202393-------------


Epoch 0: loss 16.12919333838068
Epoch 1: loss 16.129185721691435
Epoch 2: loss 16.12919253120609
Epoch 3: loss 16.129191718586377
Epoch 4: loss 16.1291882601557
Epoch 5: loss 16.129191447886
Epoch 6: loss 16.129193938122036
Epoch 7: loss 16.129188252376952
Epoch 8: loss 16.12919285117186
Epoch 9: loss 16.12918934658729
Epoch 10: loss 16.12919127390137
Epoch 11: loss 16.129184812355977
Epoch 12: loss 16.1291874716501
Epoch 13: loss 16.129188404581093
Epoch 14: loss 16.12918650941916
Epoch 15: loss 16.12919018902553
Epoch 16: loss 16.129193511587438
Epoch 17: loss 16.129187273551356
Epoch 18: loss 16.129188857304136
Epoch 19: loss 16.129192675631483
-----------Time: 0:02:41.334146, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 50, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12918720250547
Epoch 1: loss 16.12918913241247
Epoch 2: loss 16.12918372021997
Epoch 3: loss 16.12918902039852
Epoch 4: loss 16.129180570086945
Epoch 5: loss 16.129186696109077
Epoch 6: loss 16.12919261806876
Epoch 7: loss 16.129191062319467
Epoch 8: loss 16.129191761628775
Epoch 9: loss 16.12918031649981
Epoch 10: loss 16.129190997237288
Epoch 11: loss 16.129190489544435
Epoch 12: loss 16.12918949282772
Epoch 13: loss 16.129181995153296
Epoch 14: loss 16.129185470437925
Epoch 15: loss 16.12917799091391
Epoch 16: loss 16.129183244679272
Epoch 17: loss 16.129190882889716
Epoch 18: loss 16.12918332998619
Epoch 19: loss 16.129183956953156
-----------Time: 0:03:20.073304, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129178877172425
Epoch 1: loss 16.12917179825385
Epoch 2: loss 16.129179324709636
Epoch 3: loss 16.12918004813306
Epoch 4: loss 16.12918635306636
Epoch 5: loss 16.12917048857224
Epoch 6: loss 16.129179640267452
Epoch 7: loss 16.129182156951224
Epoch 8: loss 16.129179647786906
Epoch 9: loss 16.12918510950409
Epoch 10: loss 16.12917613438642
Epoch 11: loss 16.12917490352944
Epoch 12: loss 16.129176648820852
Epoch 13: loss 16.129182064902725
Epoch 14: loss 16.129182634047673
Epoch 15: loss 16.129177013644064
Epoch 16: loss 16.129174244928905
Epoch 17: loss 16.129182740097917
Epoch 18: loss 16.129182997574425
Epoch 19: loss 16.129183950211576
-----------Time: 0:04:06.427745, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129181916328665
Epoch 1: loss 16.129187513914623
Epoch 2: loss 16.12918426914018
Epoch 3: loss 16.12918065280095
Epoch 4: loss 16.129182084868173
Epoch 5: loss 16.129176665934096
Epoch 6: loss 16.129177874492004
Epoch 7: loss 16.129182587893776
Epoch 8: loss 16.129183406995782
Epoch 9: loss 16.129180594978934
Epoch 10: loss 16.129177248562204
Epoch 11: loss 16.129181468532163
Epoch 12: loss 16.129178677777222
Epoch 13: loss 16.12917800906432
Epoch 14: loss 16.129183686771363
Epoch 15: loss 16.129178983222666
Epoch 16: loss 16.12917609912277
Epoch 17: loss 16.12917975383715
Epoch 18: loss 16.1291800621348
Epoch 19: loss 16.129186752893926
-----------Time: 0:04:38.990892, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129321834974927
Epoch 1: loss 16.12932516038904
Epoch 2: loss 16.129329293496326
Epoch 3: loss 16.129322038259502
Epoch 4: loss 16.129326250969292
Epoch 5: loss 16.129321061508236
Epoch 6: loss 16.129321239900822
Epoch 7: loss 16.129321670584083
Epoch 8: loss 16.129327980703216
Epoch 9: loss 16.129327532388128
Epoch 10: loss 16.12932630101256
Epoch 11: loss 16.12932149063575
Epoch 12: loss 16.129321409736786
Epoch 13: loss 16.129321733591933
Epoch 14: loss 16.129323854078216
Epoch 15: loss 16.129321857792583
Epoch 16: loss 16.1293241644502
Epoch 17: loss 16.129322566695677
Epoch 18: loss 16.129324264018155
Epoch 19: loss 16.12932431146851
-----------Time: 0:02:51.604503, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 20, rmse: 4.0172014236450195-------------


Epoch 0: loss 16.129187362747647
Epoch 1: loss 16.12918689239278
Epoch 2: loss 16.129187836991893
Epoch 3: loss 16.12918103292236
Epoch 4: loss 16.129182121946865
Epoch 5: loss 16.129185391872586
Epoch 6: loss 16.129180804486506
Epoch 7: loss 16.12918076014765
Epoch 8: loss 16.129187223248795
Epoch 9: loss 16.129189395593393
Epoch 10: loss 16.129186834311472
Epoch 11: loss 16.129187674934673
Epoch 12: loss 16.12918742290329
Epoch 13: loss 16.129185395502667
Epoch 14: loss 16.129186680033
Epoch 15: loss 16.12918995436668
Epoch 16: loss 16.129188696543377
Epoch 17: loss 16.12919413622078
Epoch 18: loss 16.129184451422137
Epoch 19: loss 16.12918279091906
-----------Time: 0:03:29.890754, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 50, rmse: 4.017184257507324-------------


Epoch 0: loss 16.129175447782398
Epoch 1: loss 16.1291855080352
Epoch 2: loss 16.129175520902617
Epoch 3: loss 16.12918268383165
Epoch 4: loss 16.129183191524504
Epoch 5: loss 16.129178016843063
Epoch 6: loss 16.129175871205497
Epoch 7: loss 16.129177326090378
Epoch 8: loss 16.12917205728611
Epoch 9: loss 16.129171630232925
Epoch 10: loss 16.129177257378117
Epoch 11: loss 16.129176560143144
Epoch 12: loss 16.129168687792433
Epoch 13: loss 16.129175632657272
Epoch 14: loss 16.129172289611336
Epoch 15: loss 16.129176401456714
Epoch 16: loss 16.12917248330212
Epoch 17: loss 16.129182614082225
Epoch 18: loss 16.129185047274117
Epoch 19: loss 16.12917840318747
-----------Time: 0:04:05.095474, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129172646137214
Epoch 1: loss 16.129169376730076
Epoch 2: loss 16.129169263938252
Epoch 3: loss 16.129172272757383
Epoch 4: loss 16.12917489393565
Epoch 5: loss 16.129176850031094
Epoch 6: loss 16.129170969817352
Epoch 7: loss 16.12917561709978
Epoch 8: loss 16.129181542948835
Epoch 9: loss 16.129169470852908
Epoch 10: loss 16.129177636721653
Epoch 11: loss 16.129172661694707
Epoch 12: loss 16.129177102321773
Epoch 13: loss 16.129168129537728
Epoch 14: loss 16.12916973558958
Epoch 15: loss 16.129172739741463
Epoch 16: loss 16.129178860318472
Epoch 17: loss 16.12917284942179
Epoch 18: loss 16.12917604078217
Epoch 19: loss 16.129178502755426
-----------Time: 0:04:58.921162, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917312090004
Epoch 1: loss 16.12917319453884
Epoch 2: loss 16.12917896507226
Epoch 3: loss 16.1291798290317
Epoch 4: loss 16.129176685380962
Epoch 5: loss 16.129168586928017
Epoch 6: loss 16.129183365768423
Epoch 7: loss 16.129170368001667
Epoch 8: loss 16.12917292150484
Epoch 9: loss 16.12917233602452
Epoch 10: loss 16.12917970042309
Epoch 11: loss 16.129177228856047
Epoch 12: loss 16.12917150681015
Epoch 13: loss 16.12917560854316
Epoch 14: loss 16.129176253660532
Epoch 15: loss 16.129176148128874
Epoch 16: loss 16.129179356861787
Epoch 17: loss 16.129172970770234
Epoch 18: loss 16.129168003522032
Epoch 19: loss 16.129171485807532
-----------Time: 0:05:32.687369, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129324123222844
Epoch 1: loss 16.12931999763501
Epoch 2: loss 16.12932305934962
Epoch 3: loss 16.1293156412777
Epoch 4: loss 16.129320363236097
Epoch 5: loss 16.129320427281108
Epoch 6: loss 16.12931618864216
Epoch 7: loss 16.129316222868646
Epoch 8: loss 16.129320486658873
Epoch 9: loss 16.129322865918123
Epoch 10: loss 16.129323438174573
Epoch 11: loss 16.129315928313446
Epoch 12: loss 16.129318336872643
Epoch 13: loss 16.129316886136426
Epoch 14: loss 16.129318324167357
Epoch 15: loss 16.129320019415502
Epoch 16: loss 16.129319248541726
Epoch 17: loss 16.12931687446831
Epoch 18: loss 16.12931626020663
Epoch 19: loss 16.129315625460915
-----------Time: 0:03:41.757820, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 20, rmse: 4.017202854156494-------------


Epoch 0: loss 16.129228976966896
Epoch 1: loss 16.129228681633823
Epoch 2: loss 16.129230421998695
Epoch 3: loss 16.12923991206938
Epoch 4: loss 16.129233688294338
Epoch 5: loss 16.129235385357525
Epoch 6: loss 16.12922833988756
Epoch 7: loss 16.129230218973415
Epoch 8: loss 16.129230813528935
Epoch 9: loss 16.129236388815816
Epoch 10: loss 16.129229650606337
Epoch 11: loss 16.129234082936073
Epoch 12: loss 16.12923060142845
Epoch 13: loss 16.129232687169665
Epoch 14: loss 16.129242439384107
Epoch 15: loss 16.12922666227124
Epoch 16: loss 16.129231282068766
Epoch 17: loss 16.129229705316856
Epoch 18: loss 16.129227314648777
Epoch 19: loss 16.12922754282534
-----------Time: 0:04:17.891001, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 50, rmse: 4.017177104949951-------------


Epoch 0: loss 16.129178161268456
Epoch 1: loss 16.129176642597855
Epoch 2: loss 16.129176211914594
Epoch 3: loss 16.129174457288684
Epoch 4: loss 16.12917984407061
Epoch 5: loss 16.129178778382343
Epoch 6: loss 16.129181370779246
Epoch 7: loss 16.1291752973933
Epoch 8: loss 16.129173856250873
Epoch 9: loss 16.129173138272574
Epoch 10: loss 16.129184964300823
Epoch 11: loss 16.129171743543335
Epoch 12: loss 16.129171295228247
Epoch 13: loss 16.129171839221915
Epoch 14: loss 16.129175378810846
Epoch 15: loss 16.129180324278558
Epoch 16: loss 16.129176284775518
Epoch 17: loss 16.129172768782116
Epoch 18: loss 16.129177239227708
Epoch 19: loss 16.129176014334433
-----------Time: 0:04:58.844913, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917081035305
Epoch 1: loss 16.12917141476165
Epoch 2: loss 16.12916967024811
Epoch 3: loss 16.12917276722637
Epoch 4: loss 16.129175895578904
Epoch 5: loss 16.129171648642625
Epoch 6: loss 16.12917294328533
Epoch 7: loss 16.12917372867943
Epoch 8: loss 16.12916937050708
Epoch 9: loss 16.129173005774593
Epoch 10: loss 16.129171649420503
Epoch 11: loss 16.129168807325836
Epoch 12: loss 16.129170659186077
Epoch 13: loss 16.129174372241057
Epoch 14: loss 16.129177866713256
Epoch 15: loss 16.129165800581035
Epoch 16: loss 16.129165135757503
Epoch 17: loss 16.129167372406403
Epoch 18: loss 16.129164280354686
Epoch 19: loss 16.129169395139776
-----------Time: 0:04:24.671125, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 150, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129178405521095
Epoch 1: loss 16.12917746844144
Epoch 2: loss 16.12917187681919
Epoch 3: loss 16.129179902151915
Epoch 4: loss 16.129178158156957
Epoch 5: loss 16.129176149943913
Epoch 6: loss 16.12916776652956
Epoch 7: loss 16.129171433430642
Epoch 8: loss 16.12917679220908
Epoch 9: loss 16.129184778648074
Epoch 10: loss 16.1291780279926
Epoch 11: loss 16.12917329825546
Epoch 12: loss 16.129176509322
Epoch 13: loss 16.129181245282137
Epoch 14: loss 16.129178345365457
Epoch 15: loss 16.129174111653047
Epoch 16: loss 16.129181461012706
Epoch 17: loss 16.129179407423642
Epoch 18: loss 16.12918088045893
Epoch 19: loss 16.129176545104233
-----------Time: 0:04:17.986227, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129245343968037
Epoch 1: loss 16.129246904643868
Epoch 2: loss 16.12923776565394
Epoch 3: loss 16.129242153904112
Epoch 4: loss 16.129241010687675
Epoch 5: loss 16.129244156672037
Epoch 6: loss 16.1292441854534
Epoch 7: loss 16.129239585102738
Epoch 8: loss 16.129236704373632
Epoch 9: loss 16.129244539645654
Epoch 10: loss 16.129240255371393
Epoch 11: loss 16.12924725105738
Epoch 12: loss 16.129240130392866
Epoch 13: loss 16.129241220195244
Epoch 14: loss 16.129244081477488
Epoch 15: loss 16.129238970063184
Epoch 16: loss 16.12924553558449
Epoch 17: loss 16.1292411940068
Epoch 18: loss 16.129237532291548
Epoch 19: loss 16.12923832468652
-----------Time: 0:02:31.042881, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 20, rmse: 4.0172119140625-------------


Epoch 0: loss 16.129173511393113
Epoch 1: loss 16.12917209773559
Epoch 2: loss 16.129178820387573
Epoch 3: loss 16.129175850721467
Epoch 4: loss 16.129175407073625
Epoch 5: loss 16.129171373793586
Epoch 6: loss 16.129173192983092
Epoch 7: loss 16.129173222283036
Epoch 8: loss 16.12917076134695
Epoch 9: loss 16.129177239227708
Epoch 10: loss 16.129169815710668
Epoch 11: loss 16.129169944319276
Epoch 12: loss 16.12916813213064
Epoch 13: loss 16.129171651494833
Epoch 14: loss 16.12916909462087
Epoch 15: loss 16.129163625124942
Epoch 16: loss 16.129175214160714
Epoch 17: loss 16.129172103440002
Epoch 18: loss 16.129164613285035
Epoch 19: loss 16.129173357892515
-----------Time: 0:02:58.714186, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12918953612941
Epoch 1: loss 16.129180969136637
Epoch 2: loss 16.12918709852956
Epoch 3: loss 16.12918691572902
Epoch 4: loss 16.129191136476848
Epoch 5: loss 16.129184346668353
Epoch 6: loss 16.12919266137045
Epoch 7: loss 16.129182961792193
Epoch 8: loss 16.129193454802586
Epoch 9: loss 16.129182339233182
Epoch 10: loss 16.12918293793737
Epoch 11: loss 16.12919253950342
Epoch 12: loss 16.12919212774844
Epoch 13: loss 16.129184859287744
Epoch 14: loss 16.129187077526947
Epoch 15: loss 16.129191652467032
Epoch 16: loss 16.129180648392992
Epoch 17: loss 16.12919233336664
Epoch 18: loss 16.12918883785727
Epoch 19: loss 16.129187876144915
-----------Time: 0:03:48.699049, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129169081656293
Epoch 1: loss 16.129179398089146
Epoch 2: loss 16.12917409687343
Epoch 3: loss 16.129173792465153
Epoch 4: loss 16.129169775001895
Epoch 5: loss 16.129166326424297
Epoch 6: loss 16.12917148840045
Epoch 7: loss 16.129177217965804
Epoch 8: loss 16.129173269214807
Epoch 9: loss 16.12917567051384
Epoch 10: loss 16.129175776823374
Epoch 11: loss 16.129173444495894
Epoch 12: loss 16.1291701460481
Epoch 13: loss 16.129174173623728
Epoch 14: loss 16.129173211133498
Epoch 15: loss 16.129168866444306
Epoch 16: loss 16.129173799984606
Epoch 17: loss 16.12917700793965
Epoch 18: loss 16.129169666358735
Epoch 19: loss 16.12918102125424
-----------Time: 0:04:30.523994, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 150, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129172107070083
Epoch 1: loss 16.129169006721035
Epoch 2: loss 16.129162159090523
Epoch 3: loss 16.129165154945078
Epoch 4: loss 16.12917295469416
Epoch 5: loss 16.129170635331253
Epoch 6: loss 16.129173063078024
Epoch 7: loss 16.12916938580528
Epoch 8: loss 16.12916739003823
Epoch 9: loss 16.129169823748708
Epoch 10: loss 16.129174923494887
Epoch 11: loss 16.129172221158367
Epoch 12: loss 16.12917370845469
Epoch 13: loss 16.129170081743798
Epoch 14: loss 16.129172247346812
Epoch 15: loss 16.129174399466667
Epoch 16: loss 16.12917370845469
Epoch 17: loss 16.12916784716923
Epoch 18: loss 16.129169664025113
Epoch 19: loss 16.129177332831958
-----------Time: 0:04:59.780186, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129424238023454
Epoch 1: loss 16.129429282540535
Epoch 2: loss 16.129424221428792
Epoch 3: loss 16.129420719437135
Epoch 4: loss 16.129427965339467
Epoch 5: loss 16.129431203372327
Epoch 6: loss 16.129425420392913
Epoch 7: loss 16.12942442860274
Epoch 8: loss 16.129422227217493
Epoch 9: loss 16.12942944796854
Epoch 10: loss 16.12942739489806
Epoch 11: loss 16.129422957641783
Epoch 12: loss 16.129424995673357
Epoch 13: loss 16.129434462407804
Epoch 14: loss 16.12943270881906
Epoch 15: loss 16.129432936217746
Epoch 16: loss 16.12942788029184
Epoch 17: loss 16.129428346757333
Epoch 18: loss 16.129423562568967
Epoch 19: loss 16.129427080895994
-----------Time: 0:03:10.498689, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 20, rmse: 4.017213821411133-------------


Epoch 0: loss 16.12915886530998
Epoch 1: loss 16.129156813795245
Epoch 2: loss 16.129156879396007
Epoch 3: loss 16.129154138425047
Epoch 4: loss 16.1291551369568
Epoch 5: loss 16.129156115523106
Epoch 6: loss 16.129160721578177
Epoch 7: loss 16.129162698676236
Epoch 8: loss 16.1291564225243
Epoch 9: loss 16.12916596367542
Epoch 10: loss 16.12916122901174
Epoch 11: loss 16.129161944656413
Epoch 12: loss 16.12915254196698
Epoch 13: loss 16.129154320966297
Epoch 14: loss 16.129158976546055
Epoch 15: loss 16.12915666548048
Epoch 16: loss 16.12914995708953
Epoch 17: loss 16.129160995649347
Epoch 18: loss 16.12915375441426
Epoch 19: loss 16.129156813276662
-----------Time: 0:03:36.653928, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 50, rmse: 4.017179012298584-------------


Epoch 0: loss 16.129174512517782
Epoch 1: loss 16.12918594701579
Epoch 2: loss 16.129186595244665
Epoch 3: loss 16.129178581839348
Epoch 4: loss 16.12918164562829
Epoch 5: loss 16.129182982794806
Epoch 6: loss 16.129172786673234
Epoch 7: loss 16.129168766357772
Epoch 8: loss 16.129172657546043
Epoch 9: loss 16.129176453833608
Epoch 10: loss 16.12918337199142
Epoch 11: loss 16.12917340197208
Epoch 12: loss 16.12917638693639
Epoch 13: loss 16.129182470434706
Epoch 14: loss 16.129176023409638
Epoch 15: loss 16.12917413524858
Epoch 16: loss 16.12917889973079
Epoch 17: loss 16.12917900215095
Epoch 18: loss 16.129177020385644
Epoch 19: loss 16.129180107251532
-----------Time: 0:04:31.404408, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 100, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129176781318833
Epoch 1: loss 16.129176679157965
Epoch 2: loss 16.129177562564273
Epoch 3: loss 16.12917299047639
Epoch 4: loss 16.129171130837403
Epoch 5: loss 16.12916865927036
Epoch 6: loss 16.129176517360037
Epoch 7: loss 16.129171513292437
Epoch 8: loss 16.129176769910007
Epoch 9: loss 16.1291743432004
Epoch 10: loss 16.1291767670578
Epoch 11: loss 16.129170153567557
Epoch 12: loss 16.129170791684057
Epoch 13: loss 16.129166210780266
Epoch 14: loss 16.129171963163277
Epoch 15: loss 16.12918374200046
Epoch 16: loss 16.129166095395526
Epoch 17: loss 16.12916773022874
Epoch 18: loss 16.129179207250566
Epoch 19: loss 16.12917451900007
-----------Time: 0:05:24.907352, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917006826064
Epoch 1: loss 16.129170378891914
Epoch 2: loss 16.129166520633667
Epoch 3: loss 16.12917232046703
Epoch 4: loss 16.129173155385818
Epoch 5: loss 16.12917479981282
Epoch 6: loss 16.129175899468276
Epoch 7: loss 16.129173483130334
Epoch 8: loss 16.129175275872104
Epoch 9: loss 16.129170625478174
Epoch 10: loss 16.129167546391034
Epoch 11: loss 16.129177752884267
Epoch 12: loss 16.12916816843146
Epoch 13: loss 16.12917391096139
Epoch 14: loss 16.129173228506033
Epoch 15: loss 16.129171778288402
Epoch 16: loss 16.129169654690617
Epoch 17: loss 16.12917347587017
Epoch 18: loss 16.129172834382878
Epoch 19: loss 16.129172708367186
-----------Time: 0:06:00.750885, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12931745579996
Epoch 1: loss 16.129325890035457
Epoch 2: loss 16.129321893574815
Epoch 3: loss 16.12932355148498
Epoch 4: loss 16.12931406374792
Epoch 5: loss 16.129320333417567
Epoch 6: loss 16.129317353120506
Epoch 7: loss 16.12931938311404
Epoch 8: loss 16.129322290550178
Epoch 9: loss 16.12931585830473
Epoch 10: loss 16.129317289594077
Epoch 11: loss 16.1293120301243
Epoch 12: loss 16.12931714257577
Epoch 13: loss 16.12932400680094
Epoch 14: loss 16.12931388691108
Epoch 15: loss 16.12931408397266
Epoch 16: loss 16.129318491151114
Epoch 17: loss 16.129317711461425
Epoch 18: loss 16.129322281215682
Epoch 19: loss 16.129316430561175
-----------Time: 0:04:00.814025, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 20, rmse: 4.017229080200195-------------


Epoch 0: loss 16.12916509971598
Epoch 1: loss 16.12916878476747
Epoch 2: loss 16.12917071804526
Epoch 3: loss 16.12916791276999
Epoch 4: loss 16.129168341897504
Epoch 5: loss 16.12917172254072
Epoch 6: loss 16.12916940265923
Epoch 7: loss 16.129170651148037
Epoch 8: loss 16.129165102568187
Epoch 9: loss 16.129172039654282
Epoch 10: loss 16.12917088762193
Epoch 11: loss 16.12916691605328
Epoch 12: loss 16.12917131078574
Epoch 13: loss 16.12916731017643
Epoch 14: loss 16.12916621311389
Epoch 15: loss 16.129170181052462
Epoch 16: loss 16.12917464812726
Epoch 17: loss 16.12917037681758
Epoch 18: loss 16.129165499543547
Epoch 19: loss 16.12917861995521
-----------Time: 0:02:46.646991, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129175781490623
Epoch 1: loss 16.129177899643285
Epoch 2: loss 16.12917560698741
Epoch 3: loss 16.129181085558546
Epoch 4: loss 16.12917629022064
Epoch 5: loss 16.129170173533005
Epoch 6: loss 16.129178140784425
Epoch 7: loss 16.12918203275057
Epoch 8: loss 16.12917625832778
Epoch 9: loss 16.12917725089583
Epoch 10: loss 16.12917760638454
Epoch 11: loss 16.129183535345096
Epoch 12: loss 16.12917684743818
Epoch 13: loss 16.12917856654115
Epoch 14: loss 16.129171934641207
Epoch 15: loss 16.129176247696826
Epoch 16: loss 16.129175232051832
Epoch 17: loss 16.129179764727393
Epoch 18: loss 16.12917172124426
Epoch 19: loss 16.129169642503914
-----------Time: 0:03:17.203450, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129176522027286
Epoch 1: loss 16.12917720551981
Epoch 2: loss 16.1291760737122
Epoch 3: loss 16.129174388835715
Epoch 4: loss 16.129179508028763
Epoch 5: loss 16.129179231623972
Epoch 6: loss 16.129174278636807
Epoch 7: loss 16.12917780007533
Epoch 8: loss 16.12917064492504
Epoch 9: loss 16.129174087798226
Epoch 10: loss 16.129182170434383
Epoch 11: loss 16.129178313472597
Epoch 12: loss 16.12917725271087
Epoch 13: loss 16.1291739412985
Epoch 14: loss 16.129170403524608
Epoch 15: loss 16.12917726230466
Epoch 16: loss 16.129173037667453
Epoch 17: loss 16.12917898996425
Epoch 18: loss 16.129174555041597
Epoch 19: loss 16.12918220154937
-----------Time: 0:04:02.306684, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129171011304
Epoch 1: loss 16.12917012115611
Epoch 2: loss 16.129174490996583
Epoch 3: loss 16.129167595915717
Epoch 4: loss 16.129181922810954
Epoch 5: loss 16.129178675184306
Epoch 6: loss 16.12917697967687
Epoch 7: loss 16.129180067839215
Epoch 8: loss 16.129173576216
Epoch 9: loss 16.129176305778135
Epoch 10: loss 16.129183848050705
Epoch 11: loss 16.129177703878167
Epoch 12: loss 16.12917661485366
Epoch 13: loss 16.129179479765984
Epoch 14: loss 16.12917556031493
Epoch 15: loss 16.12918015910984
Epoch 16: loss 16.129175965846915
Epoch 17: loss 16.129172126776243
Epoch 18: loss 16.12917540940725
Epoch 19: loss 16.129178505607634
-----------Time: 0:04:40.775764, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129356344346444
Epoch 1: loss 16.12935128919841
Epoch 2: loss 16.1293540231685
Epoch 3: loss 16.129358239767665
Epoch 4: loss 16.12935349265799
Epoch 5: loss 16.129354579089583
Epoch 6: loss 16.129356252038654
Epoch 7: loss 16.129351857824776
Epoch 8: loss 16.12934999663004
Epoch 9: loss 16.129352644774627
Epoch 10: loss 16.12935144658838
Epoch 11: loss 16.129350919967244
Epoch 12: loss 16.1293561094283
Epoch 13: loss 16.129356770881042
Epoch 14: loss 16.129352502164277
Epoch 15: loss 16.12934799593645
Epoch 16: loss 16.12935533751736
Epoch 17: loss 16.129352069925265
Epoch 18: loss 16.129355347889025
Epoch 19: loss 16.12935203569878
-----------Time: 0:02:52.387058, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 20, rmse: 4.017237186431885-------------


Epoch 0: loss 16.129166379838356
Epoch 1: loss 16.129170792721226
Epoch 2: loss 16.12916566704589
Epoch 3: loss 16.12916521717505
Epoch 4: loss 16.129173149681403
Epoch 5: loss 16.12916970058522
Epoch 6: loss 16.129172419775692
Epoch 7: loss 16.12916538597385
Epoch 8: loss 16.129162429790902
Epoch 9: loss 16.129174315456204
Epoch 10: loss 16.129168107757238
Epoch 11: loss 16.129170228502815
Epoch 12: loss 16.129169306202776
Epoch 13: loss 16.12916877958164
Epoch 14: loss 16.129170672409945
Epoch 15: loss 16.129175316321582
Epoch 16: loss 16.12916434621474
Epoch 17: loss 16.12916675218102
Epoch 18: loss 16.12916671847312
Epoch 19: loss 16.129172387882832
-----------Time: 0:03:19.569404, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129166882345377
Epoch 1: loss 16.12917579315874
Epoch 2: loss 16.129163696170824
Epoch 3: loss 16.129162896515687
Epoch 4: loss 16.12915430981676
Epoch 5: loss 16.12917377016608
Epoch 6: loss 16.129158594609603
Epoch 7: loss 16.129166706545707
Epoch 8: loss 16.12916477197146
Epoch 9: loss 16.129164138522206
Epoch 10: loss 16.129162548546432
Epoch 11: loss 16.129165707754662
Epoch 12: loss 16.129167263503955
Epoch 13: loss 16.12915980109318
Epoch 14: loss 16.129164283466185
Epoch 15: loss 16.129164950623338
Epoch 16: loss 16.129161618208354
Epoch 17: loss 16.129172666361956
Epoch 18: loss 16.129156266690078
Epoch 19: loss 16.129168288742736
-----------Time: 0:04:06.231933, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129188894123537
Epoch 1: loss 16.12917381683856
Epoch 2: loss 16.129173863251744
Epoch 3: loss 16.12917139298116
Epoch 4: loss 16.129174953313417
Epoch 5: loss 16.129182280374
Epoch 6: loss 16.129177186591527
Epoch 7: loss 16.129173627555726
Epoch 8: loss 16.129171412168734
Epoch 9: loss 16.12918125746884
Epoch 10: loss 16.129171611823228
Epoch 11: loss 16.129172977252523
Epoch 12: loss 16.129179008633237
Epoch 13: loss 16.129167647514738
Epoch 14: loss 16.129170605253435
Epoch 15: loss 16.129176235769418
Epoch 16: loss 16.129171132133862
Epoch 17: loss 16.129178081665952
Epoch 18: loss 16.12917814182159
Epoch 19: loss 16.129178234907258
-----------Time: 0:04:52.784086, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129179727648705
Epoch 1: loss 16.129179048823428
Epoch 2: loss 16.129183518231855
Epoch 3: loss 16.129178627993245
Epoch 4: loss 16.129186694553326
Epoch 5: loss 16.12917989800325
Epoch 6: loss 16.129180461184497
Epoch 7: loss 16.129175578983922
Epoch 8: loss 16.129180837935117
Epoch 9: loss 16.129175793936618
Epoch 10: loss 16.129181423674723
Epoch 11: loss 16.129174968352327
Epoch 12: loss 16.129179922117366
Epoch 13: loss 16.12918324104919
Epoch 14: loss 16.129184114083834
Epoch 15: loss 16.129180175963793
Epoch 16: loss 16.129182721169634
Epoch 17: loss 16.129184775277285
Epoch 18: loss 16.129180681582312
Epoch 19: loss 16.129177882011458
-----------Time: 0:05:32.940934, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12933872963437
Epoch 1: loss 16.12933616809316
Epoch 2: loss 16.129336583737512
Epoch 3: loss 16.129338365070453
Epoch 4: loss 16.12934309273326
Epoch 5: loss 16.129335747262978
Epoch 6: loss 16.129342734651633
Epoch 7: loss 16.12933772150883
Epoch 8: loss 16.129342666976537
Epoch 9: loss 16.129338149599175
Epoch 10: loss 16.12934046403554
Epoch 11: loss 16.129340501892106
Epoch 12: loss 16.12934010569462
Epoch 13: loss 16.12933442332033
Epoch 14: loss 16.129339263774963
Epoch 15: loss 16.1293404101029
Epoch 16: loss 16.129341504053944
Epoch 17: loss 16.129341209239453
Epoch 18: loss 16.129339384864114
Epoch 19: loss 16.129335526087285
-----------Time: 0:03:51.380430, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 20, rmse: 4.017197608947754-------------


Epoch 0: loss 16.12918688720695
Epoch 1: loss 16.12919527321422
Epoch 2: loss 16.12918599705906
Epoch 3: loss 16.12919756042497
Epoch 4: loss 16.129184525838813
Epoch 5: loss 16.12918742342187
Epoch 6: loss 16.129191036649605
Epoch 7: loss 16.129187579515385
Epoch 8: loss 16.12918872091678
Epoch 9: loss 16.129191234748347
Epoch 10: loss 16.1291892817644
Epoch 11: loss 16.12918414493953
Epoch 12: loss 16.129184415899196
Epoch 13: loss 16.129190576407105
Epoch 14: loss 16.129186749782427
Epoch 15: loss 16.129186855054797
Epoch 16: loss 16.12918682549556
Epoch 17: loss 16.129190611670754
Epoch 18: loss 16.12918847303406
Epoch 19: loss 16.129190025412562
-----------Time: 0:04:00.553901, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 50, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129167150193545
Epoch 1: loss 16.129168415795597
Epoch 2: loss 16.129171189178003
Epoch 3: loss 16.129168446651292
Epoch 4: loss 16.129175771637545
Epoch 5: loss 16.129173010182548
Epoch 6: loss 16.1291764878008
Epoch 7: loss 16.129169039651064
Epoch 8: loss 16.129169339651384
Epoch 9: loss 16.129173095230176
Epoch 10: loss 16.12917116687893
Epoch 11: loss 16.12916815702263
Epoch 12: loss 16.129166924869192
Epoch 13: loss 16.12916703688314
Epoch 14: loss 16.12916755442907
Epoch 15: loss 16.129172061434772
Epoch 16: loss 16.129172293241417
Epoch 17: loss 16.129169639392416
Epoch 18: loss 16.129178008805027
Epoch 19: loss 16.129173617702648
-----------Time: 0:05:03.943768, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129169501708603
Epoch 1: loss 16.12916933187264
Epoch 2: loss 16.129172584425827
Epoch 3: loss 16.129170329107936
Epoch 4: loss 16.12916995313519
Epoch 5: loss 16.12916731484368
Epoch 6: loss 16.12916915140572
Epoch 7: loss 16.129173302404126
Epoch 8: loss 16.129173496613493
Epoch 9: loss 16.129170575694197
Epoch 10: loss 16.12916785339223
Epoch 11: loss 16.129174194367053
Epoch 12: loss 16.129168878112427
Epoch 13: loss 16.129167390556812
Epoch 14: loss 16.129165455723275
Epoch 15: loss 16.129167642588197
Epoch 16: loss 16.129175050288456
Epoch 17: loss 16.12917058269507
Epoch 18: loss 16.12917536714273
Epoch 19: loss 16.129170256506303
-----------Time: 0:04:48.389228, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 150, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129172460743757
Epoch 1: loss 16.129170181571045
Epoch 2: loss 16.12917506091941
Epoch 3: loss 16.129175174229815
Epoch 4: loss 16.129172277424633
Epoch 5: loss 16.129172096698422
Epoch 6: loss 16.129168653306657
Epoch 7: loss 16.12917479333053
Epoch 8: loss 16.12917867259139
Epoch 9: loss 16.12917086895294
Epoch 10: loss 16.12917559324496
Epoch 11: loss 16.129166081393784
Epoch 12: loss 16.129172615540813
Epoch 13: loss 16.129170519946516
Epoch 14: loss 16.129176912520357
Epoch 15: loss 16.12916815676334
Epoch 16: loss 16.12916704543976
Epoch 17: loss 16.129169701622388
Epoch 18: loss 16.129175649511225
Epoch 19: loss 16.12916928545945
-----------Time: 0:04:18.157644, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 200, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129277974512995
Epoch 1: loss 16.12927802896422
Epoch 2: loss 16.12927722075246
Epoch 3: loss 16.12928411142537
Epoch 4: loss 16.129274799487977
Epoch 5: loss 16.129282674172316
Epoch 6: loss 16.12928077978826
Epoch 7: loss 16.12927464365376
Epoch 8: loss 16.129276558003262
Epoch 9: loss 16.129278802430907
Epoch 10: loss 16.129274772262367
Epoch 11: loss 16.129278492058926
Epoch 12: loss 16.12927513475195
Epoch 13: loss 16.12927618591989
Epoch 14: loss 16.129281292926233
Epoch 15: loss 16.129273881336605
Epoch 16: loss 16.129276884451322
Epoch 17: loss 16.12927071616467
Epoch 18: loss 16.129282571492862
Epoch 19: loss 16.129280008655194
-----------Time: 0:02:26.205537, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 20, rmse: 4.017220973968506-------------


Epoch 0: loss 16.12917941753601
Epoch 1: loss 16.12917729082673
Epoch 2: loss 16.129182527478847
Epoch 3: loss 16.12917576774817
Epoch 4: loss 16.129177869565464
Epoch 5: loss 16.129176426089412
Epoch 6: loss 16.12917765798356
Epoch 7: loss 16.129178408891885
Epoch 8: loss 16.12917735850182
Epoch 9: loss 16.12917375357142
Epoch 10: loss 16.12918225651918
Epoch 11: loss 16.129183859978117
Epoch 12: loss 16.1291802646415
Epoch 13: loss 16.129176446054863
Epoch 14: loss 16.129177354093866
Epoch 15: loss 16.129185821777973
Epoch 16: loss 16.129176118828926
Epoch 17: loss 16.1291766565996
Epoch 18: loss 16.129186049176663
Epoch 19: loss 16.12918221321749
-----------Time: 0:03:04.656615, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 50, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12918713197817
Epoch 1: loss 16.12918256455754
Epoch 2: loss 16.129180440700463
Epoch 3: loss 16.129184074412226
Epoch 4: loss 16.129183948915117
Epoch 5: loss 16.129189305878516
Epoch 6: loss 16.12918771305053
Epoch 7: loss 16.129183926616044
Epoch 8: loss 16.129179712869085
Epoch 9: loss 16.12919209767062
Epoch 10: loss 16.12919160216447
Epoch 11: loss 16.129188818669697
Epoch 12: loss 16.129187264735442
Epoch 13: loss 16.129191836823324
Epoch 14: loss 16.129187402678546
Epoch 15: loss 16.129184078301602
Epoch 16: loss 16.129187003369562
Epoch 17: loss 16.129183826010923
Epoch 18: loss 16.12919249801677
Epoch 19: loss 16.129188455142945
-----------Time: 0:03:50.644900, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129173958152453
Epoch 1: loss 16.12917721926226
Epoch 2: loss 16.12917615720408
Epoch 3: loss 16.129171795401643
Epoch 4: loss 16.12917820016219
Epoch 5: loss 16.129176579330718
Epoch 6: loss 16.129178854614057
Epoch 7: loss 16.129170749160245
Epoch 8: loss 16.129179654787777
Epoch 9: loss 16.12918264027067
Epoch 10: loss 16.129175716927026
Epoch 11: loss 16.129170991857134
Epoch 12: loss 16.129179354528166
Epoch 13: loss 16.129178765417766
Epoch 14: loss 16.12917493905238
Epoch 15: loss 16.12917523697837
Epoch 16: loss 16.12917447362405
Epoch 17: loss 16.129173171461893
Epoch 18: loss 16.129170637664878
Epoch 19: loss 16.129182837072957
-----------Time: 0:04:31.920917, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129170834207873
Epoch 1: loss 16.129177576825306
Epoch 2: loss 16.129173303959874
Epoch 3: loss 16.129169014499784
Epoch 4: loss 16.129170878028145
Epoch 5: loss 16.12917723274542
Epoch 6: loss 16.129171763508783
Epoch 7: loss 16.129169652616284
Epoch 8: loss 16.12917698304766
Epoch 9: loss 16.129172421850026
Epoch 10: loss 16.12917678909758
Epoch 11: loss 16.129179907078456
Epoch 12: loss 16.129181909587086
Epoch 13: loss 16.12917745884765
Epoch 14: loss 16.12917493905238
Epoch 15: loss 16.129175778897707
Epoch 16: loss 16.129176070082117
Epoch 17: loss 16.129168593928892
Epoch 18: loss 16.12916954449171
Epoch 19: loss 16.129172527122392
-----------Time: 0:05:06.042762, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12912586034803
Epoch 1: loss 16.129129386972384
Epoch 2: loss 16.12913389838604
Epoch 3: loss 16.12912445265421
Epoch 4: loss 16.129129092157893
Epoch 5: loss 16.129129137793203
Epoch 6: loss 16.12912686017624
Epoch 7: loss 16.129126528283056
Epoch 8: loss 16.12912530961278
Epoch 9: loss 16.12911823795437
Epoch 10: loss 16.12911857632984
Epoch 11: loss 16.129129065710153
Epoch 12: loss 16.129127068646646
Epoch 13: loss 16.12912463882554
Epoch 14: loss 16.12912952724911
Epoch 15: loss 16.12912456000091
Epoch 16: loss 16.12913382500653
Epoch 17: loss 16.129125010649624
Epoch 18: loss 16.129132291037727
Epoch 19: loss 16.129129790948618
-----------Time: 0:03:22.320388, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 20, rmse: 4.017223834991455-------------


Epoch 0: loss 16.129172457113675
Epoch 1: loss 16.12916638891356
Epoch 2: loss 16.12917426930231
Epoch 3: loss 16.129176620558074
Epoch 4: loss 16.12916789591604
Epoch 5: loss 16.129175222458045
Epoch 6: loss 16.12917612738555
Epoch 7: loss 16.12916989272026
Epoch 8: loss 16.129167767566724
Epoch 9: loss 16.129167712337626
Epoch 10: loss 16.129171002228794
Epoch 11: loss 16.12917049064657
Epoch 12: loss 16.129175860055962
Epoch 13: loss 16.129170446307715
Epoch 14: loss 16.12917857354202
Epoch 15: loss 16.129169316315146
Epoch 16: loss 16.129175570945886
Epoch 17: loss 16.12917184492633
Epoch 18: loss 16.129176819175402
Epoch 19: loss 16.12917319116805
-----------Time: 0:03:51.260306, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 50, rmse: 4.017181873321533-------------


Epoch 0: loss 16.12917420033076
Epoch 1: loss 16.12917116350814
Epoch 2: loss 16.12917344942243
Epoch 3: loss 16.12916787387626
Epoch 4: loss 16.1291721249612
Epoch 5: loss 16.129173146310613
Epoch 6: loss 16.12916509323369
Epoch 7: loss 16.129173989267436
Epoch 8: loss 16.12917121588503
Epoch 9: loss 16.129166120806097
Epoch 10: loss 16.129170084336714
Epoch 11: loss 16.12917757241735
Epoch 12: loss 16.12917312375225
Epoch 13: loss 16.129171879930688
Epoch 14: loss 16.129177971467044
Epoch 15: loss 16.129170949851904
Epoch 16: loss 16.129168849590357
Epoch 17: loss 16.129169832046035
Epoch 18: loss 16.12917130611849
Epoch 19: loss 16.129171011044708
-----------Time: 0:04:33.000977, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129174777773038
Epoch 1: loss 16.12917439661446
Epoch 2: loss 16.129176075267946
Epoch 3: loss 16.129173948558662
Epoch 4: loss 16.129169290385992
Epoch 5: loss 16.12917417569806
Epoch 6: loss 16.129175914247895
Epoch 7: loss 16.12917359747791
Epoch 8: loss 16.12917443680465
Epoch 9: loss 16.129173649336217
Epoch 10: loss 16.129172521158686
Epoch 11: loss 16.129178326696465
Epoch 12: loss 16.129172568090457
Epoch 13: loss 16.12917810992873
Epoch 14: loss 16.129169654431326
Epoch 15: loss 16.12918066991419
Epoch 16: loss 16.129176367748816
Epoch 17: loss 16.12917552971853
Epoch 18: loss 16.129175068698157
Epoch 19: loss 16.1291696705074
-----------Time: 0:05:20.225473, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12916862867396
Epoch 1: loss 16.129168880705343
Epoch 2: loss 16.129171497216362
Epoch 3: loss 16.12917099470934
Epoch 4: loss 16.12916519669102
Epoch 5: loss 16.129173322888157
Epoch 6: loss 16.12917325988031
Epoch 7: loss 16.12916972003209
Epoch 8: loss 16.12916904561477
Epoch 9: loss 16.129173193760966
Epoch 10: loss 16.12916980430184
Epoch 11: loss 16.1291652524387
Epoch 12: loss 16.12916949470773
Epoch 13: loss 16.129168797472754
Epoch 14: loss 16.12916516842824
Epoch 15: loss 16.12917175832295
Epoch 16: loss 16.12916755416978
Epoch 17: loss 16.12916776678885
Epoch 18: loss 16.129171989351722
Epoch 19: loss 16.1291692901267
-----------Time: 0:06:02.049227, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12919022740068
Epoch 1: loss 16.129184698267693
Epoch 2: loss 16.12919382195942
Epoch 3: loss 16.129191132846767
Epoch 4: loss 16.129192144602392
Epoch 5: loss 16.12919037467828
Epoch 6: loss 16.129186048139495
Epoch 7: loss 16.129194559125292
Epoch 8: loss 16.129190644082197
Epoch 9: loss 16.129198254029863
Epoch 10: loss 16.129190179950328
Epoch 11: loss 16.129185524370566
Epoch 12: loss 16.129187909593526
Epoch 13: loss 16.129197030433044
Epoch 14: loss 16.129194847457494
Epoch 15: loss 16.129194648580878
Epoch 16: loss 16.12919525687885
Epoch 17: loss 16.129187989973904
Epoch 18: loss 16.129191579346816
Epoch 19: loss 16.129188411322673
-----------Time: 0:04:09.168208, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 20, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129188200777936
Epoch 1: loss 16.129192806055134
Epoch 2: loss 16.129184246581815
Epoch 3: loss 16.129181266025462
Epoch 4: loss 16.12919219749787
Epoch 5: loss 16.12918462203598
Epoch 6: loss 16.1291893216953
Epoch 7: loss 16.129183517972564
Epoch 8: loss 16.129186856610545
Epoch 9: loss 16.129188049870255
Epoch 10: loss 16.12918841443417
Epoch 11: loss 16.129184736902133
Epoch 12: loss 16.129189210459224
Epoch 13: loss 16.12918633180445
Epoch 14: loss 16.12918528374801
Epoch 15: loss 16.129182491955905
Epoch 16: loss 16.12918482169047
Epoch 17: loss 16.129183875017027
Epoch 18: loss 16.129189135264678
Epoch 19: loss 16.129186610542867
-----------Time: 0:02:46.614883, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129179763690228
Epoch 1: loss 16.129176496616715
Epoch 2: loss 16.129173240433445
Epoch 3: loss 16.12918020604161
Epoch 4: loss 16.129168690126054
Epoch 5: loss 16.129165752352808
Epoch 6: loss 16.129174325309286
Epoch 7: loss 16.129179813733497
Epoch 8: loss 16.12917622980571
Epoch 9: loss 16.129166642759987
Epoch 10: loss 16.12917204432153
Epoch 11: loss 16.129177332313375
Epoch 12: loss 16.129178943810352
Epoch 13: loss 16.12918135444388
Epoch 14: loss 16.129169095398744
Epoch 15: loss 16.129174246743947
Epoch 16: loss 16.129170581917194
Epoch 17: loss 16.12917525227657
Epoch 18: loss 16.129174948127584
Epoch 19: loss 16.12917482859418
-----------Time: 0:03:19.412220, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 100, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129169564457158
Epoch 1: loss 16.129168304040938
Epoch 2: loss 16.129162559436676
Epoch 3: loss 16.129170085114588
Epoch 4: loss 16.129171800587475
Epoch 5: loss 16.12916748571681
Epoch 6: loss 16.12916674025361
Epoch 7: loss 16.129164892801324
Epoch 8: loss 16.129170931442204
Epoch 9: loss 16.12916839971952
Epoch 10: loss 16.129158014315117
Epoch 11: loss 16.12916343713857
Epoch 12: loss 16.129167567134356
Epoch 13: loss 16.129166664281186
Epoch 14: loss 16.129165991678907
Epoch 15: loss 16.129165141202627
Epoch 16: loss 16.12916842694513
Epoch 17: loss 16.129168659788945
Epoch 18: loss 16.129167762380895
Epoch 19: loss 16.12917073541779
-----------Time: 0:04:05.369874, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129174043459372
Epoch 1: loss 16.129180173630168
Epoch 2: loss 16.129177085986406
Epoch 3: loss 16.129175312950792
Epoch 4: loss 16.12917327102985
Epoch 5: loss 16.129168044230806
Epoch 6: loss 16.129172135851448
Epoch 7: loss 16.129169362987625
Epoch 8: loss 16.129174459622305
Epoch 9: loss 16.129171682091236
Epoch 10: loss 16.12917828598769
Epoch 11: loss 16.129177443290157
Epoch 12: loss 16.129171398166992
Epoch 13: loss 16.129174685724536
Epoch 14: loss 16.129173583735454
Epoch 15: loss 16.129170433083846
Epoch 16: loss 16.12917162582497
Epoch 17: loss 16.129172949508327
Epoch 18: loss 16.129174004306346
Epoch 19: loss 16.12917945228108
-----------Time: 0:04:52.039697, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12942118771767
Epoch 1: loss 16.129420489704824
Epoch 2: loss 16.129420751848578
Epoch 3: loss 16.129415700330625
Epoch 4: loss 16.129417638534953
Epoch 5: loss 16.12942242272332
Epoch 6: loss 16.12941826109396
Epoch 7: loss 16.129414263596153
Epoch 8: loss 16.129414514590373
Epoch 9: loss 16.129416198948274
Epoch 10: loss 16.12942163421772
Epoch 11: loss 16.129416464722112
Epoch 12: loss 16.129420043204775
Epoch 13: loss 16.12941410387256
Epoch 14: loss 16.129418048993475
Epoch 15: loss 16.12942731296193
Epoch 16: loss 16.129419979678346
Epoch 17: loss 16.1294150046514
Epoch 18: loss 16.12941453559299
Epoch 19: loss 16.129415058843335
-----------Time: 0:02:54.491314, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 20, rmse: 4.0171966552734375-------------


Epoch 0: loss 16.129187150647162
Epoch 1: loss 16.12917795928034
Epoch 2: loss 16.129184058595442
Epoch 3: loss 16.12918359135207
Epoch 4: loss 16.12918681564248
Epoch 5: loss 16.129182349604843
Epoch 6: loss 16.129186573464175
Epoch 7: loss 16.12918366136079
Epoch 8: loss 16.1291837500385
Epoch 9: loss 16.129181499387855
Epoch 10: loss 16.12917643412745
Epoch 11: loss 16.129182879337478
Epoch 12: loss 16.12918105444356
Epoch 13: loss 16.129183570349458
Epoch 14: loss 16.12917963819312
Epoch 15: loss 16.129176567403306
Epoch 16: loss 16.12918530008338
Epoch 17: loss 16.129187498875712
Epoch 18: loss 16.129184812096682
Epoch 19: loss 16.12917617820669
-----------Time: 0:03:25.217010, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12916869583047
Epoch 1: loss 16.12917300032947
Epoch 2: loss 16.12916989972113
Epoch 3: loss 16.12917456204247
Epoch 4: loss 16.129168759356897
Epoch 5: loss 16.129169349504462
Epoch 6: loss 16.12917463568127
Epoch 7: loss 16.129172066879896
Epoch 8: loss 16.129174801627858
Epoch 9: loss 16.129175205604092
Epoch 10: loss 16.129174969648783
Epoch 11: loss 16.12917362625927
Epoch 12: loss 16.12917796757767
Epoch 13: loss 16.12915998337514
Epoch 14: loss 16.129171726689382
Epoch 15: loss 16.129179449688163
Epoch 16: loss 16.129175349510902
Epoch 17: loss 16.129170173533005
Epoch 18: loss 16.129179993941126
Epoch 19: loss 16.129167141118344
-----------Time: 0:04:04.878941, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129178050550966
Epoch 1: loss 16.129176105345767
Epoch 2: loss 16.129177504482964
Epoch 3: loss 16.129184723937556
Epoch 4: loss 16.129179930155402
Epoch 5: loss 16.12917977406189
Epoch 6: loss 16.129178806904413
Epoch 7: loss 16.129183613391852
Epoch 8: loss 16.12918415868198
Epoch 9: loss 16.129187521693368
Epoch 10: loss 16.12918072332825
Epoch 11: loss 16.129182254444846
Epoch 12: loss 16.129181403449984
Epoch 13: loss 16.12918087734743
Epoch 14: loss 16.129183477782373
Epoch 15: loss 16.129185712875525
Epoch 16: loss 16.129189338289958
Epoch 17: loss 16.129188279861857
Epoch 18: loss 16.12918508590856
Epoch 19: loss 16.129185905269853
-----------Time: 0:04:55.518057, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129171312082196
Epoch 1: loss 16.129169639651707
Epoch 2: loss 16.129170561692455
Epoch 3: loss 16.12917389462602
Epoch 4: loss 16.129174460400183
Epoch 5: loss 16.129171638530256
Epoch 6: loss 16.129170349591966
Epoch 7: loss 16.129170866360024
Epoch 8: loss 16.129169610611054
Epoch 9: loss 16.12917144665451
Epoch 10: loss 16.129174284600513
Epoch 11: loss 16.129174732137724
Epoch 12: loss 16.129170515538558
Epoch 13: loss 16.129175212864258
Epoch 14: loss 16.129172242160983
Epoch 15: loss 16.12917096437223
Epoch 16: loss 16.129166400581678
Epoch 17: loss 16.12916738303736
Epoch 18: loss 16.12916595693384
Epoch 19: loss 16.129170533429676
-----------Time: 0:05:33.340436, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129414655126393
Epoch 1: loss 16.12941165304884
Epoch 2: loss 16.129409562640376
Epoch 3: loss 16.129405895998584
Epoch 4: loss 16.12941061147469
Epoch 5: loss 16.12940579176338
Epoch 6: loss 16.12940762676967
Epoch 7: loss 16.12941629488615
Epoch 8: loss 16.12941160144982
Epoch 9: loss 16.12941155892601
Epoch 10: loss 16.129415033692055
Epoch 11: loss 16.12941382772706
Epoch 12: loss 16.129409367393837
Epoch 13: loss 16.12941503161772
Epoch 14: loss 16.129406572749524
Epoch 15: loss 16.129406989690334
Epoch 16: loss 16.129412929281845
Epoch 17: loss 16.12941339730309
Epoch 18: loss 16.129408552959084
Epoch 19: loss 16.129412981658735
-----------Time: 0:03:54.289772, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 20, rmse: 4.017153263092041-------------


Epoch 0: loss 16.12916935572746
Epoch 1: loss 16.129166536709743
Epoch 2: loss 16.12917415365828
Epoch 3: loss 16.12917223671586
Epoch 4: loss 16.129166778110175
Epoch 5: loss 16.129167276727824
Epoch 6: loss 16.129171989870304
Epoch 7: loss 16.12917137431217
Epoch 8: loss 16.12916630360664
Epoch 9: loss 16.129164208530927
Epoch 10: loss 16.129164358142148
Epoch 11: loss 16.129169907240584
Epoch 12: loss 16.12917395944891
Epoch 13: loss 16.12916849176802
Epoch 14: loss 16.129162917777595
Epoch 15: loss 16.12916987431056
Epoch 16: loss 16.129172177338095
Epoch 17: loss 16.12916523247325
Epoch 18: loss 16.129169990991755
Epoch 19: loss 16.129167850799313
-----------Time: 0:04:10.977245, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 50, rmse: 4.01719331741333-------------


Epoch 0: loss 16.129189769751097
Epoch 1: loss 16.129191868716184
Epoch 2: loss 16.129177827300943
Epoch 3: loss 16.129192482718572
Epoch 4: loss 16.129182066458473
Epoch 5: loss 16.129187593517127
Epoch 6: loss 16.12918515980665
Epoch 7: loss 16.129185532408606
Epoch 8: loss 16.129184946928287
Epoch 9: loss 16.129183870090486
Epoch 10: loss 16.129193911933587
Epoch 11: loss 16.129189128004512
Epoch 12: loss 16.12918317207764
Epoch 13: loss 16.12918179757314
Epoch 14: loss 16.129186365512354
Epoch 15: loss 16.12918805920475
Epoch 16: loss 16.129192948924775
Epoch 17: loss 16.129188992654324
Epoch 18: loss 16.1291891744177
Epoch 19: loss 16.129186501381124
-----------Time: 0:05:02.555344, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129177943204265
Epoch 1: loss 16.12918960924963
Epoch 2: loss 16.129182879337478
Epoch 3: loss 16.129174959536414
Epoch 4: loss 16.129174181661767
Epoch 5: loss 16.129180137847932
Epoch 6: loss 16.129178944328935
Epoch 7: loss 16.129177922979522
Epoch 8: loss 16.12918839939526
Epoch 9: loss 16.129179171986916
Epoch 10: loss 16.129178464380278
Epoch 11: loss 16.129183685993485
Epoch 12: loss 16.129176818397525
Epoch 13: loss 16.129179726870827
Epoch 14: loss 16.129179839921942
Epoch 15: loss 16.129182465508165
Epoch 16: loss 16.129178021510313
Epoch 17: loss 16.129178890137002
Epoch 18: loss 16.129177253229454
Epoch 19: loss 16.129183195154585
-----------Time: 0:04:28.960426, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129173267659056
Epoch 1: loss 16.129171649939085
Epoch 2: loss 16.1291699987705
Epoch 3: loss 16.12917846930682
Epoch 4: loss 16.129169485113945
Epoch 5: loss 16.129171772065405
Epoch 6: loss 16.129173560399217
Epoch 7: loss 16.12917457422917
Epoch 8: loss 16.12917438287201
Epoch 9: loss 16.129179557034863
Epoch 10: loss 16.12917575582076
Epoch 11: loss 16.12918107933555
Epoch 12: loss 16.129179859887394
Epoch 13: loss 16.129183977437187
Epoch 14: loss 16.129170051925268
Epoch 15: loss 16.129173322369574
Epoch 16: loss 16.129175190824473
Epoch 17: loss 16.129176064636994
Epoch 18: loss 16.12916947629803
Epoch 19: loss 16.1291776782083
-----------Time: 0:04:17.674526, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129247582950562
Epoch 1: loss 16.129246800408666
Epoch 2: loss 16.129245698678876
Epoch 3: loss 16.12923975830949
Epoch 4: loss 16.129241734629677
Epoch 5: loss 16.129248255552838
Epoch 6: loss 16.12924094456833
Epoch 7: loss 16.12924316954911
Epoch 8: loss 16.129249728328837
Epoch 9: loss 16.129249083989336
Epoch 10: loss 16.129250781052523
Epoch 11: loss 16.129249920463874
Epoch 12: loss 16.129246543710032
Epoch 13: loss 16.129243317086
Epoch 14: loss 16.129244287095684
Epoch 15: loss 16.129240354680057
Epoch 16: loss 16.129251660310164
Epoch 17: loss 16.129247356070454
Epoch 18: loss 16.12923985658099
Epoch 19: loss 16.129245903000616
-----------Time: 0:02:28.928518, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 20, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129179966974803
Epoch 1: loss 16.129176693159707
Epoch 2: loss 16.129177034387386
Epoch 3: loss 16.129182207772367
Epoch 4: loss 16.12918302013279
Epoch 5: loss 16.12917770439675
Epoch 6: loss 16.129177968096254
Epoch 7: loss 16.129180132402812
Epoch 8: loss 16.129176316668378
Epoch 9: loss 16.12917217993101
Epoch 10: loss 16.129182135948607
Epoch 11: loss 16.12917369704586
Epoch 12: loss 16.12917840448393
Epoch 13: loss 16.12918838072627
Epoch 14: loss 16.129176783393167
Epoch 15: loss 16.12918207605226
Epoch 16: loss 16.12918112523015
Epoch 17: loss 16.129174606640614
Epoch 18: loss 16.129172899205766
Epoch 19: loss 16.12917979428663
-----------Time: 0:03:06.616081, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129185410022995
Epoch 1: loss 16.129181780200604
Epoch 2: loss 16.129176503358295
Epoch 3: loss 16.129175154523658
Epoch 4: loss 16.129177787370043
Epoch 5: loss 16.12918041165981
Epoch 6: loss 16.129182041307192
Epoch 7: loss 16.129183809675556
Epoch 8: loss 16.12917408209381
Epoch 9: loss 16.12917174432121
Epoch 10: loss 16.129175960661083
Epoch 11: loss 16.129179591002057
Epoch 12: loss 16.129178922807736
Epoch 13: loss 16.12917599851765
Epoch 14: loss 16.12917907941983
Epoch 15: loss 16.12918327968363
Epoch 16: loss 16.129177007680358
Epoch 17: loss 16.129171931529708
Epoch 18: loss 16.1291767455366
Epoch 19: loss 16.129168474136193
-----------Time: 0:03:49.023984, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 100, rmse: 4.017190933227539-------------


Epoch 0: loss 16.12916835823287
Epoch 1: loss 16.12916831285685
Epoch 2: loss 16.129171951235865
Epoch 3: loss 16.129179920043033
Epoch 4: loss 16.129181341738594
Epoch 5: loss 16.1291677805313
Epoch 6: loss 16.129176197394266
Epoch 7: loss 16.129171941382786
Epoch 8: loss 16.12916892219199
Epoch 9: loss 16.129169219340106
Epoch 10: loss 16.12916773334024
Epoch 11: loss 16.12917616290849
Epoch 12: loss 16.129164000579102
Epoch 13: loss 16.129170536541174
Epoch 14: loss 16.129171790993688
Epoch 15: loss 16.129171440690804
Epoch 16: loss 16.129170766532777
Epoch 17: loss 16.129171295487538
Epoch 18: loss 16.129168738613576
Epoch 19: loss 16.129172120034664
-----------Time: 0:04:32.454046, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.1291790252279
Epoch 1: loss 16.129178981407627
Epoch 2: loss 16.129179280889364
Epoch 3: loss 16.12918870095133
Epoch 4: loss 16.129183419182482
Epoch 5: loss 16.129186205529468
Epoch 6: loss 16.129178328511507
Epoch 7: loss 16.12918528348872
Epoch 8: loss 16.12918024571322
Epoch 9: loss 16.12917197223848
Epoch 10: loss 16.12917937060424
Epoch 11: loss 16.129180550380788
Epoch 12: loss 16.12918025997425
Epoch 13: loss 16.12917225486627
Epoch 14: loss 16.129179700682382
Epoch 15: loss 16.129179102237487
Epoch 16: loss 16.129172234122944
Epoch 17: loss 16.129171161174515
Epoch 18: loss 16.129185632754435
Epoch 19: loss 16.12918017388946
-----------Time: 0:05:05.025718, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129444154725896
Epoch 1: loss 16.129448714627074
Epoch 2: loss 16.129448326726916
Epoch 3: loss 16.12944758074513
Epoch 4: loss 16.12944329154433
Epoch 5: loss 16.129447455766606
Epoch 6: loss 16.12944384098312
Epoch 7: loss 16.12944735490219
Epoch 8: loss 16.129442967170604
Epoch 9: loss 16.12945061549342
Epoch 10: loss 16.12944598687998
Epoch 11: loss 16.129435832245054
Epoch 12: loss 16.12944431263445
Epoch 13: loss 16.12944239880353
Epoch 14: loss 16.129450213591518
Epoch 15: loss 16.129450297342686
Epoch 16: loss 16.129446717563564
Epoch 17: loss 16.12944286137965
Epoch 18: loss 16.129447942975425
Epoch 19: loss 16.12944571566102
-----------Time: 0:03:27.887141, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 20, rmse: 4.0172119140625-------------


Epoch 0: loss 16.129151947411458
Epoch 1: loss 16.12914425500908
Epoch 2: loss 16.12915282329831
Epoch 3: loss 16.129144620869457
Epoch 4: loss 16.12914389174162
Epoch 5: loss 16.129149350088014
Epoch 6: loss 16.12914918699363
Epoch 7: loss 16.129147431849137
Epoch 8: loss 16.129146734873455
Epoch 9: loss 16.129157178359165
Epoch 10: loss 16.12915264905439
Epoch 11: loss 16.129147616724012
Epoch 12: loss 16.129147749481284
Epoch 13: loss 16.129151129865207
Epoch 14: loss 16.129145561319906
Epoch 15: loss 16.129140546102768
Epoch 16: loss 16.129146192435535
Epoch 17: loss 16.12914669260893
Epoch 18: loss 16.12914516019588
Epoch 19: loss 16.129147138849685
-----------Time: 0:03:49.090951, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 50, rmse: 4.017176151275635-------------


Epoch 0: loss 16.12917939757056
Epoch 1: loss 16.12917703335022
Epoch 2: loss 16.12918044329338
Epoch 3: loss 16.12918169904235
Epoch 4: loss 16.129177651501273
Epoch 5: loss 16.129179746576987
Epoch 6: loss 16.12918028979278
Epoch 7: loss 16.129182435171057
Epoch 8: loss 16.129177308977138
Epoch 9: loss 16.12917940716435
Epoch 10: loss 16.1291815102781
Epoch 11: loss 16.129190005447114
Epoch 12: loss 16.129177797741704
Epoch 13: loss 16.129178624881746
Epoch 14: loss 16.12917885487335
Epoch 15: loss 16.129179503102222
Epoch 16: loss 16.129185741138304
Epoch 17: loss 16.129186712963026
Epoch 18: loss 16.129181666371615
Epoch 19: loss 16.129177835598274
-----------Time: 0:04:34.529272, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12918253136822
Epoch 1: loss 16.129181183830042
Epoch 2: loss 16.12918381045343
Epoch 3: loss 16.12918229515362
Epoch 4: loss 16.129179478469524
Epoch 5: loss 16.129182294375745
Epoch 6: loss 16.12917348598254
Epoch 7: loss 16.129176848216055
Epoch 8: loss 16.129178208978104
Epoch 9: loss 16.12918419031555
Epoch 10: loss 16.129187959896086
Epoch 11: loss 16.12918428547555
Epoch 12: loss 16.12917833577167
Epoch 13: loss 16.12917965997361
Epoch 14: loss 16.129176756167553
Epoch 15: loss 16.129178681666595
Epoch 16: loss 16.129178201458647
Epoch 17: loss 16.129173364115513
Epoch 18: loss 16.129184910886764
Epoch 19: loss 16.129171989611013
-----------Time: 0:05:25.826840, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129174166104274
Epoch 1: loss 16.12917296506582
Epoch 2: loss 16.12917617820669
Epoch 3: loss 16.129178926697108
Epoch 4: loss 16.12917387725349
Epoch 5: loss 16.12917289298277
Epoch 6: loss 16.12917598192299
Epoch 7: loss 16.1291749875399
Epoch 8: loss 16.129173730494472
Epoch 9: loss 16.129169378285827
Epoch 10: loss 16.12917031199469
Epoch 11: loss 16.129175868094
Epoch 12: loss 16.12918158028682
Epoch 13: loss 16.12917191726867
Epoch 14: loss 16.129178797051335
Epoch 15: loss 16.129175651585555
Epoch 16: loss 16.129173678895455
Epoch 17: loss 16.12917678520821
Epoch 18: loss 16.129173203873336
Epoch 19: loss 16.12917061795872
-----------Time: 0:06:01.529619, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129305333401472
Epoch 1: loss 16.129304317497184
Epoch 2: loss 16.12930601093029
Epoch 3: loss 16.12930550712681
Epoch 4: loss 16.129305948441026
Epoch 5: loss 16.129306342564178
Epoch 6: loss 16.12931006313861
Epoch 7: loss 16.129304969096847
Epoch 8: loss 16.129302730632904
Epoch 9: loss 16.12930191334594
Epoch 10: loss 16.129306623117635
Epoch 11: loss 16.129312263745987
Epoch 12: loss 16.129307248010267
Epoch 13: loss 16.12929784298721
Epoch 14: loss 16.129302742819608
Epoch 15: loss 16.12929860089641
Epoch 16: loss 16.129304828042244
Epoch 17: loss 16.129299260015525
Epoch 18: loss 16.129297651111465
Epoch 19: loss 16.129306080161133
-----------Time: 0:03:58.478013, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 20, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12918801875527
Epoch 1: loss 16.129183054877856
Epoch 2: loss 16.129182590745984
Epoch 3: loss 16.12917994337927
Epoch 4: loss 16.12918439930454
Epoch 5: loss 16.129185988761733
Epoch 6: loss 16.129182024712534
Epoch 7: loss 16.129181168013258
Epoch 8: loss 16.129171218477946
Epoch 9: loss 16.129190583926558
Epoch 10: loss 16.129183836641875
Epoch 11: loss 16.129187595072878
Epoch 12: loss 16.1291849925636
Epoch 13: loss 16.129181192905246
Epoch 14: loss 16.129174614678654
Epoch 15: loss 16.12918491659118
Epoch 16: loss 16.129183511749567
Epoch 17: loss 16.129176851068262
Epoch 18: loss 16.12918137596508
Epoch 19: loss 16.129180788929013
-----------Time: 0:02:46.038135, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129180452887166
Epoch 1: loss 16.129182227219232
Epoch 2: loss 16.129186153152574
Epoch 3: loss 16.129182422725062
Epoch 4: loss 16.129176069563535
Epoch 5: loss 16.12917901226332
Epoch 6: loss 16.129181039145358
Epoch 7: loss 16.129192151862554
Epoch 8: loss 16.129186512012076
Epoch 9: loss 16.129181576397446
Epoch 10: loss 16.129178954700595
Epoch 11: loss 16.129178909583867
Epoch 12: loss 16.12918451339282
Epoch 13: loss 16.129178437673247
Epoch 14: loss 16.12917888650692
Epoch 15: loss 16.129179180024952
Epoch 16: loss 16.129177533782908
Epoch 17: loss 16.129175497047793
Epoch 18: loss 16.129182659976827
Epoch 19: loss 16.129177190221608
-----------Time: 0:03:17.430537, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129168943453898
Epoch 1: loss 16.129171321935274
Epoch 2: loss 16.12917907941983
Epoch 3: loss 16.129172592723155
Epoch 4: loss 16.12917814208088
Epoch 5: loss 16.12917223178932
Epoch 6: loss 16.129172684771657
Epoch 7: loss 16.129176945709677
Epoch 8: loss 16.129175256425235
Epoch 9: loss 16.12917247267117
Epoch 10: loss 16.129172815713886
Epoch 11: loss 16.129174764030584
Epoch 12: loss 16.129176680713712
Epoch 13: loss 16.129167913547867
Epoch 14: loss 16.12917307267181
Epoch 15: loss 16.129176021335304
Epoch 16: loss 16.129174732397015
Epoch 17: loss 16.12917618131819
Epoch 18: loss 16.129175701110242
Epoch 19: loss 16.12917515141216
-----------Time: 0:04:04.694689, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129179309411434
Epoch 1: loss 16.129177271639154
Epoch 2: loss 16.129175758672964
Epoch 3: loss 16.129182997315134
Epoch 4: loss 16.129178707855043
Epoch 5: loss 16.129176446832737
Epoch 6: loss 16.129176754352514
Epoch 7: loss 16.129179356602496
Epoch 8: loss 16.129180089360414
Epoch 9: loss 16.129176477947723
Epoch 10: loss 16.129177718657782
Epoch 11: loss 16.129173458756927
Epoch 12: loss 16.129178987371333
Epoch 13: loss 16.129172683734488
Epoch 14: loss 16.129176624188155
Epoch 15: loss 16.129180588755936
Epoch 16: loss 16.1291767854675
Epoch 17: loss 16.129184649002298
Epoch 18: loss 16.129186297059384
Epoch 19: loss 16.12917888132109
-----------Time: 0:04:44.497617, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129335568092515
Epoch 1: loss 16.129333398340837
Epoch 2: loss 16.129334098428018
Epoch 3: loss 16.129337982615418
Epoch 4: loss 16.12933664026307
Epoch 5: loss 16.129337062908295
Epoch 6: loss 16.12933123896082
Epoch 7: loss 16.12933411709701
Epoch 8: loss 16.129341763864073
Epoch 9: loss 16.12933366152176
Epoch 10: loss 16.129330213722035
Epoch 11: loss 16.12933251830532
Epoch 12: loss 16.129332718478395
Epoch 13: loss 16.129334842594762
Epoch 14: loss 16.12933606385796
Epoch 15: loss 16.12932896445535
Epoch 16: loss 16.129339787284596
Epoch 17: loss 16.12933350750258
Epoch 18: loss 16.129333241988032
Epoch 19: loss 16.129337957204847
-----------Time: 0:02:57.850029, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 20, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129181807426217
Epoch 1: loss 16.12918159376998
Epoch 2: loss 16.12919056240536
Epoch 3: loss 16.12918995747818
Epoch 4: loss 16.129190146761008
Epoch 5: loss 16.12918597683432
Epoch 6: loss 16.12918794044922
Epoch 7: loss 16.129189151340753
Epoch 8: loss 16.129189249093667
Epoch 9: loss 16.129190553330158
Epoch 10: loss 16.129195709861186
Epoch 11: loss 16.12919004330368
Epoch 12: loss 16.12918717553915
Epoch 13: loss 16.129185592823536
Epoch 14: loss 16.129194325503608
Epoch 15: loss 16.129193670533155
Epoch 16: loss 16.129180126698397
Epoch 17: loss 16.12919023388297
Epoch 18: loss 16.12918410967588
Epoch 19: loss 16.129183806045475
-----------Time: 0:03:19.637983, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 50, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917481718535
Epoch 1: loss 16.12916442478008
Epoch 2: loss 16.129170933775825
Epoch 3: loss 16.129170538356213
Epoch 4: loss 16.129173155385818
Epoch 5: loss 16.129169159184467
Epoch 6: loss 16.129169518562552
Epoch 7: loss 16.129173563770006
Epoch 8: loss 16.12916463921419
Epoch 9: loss 16.129162649410844
Epoch 10: loss 16.1291747666235
Epoch 11: loss 16.129166922016985
Epoch 12: loss 16.129164236793706
Epoch 13: loss 16.129175119000717
Epoch 14: loss 16.129174389354297
Epoch 15: loss 16.129169495485606
Epoch 16: loss 16.129168299632983
Epoch 17: loss 16.1291680750865
Epoch 18: loss 16.129170032478402
Epoch 19: loss 16.129164087701064
-----------Time: 0:04:06.048719, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129172298945832
Epoch 1: loss 16.12917176195303
Epoch 2: loss 16.12917511251843
Epoch 3: loss 16.12917056091458
Epoch 4: loss 16.129177009236106
Epoch 5: loss 16.129163245522115
Epoch 6: loss 16.129177158328748
Epoch 7: loss 16.12917115702585
Epoch 8: loss 16.129170277508916
Epoch 9: loss 16.129168259702084
Epoch 10: loss 16.129172798600646
Epoch 11: loss 16.12916123756836
Epoch 12: loss 16.129176324706417
Epoch 13: loss 16.12916455753735
Epoch 14: loss 16.12916537482431
Epoch 15: loss 16.129172083215263
Epoch 16: loss 16.129171490734073
Epoch 17: loss 16.12917437613043
Epoch 18: loss 16.12916976748244
Epoch 19: loss 16.12917517008115
-----------Time: 0:04:56.889617, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129175940954926
Epoch 1: loss 16.12917203576491
Epoch 2: loss 16.129175314506544
Epoch 3: loss 16.12918300664963
Epoch 4: loss 16.129185136988994
Epoch 5: loss 16.129175180452812
Epoch 6: loss 16.129182237850188
Epoch 7: loss 16.12918662635965
Epoch 8: loss 16.129180781150264
Epoch 9: loss 16.129184170868683
Epoch 10: loss 16.12918082704487
Epoch 11: loss 16.129178319695594
Epoch 12: loss 16.129179727907996
Epoch 13: loss 16.129181569915158
Epoch 14: loss 16.12918005202243
Epoch 15: loss 16.12917658762805
Epoch 16: loss 16.12917790742203
Epoch 17: loss 16.129177994025408
Epoch 18: loss 16.129182160062722
Epoch 19: loss 16.129181073890425
-----------Time: 0:05:34.756547, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129360957920973
Epoch 1: loss 16.129358268030444
Epoch 2: loss 16.12936764893939
Epoch 3: loss 16.129363421449977
Epoch 4: loss 16.12936964133565
Epoch 5: loss 16.129357079178696
Epoch 6: loss 16.12936164374712
Epoch 7: loss 16.12935987278584
Epoch 8: loss 16.129361927930656
Epoch 9: loss 16.129359272525907
Epoch 10: loss 16.1293598004435
Epoch 11: loss 16.12936476302445
Epoch 12: loss 16.12936245610754
Epoch 13: loss 16.129358255843744
Epoch 14: loss 16.12936087079901
Epoch 15: loss 16.129365540899098
Epoch 16: loss 16.129357458003646
Epoch 17: loss 16.129358100787396
Epoch 18: loss 16.129362009348203
Epoch 19: loss 16.12935968635522
-----------Time: 0:03:55.516136, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 20, rmse: 4.017171859741211-------------


Epoch 0: loss 16.12917129600612
Epoch 1: loss 16.129170306290277
Epoch 2: loss 16.12917271251585
Epoch 3: loss 16.12917496212933
Epoch 4: loss 16.129171695055813
Epoch 5: loss 16.129173970339153
Epoch 6: loss 16.129174664981214
Epoch 7: loss 16.129174578637127
Epoch 8: loss 16.12917226860872
Epoch 9: loss 16.12917514337412
Epoch 10: loss 16.129174132914954
Epoch 11: loss 16.12917306567094
Epoch 12: loss 16.12917437198176
Epoch 13: loss 16.129175100072434
Epoch 14: loss 16.129179192730238
Epoch 15: loss 16.129172100069212
Epoch 16: loss 16.129173518393983
Epoch 17: loss 16.12917433256945
Epoch 18: loss 16.129173774314744
Epoch 19: loss 16.12917385910308
-----------Time: 0:04:40.635931, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 50, rmse: 4.0171990394592285-------------


Epoch 0: loss 16.129182793252685
Epoch 1: loss 16.129188094986983
Epoch 2: loss 16.12918313914761
Epoch 3: loss 16.129185369573513
Epoch 4: loss 16.129178958849263
Epoch 5: loss 16.129180042428644
Epoch 6: loss 16.1291806665434
Epoch 7: loss 16.129184608293524
Epoch 8: loss 16.129184168275767
Epoch 9: loss 16.12917874804523
Epoch 10: loss 16.12918833846175
Epoch 11: loss 16.129179295409692
Epoch 12: loss 16.129179880371424
Epoch 13: loss 16.129180897831464
Epoch 14: loss 16.129183730850926
Epoch 15: loss 16.12918034320684
Epoch 16: loss 16.129178949774058
Epoch 17: loss 16.129177166626075
Epoch 18: loss 16.129181632923004
Epoch 19: loss 16.129176742943685
-----------Time: 0:05:07.693147, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129183564904334
Epoch 1: loss 16.129184000514137
Epoch 2: loss 16.129173638186682
Epoch 3: loss 16.129180397917356
Epoch 4: loss 16.129170722193923
Epoch 5: loss 16.129171850630744
Epoch 6: loss 16.12918411175021
Epoch 7: loss 16.12917958114898
Epoch 8: loss 16.129171896266055
Epoch 9: loss 16.129176741128646
Epoch 10: loss 16.12918253603547
Epoch 11: loss 16.129172965584402
Epoch 12: loss 16.129179575963146
Epoch 13: loss 16.129175653400598
Epoch 14: loss 16.129181047701977
Epoch 15: loss 16.12918140474644
Epoch 16: loss 16.129179034043812
Epoch 17: loss 16.12917644890707
Epoch 18: loss 16.129176967490167
Epoch 19: loss 16.129182969052355
-----------Time: 0:04:23.490305, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.1291676547749
Epoch 1: loss 16.12917787241767
Epoch 2: loss 16.129173860399536
Epoch 3: loss 16.129175343806487
Epoch 4: loss 16.12917233732098
Epoch 5: loss 16.12917608797323
Epoch 6: loss 16.129170792721226
Epoch 7: loss 16.129177264638283
Epoch 8: loss 16.129169363506207
Epoch 9: loss 16.12917409687343
Epoch 10: loss 16.129172985809145
Epoch 11: loss 16.129179288927403
Epoch 12: loss 16.129167305768476
Epoch 13: loss 16.129173110528377
Epoch 14: loss 16.129167645699695
Epoch 15: loss 16.129174146398118
Epoch 16: loss 16.12917313723541
Epoch 17: loss 16.129177457551194
Epoch 18: loss 16.12916583169602
Epoch 19: loss 16.129175866797542
-----------Time: 0:04:17.987940, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129285130441158
Epoch 1: loss 16.12927724045862
Epoch 2: loss 16.129285526897934
Epoch 3: loss 16.12928614375253
Epoch 4: loss 16.1292829936195
Epoch 5: loss 16.12928859172404
Epoch 6: loss 16.12928007970108
Epoch 7: loss 16.129288823789977
Epoch 8: loss 16.129274825417134
Epoch 9: loss 16.129283574173282
Epoch 10: loss 16.129283904770006
Epoch 11: loss 16.12928580200627
Epoch 12: loss 16.129282052909765
Epoch 13: loss 16.129280916434904
Epoch 14: loss 16.129276377795637
Epoch 15: loss 16.12928174124132
Epoch 16: loss 16.129284102609457
Epoch 17: loss 16.129289984897532
Epoch 18: loss 16.12928155143991
Epoch 19: loss 16.129279408913842
-----------Time: 0:02:30.442073, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 20, rmse: 4.017205238342285-------------


Epoch 0: loss 16.129142164341324
Epoch 1: loss 16.12913848058629
Epoch 2: loss 16.12914047790909
Epoch 3: loss 16.129137603662272
Epoch 4: loss 16.129139279722843
Epoch 5: loss 16.129135022155612
Epoch 6: loss 16.129143023633517
Epoch 7: loss 16.129140413604787
Epoch 8: loss 16.129141471514306
Epoch 9: loss 16.129138598045362
Epoch 10: loss 16.12914175206776
Epoch 11: loss 16.129138742989337
Epoch 12: loss 16.129137666151536
Epoch 13: loss 16.129132891297665
Epoch 14: loss 16.129136429071558
Epoch 15: loss 16.129135219735772
Epoch 16: loss 16.129136929763536
Epoch 17: loss 16.129141220001504
Epoch 18: loss 16.129141032274422
Epoch 19: loss 16.12913869450182
-----------Time: 0:03:07.286948, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 50, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129172887019063
Epoch 1: loss 16.129172467226045
Epoch 2: loss 16.12917496575941
Epoch 3: loss 16.129173792983735
Epoch 4: loss 16.1291703148469
Epoch 5: loss 16.129177022459974
Epoch 6: loss 16.129180743812285
Epoch 7: loss 16.12917471113511
Epoch 8: loss 16.12917221156458
Epoch 9: loss 16.12917182833167
Epoch 10: loss 16.12917517345194
Epoch 11: loss 16.129171700500937
Epoch 12: loss 16.129179069826044
Epoch 13: loss 16.129174390391462
Epoch 14: loss 16.129174136026453
Epoch 15: loss 16.129171947087197
Epoch 16: loss 16.1291722722388
Epoch 17: loss 16.12916950637585
Epoch 18: loss 16.129172617355852
Epoch 19: loss 16.12917120940274
-----------Time: 0:03:49.407994, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129175724446483
Epoch 1: loss 16.129169763074483
Epoch 2: loss 16.12917188174573
Epoch 3: loss 16.129176113643098
Epoch 4: loss 16.129176088491818
Epoch 5: loss 16.129167934031898
Epoch 6: loss 16.129174884341865
Epoch 7: loss 16.129172065583436
Epoch 8: loss 16.129169662728653
Epoch 9: loss 16.12918183283679
Epoch 10: loss 16.12916268726741
Epoch 11: loss 16.12917174095042
Epoch 12: loss 16.12917574441193
Epoch 13: loss 16.12916331864233
Epoch 14: loss 16.129171219774406
Epoch 15: loss 16.129170831355665
Epoch 16: loss 16.12917422081479
Epoch 17: loss 16.129167468344278
Epoch 18: loss 16.129175084774232
Epoch 19: loss 16.129175304912756
-----------Time: 0:04:34.351406, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 150, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129176322372793
Epoch 1: loss 16.12917078986902
Epoch 2: loss 16.129180944763235
Epoch 3: loss 16.12917645201857
Epoch 4: loss 16.12917607889803
Epoch 5: loss 16.12917133256623
Epoch 6: loss 16.12917800595282
Epoch 7: loss 16.129172122627576
Epoch 8: loss 16.12917451718503
Epoch 9: loss 16.129176770947172
Epoch 10: loss 16.129178111743773
Epoch 11: loss 16.129173177166308
Epoch 12: loss 16.129179546663202
Epoch 13: loss 16.129181075705464
Epoch 14: loss 16.129175189268725
Epoch 15: loss 16.129177461699857
Epoch 16: loss 16.12917394259496
Epoch 17: loss 16.12917060266052
Epoch 18: loss 16.129177884863665
Epoch 19: loss 16.129176590480256
-----------Time: 0:05:05.385972, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129205814452842
Epoch 1: loss 16.12920734038361
Epoch 2: loss 16.12920901644418
Epoch 3: loss 16.12920783614905
Epoch 4: loss 16.129202203558734
Epoch 5: loss 16.129209545917522
Epoch 6: loss 16.12920652906035
Epoch 7: loss 16.129205641246088
Epoch 8: loss 16.12919606509061
Epoch 9: loss 16.129207002008137
Epoch 10: loss 16.129211091554442
Epoch 11: loss 16.129206077633764
Epoch 12: loss 16.129210587232382
Epoch 13: loss 16.129204142022353
Epoch 14: loss 16.12920921117213
Epoch 15: loss 16.129207951015204
Epoch 16: loss 16.129198733200646
Epoch 17: loss 16.129206725862637
Epoch 18: loss 16.129210654907475
Epoch 19: loss 16.129209292589678
-----------Time: 0:03:27.626224, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 20, rmse: 4.017205715179443-------------


Epoch 0: loss 16.12920347823599
Epoch 1: loss 16.129204290855704
Epoch 2: loss 16.129208587057374
Epoch 3: loss 16.1292061839433
Epoch 4: loss 16.12920864021214
Epoch 5: loss 16.129204072013636
Epoch 6: loss 16.129207324826115
Epoch 7: loss 16.12920863347056
Epoch 8: loss 16.1291992224838
Epoch 9: loss 16.12920216103492
Epoch 10: loss 16.12920131678164
Epoch 11: loss 16.12920394366432
Epoch 12: loss 16.129204469766872
Epoch 13: loss 16.129208541940645
Epoch 14: loss 16.12920219448353
Epoch 15: loss 16.129202741329408
Epoch 16: loss 16.12920546259421
Epoch 17: loss 16.129204473396953
Epoch 18: loss 16.129203551356206
Epoch 19: loss 16.129207969165613
-----------Time: 0:03:53.317954, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 50, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917378727932
Epoch 1: loss 16.12917425763419
Epoch 2: loss 16.12917972842658
Epoch 3: loss 16.129179022634982
Epoch 4: loss 16.129170375521124
Epoch 5: loss 16.12917204276578
Epoch 6: loss 16.129171820552923
Epoch 7: loss 16.129171404389986
Epoch 8: loss 16.12917110879762
Epoch 9: loss 16.129164065920573
Epoch 10: loss 16.12917785504514
Epoch 11: loss 16.129167846909937
Epoch 12: loss 16.129173164979605
Epoch 13: loss 16.1291725883152
Epoch 14: loss 16.129173167313226
Epoch 15: loss 16.129175434039947
Epoch 16: loss 16.12917787578846
Epoch 17: loss 16.12917894303248
Epoch 18: loss 16.129178488494393
Epoch 19: loss 16.129170370853874
-----------Time: 0:04:34.231014, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.1291736682645
Epoch 1: loss 16.129177800853203
Epoch 2: loss 16.129173943372834
Epoch 3: loss 16.1291813692235
Epoch 4: loss 16.129175648214765
Epoch 5: loss 16.12917624328887
Epoch 6: loss 16.12917559713433
Epoch 7: loss 16.129168092718327
Epoch 8: loss 16.129175348214446
Epoch 9: loss 16.129172518047188
Epoch 10: loss 16.129175432224905
Epoch 11: loss 16.129169013721906
Epoch 12: loss 16.129171919861587
Epoch 13: loss 16.129170131787067
Epoch 14: loss 16.1291682654065
Epoch 15: loss 16.129182153839725
Epoch 16: loss 16.12917524890578
Epoch 17: loss 16.129164124520464
Epoch 18: loss 16.129184011922963
Epoch 19: loss 16.12917348909404
-----------Time: 0:05:25.859752, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917415702907
Epoch 1: loss 16.129172213898205
Epoch 2: loss 16.129173956855993
Epoch 3: loss 16.129185184439347
Epoch 4: loss 16.129171841036957
Epoch 5: loss 16.12917989437317
Epoch 6: loss 16.12917489030557
Epoch 7: loss 16.129179400941354
Epoch 8: loss 16.129171229108902
Epoch 9: loss 16.129175679589043
Epoch 10: loss 16.12917381217131
Epoch 11: loss 16.129177644500402
Epoch 12: loss 16.129172540086973
Epoch 13: loss 16.129181134564647
Epoch 14: loss 16.12917743162204
Epoch 15: loss 16.129174037236375
Epoch 16: loss 16.12917307344969
Epoch 17: loss 16.1291721680036
Epoch 18: loss 16.129171720725676
Epoch 19: loss 16.129176194542058
-----------Time: 0:06:00.532247, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129399730823426
Epoch 1: loss 16.12941078079207
Epoch 2: loss 16.129402662114387
Epoch 3: loss 16.129409705250726
Epoch 4: loss 16.129400960643242
Epoch 5: loss 16.129399782681737
Epoch 6: loss 16.12940672573154
Epoch 7: loss 16.129410337662815
Epoch 8: loss 16.129408896520385
Epoch 9: loss 16.129404383032394
Epoch 10: loss 16.129403606454208
Epoch 11: loss 16.129398429179854
Epoch 12: loss 16.129407974479637
Epoch 13: loss 16.12939858942203
Epoch 14: loss 16.129401127886293
Epoch 15: loss 16.129408559441373
Epoch 16: loss 16.129404263758282
Epoch 17: loss 16.1294039471633
Epoch 18: loss 16.129405508357717
Epoch 19: loss 16.129402463756353
-----------Time: 0:03:46.584419, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 20, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129177364206235
Epoch 1: loss 16.129174780106663
Epoch 2: loss 16.129177445883073
Epoch 3: loss 16.1291771842579
Epoch 4: loss 16.12917432453141
Epoch 5: loss 16.12917976861677
Epoch 6: loss 16.12917029488145
Epoch 7: loss 16.1291706062906
Epoch 8: loss 16.129172166188557
Epoch 9: loss 16.129174855301212
Epoch 10: loss 16.129172646655796
Epoch 11: loss 16.1291725299746
Epoch 12: loss 16.129169061690845
Epoch 13: loss 16.12917394778079
Epoch 14: loss 16.129171764286657
Epoch 15: loss 16.129184190056257
Epoch 16: loss 16.129177534301494
Epoch 17: loss 16.129175624100654
Epoch 18: loss 16.12917980880696
Epoch 19: loss 16.12917211770104
-----------Time: 0:02:44.860303, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 50, rmse: 4.017199993133545-------------


Epoch 0: loss 16.12916264059493
Epoch 1: loss 16.129158261160672
Epoch 2: loss 16.129162871882993
Epoch 3: loss 16.129160893747766
Epoch 4: loss 16.129160800143516
Epoch 5: loss 16.129155668245183
Epoch 6: loss 16.12915692166053
Epoch 7: loss 16.129163888565156
Epoch 8: loss 16.12915689599067
Epoch 9: loss 16.129161240161274
Epoch 10: loss 16.12915641111547
Epoch 11: loss 16.129161874647696
Epoch 12: loss 16.129156569024023
Epoch 13: loss 16.12915690973312
Epoch 14: loss 16.12916519850606
Epoch 15: loss 16.12916107499256
Epoch 16: loss 16.129161781302738
Epoch 17: loss 16.12915721128919
Epoch 18: loss 16.129158222526232
Epoch 19: loss 16.12916406877278
-----------Time: 0:03:19.579898, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 100, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129180608462093
Epoch 1: loss 16.12917441139408
Epoch 2: loss 16.129178713040872
Epoch 3: loss 16.129173866363242
Epoch 4: loss 16.129183217194367
Epoch 5: loss 16.129171120725033
Epoch 6: loss 16.129179364121953
Epoch 7: loss 16.129183767411032
Epoch 8: loss 16.1291816606672
Epoch 9: loss 16.12918370155098
Epoch 10: loss 16.129172758151164
Epoch 11: loss 16.12917777829484
Epoch 12: loss 16.12917447699484
Epoch 13: loss 16.1291792798522
Epoch 14: loss 16.129183284869463
Epoch 15: loss 16.12917741165659
Epoch 16: loss 16.1291806328355
Epoch 17: loss 16.129172078288722
Epoch 18: loss 16.12917576956321
Epoch 19: loss 16.12917093610945
-----------Time: 0:04:12.218992, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917319609459
Epoch 1: loss 16.129175952623044
Epoch 2: loss 16.129180082618834
Epoch 3: loss 16.129170268952297
Epoch 4: loss 16.12917260205765
Epoch 5: loss 16.12917319765034
Epoch 6: loss 16.129174384427756
Epoch 7: loss 16.129180926612825
Epoch 8: loss 16.129169428588387
Epoch 9: loss 16.129171328676854
Epoch 10: loss 16.129175482268174
Epoch 11: loss 16.129169947171484
Epoch 12: loss 16.129168358492166
Epoch 13: loss 16.12916991761225
Epoch 14: loss 16.129175882614327
Epoch 15: loss 16.129171467397835
Epoch 16: loss 16.129165115273473
Epoch 17: loss 16.129175683219124
Epoch 18: loss 16.129182282189042
Epoch 19: loss 16.129174047348744
-----------Time: 0:04:47.684072, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12936284893424
Epoch 1: loss 16.129372425867594
Epoch 2: loss 16.129368737704603
Epoch 3: loss 16.12936909215615
Epoch 4: loss 16.129362673912443
Epoch 5: loss 16.12936879423016
Epoch 6: loss 16.129365630873266
Epoch 7: loss 16.129372418088845
Epoch 8: loss 16.129363097594833
Epoch 9: loss 16.12936527979251
Epoch 10: loss 16.12936755377939
Epoch 11: loss 16.12936664781472
Epoch 12: loss 16.129365748591628
Epoch 13: loss 16.12936294253849
Epoch 14: loss 16.129367182473892
Epoch 15: loss 16.129367583338627
Epoch 16: loss 16.1293711799717
Epoch 17: loss 16.129362739253914
Epoch 18: loss 16.129373085505293
Epoch 19: loss 16.12936393718087
-----------Time: 0:03:03.286000, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 20, rmse: 4.017227649688721-------------


Epoch 0: loss 16.129188746845937
Epoch 1: loss 16.12919216119705
Epoch 2: loss 16.12918305721148
Epoch 3: loss 16.129191160850255
Epoch 4: loss 16.129188280899022
Epoch 5: loss 16.129186247534697
Epoch 6: loss 16.12917986144314
Epoch 7: loss 16.129187330336205
Epoch 8: loss 16.129189803459
Epoch 9: loss 16.12919008012308
Epoch 10: loss 16.129189714521996
Epoch 11: loss 16.12918321123066
Epoch 12: loss 16.129189056440044
Epoch 13: loss 16.12918489014344
Epoch 14: loss 16.12918610414647
Epoch 15: loss 16.129180908462416
Epoch 16: loss 16.12918647908205
Epoch 17: loss 16.129183679251906
Epoch 18: loss 16.129182545369964
Epoch 19: loss 16.12918697640324
-----------Time: 0:03:19.161756, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 50, rmse: 4.017193794250488-------------


Epoch 0: loss 16.129177741994024
Epoch 1: loss 16.129177304050597
Epoch 2: loss 16.12917617924386
Epoch 3: loss 16.129180014165865
Epoch 4: loss 16.129186165079986
Epoch 5: loss 16.129180210190274
Epoch 6: loss 16.12917791909015
Epoch 7: loss 16.129174938533797
Epoch 8: loss 16.129180806301548
Epoch 9: loss 16.129182090572588
Epoch 10: loss 16.129179195582445
Epoch 11: loss 16.1291807801131
Epoch 12: loss 16.12918039506515
Epoch 13: loss 16.129178712263
Epoch 14: loss 16.1291805254888
Epoch 15: loss 16.129179327302552
Epoch 16: loss 16.12917302366571
Epoch 17: loss 16.12918165288845
Epoch 18: loss 16.12917767639326
Epoch 19: loss 16.1291778285974
-----------Time: 0:04:06.816780, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129176140090834
Epoch 1: loss 16.129173581661124
Epoch 2: loss 16.12917602703972
Epoch 3: loss 16.12917626325432
Epoch 4: loss 16.129175308283546
Epoch 5: loss 16.129169448294544
Epoch 6: loss 16.129170553395124
Epoch 7: loss 16.12917127059555
Epoch 8: loss 16.12916773334024
Epoch 9: loss 16.12917541770458
Epoch 10: loss 16.129180171296543
Epoch 11: loss 16.12917128563446
Epoch 12: loss 16.129172744668004
Epoch 13: loss 16.129172848125332
Epoch 14: loss 16.12917465279451
Epoch 15: loss 16.129174273710266
Epoch 16: loss 16.129168648639407
Epoch 17: loss 16.12917128537517
Epoch 18: loss 16.129174357202146
Epoch 19: loss 16.129172837235085
-----------Time: 0:05:00.677332, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129173793243027
Epoch 1: loss 16.12917159704361
Epoch 2: loss 16.129168937749483
Epoch 3: loss 16.129175411222292
Epoch 4: loss 16.129172301020162
Epoch 5: loss 16.1291702658408
Epoch 6: loss 16.129175626434275
Epoch 7: loss 16.129175888837324
Epoch 8: loss 16.1291708824361
Epoch 9: loss 16.129171570336577
Epoch 10: loss 16.12917255849667
Epoch 11: loss 16.129172104736462
Epoch 12: loss 16.129168648639407
Epoch 13: loss 16.129166601273337
Epoch 14: loss 16.12916597171346
Epoch 15: loss 16.129170856766237
Epoch 16: loss 16.1291722201212
Epoch 17: loss 16.12916381388919
Epoch 18: loss 16.12916833619309
Epoch 19: loss 16.129173738013925
-----------Time: 0:05:33.667822, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129414598600835
Epoch 1: loss 16.129415390217932
Epoch 2: loss 16.129413201278677
Epoch 3: loss 16.129416798171043
Epoch 4: loss 16.129420205002702
Epoch 5: loss 16.129412081139186
Epoch 6: loss 16.129419579591488
Epoch 7: loss 16.129419587370233
Epoch 8: loss 16.12941041052374
Epoch 9: loss 16.129414700502412
Epoch 10: loss 16.1294172350773
Epoch 11: loss 16.129416651412026
Epoch 12: loss 16.12941448451255
Epoch 13: loss 16.12941532591363
Epoch 14: loss 16.12941793931315
Epoch 15: loss 16.129413848470385
Epoch 16: loss 16.12941613334751
Epoch 17: loss 16.1294058402509
Epoch 18: loss 16.129411635417014
Epoch 19: loss 16.129403805590115
-----------Time: 0:03:57.882584, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 20, rmse: 4.017208099365234-------------


Epoch 0: loss 16.129178482530687
Epoch 1: loss 16.129177842599145
Epoch 2: loss 16.12917739946989
Epoch 3: loss 16.129178301545185
Epoch 4: loss 16.129184004144218
Epoch 5: loss 16.129175896875363
Epoch 6: loss 16.129182870521568
Epoch 7: loss 16.129169882089304
Epoch 8: loss 16.12918398262302
Epoch 9: loss 16.12918080344934
Epoch 10: loss 16.12917583671972
Epoch 11: loss 16.129180775445853
Epoch 12: loss 16.12918237397825
Epoch 13: loss 16.129180140181557
Epoch 14: loss 16.129174243373154
Epoch 15: loss 16.129175585984793
Epoch 16: loss 16.129172216491117
Epoch 17: loss 16.129183848309996
Epoch 18: loss 16.129177129547383
Epoch 19: loss 16.129173731531637
-----------Time: 0:04:18.508085, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 50, rmse: 4.0172014236450195-------------


Epoch 0: loss 16.12917805651467
Epoch 1: loss 16.129180243638885
Epoch 2: loss 16.129183946840786
Epoch 3: loss 16.129182708205057
Epoch 4: loss 16.129178018398814
Epoch 5: loss 16.129176964119377
Epoch 6: loss 16.12918060794351
Epoch 7: loss 16.129182584782278
Epoch 8: loss 16.129184380116964
Epoch 9: loss 16.12918099299146
Epoch 10: loss 16.129177211224224
Epoch 11: loss 16.12917953266146
Epoch 12: loss 16.129179995237582
Epoch 13: loss 16.12918278780756
Epoch 14: loss 16.129177428769832
Epoch 15: loss 16.12917844804491
Epoch 16: loss 16.1291784729369
Epoch 17: loss 16.129173736198887
Epoch 18: loss 16.12917985573873
Epoch 19: loss 16.129187169056863
-----------Time: 0:05:05.212056, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12916149426699
Epoch 1: loss 16.129164091849727
Epoch 2: loss 16.129165872404794
Epoch 3: loss 16.12916074154363
Epoch 4: loss 16.12916947240866
Epoch 5: loss 16.129161710775435
Epoch 6: loss 16.129167399113435
Epoch 7: loss 16.129162745608006
Epoch 8: loss 16.129162804207898
Epoch 9: loss 16.129167933772607
Epoch 10: loss 16.12917461753086
Epoch 11: loss 16.129170723231088
Epoch 12: loss 16.12916804630514
Epoch 13: loss 16.12916614129013
Epoch 14: loss 16.12915752269834
Epoch 15: loss 16.129161923394506
Epoch 16: loss 16.12916472270607
Epoch 17: loss 16.12916612936272
Epoch 18: loss 16.129163751918508
Epoch 19: loss 16.129161921320176
-----------Time: 0:04:13.067853, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129170351147717
Epoch 1: loss 16.129170702487766
Epoch 2: loss 16.129173528247065
Epoch 3: loss 16.129169821933665
Epoch 4: loss 16.12917941390593
Epoch 5: loss 16.129163811814855
Epoch 6: loss 16.129163249411487
Epoch 7: loss 16.129172973622442
Epoch 8: loss 16.12917552168049
Epoch 9: loss 16.129170988227052
Epoch 10: loss 16.129169599720807
Epoch 11: loss 16.12916791095495
Epoch 12: loss 16.129162465313843
Epoch 13: loss 16.129170126341943
Epoch 14: loss 16.129163669204505
Epoch 15: loss 16.129165003259523
Epoch 16: loss 16.129167849502853
Epoch 17: loss 16.12917069963556
Epoch 18: loss 16.129162965746534
Epoch 19: loss 16.129171936715537
-----------Time: 0:04:19.316576, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129216860791406
Epoch 1: loss 16.12920850771416
Epoch 2: loss 16.12922082095123
Epoch 3: loss 16.129214148342513
Epoch 4: loss 16.129211407890132
Epoch 5: loss 16.129218056384737
Epoch 6: loss 16.129213651539903
Epoch 7: loss 16.129208510566368
Epoch 8: loss 16.12921634946847
Epoch 9: loss 16.129214588878853
Epoch 10: loss 16.12921422146273
Epoch 11: loss 16.12920837210468
Epoch 12: loss 16.129204604339186
Epoch 13: loss 16.129210273230317
Epoch 14: loss 16.129212740648693
Epoch 15: loss 16.129215272889958
Epoch 16: loss 16.12920681350318
Epoch 17: loss 16.129210356981485
Epoch 18: loss 16.129212343932625
Epoch 19: loss 16.129207814887142
-----------Time: 0:02:32.842720, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 20, rmse: 4.017205238342285-------------


Epoch 0: loss 16.129210882565456
Epoch 1: loss 16.129206127677033
Epoch 2: loss 16.129211048512047
Epoch 3: loss 16.129204058530476
Epoch 4: loss 16.129212145315297
Epoch 5: loss 16.12920467901515
Epoch 6: loss 16.1291989831577
Epoch 7: loss 16.129207494143497
Epoch 8: loss 16.129205237788437
Epoch 9: loss 16.12920263761279
Epoch 10: loss 16.129204658012537
Epoch 11: loss 16.129199571749513
Epoch 12: loss 16.129205761298074
Epoch 13: loss 16.12920393018116
Epoch 14: loss 16.129206296994415
Epoch 15: loss 16.129201793618797
Epoch 16: loss 16.129200366996695
Epoch 17: loss 16.12920310174466
Epoch 18: loss 16.12919763950889
Epoch 19: loss 16.12920244573704
-----------Time: 0:03:08.614765, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 50, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129161629617183
Epoch 1: loss 16.129164681219418
Epoch 2: loss 16.12915678942184
Epoch 3: loss 16.129162703343486
Epoch 4: loss 16.129167255725207
Epoch 5: loss 16.12916066505262
Epoch 6: loss 16.129164199974305
Epoch 7: loss 16.129162437310356
Epoch 8: loss 16.129157152689302
Epoch 9: loss 16.129163165141733
Epoch 10: loss 16.129166580530015
Epoch 11: loss 16.12916472866977
Epoch 12: loss 16.129157863666727
Epoch 13: loss 16.129157609042426
Epoch 14: loss 16.129162033334122
Epoch 15: loss 16.129157233847554
Epoch 16: loss 16.129164150190327
Epoch 17: loss 16.129163225297372
Epoch 18: loss 16.129158103770703
Epoch 19: loss 16.129157757875774
-----------Time: 0:03:50.488508, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129169192114492
Epoch 1: loss 16.129171867743985
Epoch 2: loss 16.129174115542423
Epoch 3: loss 16.129179756689357
Epoch 4: loss 16.129171241554896
Epoch 5: loss 16.12917015382685
Epoch 6: loss 16.129177640351735
Epoch 7: loss 16.129173288920963
Epoch 8: loss 16.1291754446709
Epoch 9: loss 16.12917386221458
Epoch 10: loss 16.12917467301925
Epoch 11: loss 16.12917272444326
Epoch 12: loss 16.129172438963266
Epoch 13: loss 16.12916737992586
Epoch 14: loss 16.129173677080413
Epoch 15: loss 16.129176549771483
Epoch 16: loss 16.129168066789173
Epoch 17: loss 16.129172595575362
Epoch 18: loss 16.129167318733053
Epoch 19: loss 16.12916631242255
-----------Time: 0:04:34.629972, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129177397914138
Epoch 1: loss 16.129177846229226
Epoch 2: loss 16.129178423671505
Epoch 3: loss 16.129170822539752
Epoch 4: loss 16.129171031010156
Epoch 5: loss 16.12917486385783
Epoch 6: loss 16.129172116663874
Epoch 7: loss 16.129167401965642
Epoch 8: loss 16.129171708538973
Epoch 9: loss 16.129168478544152
Epoch 10: loss 16.129163344571484
Epoch 11: loss 16.12917292954288
Epoch 12: loss 16.129175972329204
Epoch 13: loss 16.129174318567703
Epoch 14: loss 16.12916957145803
Epoch 15: loss 16.129172376474003
Epoch 16: loss 16.129173496354202
Epoch 17: loss 16.129169751406366
Epoch 18: loss 16.129171454433255
Epoch 19: loss 16.129164481305637
-----------Time: 0:05:04.889310, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129307530638055
Epoch 1: loss 16.12931278829279
Epoch 2: loss 16.12931093980334
Epoch 3: loss 16.129307713179305
Epoch 4: loss 16.129312293564514
Epoch 5: loss 16.129311305663713
Epoch 6: loss 16.129308888029314
Epoch 7: loss 16.129306118795572
Epoch 8: loss 16.129311594255206
Epoch 9: loss 16.12930663193355
Epoch 10: loss 16.12931124447091
Epoch 11: loss 16.12930349346864
Epoch 12: loss 16.129312546892358
Epoch 13: loss 16.129305791051056
Epoch 14: loss 16.129306188544998
Epoch 15: loss 16.129305863393398
Epoch 16: loss 16.129302576873016
Epoch 17: loss 16.129303610668423
Epoch 18: loss 16.129305304360816
Epoch 19: loss 16.129307046281443
-----------Time: 0:03:31.415880, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 20, rmse: 4.017209053039551-------------


Epoch 0: loss 16.129178517794337
Epoch 1: loss 16.129173339742106
Epoch 2: loss 16.12917344371802
Epoch 3: loss 16.129173581661124
Epoch 4: loss 16.12916968684277
Epoch 5: loss 16.129179327561843
Epoch 6: loss 16.12917277267149
Epoch 7: loss 16.12917248122779
Epoch 8: loss 16.129176426089412
Epoch 9: loss 16.1291667835553
Epoch 10: loss 16.129177112952725
Epoch 11: loss 16.129168631526166
Epoch 12: loss 16.129174506035493
Epoch 13: loss 16.129172800415688
Epoch 14: loss 16.12917678572679
Epoch 15: loss 16.129169197041033
Epoch 16: loss 16.129161774301867
Epoch 17: loss 16.129171696870856
Epoch 18: loss 16.12916676099693
Epoch 19: loss 16.12917230724316
-----------Time: 0:03:40.179583, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 50, rmse: 4.017199993133545-------------


Epoch 0: loss 16.129179687977096
Epoch 1: loss 16.129180030760523
Epoch 2: loss 16.12917807466508
Epoch 3: loss 16.129172283129048
Epoch 4: loss 16.129176564291807
Epoch 5: loss 16.129176624447446
Epoch 6: loss 16.129167495310597
Epoch 7: loss 16.129175204566927
Epoch 8: loss 16.129172623060267
Epoch 9: loss 16.129176522286578
Epoch 10: loss 16.1291739843409
Epoch 11: loss 16.12918053923125
Epoch 12: loss 16.129173473795838
Epoch 13: loss 16.129165643709648
Epoch 14: loss 16.129171344234347
Epoch 15: loss 16.129175945881464
Epoch 16: loss 16.12917442176574
Epoch 17: loss 16.129172754521083
Epoch 18: loss 16.129171253223014
Epoch 19: loss 16.129169066617383
-----------Time: 0:04:35.170948, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129175267574773
Epoch 1: loss 16.12917255331084
Epoch 2: loss 16.129171487881866
Epoch 3: loss 16.12917562202632
Epoch 4: loss 16.12917245789155
Epoch 5: loss 16.129170049332355
Epoch 6: loss 16.129171977942892
Epoch 7: loss 16.129180441737628
Epoch 8: loss 16.129169961951103
Epoch 9: loss 16.12917718918444
Epoch 10: loss 16.129173423493278
Epoch 11: loss 16.129168016486613
Epoch 12: loss 16.12916313947187
Epoch 13: loss 16.129173910442805
Epoch 14: loss 16.129174064202694
Epoch 15: loss 16.129170855210486
Epoch 16: loss 16.12916561674333
Epoch 17: loss 16.129167750453483
Epoch 18: loss 16.129163257708818
Epoch 19: loss 16.129163783552077
-----------Time: 0:05:29.193712, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129174213036045
Epoch 1: loss 16.12917215789123
Epoch 2: loss 16.12917410179997
Epoch 3: loss 16.129171450803174
Epoch 4: loss 16.12917519471385
Epoch 5: loss 16.129167924697402
Epoch 6: loss 16.12917345071889
Epoch 7: loss 16.12917620439514
Epoch 8: loss 16.129178075961537
Epoch 9: loss 16.12917129237604
Epoch 10: loss 16.12917215633548
Epoch 11: loss 16.12917334648369
Epoch 12: loss 16.12916522158301
Epoch 13: loss 16.12916844302121
Epoch 14: loss 16.12917131649015
Epoch 15: loss 16.12916760421305
Epoch 16: loss 16.129169539046586
Epoch 17: loss 16.12917190378551
Epoch 18: loss 16.129172639136343
Epoch 19: loss 16.129167756157898
-----------Time: 0:06:05.836444, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12947145268086
Epoch 1: loss 16.129474368414325
Epoch 2: loss 16.129469141615285
Epoch 3: loss 16.129474620445713
Epoch 4: loss 16.129466255181764
Epoch 5: loss 16.129468690447993
Epoch 6: loss 16.12946505206898
Epoch 7: loss 16.129467124327036
Epoch 8: loss 16.129468513870446
Epoch 9: loss 16.129467229858697
Epoch 10: loss 16.129471947668428
Epoch 11: loss 16.129460024146557
Epoch 12: loss 16.12946643694514
Epoch 13: loss 16.12946399234442
Epoch 14: loss 16.129467185519843
Epoch 15: loss 16.129461589748928
Epoch 16: loss 16.12946663115451
Epoch 17: loss 16.129466637377508
Epoch 18: loss 16.12946000340323
Epoch 19: loss 16.129461610751544
-----------Time: 0:03:47.162400, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 20, rmse: 4.01716423034668-------------


Epoch 0: loss 16.129196387908586
Epoch 1: loss 16.129199774774797
Epoch 2: loss 16.12919326396401
Epoch 3: loss 16.129186771044335
Epoch 4: loss 16.12919067623435
Epoch 5: loss 16.129191047021266
Epoch 6: loss 16.12918938651819
Epoch 7: loss 16.129192193608493
Epoch 8: loss 16.129194608390687
Epoch 9: loss 16.129191143996305
Epoch 10: loss 16.129186128779168
Epoch 11: loss 16.129187098010977
Epoch 12: loss 16.129191545638914
Epoch 13: loss 16.129186139410123
Epoch 14: loss 16.12919082299337
Epoch 15: loss 16.129186884614032
Epoch 16: loss 16.12918255159296
Epoch 17: loss 16.129190310892557
Epoch 18: loss 16.12918857182414
Epoch 19: loss 16.129183856607327
-----------Time: 0:02:46.227911, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 50, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12916309954097
Epoch 1: loss 16.129163404727123
Epoch 2: loss 16.129171164804596
Epoch 3: loss 16.129159855544405
Epoch 4: loss 16.12916481864394
Epoch 5: loss 16.129163120284296
Epoch 6: loss 16.12916626886157
Epoch 7: loss 16.12916255010218
Epoch 8: loss 16.129162067560607
Epoch 9: loss 16.129157299448316
Epoch 10: loss 16.12917160871173
Epoch 11: loss 16.12916098812989
Epoch 12: loss 16.129163877156326
Epoch 13: loss 16.12916353774369
Epoch 14: loss 16.12915729166957
Epoch 15: loss 16.129160469806084
Epoch 16: loss 16.129170522280138
Epoch 17: loss 16.1291552689362
Epoch 18: loss 16.12915794767719
Epoch 19: loss 16.129150811195892
-----------Time: 0:03:17.315814, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129170327811476
Epoch 1: loss 16.129175576909592
Epoch 2: loss 16.129164062549783
Epoch 3: loss 16.129167474048693
Epoch 4: loss 16.129170556506622
Epoch 5: loss 16.129168970938803
Epoch 6: loss 16.129164459006564
Epoch 7: loss 16.129166592716718
Epoch 8: loss 16.129167925993862
Epoch 9: loss 16.12917385910308
Epoch 10: loss 16.12916382244581
Epoch 11: loss 16.129164914322523
Epoch 12: loss 16.12916392616243
Epoch 13: loss 16.129158756666822
Epoch 14: loss 16.129170642072832
Epoch 15: loss 16.129167622363457
Epoch 16: loss 16.129165897296783
Epoch 17: loss 16.129165457279026
Epoch 18: loss 16.12916043583889
Epoch 19: loss 16.12916447378618
-----------Time: 0:04:07.934906, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129179498434976
Epoch 1: loss 16.12917320983704
Epoch 2: loss 16.129176747610934
Epoch 3: loss 16.129181127045193
Epoch 4: loss 16.129175882095744
Epoch 5: loss 16.12916985953094
Epoch 6: loss 16.129177314940844
Epoch 7: loss 16.12917137431217
Epoch 8: loss 16.12917910120032
Epoch 9: loss 16.129177396876972
Epoch 10: loss 16.12917268892032
Epoch 11: loss 16.12917842937592
Epoch 12: loss 16.1291785587624
Epoch 13: loss 16.129173836544716
Epoch 14: loss 16.12917607397149
Epoch 15: loss 16.129174576044214
Epoch 16: loss 16.129168601189054
Epoch 17: loss 16.129170884251142
Epoch 18: loss 16.129171333603395
Epoch 19: loss 16.129169330835474
-----------Time: 0:04:44.667850, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129184524542357
Epoch 1: loss 16.129190582889393
Epoch 2: loss 16.129189226794594
Epoch 3: loss 16.129193057049353
Epoch 4: loss 16.12919013664864
Epoch 5: loss 16.129186081847397
Epoch 6: loss 16.12918521840654
Epoch 7: loss 16.12919106957963
Epoch 8: loss 16.129188008383604
Epoch 9: loss 16.12918689550428
Epoch 10: loss 16.129188517372917
Epoch 11: loss 16.12919089377996
Epoch 12: loss 16.12917888158038
Epoch 13: loss 16.12918530034267
Epoch 14: loss 16.12918616119061
Epoch 15: loss 16.129183295759706
Epoch 16: loss 16.129180295237905
Epoch 17: loss 16.1291827621377
Epoch 18: loss 16.12917201735521
Epoch 19: loss 16.12917775003206
-----------Time: 0:03:01.174253, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 20, rmse: 4.017192363739014-------------


Epoch 0: loss 16.129203239687765
Epoch 1: loss 16.129199113062764
Epoch 2: loss 16.129193479175992
Epoch 3: loss 16.129192945813276
Epoch 4: loss 16.12919778626791
Epoch 5: loss 16.129198161722073
Epoch 6: loss 16.12919776137592
Epoch 7: loss 16.12919737088285
Epoch 8: loss 16.12918978427142
Epoch 9: loss 16.12919195220806
Epoch 10: loss 16.129188405358967
Epoch 11: loss 16.1291865174572
Epoch 12: loss 16.129185840187674
Epoch 13: loss 16.129187631632984
Epoch 14: loss 16.12918859645684
Epoch 15: loss 16.129195038036784
Epoch 16: loss 16.12919006482488
Epoch 17: loss 16.129191353503877
Epoch 18: loss 16.129185798441735
Epoch 19: loss 16.12918568539062
-----------Time: 0:03:26.124285, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 50, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129169706808216
Epoch 1: loss 16.12916586151455
Epoch 2: loss 16.12917325599094
Epoch 3: loss 16.12916074050646
Epoch 4: loss 16.12916578969079
Epoch 5: loss 16.12917062288526
Epoch 6: loss 16.129166200667896
Epoch 7: loss 16.12916712426439
Epoch 8: loss 16.12916928182937
Epoch 9: loss 16.129165941117055
Epoch 10: loss 16.12916072209676
Epoch 11: loss 16.129161102996047
Epoch 12: loss 16.129158940504528
Epoch 13: loss 16.129166215447512
Epoch 14: loss 16.129163672316004
Epoch 15: loss 16.12915955839629
Epoch 16: loss 16.129158584497233
Epoch 17: loss 16.129160528405976
Epoch 18: loss 16.129147325539602
Epoch 19: loss 16.129160122355408
-----------Time: 0:04:06.561687, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12918359939011
Epoch 1: loss 16.129175769822503
Epoch 2: loss 16.1291782273878
Epoch 3: loss 16.129177293160353
Epoch 4: loss 16.129178614769376
Epoch 5: loss 16.12917516178382
Epoch 6: loss 16.129169896609632
Epoch 7: loss 16.129173973709946
Epoch 8: loss 16.12917283153067
Epoch 9: loss 16.129171960051778
Epoch 10: loss 16.129176416236334
Epoch 11: loss 16.12916994172636
Epoch 12: loss 16.12916718130853
Epoch 13: loss 16.12917756048994
Epoch 14: loss 16.129166717954536
Epoch 15: loss 16.129159600920104
Epoch 16: loss 16.129164815791732
Epoch 17: loss 16.12916947940953
Epoch 18: loss 16.129163614753278
Epoch 19: loss 16.12916178830361
-----------Time: 0:04:59.124194, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129166771887178
Epoch 1: loss 16.129170163420635
Epoch 2: loss 16.129172395661577
Epoch 3: loss 16.12916197084486
Epoch 4: loss 16.12917041208123
Epoch 5: loss 16.12916462287882
Epoch 6: loss 16.129171728245133
Epoch 7: loss 16.12916449712242
Epoch 8: loss 16.129170411303356
Epoch 9: loss 16.129168370419574
Epoch 10: loss 16.129170757976155
Epoch 11: loss 16.12916263722414
Epoch 12: loss 16.129163943794254
Epoch 13: loss 16.1291631630674
Epoch 14: loss 16.129166308273888
Epoch 15: loss 16.129153138337543
Epoch 16: loss 16.12915262027303
Epoch 17: loss 16.129157022524943
Epoch 18: loss 16.12916196877053
Epoch 19: loss 16.12915711794423
-----------Time: 0:05:30.167814, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12932905468881
Epoch 1: loss 16.12933913257344
Epoch 2: loss 16.129338505865764
Epoch 3: loss 16.129335105775684
Epoch 4: loss 16.129334278635643
Epoch 5: loss 16.12933236765693
Epoch 6: loss 16.12933857198511
Epoch 7: loss 16.12933032184661
Epoch 8: loss 16.129333936889385
Epoch 9: loss 16.129320867298865
Epoch 10: loss 16.12932641769376
Epoch 11: loss 16.129325325298463
Epoch 12: loss 16.129322836358888
Epoch 13: loss 16.129329796521933
Epoch 14: loss 16.129323664795386
Epoch 15: loss 16.12932823740185
Epoch 16: loss 16.129324757968558
Epoch 17: loss 16.129323244224494
Epoch 18: loss 16.129323252781116
Epoch 19: loss 16.12932543290446
-----------Time: 0:03:57.573845, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 20, rmse: 4.017245292663574-------------


Epoch 0: loss 16.129195169238308
Epoch 1: loss 16.12919968480063
Epoch 2: loss 16.129197426889824
Epoch 3: loss 16.12919487623886
Epoch 4: loss 16.1291902359573
Epoch 5: loss 16.129186183748978
Epoch 6: loss 16.129191827229537
Epoch 7: loss 16.129187104493266
Epoch 8: loss 16.12918280932876
Epoch 9: loss 16.129190138204386
Epoch 10: loss 16.129186755746133
Epoch 11: loss 16.129186670957797
Epoch 12: loss 16.129183659286458
Epoch 13: loss 16.129185300861252
Epoch 14: loss 16.129172545532093
Epoch 15: loss 16.129177702581707
Epoch 16: loss 16.12918877433084
Epoch 17: loss 16.1291816821884
Epoch 18: loss 16.129181748826326
Epoch 19: loss 16.129177680023343
-----------Time: 0:04:14.336523, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129158916909
Epoch 1: loss 16.129163815444937
Epoch 2: loss 16.129163169031106
Epoch 3: loss 16.129154613447163
Epoch 4: loss 16.129154273775235
Epoch 5: loss 16.129157507918723
Epoch 6: loss 16.12915381897786
Epoch 7: loss 16.129159718379174
Epoch 8: loss 16.12915898562126
Epoch 9: loss 16.12915138137801
Epoch 10: loss 16.129152683280875
Epoch 11: loss 16.129154655452393
Epoch 12: loss 16.12915647645694
Epoch 13: loss 16.129149839630458
Epoch 14: loss 16.12914985155787
Epoch 15: loss 16.129158416476308
Epoch 16: loss 16.129144443514036
Epoch 17: loss 16.12914476192406
Epoch 18: loss 16.129148238764436
Epoch 19: loss 16.12914611464807
-----------Time: 0:05:01.348571, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129168592632432
Epoch 1: loss 16.12916200118197
Epoch 2: loss 16.12917058113932
Epoch 3: loss 16.129174557634514
Epoch 4: loss 16.129168066011296
Epoch 5: loss 16.129167008101778
Epoch 6: loss 16.129163895306736
Epoch 7: loss 16.129161831346007
Epoch 8: loss 16.129155570232978
Epoch 9: loss 16.129163858228043
Epoch 10: loss 16.129160148284562
Epoch 11: loss 16.129159773089693
Epoch 12: loss 16.12915926513755
Epoch 13: loss 16.12915413920292
Epoch 14: loss 16.129163503517205
Epoch 15: loss 16.129160262632137
Epoch 16: loss 16.129151138681117
Epoch 17: loss 16.129158634540502
Epoch 18: loss 16.129154955193425
Epoch 19: loss 16.12915935433384
-----------Time: 0:04:13.692245, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129174074574358
Epoch 1: loss 16.12916830248519
Epoch 2: loss 16.129177757292226
Epoch 3: loss 16.12916467836721
Epoch 4: loss 16.129170943888198
Epoch 5: loss 16.129166917090444
Epoch 6: loss 16.129164762636965
Epoch 7: loss 16.129166644315735
Epoch 8: loss 16.12916905909793
Epoch 9: loss 16.129163362721894
Epoch 10: loss 16.129163394614753
Epoch 11: loss 16.129156660553942
Epoch 12: loss 16.129159055370685
Epoch 13: loss 16.129159422268227
Epoch 14: loss 16.129166840080856
Epoch 15: loss 16.129166910867447
Epoch 16: loss 16.129159666261575
Epoch 17: loss 16.12916403843567
Epoch 18: loss 16.129160785623192
Epoch 19: loss 16.129156809905872
-----------Time: 0:04:17.519499, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 200, rmse: 4.017185688018799-------------


Epoch 0: loss 16.1293316260831
Epoch 1: loss 16.12932841501656
Epoch 2: loss 16.129332025910667
Epoch 3: loss 16.129336299294685
Epoch 4: loss 16.12933594354668
Epoch 5: loss 16.12933393766726
Epoch 6: loss 16.12933169816615
Epoch 7: loss 16.129329006460583
Epoch 8: loss 16.129326989431625
Epoch 9: loss 16.129335608282705
Epoch 10: loss 16.12932536782228
Epoch 11: loss 16.129323969722247
Epoch 12: loss 16.129324722964196
Epoch 13: loss 16.129318093138586
Epoch 14: loss 16.129319321402654
Epoch 15: loss 16.129319323736276
Epoch 16: loss 16.12932458605826
Epoch 17: loss 16.129317772654232
Epoch 18: loss 16.129327590210142
Epoch 19: loss 16.12931875199841
-----------Time: 0:02:31.573749, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 20, rmse: 4.017237663269043-------------


Epoch 0: loss 16.12919105972655
Epoch 1: loss 16.12919916751399
Epoch 2: loss 16.1291914079551
Epoch 3: loss 16.1291898094227
Epoch 4: loss 16.129181895326052
Epoch 5: loss 16.129190262923622
Epoch 6: loss 16.129188303975972
Epoch 7: loss 16.129183464039922
Epoch 8: loss 16.129184311145412
Epoch 9: loss 16.129181269396252
Epoch 10: loss 16.129184659633253
Epoch 11: loss 16.129186188416224
Epoch 12: loss 16.12918453335827
Epoch 13: loss 16.129180370432454
Epoch 14: loss 16.1291759847752
Epoch 15: loss 16.12917245270572
Epoch 16: loss 16.12916502815151
Epoch 17: loss 16.129173263510392
Epoch 18: loss 16.12917371519627
Epoch 19: loss 16.129165706458203
-----------Time: 0:03:05.383399, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 50, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129189421003964
Epoch 1: loss 16.129177691691464
Epoch 2: loss 16.12918586300533
Epoch 3: loss 16.12917824346388
Epoch 4: loss 16.129179588668432
Epoch 5: loss 16.129180786595388
Epoch 6: loss 16.129181057295767
Epoch 7: loss 16.12917618442969
Epoch 8: loss 16.129184167757185
Epoch 9: loss 16.129177064465207
Epoch 10: loss 16.129176818397525
Epoch 11: loss 16.129169624353505
Epoch 12: loss 16.12917593369476
Epoch 13: loss 16.129174994540772
Epoch 14: loss 16.129173565325754
Epoch 15: loss 16.129167450971742
Epoch 16: loss 16.129175376736516
Epoch 17: loss 16.12916892322916
Epoch 18: loss 16.12916432443425
Epoch 19: loss 16.129161554681925
-----------Time: 0:03:51.596152, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 100, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12917103334378
Epoch 1: loss 16.129170880102475
Epoch 2: loss 16.12917206791706
Epoch 3: loss 16.129171290820288
Epoch 4: loss 16.12916827370383
Epoch 5: loss 16.12916994872723
Epoch 6: loss 16.129162653300217
Epoch 7: loss 16.12917422574133
Epoch 8: loss 16.129165517953247
Epoch 9: loss 16.12916220550371
Epoch 10: loss 16.12916753939016
Epoch 11: loss 16.129163282341512
Epoch 12: loss 16.129155586049762
Epoch 13: loss 16.1291574594312
Epoch 14: loss 16.129154936524433
Epoch 15: loss 16.12916054940859
Epoch 16: loss 16.12915489607495
Epoch 17: loss 16.129158053468142
Epoch 18: loss 16.129157428316216
Epoch 19: loss 16.12915715787513
-----------Time: 0:04:33.543295, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129180952023397
Epoch 1: loss 16.12917500413456
Epoch 2: loss 16.1291781814932
Epoch 3: loss 16.129170211908153
Epoch 4: loss 16.129171360569718
Epoch 5: loss 16.129174607159197
Epoch 6: loss 16.129176679157965
Epoch 7: loss 16.129166720028866
Epoch 8: loss 16.129167167825372
Epoch 9: loss 16.129172527381684
Epoch 10: loss 16.129167938699148
Epoch 11: loss 16.129174664721923
Epoch 12: loss 16.12916583299248
Epoch 13: loss 16.12916246764747
Epoch 14: loss 16.129161258830266
Epoch 15: loss 16.129161883982192
Epoch 16: loss 16.12915659832397
Epoch 17: loss 16.129160659866788
Epoch 18: loss 16.129163570155132
Epoch 19: loss 16.129150493823037
-----------Time: 0:05:02.021446, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 200, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129284647899585
Epoch 1: loss 16.129285742369213
Epoch 2: loss 16.129291236238547
Epoch 3: loss 16.12928316578909
Epoch 4: loss 16.12927622662866
Epoch 5: loss 16.129283512461893
Epoch 6: loss 16.12928204927968
Epoch 7: loss 16.1292780940464
Epoch 8: loss 16.12927523665353
Epoch 9: loss 16.12927698816794
Epoch 10: loss 16.129275682894285
Epoch 11: loss 16.12927963138599
Epoch 12: loss 16.129274422218774
Epoch 13: loss 16.12927012031269
Epoch 14: loss 16.129277695255997
Epoch 15: loss 16.12926742160625
Epoch 16: loss 16.12927111028782
Epoch 17: loss 16.1292674999123
Epoch 18: loss 16.129265271820017
Epoch 19: loss 16.129256399641093
-----------Time: 0:03:33.260680, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 20, rmse: 4.017208576202393-------------


Epoch 0: loss 16.12914716633459
Epoch 1: loss 16.129143142129752
Epoch 2: loss 16.129136996142172
Epoch 3: loss 16.12913603754132
Epoch 4: loss 16.129138243075232
Epoch 5: loss 16.129125577720238
Epoch 6: loss 16.129138383351957
Epoch 7: loss 16.129138598563944
Epoch 8: loss 16.12912737227705
Epoch 9: loss 16.12912525153147
Epoch 10: loss 16.12913403088402
Epoch 11: loss 16.12912932085304
Epoch 12: loss 16.12912447313824
Epoch 13: loss 16.129123054554178
Epoch 14: loss 16.12911854262194
Epoch 15: loss 16.129122114622316
Epoch 16: loss 16.129117051176948
Epoch 17: loss 16.129117803641023
Epoch 18: loss 16.129109709077454
Epoch 19: loss 16.129120747118687
-----------Time: 0:03:39.821992, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 50, rmse: 4.017179012298584-------------


Epoch 0: loss 16.129190427314462
Epoch 1: loss 16.129178537759785
Epoch 2: loss 16.129180822896206
Epoch 3: loss 16.12918281607034
Epoch 4: loss 16.129181388670364
Epoch 5: loss 16.129181559543497
Epoch 6: loss 16.12917448866296
Epoch 7: loss 16.12916867742077
Epoch 8: loss 16.12917299177285
Epoch 9: loss 16.12916966532157
Epoch 10: loss 16.129172743371544
Epoch 11: loss 16.129169893498133
Epoch 12: loss 16.129169950801565
Epoch 13: loss 16.129173227209577
Epoch 14: loss 16.129165461168398
Epoch 15: loss 16.129161431259146
Epoch 16: loss 16.12916488761549
Epoch 17: loss 16.12915786418531
Epoch 18: loss 16.129161449928137
Epoch 19: loss 16.129151782761326
-----------Time: 0:04:33.284866, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 100, rmse: 4.017183303833008-------------


Epoch 0: loss 16.1291736192584
Epoch 1: loss 16.129176234213666
Epoch 2: loss 16.129172467744628
Epoch 3: loss 16.129171380794457
Epoch 4: loss 16.129167275431364
Epoch 5: loss 16.129164307580297
Epoch 6: loss 16.1291679462186
Epoch 7: loss 16.129166415102006
Epoch 8: loss 16.12916294863329
Epoch 9: loss 16.129167603175883
Epoch 10: loss 16.12916539427118
Epoch 11: loss 16.12916053644401
Epoch 12: loss 16.129167205681938
Epoch 13: loss 16.12915025320048
Epoch 14: loss 16.129155364614782
Epoch 15: loss 16.129155042315386
Epoch 16: loss 16.12915564231603
Epoch 17: loss 16.12915610307711
Epoch 18: loss 16.129147236602602
Epoch 19: loss 16.129142619916575
-----------Time: 0:05:23.365743, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 150, rmse: 4.017185688018799-------------


Epoch 0: loss 16.12917399315681
Epoch 1: loss 16.129172887537646
Epoch 2: loss 16.129172578980704
Epoch 3: loss 16.129173816579264
Epoch 4: loss 16.12916723575976
Epoch 5: loss 16.129167511386676
Epoch 6: loss 16.129168727464037
Epoch 7: loss 16.129163162289526
Epoch 8: loss 16.129157352862375
Epoch 9: loss 16.129155415176633
Epoch 10: loss 16.129165047079795
Epoch 11: loss 16.12916139106896
Epoch 12: loss 16.129158324168518
Epoch 13: loss 16.129160404205322
Epoch 14: loss 16.129156908955245
Epoch 15: loss 16.129156680000808
Epoch 16: loss 16.129153856575133
Epoch 17: loss 16.129148760977618
Epoch 18: loss 16.129150905837307
Epoch 19: loss 16.129150604281236
-----------Time: 0:05:59.595974, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 200, rmse: 4.017185211181641-------------


Epoch 0: loss 16.12936202905436
Epoch 1: loss 16.12935961660579
Epoch 2: loss 16.129360022137774
Epoch 3: loss 16.129360832942446
Epoch 4: loss 16.12935783656931
Epoch 5: loss 16.129354252900814
Epoch 6: loss 16.129360330694716
Epoch 7: loss 16.129354656358462
Epoch 8: loss 16.12935059948289
Epoch 9: loss 16.12935141469552
Epoch 10: loss 16.129348511667338
Epoch 11: loss 16.129345135432082
Epoch 12: loss 16.129342750987
Epoch 13: loss 16.12934338547342
Epoch 14: loss 16.129334342162075
Epoch 15: loss 16.129337281491072
Epoch 16: loss 16.12933323654291
Epoch 17: loss 16.129337508371176
Epoch 18: loss 16.129336581663182
Epoch 19: loss 16.129333292809175
-----------Time: 0:03:48.405179, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 20, rmse: 4.01719856262207-------------


Epoch 0: loss 16.12920495516065
Epoch 1: loss 16.129197531125026
Epoch 2: loss 16.12919817779815
Epoch 3: loss 16.129201324819675
Epoch 4: loss 16.12919446137238
Epoch 5: loss 16.129192999745918
Epoch 6: loss 16.129191276234994
Epoch 7: loss 16.129188575713513
Epoch 8: loss 16.129185168622563
Epoch 9: loss 16.129187303369886
Epoch 10: loss 16.129185581933292
Epoch 11: loss 16.129184376486883
Epoch 12: loss 16.12917698667774
Epoch 13: loss 16.129177422546835
Epoch 14: loss 16.1291842592871
Epoch 15: loss 16.129178958071385
Epoch 16: loss 16.12917256160817
Epoch 17: loss 16.129176958414963
Epoch 18: loss 16.129173014071924
Epoch 19: loss 16.129167087963577
-----------Time: 0:02:50.476988, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 50, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129187685047043
Epoch 1: loss 16.129188740104357
Epoch 2: loss 16.129180809413047
Epoch 3: loss 16.129186042953666
Epoch 4: loss 16.12918430907108
Epoch 5: loss 16.129174010529344
Epoch 6: loss 16.129183543642426
Epoch 7: loss 16.129168034118436
Epoch 8: loss 16.129174425914407
Epoch 9: loss 16.129169328242558
Epoch 10: loss 16.129172037061366
Epoch 11: loss 16.12916996998914
Epoch 12: loss 16.129160983981226
Epoch 13: loss 16.129165134461047
Epoch 14: loss 16.129161188043675
Epoch 15: loss 16.129164986405574
Epoch 16: loss 16.129161008873215
Epoch 17: loss 16.129167546650326
Epoch 18: loss 16.129159416823104
Epoch 19: loss 16.129154294259266
-----------Time: 0:03:21.925426, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129167668258063
Epoch 1: loss 16.12917371208477
Epoch 2: loss 16.12916621492893
Epoch 3: loss 16.12916884492311
Epoch 4: loss 16.12916524206704
Epoch 5: loss 16.129159108784744
Epoch 6: loss 16.12915772831654
Epoch 7: loss 16.12915579296442
Epoch 8: loss 16.12914807022493
Epoch 9: loss 16.129157887002968
Epoch 10: loss 16.129155781296298
Epoch 11: loss 16.129151526321984
Epoch 12: loss 16.129148676448573
Epoch 13: loss 16.12914286339134
Epoch 14: loss 16.129148468496748
Epoch 15: loss 16.129142620435157
Epoch 16: loss 16.12914261239712
Epoch 17: loss 16.12913851870215
Epoch 18: loss 16.129136214378153
Epoch 19: loss 16.129139419221698
-----------Time: 0:04:07.218841, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 150, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129164502826836
Epoch 1: loss 16.129163786922867
Epoch 2: loss 16.129166104989313
Epoch 3: loss 16.129164445782692
Epoch 4: loss 16.129161967214777
Epoch 5: loss 16.12916489513495
Epoch 6: loss 16.12916088596902
Epoch 7: loss 16.12916013117132
Epoch 8: loss 16.1291552689362
Epoch 9: loss 16.129156831686362
Epoch 10: loss 16.129153247499286
Epoch 11: loss 16.129152567377552
Epoch 12: loss 16.129147068581677
Epoch 13: loss 16.129144362874367
Epoch 14: loss 16.129144873938007
Epoch 15: loss 16.12913839890945
Epoch 16: loss 16.129139752152046
Epoch 17: loss 16.12913432492064
Epoch 18: loss 16.12913765655775
Epoch 19: loss 16.129135891560175
-----------Time: 0:04:50.765559, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 200, rmse: 4.01718282699585-------------


Epoch 0: loss 16.12934259359703
Epoch 1: loss 16.129336755129227
Epoch 2: loss 16.129335661437473
Epoch 3: loss 16.12932908347017
Epoch 4: loss 16.12933403360513
Epoch 5: loss 16.12933077042099
Epoch 6: loss 16.12932277179529
Epoch 7: loss 16.12931937300167
Epoch 8: loss 16.1293186726552
Epoch 9: loss 16.12931750454677
Epoch 10: loss 16.1293141931344
Epoch 11: loss 16.129310281721388
Epoch 12: loss 16.129308101338754
Epoch 13: loss 16.129305518535638
Epoch 14: loss 16.12930802847783
Epoch 15: loss 16.129306301336822
Epoch 16: loss 16.129301847745182
Epoch 17: loss 16.12929870876169
Epoch 18: loss 16.129298788364196
Epoch 19: loss 16.12929612232849
-----------Time: 0:02:59.713222, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 20, rmse: 4.017196178436279-------------


Epoch 0: loss 16.129201834586862
Epoch 1: loss 16.129201030783058
Epoch 2: loss 16.129198083675316
Epoch 3: loss 16.12919975480935
Epoch 4: loss 16.129193297412616
Epoch 5: loss 16.12919342031681
Epoch 6: loss 16.129185887119444
Epoch 7: loss 16.129182297487244
Epoch 8: loss 16.12918625609132
Epoch 9: loss 16.129184994637935
Epoch 10: loss 16.1291824460613
Epoch 11: loss 16.129176917187607
Epoch 12: loss 16.129166742068648
Epoch 13: loss 16.12916834889838
Epoch 14: loss 16.129173761609458
Epoch 15: loss 16.129160146469523
Epoch 16: loss 16.129169109919072
Epoch 17: loss 16.129161522789065
Epoch 18: loss 16.129156929698567
Epoch 19: loss 16.129157884669343
-----------Time: 0:03:27.946554, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 50, rmse: 4.017179012298584-------------


Epoch 0: loss 16.129165934894058
Epoch 1: loss 16.12917079531414
Epoch 2: loss 16.129165161945952
Epoch 3: loss 16.129167858059475
Epoch 4: loss 16.129165599889376
Epoch 5: loss 16.12915619486632
Epoch 6: loss 16.12914410876865
Epoch 7: loss 16.129146091571123
Epoch 8: loss 16.129155480777396
Epoch 9: loss 16.129146955789853
Epoch 10: loss 16.12914276460126
Epoch 11: loss 16.12914420133573
Epoch 12: loss 16.129152478959135
Epoch 13: loss 16.129137711008973
Epoch 14: loss 16.129136876349477
Epoch 15: loss 16.12913697721389
Epoch 16: loss 16.129129547473852
Epoch 17: loss 16.129133012127525
Epoch 18: loss 16.12912796346178
Epoch 19: loss 16.129121523437583
-----------Time: 0:04:07.768236, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 100, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129168152096092
Epoch 1: loss 16.1291664827771
Epoch 2: loss 16.129165136016795
Epoch 3: loss 16.129157532810712
Epoch 4: loss 16.129161821233637
Epoch 5: loss 16.129161158484436
Epoch 6: loss 16.129157529699214
Epoch 7: loss 16.129152456141476
Epoch 8: loss 16.129145137637515
Epoch 9: loss 16.1291445428227
Epoch 10: loss 16.129143132017383
Epoch 11: loss 16.129142235127915
Epoch 12: loss 16.129135231144602
Epoch 13: loss 16.129143742908273
Epoch 14: loss 16.12914158456542
Epoch 15: loss 16.129134138490013
Epoch 16: loss 16.12912583908612
Epoch 17: loss 16.129125406328527
Epoch 18: loss 16.12912910149239
Epoch 19: loss 16.129127467437048
-----------Time: 0:05:00.019172, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 150, rmse: 4.017180442810059-------------


Epoch 0: loss 16.12917427941468
Epoch 1: loss 16.129165395308345
Epoch 2: loss 16.12917071337801
Epoch 3: loss 16.129165070675327
Epoch 4: loss 16.12916928857095
Epoch 5: loss 16.129167910177078
Epoch 6: loss 16.129164160561988
Epoch 7: loss 16.129153849574262
Epoch 8: loss 16.12914519079228
Epoch 9: loss 16.129155300569767
Epoch 10: loss 16.129147085694918
Epoch 11: loss 16.129144804966455
Epoch 12: loss 16.129141905568357
Epoch 13: loss 16.129139684995533
Epoch 14: loss 16.129141747141222
Epoch 15: loss 16.129129405122793
Epoch 16: loss 16.12913029708572
Epoch 17: loss 16.129136293721366
Epoch 18: loss 16.129130214112422
Epoch 19: loss 16.129125232084604
-----------Time: 0:05:33.304834, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 200, rmse: 4.01718282699585-------------


Epoch 0: loss 16.1293463375077
Epoch 1: loss 16.129344131714497
Epoch 2: loss 16.12934260007932
Epoch 3: loss 16.12933859713639
Epoch 4: loss 16.12933806662588
Epoch 5: loss 16.129334021418426
Epoch 6: loss 16.129326567823565
Epoch 7: loss 16.129326534374957
Epoch 8: loss 16.129326515965257
Epoch 9: loss 16.129319735491258
Epoch 10: loss 16.12931377230422
Epoch 11: loss 16.129314312408514
Epoch 12: loss 16.129309057605987
Epoch 13: loss 16.12930586391198
Epoch 14: loss 16.12930949684587
Epoch 15: loss 16.12929942181345
Epoch 16: loss 16.129305317066102
Epoch 17: loss 16.12929628257067
Epoch 18: loss 16.129297335553648
Epoch 19: loss 16.129293770813437
-----------Time: 0:03:59.724454, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 20, rmse: 4.017204761505127-------------


Epoch 0: loss 16.129175897653237
Epoch 1: loss 16.129172199637168
Epoch 2: loss 16.12917073489921
Epoch 3: loss 16.129170499981065
Epoch 4: loss 16.12916444007828
Epoch 5: loss 16.12915972745438
Epoch 6: loss 16.12915829434999
Epoch 7: loss 16.129155414398756
Epoch 8: loss 16.12915174153397
Epoch 9: loss 16.129150796416273
Epoch 10: loss 16.129145474716527
Epoch 11: loss 16.12914794991365
Epoch 12: loss 16.12914237125598
Epoch 13: loss 16.12914009312043
Epoch 14: loss 16.129135826477995
Epoch 15: loss 16.129128815753102
Epoch 16: loss 16.129130047128665
Epoch 17: loss 16.12912441272331
Epoch 18: loss 16.12912169093992
Epoch 19: loss 16.129119461291896
-----------Time: 0:04:40.525818, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 50, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129161497637785
Epoch 1: loss 16.129151854844377
Epoch 2: loss 16.129150880686026
Epoch 3: loss 16.129154233585044
Epoch 4: loss 16.12915271750736
Epoch 5: loss 16.129152797887738
Epoch 6: loss 16.129144028388268
Epoch 7: loss 16.129138976870316
Epoch 8: loss 16.12914221931113
Epoch 9: loss 16.129127858707996
Epoch 10: loss 16.1291356024501
Epoch 11: loss 16.129135203141114
Epoch 12: loss 16.12911412351607
Epoch 13: loss 16.12912203709414
Epoch 14: loss 16.129118260512733
Epoch 15: loss 16.129118168464235
Epoch 16: loss 16.12911438591912
Epoch 17: loss 16.129105520222485
Epoch 18: loss 16.129111596201348
Epoch 19: loss 16.12910251995997
-----------Time: 0:05:05.672158, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 100, rmse: 4.017179489135742-------------


Epoch 0: loss 16.12917008589246
Epoch 1: loss 16.129168136020017
Epoch 2: loss 16.12916271941956
Epoch 3: loss 16.129159346555095
Epoch 4: loss 16.129156563578903
Epoch 5: loss 16.129160337826686
Epoch 6: loss 16.129155798668833
Epoch 7: loss 16.12915373211519
Epoch 8: loss 16.129143317929422
Epoch 9: loss 16.129148877658814
Epoch 10: loss 16.12913964065668
Epoch 11: loss 16.129131252315783
Epoch 12: loss 16.12913891619609
Epoch 13: loss 16.12913413926789
Epoch 14: loss 16.129124040639937
Epoch 15: loss 16.12912806354832
Epoch 16: loss 16.12912959077554
Epoch 17: loss 16.129119353945196
Epoch 18: loss 16.129103764300115
Epoch 19: loss 16.12911328055925
-----------Time: 0:04:03.659946, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 150, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129170825910542
Epoch 1: loss 16.129170093152627
Epoch 2: loss 16.12916428450335
Epoch 3: loss 16.129153354586695
Epoch 4: loss 16.12916221380104
Epoch 5: loss 16.129154150352456
Epoch 6: loss 16.12916192598742
Epoch 7: loss 16.1291497799934
Epoch 8: loss 16.129147706438886
Epoch 9: loss 16.129142757341096
Epoch 10: loss 16.129144820264656
Epoch 11: loss 16.129143100902397
Epoch 12: loss 16.129137331665437
Epoch 13: loss 16.129125134850273
Epoch 14: loss 16.1291364798927
Epoch 15: loss 16.12913025041324
Epoch 16: loss 16.129122101657735
Epoch 17: loss 16.129117395256834
Epoch 18: loss 16.129113924120873
Epoch 19: loss 16.129109153156374
-----------Time: 0:04:19.473119, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 200, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129418217273688
Epoch 1: loss 16.129416598257258
Epoch 2: loss 16.129406411729473
Epoch 3: loss 16.12940130290809
Epoch 4: loss 16.12940015009786
Epoch 5: loss 16.129393914654695
Epoch 6: loss 16.12938961404507
Epoch 7: loss 16.1293797368521
Epoch 8: loss 16.12937647029717
Epoch 9: loss 16.12937055041182
Epoch 10: loss 16.129378113427716
Epoch 11: loss 16.129366271841974
Epoch 12: loss 16.129370212814223
Epoch 13: loss 16.12936172231246
Epoch 14: loss 16.129364596559277
Epoch 15: loss 16.129355497759537
Epoch 16: loss 16.12934345444497
Epoch 17: loss 16.12933769013455
Epoch 18: loss 16.129341299213618
Epoch 19: loss 16.129330863247365
-----------Time: 0:02:33.716336, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 20, rmse: 4.017195224761963-------------


Epoch 0: loss 16.129208909875352
Epoch 1: loss 16.129203290249617
Epoch 2: loss 16.129201871146968
Epoch 3: loss 16.129198916779064
Epoch 4: loss 16.129192098707787
Epoch 5: loss 16.129186385996384
Epoch 6: loss 16.129189581505432
Epoch 7: loss 16.12918311529279
Epoch 8: loss 16.129180463518118
Epoch 9: loss 16.129178081665952
Epoch 10: loss 16.129163941460632
Epoch 11: loss 16.12916566704589
Epoch 12: loss 16.129162241285947
Epoch 13: loss 16.129158462889496
Epoch 14: loss 16.129144119658893
Epoch 15: loss 16.12914949321695
Epoch 16: loss 16.129143622856287
Epoch 17: loss 16.129141963649666
Epoch 18: loss 16.12913450798047
Epoch 19: loss 16.129129684639082
-----------Time: 0:03:03.745955, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 50, rmse: 4.017181873321533-------------


Epoch 0: loss 16.129162495910247
Epoch 1: loss 16.129155250267207
Epoch 2: loss 16.129158068247758
Epoch 3: loss 16.129149847409206
Epoch 4: loss 16.129145655183443
Epoch 5: loss 16.12914275500747
Epoch 6: loss 16.129143143944795
Epoch 7: loss 16.129140438496776
Epoch 8: loss 16.129134541947664
Epoch 9: loss 16.12912687495586
Epoch 10: loss 16.12912099733503
Epoch 11: loss 16.129120307360218
Epoch 12: loss 16.129115973820564
Epoch 13: loss 16.129108104840643
Epoch 14: loss 16.12910567864962
Epoch 15: loss 16.12910708997352
Epoch 16: loss 16.12909949247185
Epoch 17: loss 16.129088958752682
Epoch 18: loss 16.12908987638547
Epoch 19: loss 16.12909286783207
-----------Time: 0:03:52.571102, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 100, rmse: 4.017177581787109-------------


Epoch 0: loss 16.1291766781208
Epoch 1: loss 16.129180584866564
Epoch 2: loss 16.129172978030397
Epoch 3: loss 16.12916825244192
Epoch 4: loss 16.1291590579636
Epoch 5: loss 16.12915616997433
Epoch 6: loss 16.12915444412978
Epoch 7: loss 16.12915215017745
Epoch 8: loss 16.12915255026431
Epoch 9: loss 16.129147794857307
Epoch 10: loss 16.129143902372576
Epoch 11: loss 16.129135073495338
Epoch 12: loss 16.129126793797603
Epoch 13: loss 16.12912340330131
Epoch 14: loss 16.129120890766202
Epoch 15: loss 16.129112812538
Epoch 16: loss 16.129114639765547
Epoch 17: loss 16.129106432410154
Epoch 18: loss 16.12910127847204
Epoch 19: loss 16.129098600768213
-----------Time: 0:04:34.233749, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 150, rmse: 4.017178535461426-------------


Epoch 0: loss 16.12917842263434
Epoch 1: loss 16.12916098138831
Epoch 2: loss 16.12916217698164
Epoch 3: loss 16.129157801955337
Epoch 4: loss 16.129158680435108
Epoch 5: loss 16.12914899797009
Epoch 6: loss 16.129143788284292
Epoch 7: loss 16.129145050515554
Epoch 8: loss 16.129140804875732
Epoch 9: loss 16.129139376957173
Epoch 10: loss 16.129127408577865
Epoch 11: loss 16.129123193015865
Epoch 12: loss 16.129121872184715
Epoch 13: loss 16.129119114359803
Epoch 14: loss 16.129113156877178
Epoch 15: loss 16.129114515564893
Epoch 16: loss 16.12910697614453
Epoch 17: loss 16.12909556342701
Epoch 18: loss 16.12910059938747
Epoch 19: loss 16.129102224108316
-----------Time: 0:05:04.128146, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 200, rmse: 4.017178535461426-------------


Epoch 0: loss 16.129358125679385
Epoch 1: loss 16.129358087563528
Epoch 2: loss 16.129350712534006
Epoch 3: loss 16.129343190226884
Epoch 4: loss 16.129337364464366
Epoch 5: loss 16.129332460224013
Epoch 6: loss 16.129326669206563
Epoch 7: loss 16.12932756817036
Epoch 8: loss 16.129319902215723
Epoch 9: loss 16.129314599444257
Epoch 10: loss 16.12930778759598
Epoch 11: loss 16.12930039571251
Epoch 12: loss 16.129297825355383
Epoch 13: loss 16.129294260874463
Epoch 14: loss 16.129284215660572
Epoch 15: loss 16.129275226022575
Epoch 16: loss 16.129274203117415
Epoch 17: loss 16.12927195013315
Epoch 18: loss 16.129263089363054
Epoch 19: loss 16.129252566534127
-----------Time: 0:03:37.706639, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 20, rmse: 4.0171685218811035-------------


Epoch 0: loss 16.12920488800414
Epoch 1: loss 16.12919956397077
Epoch 2: loss 16.1291919571346
Epoch 3: loss 16.129190374159695
Epoch 4: loss 16.129181319958104
Epoch 5: loss 16.129181934479075
Epoch 6: loss 16.129165497728508
Epoch 7: loss 16.12917032184777
Epoch 8: loss 16.129165926856018
Epoch 9: loss 16.129156527537376
Epoch 10: loss 16.12914880713151
Epoch 11: loss 16.12914723634331
Epoch 12: loss 16.12913747272004
Epoch 13: loss 16.129134815240956
Epoch 14: loss 16.12911912862084
Epoch 15: loss 16.129111889200797
Epoch 16: loss 16.129119072354573
Epoch 17: loss 16.129113194733744
Epoch 18: loss 16.129103915467088
Epoch 19: loss 16.129098680630012
-----------Time: 0:04:11.984336, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129167298767605
Epoch 1: loss 16.12916617629449
Epoch 2: loss 16.12916338891034
Epoch 3: loss 16.1291599753371
Epoch 4: loss 16.129151218283624
Epoch 5: loss 16.129147814822755
Epoch 6: loss 16.129143399087678
Epoch 7: loss 16.129138328641442
Epoch 8: loss 16.129123080483332
Epoch 9: loss 16.129123369074826
Epoch 10: loss 16.129113568632157
Epoch 11: loss 16.129113048752604
Epoch 12: loss 16.129111415215846
Epoch 13: loss 16.12910477942653
Epoch 14: loss 16.129099046231094
Epoch 15: loss 16.129082941892293
Epoch 16: loss 16.129085420460207
Epoch 17: loss 16.12908883118124
Epoch 18: loss 16.129082199022005
Epoch 19: loss 16.12906373720444
-----------Time: 0:04:36.349638, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 100, rmse: 4.01717472076416-------------


Epoch 0: loss 16.129172929024296
Epoch 1: loss 16.129178190568403
Epoch 2: loss 16.129162102046383
Epoch 3: loss 16.12916448908438
Epoch 4: loss 16.12914867126274
Epoch 5: loss 16.12914986348528
Epoch 6: loss 16.12914154930177
Epoch 7: loss 16.12913987894561
Epoch 8: loss 16.129128358881392
Epoch 9: loss 16.12913790106968
Epoch 10: loss 16.129122934502192
Epoch 11: loss 16.129112714525796
Epoch 12: loss 16.12911418470888
Epoch 13: loss 16.129101933701783
Epoch 14: loss 16.12910664451064
Epoch 15: loss 16.129097259971616
Epoch 16: loss 16.129091028677113
Epoch 17: loss 16.129093084599806
Epoch 18: loss 16.129088563073775
Epoch 19: loss 16.129076350182537
-----------Time: 0:05:33.748614, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 150, rmse: 4.017175197601318-------------


Epoch 0: loss 16.129168160912005
Epoch 1: loss 16.129164728929066
Epoch 2: loss 16.12916446341452
Epoch 3: loss 16.129156690113177
Epoch 4: loss 16.129146691312474
Epoch 5: loss 16.12914340401422
Epoch 6: loss 16.129138881969606
Epoch 7: loss 16.12913415249176
Epoch 8: loss 16.12913635439559
Epoch 9: loss 16.12912980520965
Epoch 10: loss 16.1291216486754
Epoch 11: loss 16.129109540797238
Epoch 12: loss 16.129106400258
Epoch 13: loss 16.129097554786107
Epoch 14: loss 16.129096471725305
Epoch 15: loss 16.129090502055977
Epoch 16: loss 16.12908371847048
Epoch 17: loss 16.129079006365163
Epoch 18: loss 16.12907554508228
Epoch 19: loss 16.12906684611011
-----------Time: 0:06:02.178413, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 200, rmse: 4.01717472076416-------------


Epoch 0: loss 16.129281822658868
Epoch 1: loss 16.129269815126538
Epoch 2: loss 16.129271104064824
Epoch 3: loss 16.129257780887173
Epoch 4: loss 16.129246805075915
Epoch 5: loss 16.12924576505751
Epoch 6: loss 16.12924254050781
Epoch 7: loss 16.12922374964927
Epoch 8: loss 16.129212727943408
Epoch 9: loss 16.129213549638326
Epoch 10: loss 16.129206131825697
Epoch 11: loss 16.129191709511172
Epoch 12: loss 16.129190525585962
Epoch 13: loss 16.129183054877856
Epoch 14: loss 16.129178563170356
Epoch 15: loss 16.12916704492118
Epoch 16: loss 16.12916343065628
Epoch 17: loss 16.12915237109385
Epoch 18: loss 16.129142777825127
Epoch 19: loss 16.1291393977005
-----------Time: 0:03:16.112152, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 20, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129213844193526
Epoch 1: loss 16.129208910653226
Epoch 2: loss 16.129192387817863
Epoch 3: loss 16.12919764054606
Epoch 4: loss 16.129186962142207
Epoch 5: loss 16.129174911567475
Epoch 6: loss 16.129174671463502
Epoch 7: loss 16.12916080221785
Epoch 8: loss 16.12916262374098
Epoch 9: loss 16.129149434098476
Epoch 10: loss 16.129143660972144
Epoch 11: loss 16.129140082230187
Epoch 12: loss 16.129128876945906
Epoch 13: loss 16.12911533207398
Epoch 14: loss 16.12910865998385
Epoch 15: loss 16.129104336815857
Epoch 16: loss 16.12910010025124
Epoch 17: loss 16.129097085468402
Epoch 18: loss 16.129085174392525
Epoch 19: loss 16.129080416392608
-----------Time: 0:02:46.458512, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 50, rmse: 4.017177104949951-------------


Epoch 0: loss 16.129181395930527
Epoch 1: loss 16.129172821677592
Epoch 2: loss 16.129158086138876
Epoch 3: loss 16.129161202564
Epoch 4: loss 16.129152629088942
Epoch 5: loss 16.12914908431418
Epoch 6: loss 16.12914016131411
Epoch 7: loss 16.129133508152258
Epoch 8: loss 16.129114725331757
Epoch 9: loss 16.129117346510025
Epoch 10: loss 16.129100107770693
Epoch 11: loss 16.129102273633002
Epoch 12: loss 16.129092928506292
Epoch 13: loss 16.12908535900811
Epoch 14: loss 16.129076752084437
Epoch 15: loss 16.129075150699833
Epoch 16: loss 16.12906786071794
Epoch 17: loss 16.129057926740124
Epoch 18: loss 16.12905158472813
Epoch 19: loss 16.129046140902066
-----------Time: 0:03:21.794161, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 100, rmse: 4.01717472076416-------------


Epoch 0: loss 16.129168393237233
Epoch 1: loss 16.129162822358307
Epoch 2: loss 16.129152742140057
Epoch 3: loss 16.129149147062734
Epoch 4: loss 16.129143754576393
Epoch 5: loss 16.129128290947005
Epoch 6: loss 16.129123921365824
Epoch 7: loss 16.129119887826494
Epoch 8: loss 16.129111300349688
Epoch 9: loss 16.129104650040045
Epoch 10: loss 16.1291037095896
Epoch 11: loss 16.129088666531104
Epoch 12: loss 16.129086158144663
Epoch 13: loss 16.129078676805605
Epoch 14: loss 16.129068502983106
Epoch 15: loss 16.12906363322853
Epoch 16: loss 16.12905623356631
Epoch 17: loss 16.129046685155025
Epoch 18: loss 16.129041771320885
Epoch 19: loss 16.12903231962535
-----------Time: 0:04:23.451464, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 150, rmse: 4.01716947555542-------------


Epoch 0: loss 16.129170866878606
Epoch 1: loss 16.129170658667494
Epoch 2: loss 16.129156457010076
Epoch 3: loss 16.129156225462722
Epoch 4: loss 16.129149797106646
Epoch 5: loss 16.129132037191305
Epoch 6: loss 16.12913135240232
Epoch 7: loss 16.129121915745696
Epoch 8: loss 16.12911427494234
Epoch 9: loss 16.129113939678366
Epoch 10: loss 16.129092968696483
Epoch 11: loss 16.129091310267736
Epoch 12: loss 16.12909031666252
Epoch 13: loss 16.129078471965283
Epoch 14: loss 16.129067620354675
Epoch 15: loss 16.129069825629298
Epoch 16: loss 16.12906393685893
Epoch 17: loss 16.129058424320604
Epoch 18: loss 16.129039052389704
Epoch 19: loss 16.129041742020938
-----------Time: 0:04:51.558721, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 200, rmse: 4.017169952392578-------------


Epoch 0: loss 16.129264276659054
Epoch 1: loss 16.12924873757583
Epoch 2: loss 16.129243308010796
Epoch 3: loss 16.129230721480436
Epoch 4: loss 16.129228777312402
Epoch 5: loss 16.12920689543931
Epoch 6: loss 16.129207745397007
Epoch 7: loss 16.12919263984925
Epoch 8: loss 16.129184626962516
Epoch 9: loss 16.129182492474488
Epoch 10: loss 16.12916091941763
Epoch 11: loss 16.129157185360036
Epoch 12: loss 16.12914592536524
Epoch 13: loss 16.129135216624274
Epoch 14: loss 16.129128198379924
Epoch 15: loss 16.12911659041587
Epoch 16: loss 16.129111853159273
Epoch 17: loss 16.12909902030194
Epoch 18: loss 16.129088671457644
Epoch 19: loss 16.129072373946638
-----------Time: 0:03:16.367992, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 20, rmse: 4.017213344573975-------------


Epoch 0: loss 16.129188312791882
Epoch 1: loss 16.129174638274183
Epoch 2: loss 16.12916612936272
Epoch 3: loss 16.129153545425275
Epoch 4: loss 16.12914055777089
Epoch 5: loss 16.129135373754952
Epoch 6: loss 16.129126529060933
Epoch 7: loss 16.12912492612058
Epoch 8: loss 16.12910595272079
Epoch 9: loss 16.129095066883696
Epoch 10: loss 16.129084060476032
Epoch 11: loss 16.129080345606013
Epoch 12: loss 16.129067578349442
Epoch 13: loss 16.12905967280941
Epoch 14: loss 16.12905181031178
Epoch 15: loss 16.129039941241132
Epoch 16: loss 16.129031773557347
Epoch 17: loss 16.129021068446463
Epoch 18: loss 16.129008015450605
Epoch 19: loss 16.129005649933806
-----------Time: 0:03:26.909680, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 50, rmse: 4.0171589851379395-------------


Epoch 0: loss 16.12916855555374
Epoch 1: loss 16.12915899625221
Epoch 2: loss 16.129150590798076
Epoch 3: loss 16.129143991050285
Epoch 4: loss 16.129129944967797
Epoch 5: loss 16.129119477627263
Epoch 6: loss 16.12911447485612
Epoch 7: loss 16.12909873248832
Epoch 8: loss 16.129090369817288
Epoch 9: loss 16.129081025468455
Epoch 10: loss 16.129072354499773
Epoch 11: loss 16.129061667539297
Epoch 12: loss 16.129053803226622
Epoch 13: loss 16.12904737409267
Epoch 14: loss 16.129029312102674
Epoch 15: loss 16.12902397562331
Epoch 16: loss 16.129011727987002
Epoch 17: loss 16.12900949289385
Epoch 18: loss 16.12899471794282
Epoch 19: loss 16.12899062658147
-----------Time: 0:04:17.014450, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 100, rmse: 4.017166614532471-------------


Epoch 0: loss 16.129180773890102
Epoch 1: loss 16.12916532892971
Epoch 2: loss 16.1291510901936
Epoch 3: loss 16.1291515105052
Epoch 4: loss 16.129138048865862
Epoch 5: loss 16.129120388518473
Epoch 6: loss 16.129118278922434
Epoch 7: loss 16.129102083572295
Epoch 8: loss 16.129092635766135
Epoch 9: loss 16.129082352781893
Epoch 10: loss 16.12908001682433
Epoch 11: loss 16.129071113011836
Epoch 12: loss 16.12905513054006
Epoch 13: loss 16.129048876427905
Epoch 14: loss 16.129041719721865
Epoch 15: loss 16.129030226623968
Epoch 16: loss 16.129018543724655
Epoch 17: loss 16.129008412425968
Epoch 18: loss 16.12900217464918
Epoch 19: loss 16.128991410679113
-----------Time: 0:05:02.709814, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 150, rmse: 4.017164707183838-------------


Epoch 0: loss 16.129171617268348
Epoch 1: loss 16.129162716567354
Epoch 2: loss 16.12915601725161
Epoch 3: loss 16.12914339960626
Epoch 4: loss 16.12913734048135
Epoch 5: loss 16.12912220589294
Epoch 6: loss 16.129113777621146
Epoch 7: loss 16.12910729559172
Epoch 8: loss 16.12909163541934
Epoch 9: loss 16.12908325511648
Epoch 10: loss 16.12908496929291
Epoch 11: loss 16.129069266337424
Epoch 12: loss 16.129062592432252
Epoch 13: loss 16.129045444704257
Epoch 14: loss 16.129038925596138
Epoch 15: loss 16.129027830510765
Epoch 16: loss 16.12902055738282
Epoch 17: loss 16.129013737496503
Epoch 18: loss 16.12900073687754
Epoch 19: loss 16.12898639209119
-----------Time: 0:05:32.749271, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 200, rmse: 4.017165184020996-------------


Epoch 0: loss 16.129395190628408
Epoch 1: loss 16.129373006939954
Epoch 2: loss 16.129370084724197
Epoch 3: loss 16.129355808390812
Epoch 4: loss 16.129340644243168
Epoch 5: loss 16.129322831432347
Epoch 6: loss 16.129317304632988
Epoch 7: loss 16.129291698555377
Epoch 8: loss 16.12929510746137
Epoch 9: loss 16.129273799141185
Epoch 10: loss 16.129261205091368
Epoch 11: loss 16.129252828159302
Epoch 12: loss 16.12924123782707
Epoch 13: loss 16.129229981980938
Epoch 14: loss 16.129218709540147
Epoch 15: loss 16.12920072118895
Epoch 16: loss 16.129184908293848
Epoch 17: loss 16.129182324453563
Epoch 18: loss 16.12917202772687
Epoch 19: loss 16.12914920903341
-----------Time: 0:04:12.665912, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 20, rmse: 4.017152309417725-------------


Epoch 0: loss 16.12916903057586
Epoch 1: loss 16.129157190286577
Epoch 2: loss 16.129149126060117
Epoch 3: loss 16.129135936676906
Epoch 4: loss 16.129117859647998
Epoch 5: loss 16.129104535951765
Epoch 6: loss 16.129097925832312
Epoch 7: loss 16.12908649781659
Epoch 8: loss 16.129074257440447
Epoch 9: loss 16.129054799684045
Epoch 10: loss 16.129050177812186
Epoch 11: loss 16.129034999403505
Epoch 12: loss 16.12902111615611
Epoch 13: loss 16.12901099082113
Epoch 14: loss 16.128990300651996
Epoch 15: loss 16.12898311127522
Epoch 16: loss 16.12897041117516
Epoch 17: loss 16.128954433111343
Epoch 18: loss 16.12894179420409
Epoch 19: loss 16.128930086672078
-----------Time: 0:04:10.102839, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 50, rmse: 4.0171613693237305-------------


Epoch 0: loss 16.129170624959592
Epoch 1: loss 16.12915320938343
Epoch 2: loss 16.12914237125598
Epoch 3: loss 16.129127746434754
Epoch 4: loss 16.12910995566372
Epoch 5: loss 16.12910259982177
Epoch 6: loss 16.129092978808853
Epoch 7: loss 16.129077054159094
Epoch 8: loss 16.1290638396246
Epoch 9: loss 16.129053305386847
Epoch 10: loss 16.129037784972613
Epoch 11: loss 16.12902444623747
Epoch 12: loss 16.129009883905507
Epoch 13: loss 16.129000477585993
Epoch 14: loss 16.128985245244667
Epoch 15: loss 16.128976772115436
Epoch 16: loss 16.128966572882366
Epoch 17: loss 16.128950551257567
Epoch 18: loss 16.128940152370003
Epoch 19: loss 16.128921491157236
-----------Time: 0:04:41.124901, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 100, rmse: 4.017160892486572-------------


Epoch 0: loss 16.12915725977671
Epoch 1: loss 16.129138792254732
Epoch 2: loss 16.12913261255925
Epoch 3: loss 16.129115891884435
Epoch 4: loss 16.129101486942442
Epoch 5: loss 16.12909437146376
Epoch 6: loss 16.129077436095542
Epoch 7: loss 16.129065309807682
Epoch 8: loss 16.129060348263895
Epoch 9: loss 16.129042818340157
Epoch 10: loss 16.129031589719638
Epoch 11: loss 16.129013721161137
Epoch 12: loss 16.12900265044917
Epoch 13: loss 16.128984992435406
Epoch 14: loss 16.128976309020732
Epoch 15: loss 16.128960465529225
Epoch 16: loss 16.128951369322404
Epoch 17: loss 16.12894228659874
Epoch 18: loss 16.12893026273104
Epoch 19: loss 16.12891982106037
-----------Time: 0:03:50.529397, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 150, rmse: 4.017155647277832-------------


Epoch 0: loss 16.12916463117615
Epoch 1: loss 16.129162123049
Epoch 2: loss 16.12914021343171
Epoch 3: loss 16.12912950702437
Epoch 4: loss 16.12911688834186
Epoch 5: loss 16.129101005178747
Epoch 6: loss 16.129091757286368
Epoch 7: loss 16.12907746072824
Epoch 8: loss 16.12906162112611
Epoch 9: loss 16.129051245315495
Epoch 10: loss 16.12904756000471
Epoch 11: loss 16.12902928643281
Epoch 12: loss 16.12901517474956
Epoch 13: loss 16.129004095740264
Epoch 14: loss 16.12898974550879
Epoch 15: loss 16.128974128638095
Epoch 16: loss 16.128970165885356
Epoch 17: loss 16.128949313918294
Epoch 18: loss 16.12893446455059
Epoch 19: loss 16.12893206065864
-----------Time: 0:04:19.240264, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 200, rmse: 4.017156600952148-------------


Epoch 0: loss 16.12931287204396
Epoch 1: loss 16.12929519276829
Epoch 2: loss 16.129284378236374
Epoch 3: loss 16.129261053405813
Epoch 4: loss 16.12924826047938
Epoch 5: loss 16.129227022686493
Epoch 6: loss 16.129217346703765
Epoch 7: loss 16.129194335097395
Epoch 8: loss 16.12918017777883
Epoch 9: loss 16.129162691416074
Epoch 10: loss 16.12914513737822
Epoch 11: loss 16.129126183166004
Epoch 12: loss 16.12910888919758
Epoch 13: loss 16.129103847273413
Epoch 14: loss 16.129083702135112
Epoch 15: loss 16.129056226306147
Epoch 16: loss 16.12904746977125
Epoch 17: loss 16.129029019103225
Epoch 18: loss 16.129014772329075
Epoch 19: loss 16.12899465597214
-----------Time: 0:02:41.343766, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 20, rmse: 4.017164707183838-------------


Epoch 0: loss 16.1291653906411
Epoch 1: loss 16.129154895297077
Epoch 2: loss 16.12913447116107
Epoch 3: loss 16.129116642533468
Epoch 4: loss 16.129107510803703
Epoch 5: loss 16.12908304742395
Epoch 6: loss 16.129074347155324
Epoch 7: loss 16.129058701762563
Epoch 8: loss 16.12904076630684
Epoch 9: loss 16.129016820213728
Epoch 10: loss 16.129001758486243
Epoch 11: loss 16.128987178263163
Epoch 12: loss 16.12896319664711
Epoch 13: loss 16.128959851786128
Epoch 14: loss 16.12893487163832
Epoch 15: loss 16.12891645078882
Epoch 16: loss 16.128908388636695
Epoch 17: loss 16.12888197097654
Epoch 18: loss 16.12886687502257
Epoch 19: loss 16.12885061536813
-----------Time: 0:03:07.029014, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 50, rmse: 4.017141819000244-------------


Epoch 0: loss 16.129169285978033
Epoch 1: loss 16.129163574044505
Epoch 2: loss 16.12913429639857
Epoch 3: loss 16.129125483078823
Epoch 4: loss 16.12910276084182
Epoch 5: loss 16.129086192371147
Epoch 6: loss 16.12907331880504
Epoch 7: loss 16.12905724998918
Epoch 8: loss 16.12903481919588
Epoch 9: loss 16.12901689696403
Epoch 10: loss 16.129005349414903
Epoch 11: loss 16.128986161581
Epoch 12: loss 16.12897416623537
Epoch 13: loss 16.128949767419215
Epoch 14: loss 16.12893848927401
Epoch 15: loss 16.128915963320708
Epoch 16: loss 16.12890201421326
Epoch 17: loss 16.128888574354413
Epoch 18: loss 16.128867353415476
Epoch 19: loss 16.12886030250039
-----------Time: 0:03:55.748298, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 100, rmse: 4.017147541046143-------------


Epoch 0: loss 16.129166536969034
Epoch 1: loss 16.129155390803227
Epoch 2: loss 16.12912078756817
Epoch 3: loss 16.129121016522607
Epoch 4: loss 16.129103047877567
Epoch 5: loss 16.12908287966232
Epoch 6: loss 16.12906080435773
Epoch 7: loss 16.129045869683104
Epoch 8: loss 16.129031136478012
Epoch 9: loss 16.12901747492489
Epoch 10: loss 16.128995299533763
Epoch 11: loss 16.128977345668343
Epoch 12: loss 16.128967340644643
Epoch 13: loss 16.1289472735531
Epoch 14: loss 16.128930658928528
Epoch 15: loss 16.128915137477126
Epoch 16: loss 16.12889107729573
Epoch 17: loss 16.128883089041697
Epoch 18: loss 16.128863591354396
Epoch 19: loss 16.12884562996952
-----------Time: 0:04:36.562590, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 150, rmse: 4.017144203186035-------------


Epoch 0: loss 16.12916736514624
Epoch 1: loss 16.129153810161945
Epoch 2: loss 16.129131345920033
Epoch 3: loss 16.129116488254997
Epoch 4: loss 16.129105647793924
Epoch 5: loss 16.129084782084412
Epoch 6: loss 16.129059422593066
Epoch 7: loss 16.129047967351735
Epoch 8: loss 16.129036845040748
Epoch 9: loss 16.129018938884975
Epoch 10: loss 16.12899910437795
Epoch 11: loss 16.12898382769777
Epoch 12: loss 16.128967646349377
Epoch 13: loss 16.128951099918485
Epoch 14: loss 16.12893073801245
Epoch 15: loss 16.128911880515982
Epoch 16: loss 16.12889506831125
Epoch 17: loss 16.128876791887144
Epoch 18: loss 16.128865712877847
Epoch 19: loss 16.128849459964986
-----------Time: 0:05:05.607643, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 200, rmse: 4.017148017883301-------------


Epoch 0: loss 16.129231831507557
Epoch 1: loss 16.129216864680778
Epoch 2: loss 16.12919184952861
Epoch 3: loss 16.129175180971394
Epoch 4: loss 16.129146491917272
Epoch 5: loss 16.12912737590713
Epoch 6: loss 16.129098545798406
Epoch 7: loss 16.129082284069632
Epoch 8: loss 16.129065931329524
Epoch 9: loss 16.12903114244172
Epoch 10: loss 16.129016488579836
Epoch 11: loss 16.128992650611302
Epoch 12: loss 16.128970369688513
Epoch 13: loss 16.128940658507105
Epoch 14: loss 16.128920771104607
Epoch 15: loss 16.1289005862947
Epoch 16: loss 16.1288819014864
Epoch 17: loss 16.128859876484377
Epoch 18: loss 16.12883668855975
Epoch 19: loss 16.128821174627806
-----------Time: 0:03:42.330626, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 20, rmse: 4.0171074867248535-------------


Epoch 0: loss 16.129157168506087
Epoch 1: loss 16.12915003746991
Epoch 2: loss 16.129128803047816
Epoch 3: loss 16.12909493853438
Epoch 4: loss 16.12907309270281
Epoch 5: loss 16.129051093889228
Epoch 6: loss 16.129029242353248
Epoch 7: loss 16.129009905167415
Epoch 8: loss 16.128989850781156
Epoch 9: loss 16.128964858705938
Epoch 10: loss 16.128936961787495
Epoch 11: loss 16.128916363148278
Epoch 12: loss 16.128897285772574
Epoch 13: loss 16.12887686837815
Epoch 14: loss 16.12885582738755
Epoch 15: loss 16.128831978269478
Epoch 16: loss 16.128801871668458
Epoch 17: loss 16.1287871711341
Epoch 18: loss 16.12876502711725
Epoch 19: loss 16.12874965786999
-----------Time: 0:03:53.555039, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 50, rmse: 4.01714563369751-------------


Epoch 0: loss 16.129152149140285
Epoch 1: loss 16.12912639785941
Epoch 2: loss 16.129099588669014
Epoch 3: loss 16.129084951142502
Epoch 4: loss 16.129056682918563
Epoch 5: loss 16.129038915224474
Epoch 6: loss 16.129013078636678
Epoch 7: loss 16.12899216677327
Epoch 8: loss 16.128967828112753
Epoch 9: loss 16.12894874710697
Epoch 10: loss 16.12892768433588
Epoch 11: loss 16.12890440358488
Epoch 12: loss 16.128880555503972
Epoch 13: loss 16.128859346751742
Epoch 14: loss 16.128842068340806
Epoch 15: loss 16.128817601590264
Epoch 16: loss 16.128794625766126
Epoch 17: loss 16.12876575546721
Epoch 18: loss 16.12874244593485
Epoch 19: loss 16.128724636494823
-----------Time: 0:04:43.272441, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 100, rmse: 4.017137050628662-------------


Epoch 0: loss 16.129165651229105
Epoch 1: loss 16.129139003836634
Epoch 2: loss 16.12912230623877
Epoch 3: loss 16.129095226866582
Epoch 4: loss 16.129066962532015
Epoch 5: loss 16.129051471417725
Epoch 6: loss 16.12902984728043
Epoch 7: loss 16.129009804821585
Epoch 8: loss 16.12898456797514
Epoch 9: loss 16.12896626536259
Epoch 10: loss 16.12893937553252
Epoch 11: loss 16.128920824777957
Epoch 12: loss 16.12889222206792
Epoch 13: loss 16.12887219750019
Epoch 14: loss 16.128853273365795
Epoch 15: loss 16.12883154058534
Epoch 16: loss 16.128807473921658
Epoch 17: loss 16.12878117968428
Epoch 18: loss 16.12875764560544
Epoch 19: loss 16.12874151818969
-----------Time: 0:05:31.778582, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 150, rmse: 4.017133712768555-------------


Epoch 0: loss 16.1291686273775
Epoch 1: loss 16.129141816372066
Epoch 2: loss 16.12912282871124
Epoch 3: loss 16.129098554355025
Epoch 4: loss 16.129078188300326
Epoch 5: loss 16.129060887590317
Epoch 6: loss 16.12903214408497
Epoch 7: loss 16.12901219808258
Epoch 8: loss 16.128986087942202
Epoch 9: loss 16.128968195528877
Epoch 10: loss 16.128938614511828
Epoch 11: loss 16.12892033653197
Epoch 12: loss 16.128901735215553
Epoch 13: loss 16.128872874510424
Epoch 14: loss 16.12885395374682
Epoch 15: loss 16.12883342407915
Epoch 16: loss 16.128807626903672
Epoch 17: loss 16.12878704459982
Epoch 18: loss 16.128765897558978
Epoch 19: loss 16.12874121041062
-----------Time: 0:06:00.350764, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 200, rmse: 4.017133712768555-------------


Epoch 0: loss 16.12932975425741
Epoch 1: loss 16.129308620440433
Epoch 2: loss 16.129276623604024
Epoch 3: loss 16.129249502226607
Epoch 4: loss 16.129207071757563
Epoch 5: loss 16.129189966812675
Epoch 6: loss 16.12915808510171
Epoch 7: loss 16.129139239532652
Epoch 8: loss 16.129100226785514
Epoch 9: loss 16.129074855885342
Epoch 10: loss 16.12904023346271
Epoch 11: loss 16.129012883649434
Epoch 12: loss 16.12898047920671
Epoch 13: loss 16.128954620319842
Epoch 14: loss 16.128915800744906
Epoch 15: loss 16.128887615753555
Epoch 16: loss 16.128870910895525
Epoch 17: loss 16.128845402830123
Epoch 18: loss 16.12880639552811
Epoch 19: loss 16.128771810184166
-----------Time: 0:02:53.972034, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 20, rmse: 4.017153739929199-------------


Epoch 0: loss 16.129187061191576
Epoch 1: loss 16.129144441439706
Epoch 2: loss 16.129118672267712
Epoch 3: loss 16.129096111310055
Epoch 4: loss 16.129068958558356
Epoch 5: loss 16.129026661883753
Epoch 6: loss 16.12900717560528
Epoch 7: loss 16.12897713927227
Epoch 8: loss 16.128950852295056
Epoch 9: loss 16.128916988559492
Epoch 10: loss 16.12889350452392
Epoch 11: loss 16.128857168184147
Epoch 12: loss 16.128824732626438
Epoch 13: loss 16.128801427242742
Epoch 14: loss 16.128774139400146
Epoch 15: loss 16.12873929683899
Epoch 16: loss 16.128712949706134
Epoch 17: loss 16.12868592582234
Epoch 18: loss 16.128651997782473
Epoch 19: loss 16.128623688331178
-----------Time: 0:02:46.366815, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 50, rmse: 4.01711368560791-------------


Epoch 0: loss 16.129159933850453
Epoch 1: loss 16.129126559916628
Epoch 2: loss 16.12909564873393
Epoch 3: loss 16.129062688629414
Epoch 4: loss 16.12904538325216
Epoch 5: loss 16.129006716140655
Epoch 6: loss 16.12898947973495
Epoch 7: loss 16.128950905968406
Epoch 8: loss 16.12892305494457
Epoch 9: loss 16.12887949214933
Epoch 10: loss 16.12886241028139
Epoch 11: loss 16.128831251215974
Epoch 12: loss 16.128797222570988
Epoch 13: loss 16.12877291398829
Epoch 14: loss 16.1287363204313
Epoch 15: loss 16.128716916866832
Epoch 16: loss 16.128685762468663
Epoch 17: loss 16.12865526693032
Epoch 18: loss 16.128628880644442
Epoch 19: loss 16.128593328143022
-----------Time: 0:03:24.137239, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 100, rmse: 4.017120838165283-------------


Epoch 0: loss 16.129168338267423
Epoch 1: loss 16.129137086375632
Epoch 2: loss 16.12910302505991
Epoch 3: loss 16.129080536185302
Epoch 4: loss 16.129040730265316
Epoch 5: loss 16.129015121335502
Epoch 6: loss 16.128987183708286
Epoch 7: loss 16.128964523701256
Epoch 8: loss 16.128929556939443
Epoch 9: loss 16.128899408333194
Epoch 10: loss 16.128867757651
Epoch 11: loss 16.1288422679953
Epoch 12: loss 16.12880619146566
Epoch 13: loss 16.128785466032873
Epoch 14: loss 16.128752087172508
Epoch 15: loss 16.12872632526068
Epoch 16: loss 16.1286974292919
Epoch 17: loss 16.128669150437005
Epoch 18: loss 16.128637889729305
Epoch 19: loss 16.128606827638926
-----------Time: 0:04:13.450515, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 150, rmse: 4.017116546630859-------------


Epoch 0: loss 16.12915601284365
Epoch 1: loss 16.129121474950065
Epoch 2: loss 16.12910383379025
Epoch 3: loss 16.12907012433316
Epoch 4: loss 16.129041583853095
Epoch 5: loss 16.12902346870833
Epoch 6: loss 16.128979465895338
Epoch 7: loss 16.128957570539082
Epoch 8: loss 16.128918481041648
Epoch 9: loss 16.1288886145446
Epoch 10: loss 16.12886378608235
Epoch 11: loss 16.128832135659447
Epoch 12: loss 16.128803861471802
Epoch 13: loss 16.128774667577034
Epoch 14: loss 16.128745965039748
Epoch 15: loss 16.128716102691367
Epoch 16: loss 16.1286942200404
Epoch 17: loss 16.128649933821745
Epoch 18: loss 16.128629340368356
Epoch 19: loss 16.128598981995243
-----------Time: 0:04:44.520848, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 200, rmse: 4.017115116119385-------------


Epoch 0: loss 16.129303797617627
Epoch 1: loss 16.1292663369897
Epoch 2: loss 16.12923266253697
Epoch 3: loss 16.129192198535033
Epoch 4: loss 16.129143234696837
Epoch 5: loss 16.1291056117524
Epoch 6: loss 16.129064645243442
Epoch 7: loss 16.129032157049547
Epoch 8: loss 16.128995652948145
Epoch 9: loss 16.128962109437644
Epoch 10: loss 16.128908611108844
Epoch 11: loss 16.128882827157234
Epoch 12: loss 16.128837487177723
Epoch 13: loss 16.12880460615713
Epoch 14: loss 16.128754832810717
Epoch 15: loss 16.128721456543268
Epoch 16: loss 16.128682248808886
Epoch 17: loss 16.12864630192502
Epoch 18: loss 16.128611907160366
Epoch 19: loss 16.12856949924969
-----------Time: 0:03:17.993337, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 20, rmse: 4.017171859741211-------------


Epoch 0: loss 16.12911820295001
Epoch 1: loss 16.12907946816341
Epoch 2: loss 16.129050160439654
Epoch 3: loss 16.129008665753812
Epoch 4: loss 16.128975317230555
Epoch 5: loss 16.1289333903057
Epoch 6: loss 16.12889244946661
Epoch 7: loss 16.128856292556588
Epoch 8: loss 16.12881948456549
Epoch 9: loss 16.128777733181014
Epoch 10: loss 16.12873393261543
Epoch 11: loss 16.128702977871754
Epoch 12: loss 16.128664405142374
Epoch 13: loss 16.128623084181868
Epoch 14: loss 16.128582384743204
Epoch 15: loss 16.12854320630877
Epoch 16: loss 16.12850985052535
Epoch 17: loss 16.128464092567864
Epoch 18: loss 16.128422441529217
Epoch 19: loss 16.128387118500818
-----------Time: 0:03:26.219770, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 50, rmse: 4.017089366912842-------------


Epoch 0: loss 16.129157569889404
Epoch 1: loss 16.129119876676956
Epoch 2: loss 16.129083918902847
Epoch 3: loss 16.129043486534478
Epoch 4: loss 16.12900921130323
Epoch 5: loss 16.12896521056457
Epoch 6: loss 16.128928198770314
Epoch 7: loss 16.128886657411993
Epoch 8: loss 16.128848357975905
Epoch 9: loss 16.128809611002605
Epoch 10: loss 16.128767045961247
Epoch 11: loss 16.12872859846969
Epoch 12: loss 16.128691983650793
Epoch 13: loss 16.128652246183776
Epoch 14: loss 16.128614162218966
Epoch 15: loss 16.128574115935717
Epoch 16: loss 16.128531567229725
Epoch 17: loss 16.12849826849045
Epoch 18: loss 16.128457887980392
Epoch 19: loss 16.12842133928084
-----------Time: 0:04:17.979788, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 100, rmse: 4.017093181610107-------------


Epoch 0: loss 16.129161101440296
Epoch 1: loss 16.129115784537735
Epoch 2: loss 16.12907101655538
Epoch 3: loss 16.129034530604386
Epoch 4: loss 16.129003801185064
Epoch 5: loss 16.128962185410067
Epoch 6: loss 16.128923399543037
Epoch 7: loss 16.128884196735193
Epoch 8: loss 16.128841547164793
Epoch 9: loss 16.128801976422242
Epoch 10: loss 16.12876962124491
Epoch 11: loss 16.12872627884749
Epoch 12: loss 16.128683912423462
Epoch 13: loss 16.12865109129922
Epoch 14: loss 16.128613680973853
Epoch 15: loss 16.128573448000687
Epoch 16: loss 16.12853318624616
Epoch 17: loss 16.12849426062098
Epoch 18: loss 16.128460219011416
Epoch 19: loss 16.128411722935173
-----------Time: 0:05:03.591940, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 150, rmse: 4.017090797424316-------------


Epoch 0: loss 16.12915473349915
Epoch 1: loss 16.12911246093866
Epoch 2: loss 16.12907534724283
Epoch 3: loss 16.12903791643343
Epoch 4: loss 16.128992636868848
Epoch 5: loss 16.128960997595485
Epoch 6: loss 16.128912630905724
Epoch 7: loss 16.12888890391397
Epoch 8: loss 16.128841260129047
Epoch 9: loss 16.128801600968078
Epoch 10: loss 16.128765168171853
Epoch 11: loss 16.128727042720396
Epoch 12: loss 16.12868715149349
Epoch 13: loss 16.12864842474493
Epoch 14: loss 16.128612062994588
Epoch 15: loss 16.12857003131595
Epoch 16: loss 16.12853097837862
Epoch 17: loss 16.12849158395432
Epoch 18: loss 16.128452638104402
Epoch 19: loss 16.12841718180015
-----------Time: 0:05:33.863292, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 200, rmse: 4.017091751098633-------------


Epoch 0: loss 16.129265773808456
Epoch 1: loss 16.129207347903062
Epoch 2: loss 16.129161372399967
Epoch 3: loss 16.129105232408865
Epoch 4: loss 16.129056837715616
Epoch 5: loss 16.129007739045814
Epoch 6: loss 16.128953639938494
Epoch 7: loss 16.128896158632212
Epoch 8: loss 16.128849693845964
Epoch 9: loss 16.128800068555027
Epoch 10: loss 16.128745805316157
Epoch 11: loss 16.128693343634964
Epoch 12: loss 16.12863862741376
Epoch 13: loss 16.128593256059972
Epoch 14: loss 16.12854477372618
Epoch 15: loss 16.12849174445579
Epoch 16: loss 16.12843657706729
Epoch 17: loss 16.12839615792279
Epoch 18: loss 16.12833494800404
Epoch 19: loss 16.12829033715236
-----------Time: 0:04:11.377281, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 20, rmse: 4.017111778259277-------------


Epoch 0: loss 16.129166505076174
Epoch 1: loss 16.12910671477865
Epoch 2: loss 16.129054578767644
Epoch 3: loss 16.129007402744676
Epoch 4: loss 16.128958901482605
Epoch 5: loss 16.128902343254236
Epoch 6: loss 16.128855326436277
Epoch 7: loss 16.12880196449483
Epoch 8: loss 16.128755294868256
Epoch 9: loss 16.128697707511733
Epoch 10: loss 16.12865038032179
Epoch 11: loss 16.12859233998293
Epoch 12: loss 16.12854429637044
Epoch 13: loss 16.128495688280246
Epoch 14: loss 16.12844487750835
Epoch 15: loss 16.128387950567397
Epoch 16: loss 16.12833989528679
Epoch 17: loss 16.12828316774104
Epoch 18: loss 16.12823722542726
Epoch 19: loss 16.128188575331837
-----------Time: 0:04:03.330253, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 50, rmse: 4.017069339752197-------------


Epoch 0: loss 16.129152671612754
Epoch 1: loss 16.1290974015448
Epoch 2: loss 16.129048304171455
Epoch 3: loss 16.128998746555613
Epoch 4: loss 16.128938583915424
Epoch 5: loss 16.128895086980243
Epoch 6: loss 16.128856551588846
Epoch 7: loss 16.128792258434284
Epoch 8: loss 16.128737314036517
Epoch 9: loss 16.12868586203662
Epoch 10: loss 16.12863754435296
Epoch 11: loss 16.1285900245094
Epoch 12: loss 16.12853864251822
Epoch 13: loss 16.128488570986526
Epoch 14: loss 16.12843550360028
Epoch 15: loss 16.128374591866805
Epoch 16: loss 16.128327797780287
Epoch 17: loss 16.1282750783633
Epoch 18: loss 16.128230529482305
Epoch 19: loss 16.12817657894904
-----------Time: 0:04:46.153797, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 100, rmse: 4.017063617706299-------------


Epoch 0: loss 16.12915796504972
Epoch 1: loss 16.129103948137825
Epoch 2: loss 16.129050965021328
Epoch 3: loss 16.12899787896609
Epoch 4: loss 16.128949682371587
Epoch 5: loss 16.1289019639107
Epoch 6: loss 16.12884751320404
Epoch 7: loss 16.128793343569416
Epoch 8: loss 16.12874720211973
Epoch 9: loss 16.12869693015567
Epoch 10: loss 16.128638713757844
Epoch 11: loss 16.128585444902065
Epoch 12: loss 16.128536357122506
Epoch 13: loss 16.128495685687334
Epoch 14: loss 16.128429967985003
Epoch 15: loss 16.128383846241473
Epoch 16: loss 16.1283306722864
Epoch 17: loss 16.128279194875926
Epoch 18: loss 16.128232464834422
Epoch 19: loss 16.128172725876627
-----------Time: 0:03:51.201919, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 150, rmse: 4.017061710357666-------------


Epoch 0: loss 16.129151665820835
Epoch 1: loss 16.1291051056153
Epoch 2: loss 16.12904959207258
Epoch 3: loss 16.128996711635537
Epoch 4: loss 16.128943686773106
Epoch 5: loss 16.128893039614173
Epoch 6: loss 16.12884326626776
Epoch 7: loss 16.128790506919874
Epoch 8: loss 16.12874525691453
Epoch 9: loss 16.128682504729642
Epoch 10: loss 16.12863568289893
Epoch 11: loss 16.12858364775234
Epoch 12: loss 16.128544722645746
Epoch 13: loss 16.128483949892544
Epoch 14: loss 16.128436929444504
Epoch 15: loss 16.128382883491952
Epoch 16: loss 16.128326376344017
Epoch 17: loss 16.128277131693075
Epoch 18: loss 16.128232025335247
Epoch 19: loss 16.1281738312365
-----------Time: 0:04:19.954151, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 200, rmse: 4.017061710357666-------------


Epoch 0: loss 16.129327695482512
Epoch 1: loss 16.12926096576527
Epoch 2: loss 16.129193450653922
Epoch 3: loss 16.12913210538498
Epoch 4: loss 16.129053038835135
Epoch 5: loss 16.128997344047626
Epoch 6: loss 16.128923898419977
Epoch 7: loss 16.128858739750225
Epoch 8: loss 16.12878838513713
Epoch 9: loss 16.12872863088113
Epoch 10: loss 16.128654013774263
Epoch 11: loss 16.128576721297197
Epoch 12: loss 16.128519926335645
Epoch 13: loss 16.128449897392734
Epoch 14: loss 16.128379143989235
Epoch 15: loss 16.12831643380958
Epoch 16: loss 16.1282440782436
Epoch 17: loss 16.128177950860625
Epoch 18: loss 16.1281055722177
Epoch 19: loss 16.12804098969377
-----------Time: 0:02:39.645051, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 20, rmse: 4.017070770263672-------------


Epoch 0: loss 16.1291752852066
Epoch 1: loss 16.12911303630661
Epoch 2: loss 16.129041170542365
Epoch 3: loss 16.128975812995996
Epoch 4: loss 16.128907431332298
Epoch 5: loss 16.128833288728963
Epoch 6: loss 16.12876946385494
Epoch 7: loss 16.128706477011203
Epoch 8: loss 16.128640313068118
Epoch 9: loss 16.128564971759957
Epoch 10: loss 16.128497753278143
Epoch 11: loss 16.12843072770924
Epoch 12: loss 16.128359713199153
Epoch 13: loss 16.128289276131344
Epoch 14: loss 16.128223017546844
Epoch 15: loss 16.128158608748254
Epoch 16: loss 16.128085338142398
Epoch 17: loss 16.128020364866106
Epoch 18: loss 16.127960201707335
Epoch 19: loss 16.127887103789654
-----------Time: 0:03:04.820534, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 50, rmse: 4.017024040222168-------------


Epoch 0: loss 16.129126585845782
Epoch 1: loss 16.129058953793947
Epoch 2: loss 16.128992291751796
Epoch 3: loss 16.128922239472647
Epoch 4: loss 16.12885674502034
Epoch 5: loss 16.12878879378061
Epoch 6: loss 16.1287168686386
Epoch 7: loss 16.128651000547176
Epoch 8: loss 16.1285858273571
Epoch 9: loss 16.128520585714053
Epoch 10: loss 16.12844374907153
Epoch 11: loss 16.128380534051228
Epoch 12: loss 16.12830679075688
Epoch 13: loss 16.128240971152973
Epoch 14: loss 16.12818058681851
Epoch 15: loss 16.12810709218476
Epoch 16: loss 16.128033850360268
Epoch 17: loss 16.12797402246547
Epoch 18: loss 16.127900305359567
Epoch 19: loss 16.127839422148163
-----------Time: 0:03:55.506826, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 100, rmse: 4.0170207023620605-------------


Epoch 0: loss 16.12912907634111
Epoch 1: loss 16.129068283622455
Epoch 2: loss 16.128999140678765
Epoch 3: loss 16.128932520123264
Epoch 4: loss 16.12886652186748
Epoch 5: loss 16.12879165506285
Epoch 6: loss 16.128730007978547
Epoch 7: loss 16.128665503501374
Epoch 8: loss 16.12859268717431
Epoch 9: loss 16.128531507333378
Epoch 10: loss 16.128462512185873
Epoch 11: loss 16.12838375574872
Epoch 12: loss 16.12831606535629
Epoch 13: loss 16.128254962006363
Epoch 14: loss 16.12818249831581
Epoch 15: loss 16.128120342501486
Epoch 16: loss 16.12805491676144
Epoch 17: loss 16.127974447703608
Epoch 18: loss 16.12790750147792
Epoch 19: loss 16.127847358025306
-----------Time: 0:04:37.400858, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 150, rmse: 4.017022132873535-------------


Epoch 0: loss 16.129151114307714
Epoch 1: loss 16.129075797632247
Epoch 2: loss 16.129005897038653
Epoch 3: loss 16.12894579196119
Epoch 4: loss 16.128870811846152
Epoch 5: loss 16.128805154558755
Epoch 6: loss 16.128733177039855
Epoch 7: loss 16.128664596240245
Epoch 8: loss 16.128600842412105
Epoch 9: loss 16.12853659644861
Epoch 10: loss 16.128463367588694
Epoch 11: loss 16.128395644006943
Epoch 12: loss 16.128327636500945
Epoch 13: loss 16.128264793304727
Epoch 14: loss 16.1281900797414
Epoch 15: loss 16.128127638965665
Epoch 16: loss 16.12805544908699
Epoch 17: loss 16.127995879446573
Epoch 18: loss 16.127925646441213
Epoch 19: loss 16.127852791998293
-----------Time: 0:05:06.721848, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 200, rmse: 4.017019748687744-------------


Epoch 0: loss 16.12932043480056
Epoch 1: loss 16.129235614311963
Epoch 2: loss 16.129140935817965
Epoch 3: loss 16.129053952578555
Epoch 4: loss 16.128960421968245
Epoch 5: loss 16.12887385670681
Epoch 6: loss 16.128784945634735
Epoch 7: loss 16.128698880546697
Epoch 8: loss 16.12859957369856
Epoch 9: loss 16.128519648375104
Epoch 10: loss 16.1284270042826
Epoch 11: loss 16.128336223459165
Epoch 12: loss 16.12824763027853
Epoch 13: loss 16.128154436747234
Epoch 14: loss 16.12806774754444
Epoch 15: loss 16.127982369060422
Epoch 16: loss 16.127889188234413
Epoch 17: loss 16.12779917283963
Epoch 18: loss 16.127709983288433
Epoch 19: loss 16.127609143508653
-----------Time: 0:03:42.149283, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 20, rmse: 4.016986846923828-------------


Epoch 0: loss 16.129102612008474
Epoch 1: loss 16.12900903057702
Epoch 2: loss 16.128921957622733
Epoch 3: loss 16.128835274642935
Epoch 4: loss 16.12874382406943
Epoch 5: loss 16.128656748003646
Epoch 6: loss 16.12856468576138
Epoch 7: loss 16.128473818593857
Epoch 8: loss 16.1283868244642
Epoch 9: loss 16.128288720728847
Epoch 10: loss 16.12819960559432
Epoch 11: loss 16.12810828622234
Epoch 12: loss 16.128022568066395
Epoch 13: loss 16.12793495448923
Epoch 14: loss 16.12784350080423
Epoch 15: loss 16.12775932284008
Epoch 16: loss 16.127664044604735
Epoch 17: loss 16.12758074563894
Epoch 18: loss 16.127485716842063
Epoch 19: loss 16.127392526162975
-----------Time: 0:03:40.263585, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 50, rmse: 4.016967296600342-------------


Epoch 0: loss 16.1291168839339
Epoch 1: loss 16.129030781507883
Epoch 2: loss 16.128930892550212
Epoch 3: loss 16.128849754520186
Epoch 4: loss 16.12876222962073
Epoch 5: loss 16.128668421568463
Epoch 6: loss 16.128580360972666
Epoch 7: loss 16.128485388182764
Epoch 8: loss 16.128401410132405
Epoch 9: loss 16.12830367355388
Epoch 10: loss 16.12821686248406
Epoch 11: loss 16.128129686331736
Epoch 12: loss 16.128039331783608
Epoch 13: loss 16.127951327454078
Epoch 14: loss 16.127863563228523
Epoch 15: loss 16.12777324938917
Epoch 16: loss 16.12767605265565
Epoch 17: loss 16.127595835369913
Epoch 18: loss 16.12749930579355
Epoch 19: loss 16.127409618402577
-----------Time: 0:04:37.517404, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 100, rmse: 4.016963005065918-------------


Epoch 0: loss 16.12912235965283
Epoch 1: loss 16.129037903728143
Epoch 2: loss 16.128949232241457
Epoch 3: loss 16.128861319701137
Epoch 4: loss 16.128774018829578
Epoch 5: loss 16.128669736435906
Epoch 6: loss 16.128596699192446
Epoch 7: loss 16.12850549079725
Epoch 8: loss 16.128410102622286
Epoch 9: loss 16.128323034335246
Epoch 10: loss 16.128233033720083
Epoch 11: loss 16.128142429992774
Epoch 12: loss 16.128048661093533
Epoch 13: loss 16.12795857335641
Epoch 14: loss 16.127880954949866
Epoch 15: loss 16.127779000216428
Epoch 16: loss 16.12768999191002
Epoch 17: loss 16.12760990271431
Epoch 18: loss 16.12751457573215
Epoch 19: loss 16.127419677099628
-----------Time: 0:05:32.097704, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 150, rmse: 4.016964912414551-------------


Epoch 0: loss 16.12912576337299
Epoch 1: loss 16.129043893103628
Epoch 2: loss 16.128951858605557
Epoch 3: loss 16.12886081641624
Epoch 4: loss 16.128767939479925
Epoch 5: loss 16.12868163195429
Epoch 6: loss 16.12859439616491
Epoch 7: loss 16.128502135305318
Epoch 8: loss 16.128409067530423
Epoch 9: loss 16.12832409302264
Epoch 10: loss 16.128229640630874
Epoch 11: loss 16.1281357376779
Epoch 12: loss 16.128052803276024
Epoch 13: loss 16.127962633084188
Epoch 14: loss 16.127868665308327
Epoch 15: loss 16.127783387688723
Epoch 16: loss 16.127687656730334
Epoch 17: loss 16.127602085074113
Epoch 18: loss 16.12751308402787
Epoch 19: loss 16.127417807866856
-----------Time: 0:06:01.071983, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 200, rmse: 4.016966342926025-------------


Epoch 0: loss 16.12924968398998
Epoch 1: loss 16.12913173589452
Epoch 2: loss 16.129009114846774
Epoch 3: loss 16.12889845336242
Epoch 4: loss 16.12877681191814
Epoch 5: loss 16.128657190095737
Epoch 6: loss 16.128543768452847
Epoch 7: loss 16.128418027609182
Epoch 8: loss 16.12830282126256
Epoch 9: loss 16.12817385638778
Epoch 10: loss 16.128052667666545
Epoch 11: loss 16.12794586677406
Epoch 12: loss 16.127826865177042
Epoch 13: loss 16.127705746723816
Epoch 14: loss 16.12758207813821
Epoch 15: loss 16.127469943359277
Epoch 16: loss 16.127352118686595
Epoch 17: loss 16.12723399323572
Epoch 18: loss 16.127112685240373
Epoch 19: loss 16.126998691341033
-----------Time: 0:03:08.686755, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 20, rmse: 4.016863822937012-------------


Epoch 0: loss 16.129115630518555
Epoch 1: loss 16.128989894342137
Epoch 2: loss 16.12887742066915
Epoch 3: loss 16.128757624602823
Epoch 4: loss 16.128634393960642
Epoch 5: loss 16.1285140155255
Epoch 6: loss 16.128403908665767
Epoch 7: loss 16.128282479062683
Epoch 8: loss 16.128163333040273
Epoch 9: loss 16.128036417865182
Epoch 10: loss 16.12792355992212
Epoch 11: loss 16.127804676821338
Epoch 12: loss 16.127681437881826
Epoch 13: loss 16.127562200070205
Epoch 14: loss 16.12744944532518
Epoch 15: loss 16.127328125661712
Epoch 16: loss 16.127220112321947
Epoch 17: loss 16.12709089878657
Epoch 18: loss 16.126973675929573
Epoch 19: loss 16.12685381659611
-----------Time: 0:02:46.297020, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 50, rmse: 4.016899108886719-------------


Epoch 0: loss 16.129115821357132
Epoch 1: loss 16.128991457610883
Epoch 2: loss 16.12887982819118
Epoch 3: loss 16.128760045089432
Epoch 4: loss 16.128639976248397
Epoch 5: loss 16.128510552686866
Epoch 6: loss 16.128397208053563
Epoch 7: loss 16.12828446549524
Epoch 8: loss 16.12816430408712
Epoch 9: loss 16.128046517789592
Epoch 10: loss 16.127925699855272
Epoch 11: loss 16.12780068917661
Epoch 12: loss 16.127694414386678
Epoch 13: loss 16.12757419800875
Epoch 14: loss 16.127454105831475
Epoch 15: loss 16.127330793253165
Epoch 16: loss 16.127216694081458
Epoch 17: loss 16.127097697151687
Epoch 18: loss 16.12697235665417
Epoch 19: loss 16.126861852559788
-----------Time: 0:03:20.237320, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 100, rmse: 4.016890525817871-------------


Epoch 0: loss 16.129116336828734
Epoch 1: loss 16.12899936600312
Epoch 2: loss 16.128871781596548
Epoch 3: loss 16.12876007542654
Epoch 4: loss 16.128639706585187
Epoch 5: loss 16.128526219341534
Epoch 6: loss 16.12840330788725
Epoch 7: loss 16.128284203610775
Epoch 8: loss 16.128166305817878
Epoch 9: loss 16.12804444682799
Epoch 10: loss 16.127931970043505
Epoch 11: loss 16.12781402920821
Epoch 12: loss 16.12769592527853
Epoch 13: loss 16.127576255746483
Epoch 14: loss 16.12745509554732
Epoch 15: loss 16.127340583842756
Epoch 16: loss 16.12721269969515
Epoch 17: loss 16.127098346158434
Epoch 18: loss 16.12698222995777
Epoch 19: loss 16.1268560535043
-----------Time: 0:04:13.231476, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 150, rmse: 4.016892433166504-------------


Epoch 0: loss 16.129117506752202
Epoch 1: loss 16.129003765662123
Epoch 2: loss 16.128882360691737
Epoch 3: loss 16.128763086579298
Epoch 4: loss 16.12864740028402
Epoch 5: loss 16.128520945610724
Epoch 6: loss 16.128405638918274
Epoch 7: loss 16.128284444492625
Epoch 8: loss 16.128170045839177
Epoch 9: loss 16.1280452840804
Epoch 10: loss 16.12792806537207
Epoch 11: loss 16.127815851249924
Epoch 12: loss 16.1276931005564
Epoch 13: loss 16.127573445803968
Epoch 14: loss 16.127447169263963
Epoch 15: loss 16.12733649974157
Epoch 16: loss 16.12722081759496
Epoch 17: loss 16.12710600848299
Epoch 18: loss 16.126980641278447
Epoch 19: loss 16.126863841325967
-----------Time: 0:04:49.794749, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 200, rmse: 4.016894817352295-------------


Epoch 0: loss 16.12936646890355
Epoch 1: loss 16.129200335622418
Epoch 2: loss 16.129050850933048
Epoch 3: loss 16.128892533995653
Epoch 4: loss 16.128730431142355
Epoch 5: loss 16.128576466154314
Epoch 6: loss 16.128422014735328
Epoch 7: loss 16.128260254406165
Epoch 8: loss 16.128103318196267
Epoch 9: loss 16.127940873078124
Epoch 10: loss 16.12779720222386
Epoch 11: loss 16.127625680345762
Epoch 12: loss 16.127470602478393
Epoch 13: loss 16.12732174838607
Epoch 14: loss 16.12716075400414
Epoch 15: loss 16.127006358073547
Epoch 16: loss 16.126853164737156
Epoch 17: loss 16.126695979607373
Epoch 18: loss 16.126541504333563
Epoch 19: loss 16.126380671749562
-----------Time: 0:03:13.231477, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 20, rmse: 4.016835689544678-------------


Epoch 0: loss 16.12912120969481
Epoch 1: loss 16.12896319846215
Epoch 2: loss 16.128813451629025
Epoch 3: loss 16.12865159743632
Epoch 4: loss 16.128503389239246
Epoch 5: loss 16.128335854228002
Epoch 6: loss 16.128180492695677
Epoch 7: loss 16.12802919789201
Epoch 8: loss 16.12786456046382
Epoch 9: loss 16.127714304900675
Epoch 10: loss 16.127551831779044
Epoch 11: loss 16.12739438891346
Epoch 12: loss 16.1272408846865
Epoch 13: loss 16.12707970750403
Epoch 14: loss 16.126926468273037
Epoch 15: loss 16.126763500682422
Epoch 16: loss 16.126612262145017
Epoch 17: loss 16.126455996203774
Epoch 18: loss 16.126304181261254
Epoch 19: loss 16.126143234588973
-----------Time: 0:03:26.623836, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 50, rmse: 4.0167927742004395-------------


Epoch 0: loss 16.12909741658371
Epoch 1: loss 16.12893191182529
Epoch 2: loss 16.128779292041806
Epoch 3: loss 16.12862224226221
Epoch 4: loss 16.128468602944356
Epoch 5: loss 16.12830971556555
Epoch 6: loss 16.128157178755362
Epoch 7: loss 16.128000058967046
Epoch 8: loss 16.127836961471367
Epoch 9: loss 16.12768041601383
Epoch 10: loss 16.12752384203423
Epoch 11: loss 16.127368869957813
Epoch 12: loss 16.127210237981185
Epoch 13: loss 16.127049241265635
Epoch 14: loss 16.126892740406245
Epoch 15: loss 16.126745882080975
Epoch 16: loss 16.126579054417075
Epoch 17: loss 16.12642199841448
Epoch 18: loss 16.126269546133337
Epoch 19: loss 16.126105230226667
-----------Time: 0:04:14.638551, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 100, rmse: 4.016798496246338-------------


Epoch 0: loss 16.129100066024755
Epoch 1: loss 16.12894527467455
Epoch 2: loss 16.128793028530186
Epoch 3: loss 16.12862318997282
Epoch 4: loss 16.12847640891643
Epoch 5: loss 16.128320536061175
Epoch 6: loss 16.128151924471418
Epoch 7: loss 16.127998871930338
Epoch 8: loss 16.127846854999703
Epoch 9: loss 16.127694951120183
Epoch 10: loss 16.127532241265367
Epoch 11: loss 16.12737405112154
Epoch 12: loss 16.127216061410078
Epoch 13: loss 16.12706699547656
Epoch 14: loss 16.12690651786269
Epoch 15: loss 16.126738199531676
Epoch 16: loss 16.126592799202783
Epoch 17: loss 16.12643508693328
Epoch 18: loss 16.12627514008921
Epoch 19: loss 16.126117916584274
-----------Time: 0:05:02.660219, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 150, rmse: 4.016800880432129-------------


Epoch 0: loss 16.129095270168268
Epoch 1: loss 16.128942240185552
Epoch 2: loss 16.128781404490052
Epoch 3: loss 16.128627714610346
Epoch 4: loss 16.128471696551824
Epoch 5: loss 16.128306888509794
Epoch 6: loss 16.12814803613535
Epoch 7: loss 16.12799947841327
Epoch 8: loss 16.12784352517763
Epoch 9: loss 16.1276849577646
Epoch 10: loss 16.127523681792052
Epoch 11: loss 16.12737190522468
Epoch 12: loss 16.12721344619552
Epoch 13: loss 16.1270687583998
Epoch 14: loss 16.126903471705575
Epoch 15: loss 16.126748579490958
Epoch 16: loss 16.126584880438884
Epoch 17: loss 16.126437284688446
Epoch 18: loss 16.126270769730155
Epoch 19: loss 16.12611306886948
-----------Time: 0:05:31.938444, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 200, rmse: 4.016799449920654-------------


Epoch 0: loss 16.129259276740118
Epoch 1: loss 16.12906163305352
Epoch 2: loss 16.12884577439491
Epoch 3: loss 16.128638571406785
Epoch 4: loss 16.128436538173556
Epoch 5: loss 16.128225522562495
Epoch 6: loss 16.128019474458927
Epoch 7: loss 16.127805113986888
Epoch 8: loss 16.127599260870564
Epoch 9: loss 16.127392753820953
Epoch 10: loss 16.127182954546548
Epoch 11: loss 16.126974533406724
Epoch 12: loss 16.126773636648355
Epoch 13: loss 16.126556439267482
Epoch 14: loss 16.126356834299607
Epoch 15: loss 16.126149131915955
Epoch 16: loss 16.12593441491804
Epoch 17: loss 16.125729268889767
Epoch 18: loss 16.125524839024752
Epoch 19: loss 16.125320086341763
-----------Time: 0:04:09.220963, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 20, rmse: 4.0167012214660645-------------


Epoch 0: loss 16.129107959637373
Epoch 1: loss 16.12889470452521
Epoch 2: loss 16.12869133360688
Epoch 3: loss 16.1284777014848
Epoch 4: loss 16.128278154338943
Epoch 5: loss 16.128062756182125
Epoch 6: loss 16.12785418906116
Epoch 7: loss 16.12764664562323
Epoch 8: loss 16.12744442181071
Epoch 9: loss 16.12723385451474
Epoch 10: loss 16.12702734746513
Epoch 11: loss 16.12681866107005
Epoch 12: loss 16.126612814176724
Epoch 13: loss 16.126400505997292
Epoch 14: loss 16.126194136631494
Epoch 15: loss 16.125990612731155
Epoch 16: loss 16.12578478995194
Epoch 17: loss 16.125571450829312
Epoch 18: loss 16.125371274901447
Epoch 19: loss 16.12516124407969
-----------Time: 0:04:17.434093, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 50, rmse: 4.016688346862793-------------


Epoch 0: loss 16.129053922760026
Epoch 1: loss 16.1288418414607
Epoch 2: loss 16.128647891641506
Epoch 3: loss 16.12843284560261
Epoch 4: loss 16.128225537342114
Epoch 5: loss 16.128013487157776
Epoch 6: loss 16.127808537153914
Epoch 7: loss 16.127603166579163
Epoch 8: loss 16.127398997042864
Epoch 9: loss 16.127187800187013
Epoch 10: loss 16.126978395554342
Epoch 11: loss 16.1267742887666
Epoch 12: loss 16.12656193417398
Epoch 13: loss 16.126356816149197
Epoch 14: loss 16.12614824513886
Epoch 15: loss 16.125936274557027
Epoch 16: loss 16.125743922492354
Epoch 17: loss 16.12553126919587
Epoch 18: loss 16.12531786758398
Epoch 19: loss 16.125107549726476
-----------Time: 0:04:45.209136, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 100, rmse: 4.016674041748047-------------


Epoch 0: loss 16.12907563427857
Epoch 1: loss 16.12885746636943
Epoch 2: loss 16.128663129687244
Epoch 3: loss 16.128451962390628
Epoch 4: loss 16.12824200365192
Epoch 5: loss 16.128037569897533
Epoch 6: loss 16.127833995176047
Epoch 7: loss 16.127619020183037
Epoch 8: loss 16.127414834311374
Epoch 9: loss 16.127211245588143
Epoch 10: loss 16.126999542595186
Epoch 11: loss 16.1267915529165
Epoch 12: loss 16.12658309003074
Epoch 13: loss 16.126377076412947
Epoch 14: loss 16.126167368149876
Epoch 15: loss 16.12596004018322
Epoch 16: loss 16.125753810316276
Epoch 17: loss 16.125548592723536
Epoch 18: loss 16.125339910477123
Epoch 19: loss 16.12513309461128
-----------Time: 0:03:47.606911, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 150, rmse: 4.0166754722595215-------------


Epoch 0: loss 16.129067302203943
Epoch 1: loss 16.128859588411466
Epoch 2: loss 16.128657902628913
Epoch 3: loss 16.12844659038832
Epoch 4: loss 16.12823565852843
Epoch 5: loss 16.128031589597253
Epoch 6: loss 16.12782694685388
Epoch 7: loss 16.127618688289857
Epoch 8: loss 16.127409681669715
Epoch 9: loss 16.127197115754484
Epoch 10: loss 16.12699015753758
Epoch 11: loss 16.126787157406167
Epoch 12: loss 16.12658058864517
Epoch 13: loss 16.126366542693777
Epoch 14: loss 16.1261661002142
Epoch 15: loss 16.12596315297826
Epoch 16: loss 16.125753713600528
Epoch 17: loss 16.125547548037886
Epoch 18: loss 16.125334212026758
Epoch 19: loss 16.125130132723918
-----------Time: 0:04:17.062229, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 200, rmse: 4.016674518585205-------------


Epoch 0: loss 16.129058387501203
Epoch 1: loss 16.128783449003915
Epoch 2: loss 16.128518290724553
Epoch 3: loss 16.128236130179758
Epoch 4: loss 16.127963237233498
Epoch 5: loss 16.127692974540707
Epoch 6: loss 16.127414449782005
Epoch 7: loss 16.127142841884663
Epoch 8: loss 16.12686792516786
Epoch 9: loss 16.126585428062633
Epoch 10: loss 16.126323606346922
Epoch 11: loss 16.12604149843831
Epoch 12: loss 16.12577081543392
Epoch 13: loss 16.12549513717784
Epoch 14: loss 16.12522477699143
Epoch 15: loss 16.124947266840557
Epoch 16: loss 16.12467702074243
Epoch 17: loss 16.124399466252704
Epoch 18: loss 16.12412323622421
Epoch 19: loss 16.12384897577432
-----------Time: 0:02:40.630103, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 20, rmse: 4.01649808883667-------------


Epoch 0: loss 16.129046384117537
Epoch 1: loss 16.128773036892483
Epoch 2: loss 16.12849913582068
Epoch 3: loss 16.128214871643547
Epoch 4: loss 16.12794478008318
Epoch 5: loss 16.1276813066803
Epoch 6: loss 16.127396371975227
Epoch 7: loss 16.127119542983255
Epoch 8: loss 16.126845515117886
Epoch 9: loss 16.126575403073485
Epoch 10: loss 16.12629905040009
Epoch 11: loss 16.12602986091502
Epoch 12: loss 16.12575834999271
Epoch 13: loss 16.12548395678555
Epoch 14: loss 16.125209422264493
Epoch 15: loss 16.124931663971612
Epoch 16: loss 16.124659030057607
Epoch 17: loss 16.12438285473963
Epoch 18: loss 16.124104280715535
Epoch 19: loss 16.12383237307716
-----------Time: 0:03:07.788738, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 50, rmse: 4.016505241394043-------------


Epoch 0: loss 16.12904117624678
Epoch 1: loss 16.12876940655151
Epoch 2: loss 16.128496759672927
Epoch 3: loss 16.128219427655353
Epoch 4: loss 16.127947715263513
Epoch 5: loss 16.127675172879425
Epoch 6: loss 16.127395598508535
Epoch 7: loss 16.127123491474958
Epoch 8: loss 16.126855002077072
Epoch 9: loss 16.126574716210172
Epoch 10: loss 16.12630482637863
Epoch 11: loss 16.126028578199726
Epoch 12: loss 16.125748884814016
Epoch 13: loss 16.125478468361337
Epoch 14: loss 16.125205438249846
Epoch 15: loss 16.12493007970025
Epoch 16: loss 16.12465160705915
Epoch 17: loss 16.1243824507634
Epoch 18: loss 16.124110633877063
Epoch 19: loss 16.123825886899066
-----------Time: 0:03:50.408640, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 100, rmse: 4.016510963439941-------------


Epoch 0: loss 16.129034472004495
Epoch 1: loss 16.12875362477139
Epoch 2: loss 16.128489720944543
Epoch 3: loss 16.12821115029124
Epoch 4: loss 16.127937965123404
Epoch 5: loss 16.127665885574732
Epoch 6: loss 16.127388953903306
Epoch 7: loss 16.1271097389105
Epoch 8: loss 16.126835100413537
Epoch 9: loss 16.12655874618439
Epoch 10: loss 16.126288561538356
Epoch 11: loss 16.126018585103434
Epoch 12: loss 16.125741639170975
Epoch 13: loss 16.12546062028687
Epoch 14: loss 16.12519311541899
Epoch 15: loss 16.12491817355091
Epoch 16: loss 16.124642432546278
Epoch 17: loss 16.124363340976252
Epoch 18: loss 16.12409617655818
Epoch 19: loss 16.123821006254246
-----------Time: 0:04:38.539296, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 150, rmse: 4.016509056091309-------------


Epoch 0: loss 16.12904285878964
Epoch 1: loss 16.12875660273483
Epoch 2: loss 16.12849017392688
Epoch 3: loss 16.128209878466194
Epoch 4: loss 16.12793065517606
Epoch 5: loss 16.127667569414047
Epoch 6: loss 16.127390692971723
Epoch 7: loss 16.12711582889111
Epoch 8: loss 16.126842362651235
Epoch 9: loss 16.126566506002572
Epoch 10: loss 16.12628590535573
Epoch 11: loss 16.12602075381795
Epoch 12: loss 16.12574638628065
Epoch 13: loss 16.125463421932054
Epoch 14: loss 16.125196151463737
Epoch 15: loss 16.124922592916068
Epoch 16: loss 16.124646701522337
Epoch 17: loss 16.12437425403896
Epoch 18: loss 16.12410514363781
Epoch 19: loss 16.12382171360159
-----------Time: 0:05:08.658372, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 200, rmse: 4.01650857925415-------------


Epoch 0: loss 16.128985273766737
Epoch 1: loss 16.12863618955462
Epoch 2: loss 16.12827314145543
Epoch 3: loss 16.127907970795622
Epoch 4: loss 16.127544458823856
Epoch 5: loss 16.12718208929065
Epoch 6: loss 16.12681889028378
Epoch 7: loss 16.126460232249805
Epoch 8: loss 16.12609911820628
Epoch 9: loss 16.125736137522892
Epoch 10: loss 16.125367703419656
Epoch 11: loss 16.12500756534952
Epoch 12: loss 16.12464531742405
Epoch 13: loss 16.124284780822805
Epoch 14: loss 16.123922987694712
Epoch 15: loss 16.123554081162272
Epoch 16: loss 16.12320113765474
Epoch 17: loss 16.122832249272705
Epoch 18: loss 16.12247246072758
Epoch 19: loss 16.12210309110043
-----------Time: 0:03:43.517446, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 20, rmse: 4.016263484954834-------------


Epoch 0: loss 16.129015563686885
Epoch 1: loss 16.128651504350653
Epoch 2: loss 16.128293781062712
Epoch 3: loss 16.12792641316632
Epoch 4: loss 16.127561108712076
Epoch 5: loss 16.127204908761982
Epoch 6: loss 16.12684125029049
Epoch 7: loss 16.126475180926175
Epoch 8: loss 16.126117556428312
Epoch 9: loss 16.1257419329483
Epoch 10: loss 16.125390962649455
Epoch 11: loss 16.125017668644716
Epoch 12: loss 16.124653712765816
Epoch 13: loss 16.12430003675966
Epoch 14: loss 16.123936717960095
Epoch 15: loss 16.123566295090676
Epoch 16: loss 16.12321033602243
Epoch 17: loss 16.12285031074412
Epoch 18: loss 16.122484768519524
Epoch 19: loss 16.12212048385894
-----------Time: 0:04:13.414391, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 50, rmse: 4.016274452209473-------------


Epoch 0: loss 16.128992415433867
Epoch 1: loss 16.12861972894923
Epoch 2: loss 16.128257111792593
Epoch 3: loss 16.12790296620944
Epoch 4: loss 16.127531184393014
Epoch 5: loss 16.127176946761363
Epoch 6: loss 16.12681163115758
Epoch 7: loss 16.12645198625997
Epoch 8: loss 16.12609141543224
Epoch 9: loss 16.12572814615736
Epoch 10: loss 16.125358778604546
Epoch 11: loss 16.12499895038781
Epoch 12: loss 16.124634744292564
Epoch 13: loss 16.12427391132108
Epoch 14: loss 16.123908250600245
Epoch 15: loss 16.123553312881413
Epoch 16: loss 16.123184140834425
Epoch 17: loss 16.122821810194953
Epoch 18: loss 16.12246370704489
Epoch 19: loss 16.12209751451709
-----------Time: 0:04:45.329103, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 100, rmse: 4.016292095184326-------------


Epoch 0: loss 16.12899006754889
Epoch 1: loss 16.1286253934324
Epoch 2: loss 16.12826726331602
Epoch 3: loss 16.127905151000032
Epoch 4: loss 16.12754252917615
Epoch 5: loss 16.127182210898386
Epoch 6: loss 16.126814638939546
Epoch 7: loss 16.12645049092561
Epoch 8: loss 16.12609881198296
Epoch 9: loss 16.125732380647644
Epoch 10: loss 16.125370866517258
Epoch 11: loss 16.125008951262846
Epoch 12: loss 16.12463784230799
Epoch 13: loss 16.12427578185031
Epoch 14: loss 16.123908036425426
Epoch 15: loss 16.123555281163558
Epoch 16: loss 16.12319244205336
Epoch 17: loss 16.122830670446465
Epoch 18: loss 16.122465520529982
Epoch 19: loss 16.12210366750554
-----------Time: 0:05:34.393943, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 150, rmse: 4.016292572021484-------------


Epoch 0: loss 16.12900098501955
Epoch 1: loss 16.128632155237412
Epoch 2: loss 16.128272381731193
Epoch 3: loss 16.127906959299292
Epoch 4: loss 16.127546690546215
Epoch 5: loss 16.127188095001504
Epoch 6: loss 16.126826669808118
Epoch 7: loss 16.126472011605575
Epoch 8: loss 16.126100126591112
Epoch 9: loss 16.125734164314206
Epoch 10: loss 16.125375421232604
Epoch 11: loss 16.12501062965704
Epoch 12: loss 16.124658584854018
Epoch 13: loss 16.124288332339148
Epoch 14: loss 16.123926737309798
Epoch 15: loss 16.123558640804156
Epoch 16: loss 16.123194375071854
Epoch 17: loss 16.12283799932209
Epoch 18: loss 16.122475534628887
Epoch 19: loss 16.122111802259305
-----------Time: 0:06:05.342461, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 200, rmse: 4.016290187835693-------------


Epoch 0: loss 16.129028250563074
Epoch 1: loss 16.12854689654609
Epoch 2: loss 16.128070005195955
Epoch 3: loss 16.12758646768484
Epoch 4: loss 16.127108609436515
Epoch 5: loss 16.12662934764304
Epoch 6: loss 16.126152185333236
Epoch 7: loss 16.125674438061697
Epoch 8: loss 16.12519067963418
Epoch 9: loss 16.124714894681084
Epoch 10: loss 16.12423542234287
Epoch 11: loss 16.123753749915863
Epoch 12: loss 16.123275807138498
Epoch 13: loss 16.122796873867415
Epoch 14: loss 16.122310283457605
Epoch 15: loss 16.121831989080896
Epoch 16: loss 16.121361756597025
Epoch 17: loss 16.120886480373944
Epoch 18: loss 16.120403359544348
Epoch 19: loss 16.1199245940349
-----------Time: 0:02:56.577328, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 20, rmse: 4.016040325164795-------------


Epoch 0: loss 16.12893235702888
Epoch 1: loss 16.12845078105833
Epoch 2: loss 16.12796838806011
Epoch 3: loss 16.127490086682535
Epoch 4: loss 16.12701480942229
Epoch 5: loss 16.12652591079911
Epoch 6: loss 16.12604967493801
Epoch 7: loss 16.125561819963316
Epoch 8: loss 16.125093117558873
Epoch 9: loss 16.124616598032816
Epoch 10: loss 16.12413293502459
Epoch 11: loss 16.123659033824595
Epoch 12: loss 16.12317719882179
Epoch 13: loss 16.12269603642126
Epoch 14: loss 16.12222383850745
Epoch 15: loss 16.12173621093144
Epoch 16: loss 16.12125577895521
Epoch 17: loss 16.120785719418798
Epoch 18: loss 16.12030096790465
Epoch 19: loss 16.119823932388414
-----------Time: 0:02:47.391310, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 50, rmse: 4.016005516052246-------------


Epoch 0: loss 16.128944595849273
Epoch 1: loss 16.128469329479273
Epoch 2: loss 16.127983209683624
Epoch 3: loss 16.127506524988853
Epoch 4: loss 16.12703591601365
Epoch 5: loss 16.12655256700749
Epoch 6: loss 16.126067550756673
Epoch 7: loss 16.125590461048503
Epoch 8: loss 16.12511571896601
Epoch 9: loss 16.124630614037486
Epoch 10: loss 16.124152741009546
Epoch 11: loss 16.12367337757378
Epoch 12: loss 16.12319292096485
Epoch 13: loss 16.122713649059005
Epoch 14: loss 16.122232884671003
Epoch 15: loss 16.121757530660457
Epoch 16: loss 16.121278372842895
Epoch 17: loss 16.12079416091446
Epoch 18: loss 16.12031692574373
Epoch 19: loss 16.11984158936501
-----------Time: 0:03:24.364923, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 100, rmse: 4.016000270843506-------------


Epoch 0: loss 16.12892782979844
Epoch 1: loss 16.128458536727848
Epoch 2: loss 16.127982526709683
Epoch 3: loss 16.12749534459656
Epoch 4: loss 16.1270151680225
Epoch 5: loss 16.12653449931308
Epoch 6: loss 16.126060877629374
Epoch 7: loss 16.125580756025123
Epoch 8: loss 16.12510121860473
Epoch 9: loss 16.124615733814085
Epoch 10: loss 16.124147534694536
Epoch 11: loss 16.123666896322227
Epoch 12: loss 16.12317699475934
Epoch 13: loss 16.122703918884344
Epoch 14: loss 16.122234179054413
Epoch 15: loss 16.121743686047502
Epoch 16: loss 16.121262600656564
Epoch 17: loss 16.12079551519422
Epoch 18: loss 16.1203118703364
Epoch 19: loss 16.119827656074346
-----------Time: 0:04:13.108081, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 150, rmse: 4.016000747680664-------------


Epoch 0: loss 16.128935779936615
Epoch 1: loss 16.128452894803036
Epoch 2: loss 16.127968530929756
Epoch 3: loss 16.127501603375965
Epoch 4: loss 16.12701444330262
Epoch 5: loss 16.12653389827527
Epoch 6: loss 16.12605570217006
Epoch 7: loss 16.125570744519134
Epoch 8: loss 16.125101551016495
Epoch 9: loss 16.12461749621874
Epoch 10: loss 16.124134094057816
Epoch 11: loss 16.12366079130271
Epoch 12: loss 16.123182913866817
Epoch 13: loss 16.12270092640058
Epoch 14: loss 16.122223457867456
Epoch 15: loss 16.121741963573744
Epoch 16: loss 16.121257111713767
Epoch 17: loss 16.12077550722115
Epoch 18: loss 16.12030385900542
Epoch 19: loss 16.119822449759337
-----------Time: 0:04:46.131822, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 200, rmse: 4.016000747680664-------------


Epoch 0: loss 16.12915611448594
Epoch 1: loss 16.12852807716548
Epoch 2: loss 16.127895687377084
Epoch 3: loss 16.127261876671
Epoch 4: loss 16.12662916199029
Epoch 5: loss 16.125993207980265
Epoch 6: loss 16.125365154324438
Epoch 7: loss 16.12472943419539
Epoch 8: loss 16.124095083385008
Epoch 9: loss 16.12346448633838
Epoch 10: loss 16.122822980896295
Epoch 11: loss 16.122190853510947
Epoch 12: loss 16.12156491982282
Epoch 13: loss 16.120927290270807
Epoch 14: loss 16.120293980256704
Epoch 15: loss 16.11965761293595
Epoch 16: loss 16.11902717665008
Epoch 17: loss 16.11840090743869
Epoch 18: loss 16.117759395514316
Epoch 19: loss 16.117128369080884
-----------Time: 0:03:18.548794, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 20, rmse: 4.015646934509277-------------


Epoch 0: loss 16.128895999686495
Epoch 1: loss 16.128270914141027
Epoch 2: loss 16.127626564011358
Epoch 3: loss 16.126995503870024
Epoch 4: loss 16.12636169160819
Epoch 5: loss 16.12572435090696
Epoch 6: loss 16.125090544349543
Epoch 7: loss 16.12446835094201
Epoch 8: loss 16.123831779558806
Epoch 9: loss 16.12319670636217
Epoch 10: loss 16.1225662547781
Epoch 11: loss 16.121929487629778
Epoch 12: loss 16.12129394174465
Epoch 13: loss 16.120662378318418
Epoch 14: loss 16.12003415197442
Epoch 15: loss 16.11939137159579
Epoch 16: loss 16.118761274203973
Epoch 17: loss 16.118131759958853
Epoch 18: loss 16.117499647612412
Epoch 19: loss 16.116862218233475
-----------Time: 0:03:27.550825, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 50, rmse: 4.015615940093994-------------


Epoch 0: loss 16.128864573550782
Epoch 1: loss 16.128228988512628
Epoch 2: loss 16.127595112724368
Epoch 3: loss 16.126959617919674
Epoch 4: loss 16.126332094774355
Epoch 5: loss 16.125691851822822
Epoch 6: loss 16.125063174311528
Epoch 7: loss 16.124426678122873
Epoch 8: loss 16.123792895679568
Epoch 9: loss 16.123160948761136
Epoch 10: loss 16.12253019510241
Epoch 11: loss 16.121885391212533
Epoch 12: loss 16.121262476974497
Epoch 13: loss 16.120624193489196
Epoch 14: loss 16.119993757203325
Epoch 15: loss 16.11935541693318
Epoch 16: loss 16.118722122476566
Epoch 17: loss 16.118091163977518
Epoch 18: loss 16.11746116926516
Epoch 19: loss 16.116822824068468
-----------Time: 0:04:15.938220, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 100, rmse: 4.015622615814209-------------


Epoch 0: loss 16.128854683393236
Epoch 1: loss 16.128216334566467
Epoch 2: loss 16.127591489384265
Epoch 3: loss 16.1269439790092
Epoch 4: loss 16.12631561186989
Epoch 5: loss 16.125676882403127
Epoch 6: loss 16.125041486907097
Epoch 7: loss 16.124417491164007
Epoch 8: loss 16.123784324538132
Epoch 9: loss 16.123139528427
Epoch 10: loss 16.12250773241625
Epoch 11: loss 16.121876653865215
Epoch 12: loss 16.121245448779906
Epoch 13: loss 16.120610863829256
Epoch 14: loss 16.119979785278222
Epoch 15: loss 16.119344906290955
Epoch 16: loss 16.118716726100853
Epoch 17: loss 16.1180851204101
Epoch 18: loss 16.117445122229785
Epoch 19: loss 16.11680872431263
-----------Time: 0:05:03.641993, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 150, rmse: 4.01561975479126-------------


Epoch 0: loss 16.128861424714213
Epoch 1: loss 16.128221593517658
Epoch 2: loss 16.127601281788895
Epoch 3: loss 16.12695696225563
Epoch 4: loss 16.126326772556023
Epoch 5: loss 16.125691241450514
Epoch 6: loss 16.125056475773658
Epoch 7: loss 16.124424896789932
Epoch 8: loss 16.12378673880174
Epoch 9: loss 16.123161107447565
Epoch 10: loss 16.12253072872442
Epoch 11: loss 16.121894191567698
Epoch 12: loss 16.121252493990575
Epoch 13: loss 16.120621996771195
Epoch 14: loss 16.119985276036058
Epoch 15: loss 16.11935211433672
Epoch 16: loss 16.118722056875804
Epoch 17: loss 16.11808892603216
Epoch 18: loss 16.11745781947764
Epoch 19: loss 16.11683078146681
-----------Time: 0:05:33.443279, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 200, rmse: 4.015618801116943-------------


Epoch 0: loss 16.12889502475027
Epoch 1: loss 16.1280579401009
Epoch 2: loss 16.12722324223023
Epoch 3: loss 16.12638270744751
Epoch 4: loss 16.125548861349582
Epoch 5: loss 16.124712342215076
Epoch 6: loss 16.123868512096063
Epoch 7: loss 16.123025656394688
Epoch 8: loss 16.12220231238236
Epoch 9: loss 16.12135997630125
Epoch 10: loss 16.120529685090457
Epoch 11: loss 16.119680416071915
Epoch 12: loss 16.118849111549746
Epoch 13: loss 16.11801072914617
Epoch 14: loss 16.11717186964615
Epoch 15: loss 16.11633677142933
Epoch 16: loss 16.115496054883234
Epoch 17: loss 16.114662830825733
Epoch 18: loss 16.113823421886917
Epoch 19: loss 16.112985107158433
-----------Time: 0:04:11.644842, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 20, rmse: 4.015137195587158-------------


Epoch 0: loss 16.128740250772598
Epoch 1: loss 16.12789987234268
Epoch 2: loss 16.12706361820414
Epoch 3: loss 16.126225815317177
Epoch 4: loss 16.125396489189527
Epoch 5: loss 16.124554036945803
Epoch 6: loss 16.123714599225625
Epoch 7: loss 16.12287778579521
Epoch 8: loss 16.122040139779635
Epoch 9: loss 16.121203109840778
Epoch 10: loss 16.120362808420452
Epoch 11: loss 16.1195225414859
Epoch 12: loss 16.118689240418806
Epoch 13: loss 16.117862449903214
Epoch 14: loss 16.117015570515587
Epoch 15: loss 16.116177883791238
Epoch 16: loss 16.115337053934734
Epoch 17: loss 16.114504009566275
Epoch 18: loss 16.113660567606708
Epoch 19: loss 16.112823893156566
-----------Time: 0:04:15.335361, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 50, rmse: 4.015116214752197-------------


Epoch 0: loss 16.128765724870807
Epoch 1: loss 16.127928348777733
Epoch 2: loss 16.1270943831464
Epoch 3: loss 16.126255774640597
Epoch 4: loss 16.12542910936282
Epoch 5: loss 16.12458201683754
Epoch 6: loss 16.123744592516235
Epoch 7: loss 16.122912634060782
Epoch 8: loss 16.122073656323813
Epoch 9: loss 16.121234352657368
Epoch 10: loss 16.120401185903294
Epoch 11: loss 16.11955669822091
Epoch 12: loss 16.118722848752196
Epoch 13: loss 16.117887090379092
Epoch 14: loss 16.117045602959223
Epoch 15: loss 16.11621699117979
Epoch 16: loss 16.115374229341956
Epoch 17: loss 16.114541402778396
Epoch 18: loss 16.113700882775294
Epoch 19: loss 16.112858703824863
-----------Time: 0:04:35.613618, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 100, rmse: 4.015113353729248-------------


Epoch 0: loss 16.128753283543713
Epoch 1: loss 16.12792118766374
Epoch 2: loss 16.127083393074106
Epoch 3: loss 16.126250060892026
Epoch 4: loss 16.125405376666652
Epoch 5: loss 16.12457472944856
Epoch 6: loss 16.123728058012755
Epoch 7: loss 16.122896247872067
Epoch 8: loss 16.122053220001103
Epoch 9: loss 16.12122226085528
Epoch 10: loss 16.12037115034816
Epoch 11: loss 16.119546687492797
Epoch 12: loss 16.118711599906934
Epoch 13: loss 16.117864345324435
Epoch 14: loss 16.117030338465746
Epoch 15: loss 16.11619491457874
Epoch 16: loss 16.115358112297866
Epoch 17: loss 16.11451805072222
Epoch 18: loss 16.11368061965934
Epoch 19: loss 16.112840460849366
-----------Time: 0:03:49.835356, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 150, rmse: 4.015111446380615-------------


Epoch 0: loss 16.12876189150455
Epoch 1: loss 16.127927746184174
Epoch 2: loss 16.1270924962818
Epoch 3: loss 16.1262482668538
Epoch 4: loss 16.12540848868382
Epoch 5: loss 16.124570953904318
Epoch 6: loss 16.123732737188043
Epoch 7: loss 16.122903958424853
Epoch 8: loss 16.12206052683695
Epoch 9: loss 16.12121832480957
Epoch 10: loss 16.12038534733833
Epoch 11: loss 16.119541934938
Epoch 12: loss 16.118706609841077
Epoch 13: loss 16.117866524929195
Epoch 14: loss 16.117041270975317
Epoch 15: loss 16.116203894882243
Epoch 16: loss 16.115356166574085
Epoch 17: loss 16.114523569224257
Epoch 18: loss 16.113688027359597
Epoch 19: loss 16.112852117819525
-----------Time: 0:04:16.551172, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 200, rmse: 4.015113830566406-------------


Epoch 0: loss 16.128759412677343
Epoch 1: loss 16.12763310671501
Epoch 2: loss 16.12653862749383
Epoch 3: loss 16.125429355430498
Epoch 4: loss 16.12432262935089
Epoch 5: loss 16.12320866333265
Epoch 6: loss 16.122111029311196
Epoch 7: loss 16.12100084687524
Epoch 8: loss 16.11989541232683
Epoch 9: loss 16.118787980974208
Epoch 10: loss 16.117674938811756
Epoch 11: loss 16.116575437891154
Epoch 12: loss 16.115467016822688
Epoch 13: loss 16.114368084009865
Epoch 14: loss 16.113249987217962
Epoch 15: loss 16.11214794809239
Epoch 16: loss 16.111037027453392
Epoch 17: loss 16.10992870050776
Epoch 18: loss 16.108829464842405
Epoch 19: loss 16.107716042039964
-----------Time: 0:02:40.654278, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 20, rmse: 4.014448642730713-------------


Epoch 0: loss 16.12864650417243
Epoch 1: loss 16.12753393262414
Epoch 2: loss 16.126430441465892
Epoch 3: loss 16.125328007698577
Epoch 4: loss 16.124219761392617
Epoch 5: loss 16.123109636000798
Epoch 6: loss 16.122004061175662
Epoch 7: loss 16.120893489024507
Epoch 8: loss 16.119787600197306
Epoch 9: loss 16.11868025026223
Epoch 10: loss 16.117570125388994
Epoch 11: loss 16.116463288851183
Epoch 12: loss 16.115359804693806
Epoch 13: loss 16.114250426839526
Epoch 14: loss 16.113147391256526
Epoch 15: loss 16.11203780985838
Epoch 16: loss 16.110930812300516
Epoch 17: loss 16.109822138163498
Epoch 18: loss 16.108719950723156
Epoch 19: loss 16.107614544437528
-----------Time: 0:03:07.235478, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 50, rmse: 4.014431953430176-------------


Epoch 0: loss 16.12862267942776
Epoch 1: loss 16.127503697933097
Epoch 2: loss 16.1264012605357
Epoch 3: loss 16.125296024086037
Epoch 4: loss 16.12418515219385
Epoch 5: loss 16.123085307711943
Epoch 6: loss 16.121974946364816
Epoch 7: loss 16.120869147511783
Epoch 8: loss 16.119755824277295
Epoch 9: loss 16.11864952576945
Epoch 10: loss 16.117540832444856
Epoch 11: loss 16.11643761509848
Epoch 12: loss 16.11532849653575
Epoch 13: loss 16.114224951444857
Epoch 14: loss 16.113119116291006
Epoch 15: loss 16.11201809514405
Epoch 16: loss 16.110903786083096
Epoch 17: loss 16.109799059141324
Epoch 18: loss 16.108692110589562
Epoch 19: loss 16.10758650231582
-----------Time: 0:03:48.960150, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 100, rmse: 4.014443874359131-------------


Epoch 0: loss 16.1286301828066
Epoch 1: loss 16.127518044015908
Epoch 2: loss 16.126415437819713
Epoch 3: loss 16.12530486722431
Epoch 4: loss 16.124198761370078
Epoch 5: loss 16.123090768651252
Epoch 6: loss 16.121986688641893
Epoch 7: loss 16.12087862928514
Epoch 8: loss 16.119773992058246
Epoch 9: loss 16.118664715327668
Epoch 10: loss 16.11755847438254
Epoch 11: loss 16.1164626533276
Epoch 12: loss 16.115347903211717
Epoch 13: loss 16.11423680029076
Epoch 14: loss 16.11313343618537
Epoch 15: loss 16.112028468621045
Epoch 16: loss 16.11092708628096
Epoch 17: loss 16.10981756114908
Epoch 18: loss 16.108707401012193
Epoch 19: loss 16.107603294814393
-----------Time: 0:04:32.992387, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 150, rmse: 4.014445781707764-------------


Epoch 0: loss 16.12863266915326
Epoch 1: loss 16.127530452931556
Epoch 2: loss 16.126418578877534
Epoch 3: loss 16.125312530067447
Epoch 4: loss 16.124200026194256
Epoch 5: loss 16.123094262604873
Epoch 6: loss 16.121991044999206
Epoch 7: loss 16.12087984147313
Epoch 8: loss 16.119776366390955
Epoch 9: loss 16.11867003521237
Epoch 10: loss 16.117565007233114
Epoch 11: loss 16.116456145109723
Epoch 12: loss 16.11534656241512
Epoch 13: loss 16.11423684877828
Epoch 14: loss 16.113137582257234
Epoch 15: loss 16.112025562222072
Epoch 16: loss 16.110926225173728
Epoch 17: loss 16.109811100381556
Epoch 18: loss 16.108708124435616
Epoch 19: loss 16.107601287638513
-----------Time: 0:05:05.112538, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 200, rmse: 4.014444828033447-------------


Epoch 0: loss 16.128615881321934
Epoch 1: loss 16.12715168813443
Epoch 2: loss 16.125692562540955
Epoch 3: loss 16.124228483182442
Epoch 4: loss 16.122765833038944
Epoch 5: loss 16.121300985658863
Epoch 6: loss 16.119840014687433
Epoch 7: loss 16.11837508640839
Epoch 8: loss 16.116905244035912
Epoch 9: loss 16.11544477142284
Epoch 10: loss 16.113983132516385
Epoch 11: loss 16.11252326534908
Epoch 12: loss 16.11105920310381
Epoch 13: loss 16.109599438097373
Epoch 14: loss 16.10813404464929
Epoch 15: loss 16.10667351343633
Epoch 16: loss 16.10520481065021
Epoch 17: loss 16.103745874598857
Epoch 18: loss 16.102289669146806
Epoch 19: loss 16.100819646825993
-----------Time: 0:03:41.837936, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 20, rmse: 4.013537406921387-------------


Epoch 0: loss 16.128495470216055
Epoch 1: loss 16.12703951394318
Epoch 2: loss 16.12557964236792
Epoch 3: loss 16.124112775365965
Epoch 4: loss 16.12264799436452
Epoch 5: loss 16.12117901828511
Epoch 6: loss 16.119720473504703
Epoch 7: loss 16.11825236138473
Epoch 8: loss 16.116790635097022
Epoch 9: loss 16.115324111137785
Epoch 10: loss 16.113861229446933
Epoch 11: loss 16.112396653804396
Epoch 12: loss 16.11093581870174
Epoch 13: loss 16.10948076761566
Epoch 14: loss 16.108016277020752
Epoch 15: loss 16.106553121258735
Epoch 16: loss 16.105091977599134
Epoch 17: loss 16.1036333477711
Epoch 18: loss 16.102161398136207
Epoch 19: loss 16.100699311173955
-----------Time: 0:03:40.971180, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 50, rmse: 4.013556957244873-------------


Epoch 0: loss 16.128453648822862
Epoch 1: loss 16.126999043458955
Epoch 2: loss 16.125526567202925
Epoch 3: loss 16.12405704661126
Epoch 4: loss 16.122602951792416
Epoch 5: loss 16.12113858773178
Epoch 6: loss 16.11967578590273
Epoch 7: loss 16.118208316825797
Epoch 8: loss 16.116743747146963
Epoch 9: loss 16.115286673586805
Epoch 10: loss 16.113816828880704
Epoch 11: loss 16.112358541836098
Epoch 12: loss 16.110893814767294
Epoch 13: loss 16.109435237056864
Epoch 14: loss 16.107967845508103
Epoch 15: loss 16.106505997094075
Epoch 16: loss 16.105046714369923
Epoch 17: loss 16.103588021274753
Epoch 18: loss 16.102121715120415
Epoch 19: loss 16.100658513982378
-----------Time: 0:04:37.163026, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 100, rmse: 4.013560771942139-------------


Epoch 0: loss 16.128455342255965
Epoch 1: loss 16.12699157093581
Epoch 2: loss 16.125531715695917
Epoch 3: loss 16.124063084214974
Epoch 4: loss 16.1226035904275
Epoch 5: loss 16.121136171134545
Epoch 6: loss 16.11967449385294
Epoch 7: loss 16.1182071692014
Epoch 8: loss 16.11675140921223
Epoch 9: loss 16.115283125959834
Epoch 10: loss 16.11381579690034
Epoch 11: loss 16.112357114436122
Epoch 12: loss 16.110892447263666
Epoch 13: loss 16.10943164872112
Epoch 14: loss 16.107968165473874
Epoch 15: loss 16.106509241349933
Epoch 16: loss 16.105048554821337
Epoch 17: loss 16.103588137696658
Epoch 18: loss 16.102120970435088
Epoch 19: loss 16.100655970850866
-----------Time: 0:05:31.804729, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 150, rmse: 4.0135602951049805-------------


Epoch 0: loss 16.12845402712923
Epoch 1: loss 16.126989868686795
Epoch 2: loss 16.12552526581864
Epoch 3: loss 16.124069073331167
Epoch 4: loss 16.12259612023798
Epoch 5: loss 16.12113752956297
Epoch 6: loss 16.11967446844237
Epoch 7: loss 16.11821017413116
Epoch 8: loss 16.116739207470527
Epoch 9: loss 16.115284426566244
Epoch 10: loss 16.113822793882782
Epoch 11: loss 16.112356224806817
Epoch 12: loss 16.110893757463863
Epoch 13: loss 16.10943050576397
Epoch 14: loss 16.10797301785592
Epoch 15: loss 16.106503274792104
Epoch 16: loss 16.10504659820731
Epoch 17: loss 16.10358155687715
Epoch 18: loss 16.102120845975143
Epoch 19: loss 16.100655491680087
-----------Time: 0:05:59.196717, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 200, rmse: 4.0135602951049805-------------


Epoch 0: loss 16.12819167101364
Epoch 1: loss 16.126262687093995
Epoch 2: loss 16.124326995820567
Epoch 3: loss 16.122396759263452
Epoch 4: loss 16.1204603539011
Epoch 5: loss 16.118523311718707
Epoch 6: loss 16.116596139469117
Epoch 7: loss 16.114660098929974
Epoch 8: loss 16.1127271089559
Epoch 9: loss 16.11079503817037
Epoch 10: loss 16.10886027386423
Epoch 11: loss 16.10692916064239
Epoch 12: loss 16.104995921748433
Epoch 13: loss 16.103060679567967
Epoch 14: loss 16.1011255172493
Epoch 15: loss 16.099194789853286
Epoch 16: loss 16.097271486492893
Epoch 17: loss 16.09533449798385
Epoch 18: loss 16.093402143274073
Epoch 19: loss 16.091475663851497
-----------Time: 0:03:07.305276, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.0123982429504395-------------


Epoch 0: loss 16.128228477448985
Epoch 1: loss 16.12628386265691
Epoch 2: loss 16.124357031116414
Epoch 3: loss 16.12242412100414
Epoch 4: loss 16.120489424113803
Epoch 5: loss 16.11855304079123
Epoch 6: loss 16.116618217366618
Epoch 7: loss 16.114686027047686
Epoch 8: loss 16.112748833439024
Epoch 9: loss 16.110819618490613
Epoch 10: loss 16.108880135337575
Epoch 11: loss 16.106944826259895
Epoch 12: loss 16.105014146054938
Epoch 13: loss 16.10308600327708
Epoch 14: loss 16.101146695923713
Epoch 15: loss 16.09921865764035
Epoch 16: loss 16.097277781054533
Epoch 17: loss 16.09535027042947
Epoch 18: loss 16.093414569043674
Epoch 19: loss 16.09149093456797
-----------Time: 0:02:44.450595, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012386322021484-------------


Epoch 0: loss 16.128200498335126
Epoch 1: loss 16.12627524743591
Epoch 2: loss 16.12433823118267
Epoch 3: loss 16.122410667143548
Epoch 4: loss 16.120469515449397
Epoch 5: loss 16.118541353483963
Epoch 6: loss 16.116606772237656
Epoch 7: loss 16.11467359868517
Epoch 8: loss 16.11273937344616
Epoch 9: loss 16.110801414149556
Epoch 10: loss 16.108866142150564
Epoch 11: loss 16.10693244845923
Epoch 12: loss 16.104996237046954
Epoch 13: loss 16.10306773981755
Epoch 14: loss 16.10113636730416
Epoch 15: loss 16.099204428498027
Epoch 16: loss 16.097264987868808
Epoch 17: loss 16.095332637307695
Epoch 18: loss 16.093405374306062
Epoch 19: loss 16.091476514846363
-----------Time: 0:03:23.629166, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012393951416016-------------


Epoch 0: loss 16.12820409211599
Epoch 1: loss 16.1262739652392
Epoch 2: loss 16.124339650026023
Epoch 3: loss 16.122399625990816
Epoch 4: loss 16.120471862556496
Epoch 5: loss 16.118543249164475
Epoch 6: loss 16.11659649677287
Epoch 7: loss 16.11467176212313
Epoch 8: loss 16.112727276976827
Epoch 9: loss 16.110802341635427
Epoch 10: loss 16.10887009919889
Epoch 11: loss 16.106932797206362
Epoch 12: loss 16.105008617440536
Epoch 13: loss 16.10307300862182
Epoch 14: loss 16.10112873635388
Epoch 15: loss 16.099202989948516
Epoch 16: loss 16.09727157594848
Epoch 17: loss 16.095336618729426
Epoch 18: loss 16.093402581217497
Epoch 19: loss 16.091467708527492
-----------Time: 0:04:23.125419, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012393474578857-------------


Epoch 0: loss 16.128217691957722
Epoch 1: loss 16.126289406828803
Epoch 2: loss 16.124355026792742
Epoch 3: loss 16.122414964123095
Epoch 4: loss 16.12049227683942
Epoch 5: loss 16.118545320126074
Epoch 6: loss 16.116608966362744
Epoch 7: loss 16.114676652102446
Epoch 8: loss 16.112742885550187
Epoch 9: loss 16.110810352707116
Epoch 10: loss 16.10888006584744
Epoch 11: loss 16.106945474748056
Epoch 12: loss 16.105003560996042
Epoch 13: loss 16.10307785011362
Epoch 14: loss 16.101139060046894
Epoch 15: loss 16.099211164633175
Epoch 16: loss 16.097273184074663
Epoch 17: loss 16.095336302393736
Epoch 18: loss 16.093408195916695
Epoch 19: loss 16.091475012251838
-----------Time: 0:04:44.545071, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012392520904541-------------


Epoch 0: loss 16.128100239886997
Epoch 1: loss 16.125542905681996
Epoch 2: loss 16.122981686771013
Epoch 3: loss 16.120426279880093
Epoch 4: loss 16.11786032682401
Epoch 5: loss 16.11531497603723
Epoch 6: loss 16.112754012009837
Epoch 7: loss 16.110197610217245
Epoch 8: loss 16.10764476201533
Epoch 9: loss 16.105081777069607
Epoch 10: loss 16.102530115385818
Epoch 11: loss 16.099976045661418
Epoch 12: loss 16.09741573401156
Epoch 13: loss 16.094854520286408
Epoch 14: loss 16.092310742102867
Epoch 15: loss 16.089755580501752
Epoch 16: loss 16.087201752437075
Epoch 17: loss 16.08464295971711
Epoch 18: loss 16.082091005811748
Epoch 19: loss 16.079541902816963
-----------Time: 0:03:16.644540, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 20, rmse: 4.01085090637207-------------


Epoch 0: loss 16.127904446504893
Epoch 1: loss 16.12535181792292
Epoch 2: loss 16.12279238293779
Epoch 3: loss 16.12023145650767
Epoch 4: loss 16.117674445898523
Epoch 5: loss 16.115118441859167
Epoch 6: loss 16.11256311223713
Epoch 7: loss 16.110008780887554
Epoch 8: loss 16.107455206410012
Epoch 9: loss 16.104894062174992
Epoch 10: loss 16.102338080694004
Epoch 11: loss 16.099782186594265
Epoch 12: loss 16.097229846603785
Epoch 13: loss 16.094672935821883
Epoch 14: loss 16.092120313462907
Epoch 15: loss 16.0895638476253
Epoch 16: loss 16.087004074783284
Epoch 17: loss 16.084457504548347
Epoch 18: loss 16.081904206216304
Epoch 19: loss 16.079343388947926
-----------Time: 0:03:30.623632, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 50, rmse: 4.010859966278076-------------


Epoch 0: loss 16.127903280730088
Epoch 1: loss 16.125346488444425
Epoch 2: loss 16.12278954032454
Epoch 3: loss 16.120222737829344
Epoch 4: loss 16.117669529212176
Epoch 5: loss 16.11511581886586
Epoch 6: loss 16.112559973772225
Epoch 7: loss 16.10999507655133
Epoch 8: loss 16.107442724892728
Epoch 9: loss 16.10488281488548
Epoch 10: loss 16.102334638857982
Epoch 11: loss 16.099769611991313
Epoch 12: loss 16.097219126972576
Epoch 13: loss 16.09466635941033
Epoch 14: loss 16.09210347921839
Epoch 15: loss 16.08955025322869
Epoch 16: loss 16.0869992358844
Epoch 17: loss 16.08444582942778
Epoch 18: loss 16.081893031528423
Epoch 19: loss 16.079331491355212
-----------Time: 0:04:16.814743, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 100, rmse: 4.010848522186279-------------


Epoch 0: loss 16.127898694640464
Epoch 1: loss 16.125343657240006
Epoch 2: loss 16.12279030523461
Epoch 3: loss 16.120232198081503
Epoch 4: loss 16.117673622907148
Epoch 5: loss 16.115120234082355
Epoch 6: loss 16.11256640498051
Epoch 7: loss 16.110006703702958
Epoch 8: loss 16.10745032913598
Epoch 9: loss 16.10489421697205
Epoch 10: loss 16.10234482019994
Epoch 11: loss 16.0997831151173
Epoch 12: loss 16.097229556975122
Epoch 13: loss 16.094676496932014
Epoch 14: loss 16.09211690403833
Epoch 15: loss 16.08955933724881
Epoch 16: loss 16.087008498297106
Epoch 17: loss 16.08445454188311
Epoch 18: loss 16.081898192467413
Epoch 19: loss 16.079348849368653
-----------Time: 0:05:01.614135, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 150, rmse: 4.010851860046387-------------


Epoch 0: loss 16.12790868514384
Epoch 1: loss 16.125353104268292
Epoch 2: loss 16.122796225897833
Epoch 3: loss 16.120233426345568
Epoch 4: loss 16.117677181942945
Epoch 5: loss 16.115115431224996
Epoch 6: loss 16.112567498931554
Epoch 7: loss 16.11000219903088
Epoch 8: loss 16.1074516017389
Epoch 9: loss 16.104897173414287
Epoch 10: loss 16.10234455805618
Epoch 11: loss 16.099778696011434
Epoch 12: loss 16.097229735627
Epoch 13: loss 16.09467085137712
Epoch 14: loss 16.092112897206025
Epoch 15: loss 16.08955968858886
Epoch 16: loss 16.087004183685735
Epoch 17: loss 16.084456343700083
Epoch 18: loss 16.08189789272638
Epoch 19: loss 16.07934489906191
-----------Time: 0:05:33.387403, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 200, rmse: 4.010848522186279-------------


Epoch 0: loss 16.127678916012222
Epoch 1: loss 16.124297780663895
Epoch 2: loss 16.120916326386958
Epoch 3: loss 16.117540251631787
Epoch 4: loss 16.11415398360725
Epoch 5: loss 16.110781612313268
Epoch 6: loss 16.107405975760813
Epoch 7: loss 16.10401965302576
Epoch 8: loss 16.100638011799617
Epoch 9: loss 16.09725885951305
Epoch 10: loss 16.09389157759359
Epoch 11: loss 16.090513138099492
Epoch 12: loss 16.087137557554012
Epoch 13: loss 16.083754458590782
Epoch 14: loss 16.080381354798178
Epoch 15: loss 16.07700502645587
Epoch 16: loss 16.073639404520904
Epoch 17: loss 16.070256665713636
Epoch 18: loss 16.06688524135239
Epoch 19: loss 16.063509058472643
-----------Time: 0:04:12.593318, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 20, rmse: 4.008762836456299-------------


Epoch 0: loss 16.127501784102172
Epoch 1: loss 16.124129496818654
Epoch 2: loss 16.120746232168127
Epoch 3: loss 16.117361019978773
Epoch 4: loss 16.11398710953016
Epoch 5: loss 16.110603398638876
Epoch 6: loss 16.107223896827303
Epoch 7: loss 16.10384705145765
Epoch 8: loss 16.10046902553357
Epoch 9: loss 16.097094776968778
Epoch 10: loss 16.093713773859136
Epoch 11: loss 16.090332022693378
Epoch 12: loss 16.086957678709293
Epoch 13: loss 16.083578783899238
Epoch 14: loss 16.080210655392868
Epoch 15: loss 16.07682838823693
Epoch 16: loss 16.07345122730946
Epoch 17: loss 16.0700756553206
Epoch 18: loss 16.066709510653872
Epoch 19: loss 16.063330999595305
-----------Time: 0:04:16.049657, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 50, rmse: 4.0088043212890625-------------


Epoch 0: loss 16.12749962005491
Epoch 1: loss 16.12412868445823
Epoch 2: loss 16.120739830000495
Epoch 3: loss 16.117369253263323
Epoch 4: loss 16.113981083335275
Epoch 5: loss 16.11061148723874
Epoch 6: loss 16.10722406692256
Epoch 7: loss 16.10385205734029
Epoch 8: loss 16.10046653063029
Epoch 9: loss 16.097096236002322
Epoch 10: loss 16.09371106400316
Epoch 11: loss 16.090341390897038
Epoch 12: loss 16.086959604467626
Epoch 13: loss 16.08357891665651
Epoch 14: loss 16.080206945967973
Epoch 15: loss 16.076833915814166
Epoch 16: loss 16.073454514348423
Epoch 17: loss 16.070081954030904
Epoch 18: loss 16.066703630440127
Epoch 19: loss 16.063328284034917
-----------Time: 0:04:35.554518, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 100, rmse: 4.008803367614746-------------


Epoch 0: loss 16.127506081600302
Epoch 1: loss 16.1241101204798
Epoch 2: loss 16.120737578312685
Epoch 3: loss 16.117356953768706
Epoch 4: loss 16.11397048297818
Epoch 5: loss 16.110594526719243
Epoch 6: loss 16.107212699581062
Epoch 7: loss 16.103836542371177
Epoch 8: loss 16.100465157940828
Epoch 9: loss 16.097080249641174
Epoch 10: loss 16.093704838413075
Epoch 11: loss 16.09032116589694
Epoch 12: loss 16.08694407912685
Epoch 13: loss 16.08356532563069
Epoch 14: loss 16.080195678972302
Epoch 15: loss 16.076816077592778
Epoch 16: loss 16.07343606290252
Epoch 17: loss 16.07006126956618
Epoch 18: loss 16.06668839887668
Epoch 19: loss 16.06331094857984
-----------Time: 0:03:48.435544, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 150, rmse: 4.008808612823486-------------


Epoch 0: loss 16.127496375021174
Epoch 1: loss 16.12412128894468
Epoch 2: loss 16.120736633972864
Epoch 3: loss 16.117355090499636
Epoch 4: loss 16.11397666915595
Epoch 5: loss 16.110599686621065
Epoch 6: loss 16.10721151072931
Epoch 7: loss 16.103839428804697
Epoch 8: loss 16.100455791292923
Epoch 9: loss 16.097073118086417
Epoch 10: loss 16.09369628905213
Epoch 11: loss 16.09032360505254
Epoch 12: loss 16.086945090104603
Epoch 13: loss 16.08356728250401
Epoch 14: loss 16.080190645345468
Epoch 15: loss 16.076810490378485
Epoch 16: loss 16.073430281478856
Epoch 17: loss 16.07006982411296
Epoch 18: loss 16.066681128082358
Epoch 19: loss 16.063313857053142
-----------Time: 0:04:17.501127, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 200, rmse: 4.0088067054748535-------------


Epoch 0: loss 16.12703273502493
Epoch 1: loss 16.12256243437642
Epoch 2: loss 16.11809885975069
Epoch 3: loss 16.113629037754375
Epoch 4: loss 16.109160144540393
Epoch 5: loss 16.10469428503753
Epoch 6: loss 16.100224732704433
Epoch 7: loss 16.095762496541674
Epoch 8: loss 16.091296476796636
Epoch 9: loss 16.086837045390556
Epoch 10: loss 16.082374311128735
Epoch 11: loss 16.077907744278527
Epoch 12: loss 16.073447219699283
Epoch 13: loss 16.068979767627727
Epoch 14: loss 16.064526465095987
Epoch 15: loss 16.060066883300813
Epoch 16: loss 16.055595549634774
Epoch 17: loss 16.051151035012207
Epoch 18: loss 16.046693069899842
Epoch 19: loss 16.04223795362372
-----------Time: 0:02:42.197132, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 20, rmse: 4.006119728088379-------------


Epoch 0: loss 16.12694968134894
Epoch 1: loss 16.122482410003595
Epoch 2: loss 16.11801554185661
Epoch 3: loss 16.113549543114186
Epoch 4: loss 16.10908644195482
Epoch 5: loss 16.104615062134883
Epoch 6: loss 16.100149512744714
Epoch 7: loss 16.09569637615957
Epoch 8: loss 16.091225710169265
Epoch 9: loss 16.086759735022373
Epoch 10: loss 16.0823020769112
Epoch 11: loss 16.077842175409547
Epoch 12: loss 16.073383164574363
Epoch 13: loss 16.068920490208885
Epoch 14: loss 16.064460420427018
Epoch 15: loss 16.060003037164883
Epoch 16: loss 16.05554058942022
Epoch 17: loss 16.05108661532337
Epoch 18: loss 16.04663199792419
Epoch 19: loss 16.042178230482705
-----------Time: 0:03:06.700098, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 50, rmse: 4.006106853485107-------------


Epoch 0: loss 16.1269553616489
Epoch 1: loss 16.122489217703208
Epoch 2: loss 16.118019994411085
Epoch 3: loss 16.113551980195453
Epoch 4: loss 16.109086383614223
Epoch 5: loss 16.10461181710115
Epoch 6: loss 16.100156622778275
Epoch 7: loss 16.095690420751275
Epoch 8: loss 16.091229090034606
Epoch 9: loss 16.08676463122469
Epoch 10: loss 16.0823017701693
Epoch 11: loss 16.077835484909713
Epoch 12: loss 16.073370136989077
Epoch 13: loss 16.06891266764215
Epoch 14: loss 16.064452387574836
Epoch 15: loss 16.05999036454973
Epoch 16: loss 16.05553750514725
Epoch 17: loss 16.05107629707548
Epoch 18: loss 16.04661778745086
Epoch 19: loss 16.04216575855921
-----------Time: 0:03:55.070089, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 100, rmse: 4.006111145019531-------------


Epoch 0: loss 16.126959659924903
Epoch 1: loss 16.122500643644596
Epoch 2: loss 16.11802849502522
Epoch 3: loss 16.113564053069254
Epoch 4: loss 16.109099261588284
Epoch 5: loss 16.104639131132192
Epoch 6: loss 16.100166837309548
Epoch 7: loss 16.095693568291388
Epoch 8: loss 16.091239178031604
Epoch 9: loss 16.086772249210394
Epoch 10: loss 16.082313434140328
Epoch 11: loss 16.077846747238134
Epoch 12: loss 16.07338683536482
Epoch 13: loss 16.06892140680451
Epoch 14: loss 16.06446557592088
Epoch 15: loss 16.05999739783299
Epoch 16: loss 16.055539267292612
Epoch 17: loss 16.05108371540669
Epoch 18: loss 16.04662030076384
Epoch 19: loss 16.04216257990411
-----------Time: 0:04:35.636660, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 150, rmse: 4.006111145019531-------------


Epoch 0: loss 16.126951780573318
Epoch 1: loss 16.122485288917662
Epoch 2: loss 16.118014339780988
Epoch 3: loss 16.1135457750894
Epoch 4: loss 16.109079713339128
Epoch 5: loss 16.104615439144798
Epoch 6: loss 16.100146721730482
Epoch 7: loss 16.095685349267875
Epoch 8: loss 16.09121250470998
Epoch 9: loss 16.086761456718257
Epoch 10: loss 16.082287206022293
Epoch 11: loss 16.077822317825575
Epoch 12: loss 16.07335833766786
Epoch 13: loss 16.06889183745558
Epoch 14: loss 16.064428078732846
Epoch 15: loss 16.059963384745497
Epoch 16: loss 16.055500831728462
Epoch 17: loss 16.05102904896946
Epoch 18: loss 16.04656819508995
Epoch 19: loss 16.04210321510402
-----------Time: 0:05:04.428402, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 200, rmse: 4.006104946136475-------------


Epoch 0: loss 16.126507730053586
Epoch 1: loss 16.120590244187422
Epoch 2: loss 16.114685117971515
Epoch 3: loss 16.108780170926067
Epoch 4: loss 16.10287666709738
Epoch 5: loss 16.09696691589812
Epoch 6: loss 16.09106393920915
Epoch 7: loss 16.085164842299626
Epoch 8: loss 16.079259976931016
Epoch 9: loss 16.07336765634683
Epoch 10: loss 16.067466689685947
Epoch 11: loss 16.061574751556794
Epoch 12: loss 16.05567973408121
Epoch 13: loss 16.04978354279278
Epoch 14: loss 16.043893401813353
Epoch 15: loss 16.038000141815882
Epoch 16: loss 16.0321060691987
Epoch 17: loss 16.026225534711863
Epoch 18: loss 16.02032697868451
Epoch 19: loss 16.01444333969996
-----------Time: 0:03:46.670955, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 20, rmse: 4.002553462982178-------------


Epoch 0: loss 16.126233188013074
Epoch 1: loss 16.120334128960117
Epoch 2: loss 16.11442511700106
Epoch 3: loss 16.108522893035456
Epoch 4: loss 16.10261995031368
Epoch 5: loss 16.09672270863519
Epoch 6: loss 16.090814812148377
Epoch 7: loss 16.084903591543906
Epoch 8: loss 16.079022361637133
Epoch 9: loss 16.073120350809184
Epoch 10: loss 16.067217050524366
Epoch 11: loss 16.06131678472854
Epoch 12: loss 16.055422946510916
Epoch 13: loss 16.049538885140432
Epoch 14: loss 16.04363844924935
Epoch 15: loss 16.037743158999053
Epoch 16: loss 16.031858733583235
Epoch 17: loss 16.02596957402232
Epoch 18: loss 16.020086543854323
Epoch 19: loss 16.01419566907981
-----------Time: 0:04:12.002766, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 50, rmse: 4.0025434494018555-------------


Epoch 0: loss 16.126240244632573
Epoch 1: loss 16.12033195142969
Epoch 2: loss 16.114423411640544
Epoch 3: loss 16.108518097697555
Epoch 4: loss 16.102621916521496
Epoch 5: loss 16.096705441114498
Epoch 6: loss 16.090814330125387
Epoch 7: loss 16.084917804091567
Epoch 8: loss 16.0790067797708
Epoch 9: loss 16.07311739981207
Epoch 10: loss 16.067218341796277
Epoch 11: loss 16.06131766839414
Epoch 12: loss 16.055421212369037
Epoch 13: loss 16.049529588760535
Epoch 14: loss 16.04364234458629
Epoch 15: loss 16.0377531533918
Epoch 16: loss 16.031856842829264
Epoch 17: loss 16.025970882666766
Epoch 18: loss 16.020071029403795
Epoch 19: loss 16.01418818359209
-----------Time: 0:04:49.744489, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 100, rmse: 4.00254487991333-------------


Epoch 0: loss 16.12624302501585
Epoch 1: loss 16.120334140368943
Epoch 2: loss 16.114424344312244
Epoch 3: loss 16.108524091480994
Epoch 4: loss 16.10261548168313
Epoch 5: loss 16.096708578282946
Epoch 6: loss 16.090819245515277
Epoch 7: loss 16.08491297297143
Epoch 8: loss 16.079007443038584
Epoch 9: loss 16.073099839291928
Epoch 10: loss 16.067210410327093
Epoch 11: loss 16.06130523977233
Epoch 12: loss 16.055411868279496
Epoch 13: loss 16.0495132798407
Epoch 14: loss 16.043614560978252
Epoch 15: loss 16.037719760788985
Epoch 16: loss 16.031823602171293
Epoch 17: loss 16.02592814390007
Epoch 18: loss 16.020032039992895
Epoch 19: loss 16.01413673963023
-----------Time: 0:05:36.039115, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 150, rmse: 4.002537727355957-------------


Epoch 0: loss 16.12623905889232
Epoch 1: loss 16.120340167860288
Epoch 2: loss 16.11442983870016
Epoch 3: loss 16.108518695105282
Epoch 4: loss 16.10262114175835
Epoch 5: loss 16.09671701303703
Epoch 6: loss 16.090810264952484
Epoch 7: loss 16.084911841682406
Epoch 8: loss 16.079009829558
Epoch 9: loss 16.073111642761813
Epoch 10: loss 16.067208251465658
Epoch 11: loss 16.061305543662026
Epoch 12: loss 16.05539980214727
Epoch 13: loss 16.049502303251565
Epoch 14: loss 16.04359781878224
Epoch 15: loss 16.03769886474236
Epoch 16: loss 16.03179395944285
Epoch 17: loss 16.02589032752414
Epoch 18: loss 16.01999583355819
Epoch 19: loss 16.014082468353322
-----------Time: 0:06:02.004782, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 200, rmse: 4.002529144287109-------------


Epoch 0: loss 16.125355731410263
Epoch 1: loss 16.117550982671823
Epoch 2: loss 16.10974040583724
Epoch 3: loss 16.101939825210447
Epoch 4: loss 16.0941429156334
Epoch 5: loss 16.086344889546943
Epoch 6: loss 16.078536856881037
Epoch 7: loss 16.07074261515474
Epoch 8: loss 16.062947840325013
Epoch 9: loss 16.055153381830976
Epoch 10: loss 16.04737070943429
Epoch 11: loss 16.039577355781542
Epoch 12: loss 16.031789578712136
Epoch 13: loss 16.0240045270562
Epoch 14: loss 16.01622864991313
Epoch 15: loss 16.00844750889233
Epoch 16: loss 16.000670100373377
Epoch 17: loss 15.9928968467422
Epoch 18: loss 15.985127146442407
Epoch 19: loss 15.97736126913721
-----------Time: 0:02:39.828176, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 20, rmse: 3.997840166091919-------------


Epoch 0: loss 16.125293990203126
Epoch 1: loss 16.117488598421644
Epoch 2: loss 16.109683149336732
Epoch 3: loss 16.10187577371561
Epoch 4: loss 16.09408158877416
Epoch 5: loss 16.086276881781657
Epoch 6: loss 16.078474450072495
Epoch 7: loss 16.070675822429646
Epoch 8: loss 16.062871955541763
Epoch 9: loss 16.055080511571273
Epoch 10: loss 16.047292630266664
Epoch 11: loss 16.039500716200596
Epoch 12: loss 16.031698839116057
Epoch 13: loss 16.02392889326717
Epoch 14: loss 16.01614986469462
Epoch 15: loss 16.008369895930908
Epoch 16: loss 16.000589234599477
Epoch 17: loss 15.992807120976078
Epoch 18: loss 15.985038073831696
Epoch 19: loss 15.97727366852462
-----------Time: 0:02:45.185149, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 50, rmse: 3.9978349208831787-------------


Epoch 0: loss 16.125295924777372
Epoch 1: loss 16.117483630654863
Epoch 2: loss 16.109683645620755
Epoch 3: loss 16.101876352454347
Epoch 4: loss 16.09407715800017
Epoch 5: loss 16.08626709560002
Epoch 6: loss 16.078466266571926
Epoch 7: loss 16.070668961315974
Epoch 8: loss 16.062874027021948
Epoch 9: loss 16.05507270922928
Epoch 10: loss 16.047285972178273
Epoch 11: loss 16.039489897520014
Epoch 12: loss 16.031709640424104
Epoch 13: loss 16.023918565944072
Epoch 14: loss 16.016141315852256
Epoch 15: loss 16.0083493768942
Epoch 16: loss 16.00056604667485
Epoch 17: loss 15.99277803624305
Epoch 18: loss 15.984993170758445
Epoch 19: loss 15.977214820233293
-----------Time: 0:03:31.364979, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 100, rmse: 3.9978291988372803-------------


Epoch 0: loss 16.125304434985296
Epoch 1: loss 16.117493850112673
Epoch 2: loss 16.109684706641772
Epoch 3: loss 16.101874636462878
Epoch 4: loss 16.09407121270425
Epoch 5: loss 16.08626889819487
Epoch 6: loss 16.07846727703109
Epoch 7: loss 16.070660094582173
Epoch 8: loss 16.062858526054576
Epoch 9: loss 16.055060085879518
Epoch 10: loss 16.047272640184712
Epoch 11: loss 16.03947335290416
Epoch 12: loss 16.03166815896213
Epoch 13: loss 16.02388150618088
Epoch 14: loss 16.016073508260043
Epoch 15: loss 16.008281168955833
Epoch 16: loss 16.000488633367922
Epoch 17: loss 15.992678903379538
Epoch 18: loss 15.984877543581641
Epoch 19: loss 15.97707450435238
-----------Time: 0:04:13.512116, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 150, rmse: 3.9978060722351074-------------


Epoch 0: loss 16.12530421484677
Epoch 1: loss 16.117486684850014
Epoch 2: loss 16.109678053739216
Epoch 3: loss 16.101870971376837
Epoch 4: loss 16.094070178649552
Epoch 5: loss 16.08627273908058
Epoch 6: loss 16.078471999248777
Epoch 7: loss 16.070667310147392
Epoch 8: loss 16.062866925285718
Epoch 9: loss 16.055064590292304
Epoch 10: loss 16.047264342336568
Epoch 11: loss 16.039464400085567
Epoch 12: loss 16.03166353968319
Epoch 13: loss 16.023858637444153
Epoch 14: loss 16.01605403053819
Epoch 15: loss 16.008240305904202
Epoch 16: loss 16.00043125707472
Epoch 17: loss 15.992612406765398
Epoch 18: loss 15.984780576321144
Epoch 19: loss 15.976943041203526
-----------Time: 0:04:49.600865, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 200, rmse: 3.997790575027466-------------


Epoch 0: loss 16.124199296029254
Epoch 1: loss 16.113875549341298
Epoch 2: loss 16.103564145190358
Epoch 3: loss 16.09324672651265
Epoch 4: loss 16.082930444309802
Epoch 5: loss 16.072619706019555
Epoch 6: loss 16.062320919773253
Epoch 7: loss 16.05201970915097
Epoch 8: loss 16.041723865863744
Epoch 9: loss 16.031440340480835
Epoch 10: loss 16.021149411805627
Epoch 11: loss 16.010862594457215
Epoch 12: loss 16.00058397668545
Epoch 13: loss 15.990297255534202
Epoch 14: loss 15.980026592827155
Epoch 15: loss 15.96976400186602
Epoch 16: loss 15.959483836901583
Epoch 17: loss 15.949227170233755
Epoch 18: loss 15.938965283508466
Epoch 19: loss 15.928712990051901
-----------Time: 0:03:20.973046, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 20, rmse: 3.991581916809082-------------


Epoch 0: loss 16.12403342074321
Epoch 1: loss 16.113713391518193
Epoch 2: loss 16.103391522878923
Epoch 3: loss 16.093074094866722
Epoch 4: loss 16.08276980074934
Epoch 5: loss 16.07245978121527
Epoch 6: loss 16.062159531268172
Epoch 7: loss 16.05185317604227
Epoch 8: loss 16.041549671726944
Epoch 9: loss 16.031258996379577
Epoch 10: loss 16.02097364973283
Epoch 11: loss 16.01068335035821
Epoch 12: loss 16.000398140098817
Epoch 13: loss 15.990114715061738
Epoch 14: loss 15.979840438608374
Epoch 15: loss 15.969562867855883
Epoch 16: loss 15.959296602474211
Epoch 17: loss 15.949016892566442
Epoch 18: loss 15.938757947763067
Epoch 19: loss 15.928493490680655
-----------Time: 0:03:14.348301, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 50, rmse: 3.9915995597839355-------------


Epoch 0: loss 16.12404015350757
Epoch 1: loss 16.113726394989364
Epoch 2: loss 16.103405267664634
Epoch 3: loss 16.093093173020296
Epoch 4: loss 16.082780472930196
Epoch 5: loss 16.072480883917255
Epoch 6: loss 16.0621709621361
Epoch 7: loss 16.051866336384826
Epoch 8: loss 16.041572818683502
Epoch 9: loss 16.031282017579734
Epoch 10: loss 16.020982847841232
Epoch 11: loss 16.010698164202974
Epoch 12: loss 16.000405988594707
Epoch 13: loss 15.990119674531192
Epoch 14: loss 15.979843798767554
Epoch 15: loss 15.969555296024076
Epoch 16: loss 15.959287818713705
Epoch 17: loss 15.949010316673474
Epoch 18: loss 15.938740658721176
Epoch 19: loss 15.928456254858501
-----------Time: 0:04:17.023697, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 100, rmse: 3.9915921688079834-------------


Epoch 0: loss 16.12405979847247
Epoch 1: loss 16.113739094311548
Epoch 2: loss 16.10341556542849
Epoch 3: loss 16.09310192748086
Epoch 4: loss 16.08280194201114
Epoch 5: loss 16.072483207169533
Epoch 6: loss 16.062171252542633
Epoch 7: loss 16.051860424537512
Epoch 8: loss 16.04155338141113
Epoch 9: loss 16.03126171453288
Epoch 10: loss 16.020960218430602
Epoch 11: loss 16.01065805361542
Epoch 12: loss 16.000352290870705
Epoch 13: loss 15.99005566244867
Epoch 14: loss 15.979749225286636
Epoch 15: loss 15.969438406616012
Epoch 16: loss 15.959126968497877
Epoch 17: loss 15.948808020777907
Epoch 18: loss 15.938478583677623
Epoch 19: loss 15.928143919519007
-----------Time: 0:05:04.206587, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 150, rmse: 3.991549491882324-------------


Epoch 0: loss 16.124045801655374
Epoch 1: loss 16.113734399838055
Epoch 2: loss 16.10341292324761
Epoch 3: loss 16.093090027813812
Epoch 4: loss 16.08277653584732
Epoch 5: loss 16.07247243334639
Epoch 6: loss 16.0621638808839
Epoch 7: loss 16.051859666109735
Epoch 8: loss 16.041556173203237
Epoch 9: loss 16.031245330159205
Epoch 10: loss 16.02093096178728
Epoch 11: loss 16.01062692410924
Epoch 12: loss 16.00030299954728
Epoch 13: loss 15.989976212925209
Epoch 14: loss 15.979644770204795
Epoch 15: loss 15.96928971847028
Epoch 16: loss 15.958918331368194
Epoch 17: loss 15.948515399115575
Epoch 18: loss 15.938097521298083
Epoch 19: loss 15.927613803392651
-----------Time: 0:05:33.720509, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 200, rmse: 3.9914751052856445-------------


Epoch 0: loss 16.12256182504128
Epoch 1: loss 16.108921875313225
Epoch 2: loss 16.09528446560518
Epoch 3: loss 16.081650491250866
Epoch 4: loss 16.06802243470757
Epoch 5: loss 16.054408838853945
Epoch 6: loss 16.040793318279157
Epoch 7: loss 16.027187296331675
Epoch 8: loss 16.01358788502223
Epoch 9: loss 15.99999511157643
Epoch 10: loss 15.986409976600369
Epoch 11: loss 15.972837641810903
Epoch 12: loss 15.959257096813838
Epoch 13: loss 15.945706936378334
Epoch 14: loss 15.932148067636163
Epoch 15: loss 15.918595117742177
Epoch 16: loss 15.905048160594465
Epoch 17: loss 15.891516854803221
Epoch 18: loss 15.877975612181954
Epoch 19: loss 15.864445926185015
-----------Time: 0:04:13.679732, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 20, rmse: 3.983325242996216-------------


Epoch 0: loss 16.122385121740162
Epoch 1: loss 16.108748941851932
Epoch 2: loss 16.09510863456083
Epoch 3: loss 16.081476863147515
Epoch 4: loss 16.067856398401474
Epoch 5: loss 16.054233518354657
Epoch 6: loss 16.040628688889004
Epoch 7: loss 16.027023067028384
Epoch 8: loss 16.013422889771704
Epoch 9: loss 15.99982338174651
Epoch 10: loss 15.98623459776048
Epoch 11: loss 15.97265481196907
Epoch 12: loss 15.959074402322193
Epoch 13: loss 15.945505084908481
Epoch 14: loss 15.93194303154751
Epoch 15: loss 15.918372862879643
Epoch 16: loss 15.904824279973921
Epoch 17: loss 15.891281633029624
Epoch 18: loss 15.877737229125794
Epoch 19: loss 15.864193341212145
-----------Time: 0:04:01.148899, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 50, rmse: 3.9833717346191406-------------


Epoch 0: loss 16.12238897014533
Epoch 1: loss 16.108755229931283
Epoch 2: loss 16.095109198001367
Epoch 3: loss 16.08148154984226
Epoch 4: loss 16.067857078782495
Epoch 5: loss 16.054246912059607
Epoch 6: loss 16.04062793616564
Epoch 7: loss 16.027020740664607
Epoch 8: loss 16.013419898584395
Epoch 9: loss 15.999820292546996
Epoch 10: loss 15.986229157305205
Epoch 11: loss 15.972630836576013
Epoch 12: loss 15.959036641693945
Epoch 13: loss 15.945451883987086
Epoch 14: loss 15.931863363959856
Epoch 15: loss 15.918276833217387
Epoch 16: loss 15.90468634179651
Epoch 17: loss 15.891090957025526
Epoch 18: loss 15.877503509168848
Epoch 19: loss 15.863900033982958
-----------Time: 0:04:42.025586, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 100, rmse: 3.983328104019165-------------


Epoch 0: loss 16.122398491590292
Epoch 1: loss 16.108762233136723
Epoch 2: loss 16.09513310779366
Epoch 3: loss 16.081489702746428
Epoch 4: loss 16.067850951723198
Epoch 5: loss 16.05423339207967
Epoch 6: loss 16.0406133455709
Epoch 7: loss 16.02699656950643
Epoch 8: loss 16.013378184537903
Epoch 9: loss 15.999746667489596
Epoch 10: loss 15.986136333005074
Epoch 11: loss 15.972506525206656
Epoch 12: loss 15.958878128732138
Epoch 13: loss 15.945231763353291
Epoch 14: loss 15.931578249565735
Epoch 15: loss 15.917895779135177
Epoch 16: loss 15.904186391389649
Epoch 17: loss 15.89044826143523
Epoch 18: loss 15.876658747933883
Epoch 19: loss 15.862798306525823
-----------Time: 0:03:47.066039, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 150, rmse: 3.9831724166870117-------------


Epoch 0: loss 16.122408983044938
Epoch 1: loss 16.10876510738354
Epoch 2: loss 16.09512291167209
Epoch 3: loss 16.081496451068276
Epoch 4: loss 16.0678654821623
Epoch 5: loss 16.054226471588233
Epoch 6: loss 16.04058777812773
Epoch 7: loss 16.026937453366934
Epoch 8: loss 16.013291567677452
Epoch 9: loss 15.999629889576898
Epoch 10: loss 15.985934783609876
Epoch 11: loss 15.972216112967612
Epoch 12: loss 15.958432188495596
Epoch 13: loss 15.944586412617536
Epoch 14: loss 15.930655436129463
Epoch 15: loss 15.916605892617007
Epoch 16: loss 15.902378620823937
Epoch 17: loss 15.887961088930414
Epoch 18: loss 15.873288119597692
Epoch 19: loss 15.858291502553785
-----------Time: 0:04:16.493378, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 200, rmse: 3.98252272605896-------------


Epoch 0: loss 16.120356259234512
Epoch 1: loss 16.102324646280277
Epoch 2: loss 16.08430766203664
Epoch 3: loss 16.066287073381183
Epoch 4: loss 16.048286994427944
Epoch 5: loss 16.030296427844466
Epoch 6: loss 16.012326666036984
Epoch 7: loss 15.994360459894509
Epoch 8: loss 15.976425242203613
Epoch 9: loss 15.95849556816603
Epoch 10: loss 15.940570060943637
Epoch 11: loss 15.92265649487494
Epoch 12: loss 15.904737565879138
Epoch 13: loss 15.886840187122537
Epoch 14: loss 15.868948052537512
Epoch 15: loss 15.851077054621669
Epoch 16: loss 15.833216548419763
Epoch 17: loss 15.815369429440521
Epoch 18: loss 15.797522511671522
Epoch 19: loss 15.77969402953164
-----------Time: 0:02:41.286395, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 20, rmse: 3.97249174118042-------------


Epoch 0: loss 16.12025241893293
Epoch 1: loss 16.102227368129213
Epoch 2: loss 16.084201023719956
Epoch 3: loss 16.06617976168435
Epoch 4: loss 16.048170695426737
Epoch 5: loss 16.0301880032683
Epoch 6: loss 16.012208148796574
Epoch 7: loss 15.994246714137182
Epoch 8: loss 15.97627864809643
Epoch 9: loss 15.958330454678558
Epoch 10: loss 15.940393349863772
Epoch 11: loss 15.922457169423355
Epoch 12: loss 15.904529346208847
Epoch 13: loss 15.886617475388295
Epoch 14: loss 15.868712030590196
Epoch 15: loss 15.850816118127305
Epoch 16: loss 15.83293108346347
Epoch 17: loss 15.81504369469166
Epoch 18: loss 15.797171323308483
Epoch 19: loss 15.77930686316975
-----------Time: 0:03:07.157938, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 50, rmse: 3.9724650382995605-------------


Epoch 0: loss 16.12021973497392
Epoch 1: loss 16.102198370258858
Epoch 2: loss 16.084178599668235
Epoch 3: loss 16.066173398669743
Epoch 4: loss 16.048174121705262
Epoch 5: loss 16.030178777415703
Epoch 6: loss 16.0121995696171
Epoch 7: loss 15.994220645483452
Epoch 8: loss 15.976249870623889
Epoch 9: loss 15.95829347425858
Epoch 10: loss 15.940331441932168
Epoch 11: loss 15.922367372092765
Epoch 12: loss 15.904404590673067
Epoch 13: loss 15.886433842779827
Epoch 14: loss 15.86844981967787
Epoch 15: loss 15.850464577646344
Epoch 16: loss 15.832463643290286
Epoch 17: loss 15.814432073119155
Epoch 18: loss 15.79636570965226
Epoch 19: loss 15.778275450394814
-----------Time: 0:03:55.774112, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 100, rmse: 3.972325325012207-------------


Epoch 0: loss 16.120238717708204
Epoch 1: loss 16.10221496388081
Epoch 2: loss 16.084197844027695
Epoch 3: loss 16.06618485131816
Epoch 4: loss 16.048178943750194
Epoch 5: loss 16.03017770498586
Epoch 6: loss 16.01218892310611
Epoch 7: loss 15.99417425718821
Epoch 8: loss 15.976170948240146
Epoch 9: loss 15.958155780852392
Epoch 10: loss 15.940130790204313
Epoch 11: loss 15.922046720754718
Epoch 12: loss 15.903929052306232
Epoch 13: loss 15.885750567543566
Epoch 14: loss 15.867474218112351
Epoch 15: loss 15.84907346189249
Epoch 16: loss 15.830496612743815
Epoch 17: loss 15.811714379277937
Epoch 18: loss 15.792678526784494
Epoch 19: loss 15.773306337888634
-----------Time: 0:04:35.106294, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 150, rmse: 3.971607208251953-------------


Epoch 0: loss 16.120231644494044
Epoch 1: loss 16.102179552952578
Epoch 2: loss 16.08416511832274
Epoch 3: loss 16.06614276907767
Epoch 4: loss 16.048124516120485
Epoch 5: loss 16.03010686420111
Epoch 6: loss 16.01207802955582
Epoch 7: loss 15.994023188746059
Epoch 8: loss 15.975919545377447
Epoch 9: loss 15.957762129349577
Epoch 10: loss 15.93949298044467
Epoch 11: loss 15.921076821269647
Epoch 12: loss 15.902442365058288
Epoch 13: loss 15.883488779550275
Epoch 14: loss 15.864129630944987
Epoch 15: loss 15.8441838440265
Epoch 16: loss 15.823538208215245
Epoch 17: loss 15.802021736292296
Epoch 18: loss 15.779421258452407
Epoch 19: loss 15.755563687215103
-----------Time: 0:05:05.438669, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 200, rmse: 3.969058036804199-------------


Epoch 0: loss 16.11745002647014
Epoch 1: loss 16.093633958993884
Epoch 2: loss 16.069820677605318
Epoch 3: loss 16.046016755604498
Epoch 4: loss 16.02224476043137
Epoch 5: loss 15.998486338652237
Epoch 6: loss 15.97476033661403
Epoch 7: loss 15.951035208647637
Epoch 8: loss 15.927328549499677
Epoch 9: loss 15.903642537737749
Epoch 10: loss 15.879975487188906
Epoch 11: loss 15.85634060331244
Epoch 12: loss 15.83271972325394
Epoch 13: loss 15.809123396808651
Epoch 14: loss 15.785553037115514
Epoch 15: loss 15.761983604389666
Epoch 16: loss 15.738432707898056
Epoch 17: loss 15.714908716534712
Epoch 18: loss 15.691401950946121
Epoch 19: loss 15.667913215195375
-----------Time: 0:03:41.314034, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 20, rmse: 3.9581139087677-------------


Epoch 0: loss 16.11739388440471
Epoch 1: loss 16.093569467221997
Epoch 2: loss 16.069756698452824
Epoch 3: loss 16.04597447811747
Epoch 4: loss 16.02219407671233
Epoch 5: loss 15.998446776725599
Epoch 6: loss 15.974719238644258
Epoch 7: loss 15.951002257618326
Epoch 8: loss 15.927288130614471
Epoch 9: loss 15.903599135443981
Epoch 10: loss 15.879935564847015
Epoch 11: loss 15.856287571708426
Epoch 12: loss 15.832647353167635
Epoch 13: loss 15.809029411660257
Epoch 14: loss 15.785422860830618
Epoch 15: loss 15.761826959364177
Epoch 16: loss 15.738248451952703
Epoch 17: loss 15.714683323985147
Epoch 18: loss 15.691122858857511
Epoch 19: loss 15.667578734023989
-----------Time: 0:03:49.774095, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 50, rmse: 3.958091974258423-------------


Epoch 0: loss 16.117334179414105
Epoch 1: loss 16.093513324897273
Epoch 2: loss 16.06969043597895
Epoch 3: loss 16.045888438699297
Epoch 4: loss 16.02209871576298
Epoch 5: loss 15.99832788766111
Epoch 6: loss 15.974556838642844
Epoch 7: loss 15.950787036557635
Epoch 8: loss 15.927034223513994
Epoch 9: loss 15.903262628687019
Epoch 10: loss 15.879491617784609
Epoch 11: loss 15.855689824826696
Epoch 12: loss 15.831855043501488
Epoch 13: loss 15.80794775388758
Epoch 14: loss 15.783982078299177
Epoch 15: loss 15.759916120407826
Epoch 16: loss 15.735700873851517
Epoch 17: loss 15.711306239030101
Epoch 18: loss 15.686697439501764
Epoch 19: loss 15.661806547622307
-----------Time: 0:04:42.681894, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 100, rmse: 3.957261800765991-------------


Epoch 0: loss 16.117345812010857
Epoch 1: loss 16.093529397343215
Epoch 2: loss 16.06970169312154
Epoch 3: loss 16.04589405339849
Epoch 4: loss 16.022101786293497
Epoch 5: loss 15.998299426005676
Epoch 6: loss 15.974472344861594
Epoch 7: loss 15.950628964391463
Epoch 8: loss 15.926734613944422
Epoch 9: loss 15.902745873594881
Epoch 10: loss 15.878627533624845
Epoch 11: loss 15.854302744427216
Epoch 12: loss 15.829700492010485
Epoch 13: loss 15.804705514280352
Epoch 14: loss 15.77917523666722
Epoch 15: loss 15.752944515346508
Epoch 16: loss 15.72585098687691
Epoch 17: loss 15.697671583516886
Epoch 18: loss 15.668201357855493
Epoch 19: loss 15.637198669097552
-----------Time: 0:05:33.029699, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 150, rmse: 3.9537177085876465-------------


Epoch 0: loss 16.117334940434798
Epoch 1: loss 16.093509297321646
Epoch 2: loss 16.06969090477807
Epoch 3: loss 16.04587219434306
Epoch 4: loss 16.022059119091274
Epoch 5: loss 15.998207200150441
Epoch 6: loss 15.974291671806862
Epoch 7: loss 15.950276022180388
Epoch 8: loss 15.92606739222582
Epoch 9: loss 15.901504256272847
Epoch 10: loss 15.876401405738967
Epoch 11: loss 15.850522638469238
Epoch 12: loss 15.823502805843633
Epoch 13: loss 15.795024689803505
Epoch 14: loss 15.764635292714935
Epoch 15: loss 15.731956217974798
Epoch 16: loss 15.696582352356653
Epoch 17: loss 15.658171856254258
Epoch 18: loss 15.616378863004318
Epoch 19: loss 15.570954782279566
-----------Time: 0:06:00.074977, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 200, rmse: 3.944364309310913-------------


Epoch 0: loss 16.113693175593298
Epoch 1: loss 16.082191944122314
Epoch 2: loss 16.050722258579736
Epoch 3: loss 16.01930045718535
Epoch 4: loss 15.987911588410569
Epoch 5: loss 15.956551301343211
Epoch 6: loss 15.925245693159077
Epoch 7: loss 15.89396439822489
Epoch 8: loss 15.862709910147471
Epoch 9: loss 15.831488598942302
Epoch 10: loss 15.800323048903282
Epoch 11: loss 15.769184903388052
Epoch 12: loss 15.738070596619233
Epoch 13: loss 15.707015676949581
Epoch 14: loss 15.675969801887732
Epoch 15: loss 15.644960584687176
Epoch 16: loss 15.613983050839547
Epoch 17: loss 15.583044380905708
Epoch 18: loss 15.552140932098688
Epoch 19: loss 15.521281662941497
-----------Time: 0:02:57.691711, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 20, rmse: 3.9391379356384277-------------


Epoch 0: loss 16.113579858704195
Epoch 1: loss 16.082085664665136
Epoch 2: loss 16.05061028222794
Epoch 3: loss 16.01918442110577
Epoch 4: loss 15.987793243599041
Epoch 5: loss 15.956420097485905
Epoch 6: loss 15.925096403716244
Epoch 7: loss 15.893783151357962
Epoch 8: loss 15.862497858666673
Epoch 9: loss 15.831258785731123
Epoch 10: loss 15.800034095179198
Epoch 11: loss 15.768820425555264
Epoch 12: loss 15.737644144735498
Epoch 13: loss 15.706476878964299
Epoch 14: loss 15.675332672509116
Epoch 15: loss 15.64418223761164
Epoch 16: loss 15.613029578511258
Epoch 17: loss 15.581898901370506
Epoch 18: loss 15.550756327933497
Epoch 19: loss 15.519586375123978
-----------Time: 0:02:46.373176, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 50, rmse: 3.938943862915039-------------


Epoch 0: loss 16.113531552170073
Epoch 1: loss 16.08205140187988
Epoch 2: loss 16.0505745707808
Epoch 3: loss 16.019115900202507
Epoch 4: loss 15.98769270615321
Epoch 5: loss 15.956268216164752
Epoch 6: loss 15.924856338117225
Epoch 7: loss 15.89344000025506
Epoch 8: loss 15.862002544392704
Epoch 9: loss 15.830484776862487
Epoch 10: loss 15.798886492305504
Epoch 11: loss 15.767141663425315
Epoch 12: loss 15.73516907310797
Epoch 13: loss 15.702897437180171
Epoch 14: loss 15.670175701979387
Epoch 15: loss 15.636918919704867
Epoch 16: loss 15.602948204857814
Epoch 17: loss 15.5680747815226
Epoch 18: loss 15.53205797528365
Epoch 19: loss 15.49469819854562
-----------Time: 0:03:27.836606, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 100, rmse: 3.9353370666503906-------------


Epoch 0: loss 16.113524146544147
Epoch 1: loss 16.08205927863855
Epoch 2: loss 16.050578160672295
Epoch 3: loss 16.01911538136012
Epoch 4: loss 15.987651442755428
Epoch 5: loss 15.956190539158317
Epoch 6: loss 15.924666276115513
Epoch 7: loss 15.893025937062234
Epoch 8: loss 15.861165710478259
Epoch 9: loss 15.828934789028033
Epoch 10: loss 15.796084434588103
Epoch 11: loss 15.76229884249805
Epoch 12: loss 15.72713343976567
Epoch 13: loss 15.690130611811727
Epoch 14: loss 15.650725587416499
Epoch 15: loss 15.608404141136198
Epoch 16: loss 15.562599975042463
Epoch 17: loss 15.512867924699062
Epoch 18: loss 15.458779089472875
Epoch 19: loss 15.39995293542054
-----------Time: 0:04:22.195707, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 150, rmse: 3.921809434890747-------------


Epoch 0: loss 16.113539070587823
Epoch 1: loss 16.082047329187525
Epoch 2: loss 16.050555631866786
Epoch 3: loss 16.019063962290247
Epoch 4: loss 15.987507740267597
Epoch 5: loss 15.955778758249703
Epoch 6: loss 15.923698215007523
Epoch 7: loss 15.890927154516124
Epoch 8: loss 15.85690619596779
Epoch 9: loss 15.82087385220136
Epoch 10: loss 15.781858115432183
Epoch 11: loss 15.738835265287697
Epoch 12: loss 15.690778851055335
Epoch 13: loss 15.636857345222195
Epoch 14: loss 15.576386224581794
Epoch 15: loss 15.508881844276317
Epoch 16: loss 15.434052293361562
Epoch 17: loss 15.351659341244805
Epoch 18: loss 15.261597807087155
Epoch 19: loss 15.163721843022508
-----------Time: 0:04:54.973903, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 200, rmse: 3.8890950679779053-------------


Epoch 0: loss 16.108692089068363
Epoch 1: loss 16.06709140691503
Epoch 2: loss 16.025523380717225
Epoch 3: loss 15.984031970764127
Epoch 4: loss 15.942601862778282
Epoch 5: loss 15.901240854434437
Epoch 6: loss 15.859913860215254
Epoch 7: loss 15.818645683431702
Epoch 8: loss 15.777445813635888
Epoch 9: loss 15.73629034907355
Epoch 10: loss 15.695200872913919
Epoch 11: loss 15.654174002958031
Epoch 12: loss 15.613224476300354
Epoch 13: loss 15.572322730592829
Epoch 14: loss 15.531480310013789
Epoch 15: loss 15.490671556886607
Epoch 16: loss 15.449924623791196
Epoch 17: loss 15.409244984112854
Epoch 18: loss 15.368604523906116
Epoch 19: loss 15.32801219287808
-----------Time: 0:03:18.088491, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 20, rmse: 3.9140281677246094-------------


Epoch 0: loss 16.108474592205752
Epoch 1: loss 16.066874907547007
Epoch 2: loss 16.02532304532506
Epoch 3: loss 15.983851817070368
Epoch 4: loss 15.942420373473755
Epoch 5: loss 15.901039203137662
Epoch 6: loss 15.85971107944449
Epoch 7: loss 15.818426724164754
Epoch 8: loss 15.777186690108033
Epoch 9: loss 15.73597046731282
Epoch 10: loss 15.694802240939033
Epoch 11: loss 15.653684584196007
Epoch 12: loss 15.612572458660297
Epoch 13: loss 15.571477345243105
Epoch 14: loss 15.53036477839318
Epoch 15: loss 15.48922060908929
Epoch 16: loss 15.448040844500875
Epoch 17: loss 15.406774167718934
Epoch 18: loss 15.365409433096243
Epoch 19: loss 15.323911597120691
-----------Time: 0:03:25.432465, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 50, rmse: 3.9134716987609863-------------


Epoch 0: loss 16.108505014623876
Epoch 1: loss 16.0668955191508
Epoch 2: loss 16.02532099821828
Epoch 3: loss 15.983793461692274
Epoch 4: loss 15.942309860822752
Epoch 5: loss 15.90084904079012
Epoch 6: loss 15.859367261703019
Epoch 7: loss 15.817831985583572
Epoch 8: loss 15.776179395595797
Epoch 9: loss 15.734312296302633
Epoch 10: loss 15.692070796348402
Epoch 11: loss 15.649260113069452
Epoch 12: loss 15.605640351480606
Epoch 13: loss 15.560861914491056
Epoch 14: loss 15.514508891974279
Epoch 15: loss 15.466156217181988
Epoch 16: loss 15.415272507867195
Epoch 17: loss 15.361350124850748
Epoch 18: loss 15.303802688333118
Epoch 19: loss 15.242147226836643
-----------Time: 0:04:21.188425, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 100, rmse: 3.9015603065490723-------------


Epoch 0: loss 16.108490625757963
Epoch 1: loss 16.066886570999454
Epoch 2: loss 16.025298759560016
Epoch 3: loss 15.983680695279128
Epoch 4: loss 15.942002273370806
Epoch 5: loss 15.900113325134576
Epoch 6: loss 15.857702527245857
Epoch 7: loss 15.814330729276607
Epoch 8: loss 15.76926525317177
Epoch 9: loss 15.721460839987708
Epoch 10: loss 15.669695159804244
Epoch 11: loss 15.61265709660765
Epoch 12: loss 15.54914407253006
Epoch 13: loss 15.478067594354991
Epoch 14: loss 15.398501625392928
Epoch 15: loss 15.309813409217224
Epoch 16: loss 15.21152860175792
Epoch 17: loss 15.103280282655833
Epoch 18: loss 14.984926496529592
Epoch 19: loss 14.856414546001988
-----------Time: 0:05:02.359120, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 150, rmse: 3.8474504947662354-------------


Epoch 0: loss 16.108484271559266
Epoch 1: loss 16.06686736838593
Epoch 2: loss 16.025238867879498
Epoch 3: loss 15.983523642647326
Epoch 4: loss 15.941470387579631
Epoch 5: loss 15.898578669987275
Epoch 6: loss 15.853828919458934
Epoch 7: loss 15.805544072742887
Epoch 8: loss 15.751501921404309
Epoch 9: loss 15.689388874109962
Epoch 10: loss 15.617068107391324
Epoch 11: loss 15.532887858628838
Epoch 12: loss 15.43568653902196
Epoch 13: loss 15.324800070502825
Epoch 14: loss 15.199857923373896
Epoch 15: loss 15.060802164643015
Epoch 16: loss 14.907648207118422
Epoch 17: loss 14.740604010141693
Epoch 18: loss 14.559916651592493
Epoch 19: loss 14.36595098112769
-----------Time: 0:05:33.293152, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 200, rmse: 3.7790286540985107-------------


Epoch 0: loss 16.102005816983944
Epoch 1: loss 16.047055862856144
Epoch 2: loss 15.992159465397746
Epoch 3: loss 15.937371133656006
Epoch 4: loss 15.882704332381762
Epoch 5: loss 15.828140309869498
Epoch 6: loss 15.773689763969932
Epoch 7: loss 15.719339947391944
Epoch 8: loss 15.66509469998408
Epoch 9: loss 15.610950210679155
Epoch 10: loss 15.556908430905365
Epoch 11: loss 15.502941408774463
Epoch 12: loss 15.449079478545446
Epoch 13: loss 15.3953162038242
Epoch 14: loss 15.341687635989082
Epoch 15: loss 15.28810276448435
Epoch 16: loss 15.23463038454372
Epoch 17: loss 15.181229394567344
Epoch 18: loss 15.127913183333629
Epoch 19: loss 15.074711468765566
-----------Time: 0:04:14.888415, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 20, rmse: 3.880882501602173-------------


Epoch 0: loss 16.101873008889825
Epoch 1: loss 16.046896684552223
Epoch 2: loss 15.992028364479186
Epoch 3: loss 15.93724881779441
Epoch 4: loss 15.882524684565816
Epoch 5: loss 15.82789263691573
Epoch 6: loss 15.773322696592446
Epoch 7: loss 15.718857117487465
Epoch 8: loss 15.664392297328847
Epoch 9: loss 15.60993543301282
Epoch 10: loss 15.555472994187786
Epoch 11: loss 15.500933908780935
Epoch 12: loss 15.446297100538013
Epoch 13: loss 15.391429257820715
Epoch 14: loss 15.336215993903005
Epoch 15: loss 15.280591230667305
Epoch 16: loss 15.224361110083626
Epoch 17: loss 15.167288402683907
Epoch 18: loss 15.109201085120196
Epoch 19: loss 15.049795898294372
-----------Time: 0:04:21.216949, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 50, rmse: 3.877243995666504-------------


Epoch 0: loss 16.101881646150606
Epoch 1: loss 16.046930590293016
Epoch 2: loss 15.992045041333729
Epoch 3: loss 15.93721640246144
Epoch 4: loss 15.882406046236255
Epoch 5: loss 15.827526716644055
Epoch 6: loss 15.77245519341432
Epoch 7: loss 15.716939356483929
Epoch 8: loss 15.660566538167169
Epoch 9: loss 15.60272488508489
Epoch 10: loss 15.54262693528574
Epoch 11: loss 15.479205118825476
Epoch 12: loss 15.411254263105699
Epoch 13: loss 15.337494301238483
Epoch 14: loss 15.256721184913589
Epoch 15: loss 15.16782822694514
Epoch 16: loss 15.069885327026467
Epoch 17: loss 14.962282449671468
Epoch 18: loss 14.84450309959814
Epoch 19: loss 14.716232024437062
-----------Time: 0:04:17.303766, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 100, rmse: 3.82924485206604-------------


Epoch 0: loss 16.101875478901118
Epoch 1: loss 16.046891868730288
Epoch 2: loss 15.991942447965206
Epoch 3: loss 15.936907771101719
Epoch 4: loss 15.881532014874805
Epoch 5: loss 15.825130277253027
Epoch 6: loss 15.766466943772997
Epoch 7: loss 15.703410484662452
Epoch 8: loss 15.633207266456475
Epoch 9: loss 15.552874331762117
Epoch 10: loss 15.459693572130975
Epoch 11: loss 15.351497807795747
Epoch 12: loss 15.226796336638143
Epoch 13: loss 15.08482166526757
Epoch 14: loss 14.925126104007408
Epoch 15: loss 14.747720264104995
Epoch 16: loss 14.552700149034145
Epoch 17: loss 14.34038296622255
Epoch 18: loss 14.111174669779144
Epoch 19: loss 13.86558899560527
-----------Time: 0:03:48.769025, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 150, rmse: 3.7089030742645264-------------


Epoch 0: loss 16.101852789334853
Epoch 1: loss 16.046843997287386
Epoch 2: loss 15.991750202469362
Epoch 3: loss 15.936156581185836
Epoch 4: loss 15.878779008895433
Epoch 5: loss 15.816643915855735
Epoch 6: loss 15.744943790944003
Epoch 7: loss 15.658115102520062
Epoch 8: loss 15.55140697055047
Epoch 9: loss 15.421778632221564
Epoch 10: loss 15.267627855822598
Epoch 11: loss 15.08857228796141
Epoch 12: loss 14.884829610894075
Epoch 13: loss 14.65692555690473
Epoch 14: loss 14.405383285068183
Epoch 15: loss 14.131050123865025
Epoch 16: loss 13.834909222837762
Epoch 17: loss 13.51799830928842
Epoch 18: loss 13.181367152018545
Epoch 19: loss 12.826352192306725
-----------Time: 0:04:17.811448, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 200, rmse: 3.5589711666107178-------------


Epoch 0: loss 16.093244042067244
Epoch 1: loss 16.020677907237417
Epoch 2: loss 15.948257077057898
Epoch 3: loss 15.875984077028376
Epoch 4: loss 15.80392600454151
Epoch 5: loss 15.732059800540577
Epoch 6: loss 15.660360730167573
Epoch 7: loss 15.588832492475733
Epoch 8: loss 15.51746524916582
Epoch 9: loss 15.446295345134226
Epoch 10: loss 15.375317428603388
Epoch 11: loss 15.30450654898473
Epoch 12: loss 15.233866405590778
Epoch 13: loss 15.16339460049329
Epoch 14: loss 15.093095034215034
Epoch 15: loss 15.022939777944709
Epoch 16: loss 14.952945034551906
Epoch 17: loss 14.88308582985252
Epoch 18: loss 14.813399065701192
Epoch 19: loss 14.74383559312556
-----------Time: 0:02:45.865212, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 20, rmse: 3.8371176719665527-------------


Epoch 0: loss 16.093091860227187
Epoch 1: loss 16.020508340936175
Epoch 2: loss 15.948042715030107
Epoch 3: loss 15.875765840925556
Epoch 4: loss 15.803609264357185
Epoch 5: loss 15.731622632396421
Epoch 6: loss 15.659701099727645
Epoch 7: loss 15.587826212052745
Epoch 8: loss 15.51592716229487
Epoch 9: loss 15.443909330300626
Epoch 10: loss 15.371650117321336
Epoch 11: loss 15.298930316466622
Epoch 12: loss 15.225491001270205
Epoch 13: loss 15.151017087383591
Epoch 14: loss 15.075037444655567
Epoch 15: loss 14.997096998008326
Epoch 16: loss 14.916594086294916
Epoch 17: loss 14.832925194433813
Epoch 18: loss 14.745420584541225
Epoch 19: loss 14.653432830252552
-----------Time: 0:03:06.044130, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 50, rmse: 3.823721170425415-------------


Epoch 0: loss 16.093061425881647
Epoch 1: loss 16.02043017620234
Epoch 2: loss 15.947910525087043
Epoch 3: loss 15.875425860180421
Epoch 4: loss 15.802709025621025
Epoch 5: loss 15.72927668510798
Epoch 6: loss 15.654231767929268
Epoch 7: loss 15.576096579586443
Epoch 8: loss 15.492668926618617
Epoch 9: loss 15.40117499956688
Epoch 10: loss 15.298781331172775
Epoch 11: loss 15.182633236350412
Epoch 12: loss 15.050546199099534
Epoch 13: loss 14.900798088185745
Epoch 14: loss 14.732321628219994
Epoch 15: loss 14.544460469578322
Epoch 16: loss 14.336995636658411
Epoch 17: loss 14.110255490832513
Epoch 18: loss 13.864458521012706
Epoch 19: loss 13.600191448226191
-----------Time: 0:03:58.824053, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 100, rmse: 3.6716625690460205-------------


Epoch 0: loss 16.093066426837748
Epoch 1: loss 16.020446808199445
Epoch 2: loss 15.9478016703462
Epoch 3: loss 15.874729392968034
Epoch 4: loss 15.799903961418115
Epoch 5: loss 15.720137547902143
Epoch 6: loss 15.629787163477737
Epoch 7: loss 15.521873460637932
Epoch 8: loss 15.390088786373584
Epoch 9: loss 15.229991557093792
Epoch 10: loss 15.039150413577488
Epoch 11: loss 14.816635151023512
Epoch 12: loss 14.56244548576691
Epoch 13: loss 14.277153056882659
Epoch 14: loss 13.96213465426654
Epoch 15: loss 13.618590927175882
Epoch 16: loss 13.24823354494967
Epoch 17: loss 12.852596929113524
Epoch 18: loss 12.433897972884807
Epoch 19: loss 11.993800837167779
-----------Time: 0:04:33.773633, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 150, rmse: 3.4342281818389893-------------


Epoch 0: loss 16.093064200560512
Epoch 1: loss 16.02033043089219
Epoch 2: loss 15.947002632842239
Epoch 3: loss 15.870408793776628
Epoch 4: loss 15.783290518697932
Epoch 5: loss 15.673782583291665
Epoch 6: loss 15.530753032483116
Epoch 7: loss 15.347067908141327
Epoch 8: loss 15.11965581526764
Epoch 9: loss 14.848274180654471
Epoch 10: loss 14.534015782570437
Epoch 11: loss 14.17884796815699
Epoch 12: loss 13.784945002323003
Epoch 13: loss 13.35467946989631
Epoch 14: loss 12.89067175441491
Epoch 15: loss 12.395718006416143
Epoch 16: loss 11.873242272962495
Epoch 17: loss 11.32647073936566
Epoch 18: loss 10.758950819458374
Epoch 19: loss 10.174680721506986
-----------Time: 0:05:05.254408, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 200, rmse: 3.1483688354492188-------------


Epoch 0: loss 16.08159799171311
Epoch 1: loss 15.98568532647096
Epoch 2: loss 15.890108764528126
Epoch 3: loss 15.794793856539371
Epoch 4: loss 15.699813007076775
Epoch 5: loss 15.6051123631007
Epoch 6: loss 15.510738025092767
Epoch 7: loss 15.4166302473536
Epoch 8: loss 15.322769597018782
Epoch 9: loss 15.229189536181268
Epoch 10: loss 15.13585446048133
Epoch 11: loss 15.042732672564293
Epoch 12: loss 14.949792088212495
Epoch 13: loss 14.8570046723051
Epoch 14: loss 14.76438424871174
Epoch 15: loss 14.671834910415061
Epoch 16: loss 14.579329232666531
Epoch 17: loss 14.486799156879718
Epoch 18: loss 14.394129525714105
Epoch 19: loss 14.301251530193518
-----------Time: 0:03:47.548718, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 20, rmse: 3.777719020843506-------------


Epoch 0: loss 16.081440187395106
Epoch 1: loss 15.98557897648648
Epoch 2: loss 15.889944220444896
Epoch 3: loss 15.794589524945854
Epoch 4: loss 15.699426022737553
Epoch 5: loss 15.6044084775571
Epoch 6: loss 15.509407961089822
Epoch 7: loss 15.414210347781822
Epoch 8: loss 15.31844118641018
Epoch 9: loss 15.221436487066155
Epoch 10: loss 15.122461896671297
Epoch 11: loss 15.020463879955015
Epoch 12: loss 14.914092212477867
Epoch 13: loss 14.801903238239465
Epoch 14: loss 14.682416300854001
Epoch 15: loss 14.554183649588953
Epoch 16: loss 14.41578194445537
Epoch 17: loss 14.266116079524394
Epoch 18: loss 14.104165790261233
Epoch 19: loss 13.929246616985825
-----------Time: 0:04:08.857710, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 50, rmse: 3.7222177982330322-------------


Epoch 0: loss 16.081508137079087
Epoch 1: loss 15.985598204769863
Epoch 2: loss 15.889836457321474
Epoch 3: loss 15.793866142233536
Epoch 4: loss 15.696803657434245
Epoch 5: loss 15.596377403597394
Epoch 6: loss 15.488126480689575
Epoch 7: loss 15.365474664886209
Epoch 8: loss 15.221435952925566
Epoch 9: loss 15.050249312869099
Epoch 10: loss 14.847961344882286
Epoch 11: loss 14.612431211404141
Epoch 12: loss 14.342508047673286
Epoch 13: loss 14.038219413010564
Epoch 14: loss 13.700312786351216
Epoch 15: loss 13.330191876202448
Epoch 16: loss 12.929654641537773
Epoch 17: loss 12.500505170982905
Epoch 18: loss 12.04536221816917
Epoch 19: loss 11.566636535640383
-----------Time: 0:04:49.254147, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 100, rmse: 3.3686726093292236-------------


Epoch 0: loss 16.08144887547703
Epoch 1: loss 15.985463731246126
Epoch 2: loss 15.888984687695237
Epoch 3: loss 15.789410232006693
Epoch 4: loss 15.67844233894037
Epoch 5: loss 15.541019522929853
Epoch 6: loss 15.361635801648237
Epoch 7: loss 15.130839488375376
Epoch 8: loss 14.845190584951279
Epoch 9: loss 14.504733818649532
Epoch 10: loss 14.111102498050874
Epoch 11: loss 13.667225639608777
Epoch 12: loss 13.17615285497959
Epoch 13: loss 12.641943968398989
Epoch 14: loss 12.068426710681594
Epoch 15: loss 11.460930207684482
Epoch 16: loss 10.82415060118011
Epoch 17: loss 10.164025982156145
Epoch 18: loss 9.486117015266107
Epoch 19: loss 8.797445785227906
-----------Time: 0:05:33.552322, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 150, rmse: 2.9136803150177-------------


Epoch 0: loss 16.081479436356844
Epoch 1: loss 15.98535742197042
Epoch 2: loss 15.887174846167925
Epoch 3: loss 15.775727649043603
Epoch 4: loss 15.624359751603343
Epoch 5: loss 15.406067335586174
Epoch 6: loss 15.108881747612427
Epoch 7: loss 14.732915663083913
Epoch 8: loss 14.282205879331737
Epoch 9: loss 13.762029727429654
Epoch 10: loss 13.17843653288401
Epoch 11: loss 12.538144251650737
Epoch 12: loss 11.848438931393066
Epoch 13: loss 11.116714875292299
Epoch 14: loss 10.351459187616014
Epoch 15: loss 9.56207399098623
Epoch 16: loss 8.758056597193665
Epoch 17: loss 7.950232597198611
Epoch 18: loss 7.149147543588237
Epoch 19: loss 6.3667820430826145
-----------Time: 0:05:59.648146, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 200, rmse: 2.4554059505462646-------------


Epoch 0: loss 16.066197652282632
Epoch 1: loss 15.939624455261646
Epoch 2: loss 15.813559364403895
Epoch 3: loss 15.688040452524655
Epoch 4: loss 15.563007084813826
Epoch 5: loss 15.43854321442978
Epoch 6: loss 15.314580214937592
Epoch 7: loss 15.191114652539286
Epoch 8: loss 15.06804635072286
Epoch 9: loss 14.945385232835182
Epoch 10: loss 14.823039446141552
Epoch 11: loss 14.70098902234051
Epoch 12: loss 14.579204854918537
Epoch 13: loss 14.457480488165752
Epoch 14: loss 14.335675508966908
Epoch 15: loss 14.213668970779079
Epoch 16: loss 14.091208256217952
Epoch 17: loss 13.968078244309377
Epoch 18: loss 13.844034279216043
Epoch 19: loss 13.718743514599781
-----------Time: 0:02:26.385034, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 20, rmse: 3.6978702545166016-------------


Epoch 0: loss 16.066096867472663
Epoch 1: loss 15.939507544073091
Epoch 2: loss 15.813431468588169
Epoch 3: loss 15.687770258025543
Epoch 4: loss 15.562367282398633
Epoch 5: loss 15.436886204527148
Epoch 6: loss 15.310756741949499
Epoch 7: loss 15.182873920359775
Epoch 8: loss 15.0514126088452
Epoch 9: loss 14.91379583005091
Epoch 10: loss 14.766663964382003
Epoch 11: loss 14.6060332979697
Epoch 12: loss 14.428053296604125
Epoch 13: loss 14.229222513917609
Epoch 14: loss 14.00694222395805
Epoch 15: loss 13.759673623691246
Epoch 16: loss 13.486516637216125
Epoch 17: loss 13.187265172349557
Epoch 18: loss 12.862296084206411
Epoch 19: loss 12.512514089747185
-----------Time: 0:02:45.345575, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 50, rmse: 3.514650344848633-------------


Epoch 0: loss 16.066134017728604
Epoch 1: loss 15.939383791736315
Epoch 2: loss 15.812555751831653
Epoch 3: loss 15.68363474373457
Epoch 4: loss 15.546173732004586
Epoch 5: loss 15.386394756912471
Epoch 6: loss 15.186545826028778
Epoch 7: loss 14.93273936178841
Epoch 8: loss 14.618193733231927
Epoch 9: loss 14.241328714463036
Epoch 10: loss 13.803734252218192
Epoch 11: loss 13.309021873017768
Epoch 12: loss 12.761268831971808
Epoch 13: loss 12.16556448674578
Epoch 14: loss 11.528080131775534
Epoch 15: loss 10.85514128940659
Epoch 16: loss 10.153293198901327
Epoch 17: loss 9.430500920017236
Epoch 18: loss 8.694224050733432
Epoch 19: loss 7.952975019026606
-----------Time: 0:03:35.374587, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 100, rmse: 2.7609219551086426-------------


Epoch 0: loss 16.06608668846433
Epoch 1: loss 15.939046497510917
Epoch 2: loss 15.808805219630562
Epoch 3: loss 15.659623335080669
Epoch 4: loss 15.457546452972927
Epoch 5: loss 15.171911383271542
Epoch 6: loss 14.790603508049537
Epoch 7: loss 14.311881156379986
Epoch 8: loss 13.740492974240343
Epoch 9: loss 13.083984359442507
Epoch 10: loss 12.352471337103209
Epoch 11: loss 11.557185183924913
Epoch 12: loss 10.710014586995774
Epoch 13: loss 9.824551330823624
Epoch 14: loss 8.914988600864172
Epoch 15: loss 7.996375904062509
Epoch 16: loss 7.085124465678943
Epoch 17: loss 6.1987266464036335
Epoch 18: loss 5.354886708796316
Epoch 19: loss 4.571557867974286
-----------Time: 0:04:22.555162, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 150, rmse: 2.0612969398498535-------------


Epoch 0: loss 16.0661531843006
Epoch 1: loss 15.938421704446316
Epoch 2: loss 15.796277260378433
Epoch 3: loss 15.585706077625467
Epoch 4: loss 15.24914735267187
Epoch 5: loss 14.772171236238382
Epoch 6: loss 14.162102552261477
Epoch 7: loss 13.431375736383849
Epoch 8: loss 12.595661334203209
Epoch 9: loss 11.67196128989381
Epoch 10: loss 10.679572271095015
Epoch 11: loss 9.638999750459888
Epoch 12: loss 8.573162795798554
Epoch 13: loss 7.505608921533308
Epoch 14: loss 6.463181342814136
Epoch 15: loss 5.472935067887796
Epoch 16: loss 4.562167924341137
Epoch 17: loss 3.7578047666295578
Epoch 18: loss 3.082739311174701
Epoch 19: loss 2.549382536290715
-----------Time: 0:04:56.746928, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 200, rmse: 1.5383878946304321-------------


Epoch 0: loss 16.045945716721004
Epoch 1: loss 15.878904037983798
Epoch 2: loss 15.712836950871528
Epoch 3: loss 15.547746117702316
Epoch 4: loss 15.383527491000633
Epoch 5: loss 15.220203098361424
Epoch 6: loss 15.05768108549424
Epoch 7: loss 14.895904996331584
Epoch 8: loss 14.734790063798913
Epoch 9: loss 14.574127844457331
Epoch 10: loss 14.413701044210214
Epoch 11: loss 14.253192560124358
Epoch 12: loss 14.092109806969349
Epoch 13: loss 13.929991423921653
Epoch 14: loss 13.766007813116076
Epoch 15: loss 13.59930541970407
Epoch 16: loss 13.428844148533186
Epoch 17: loss 13.253518108183304
Epoch 18: loss 13.072105464240401
Epoch 19: loss 12.883342435659436
-----------Time: 0:03:20.743246, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 20, rmse: 3.57879900932312-------------


Epoch 0: loss 16.04592885006505
Epoch 1: loss 15.878755937911299
Epoch 2: loss 15.71240319422628
Epoch 3: loss 15.546447940596165
Epoch 4: loss 15.379853516444362
Epoch 5: loss 15.210478498988854
Epoch 6: loss 15.033950994946894
Epoch 7: loss 14.843193563447821
Epoch 8: loss 14.629690037790105
Epoch 9: loss 14.384747407435075
Epoch 10: loss 14.101318219616855
Epoch 11: loss 13.774389969129288
Epoch 12: loss 13.4013086083533
Epoch 13: loss 12.981616411473583
Epoch 14: loss 12.5167746429796
Epoch 15: loss 12.009720246904632
Epoch 16: loss 11.464412405025447
Epoch 17: loss 10.885176211871032
Epoch 18: loss 10.277042507671545
Epoch 19: loss 9.645601569990415
-----------Time: 0:03:41.931515, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 50, rmse: 3.05897855758667-------------


Epoch 0: loss 16.045955831943612
Epoch 1: loss 15.878485689220252
Epoch 2: loss 15.709120043598483
Epoch 3: loss 15.525329888806906
Epoch 4: loss 15.295147430903242
Epoch 5: loss 14.980199102087472
Epoch 6: loss 14.55894799878119
Epoch 7: loss 14.028588741223665
Epoch 8: loss 13.395639549720022
Epoch 9: loss 12.67008177309726
Epoch 10: loss 11.862667414077668
Epoch 11: loss 10.987550580676578
Epoch 12: loss 10.060033723541807
Epoch 13: loss 9.097783214051546
Epoch 14: loss 8.119827973058264
Epoch 15: loss 7.145388178490892
Epoch 16: loss 6.195828593509751
Epoch 17: loss 5.293894221693229
Epoch 18: loss 4.461756522205616
Epoch 19: loss 3.7213240406187804
-----------Time: 0:04:32.536279, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 100, rmse: 1.851662516593933-------------


Epoch 0: loss 16.045771815583652
Epoch 1: loss 15.876691972256484
Epoch 2: loss 15.685834621981217
Epoch 3: loss 15.397465698352127
Epoch 4: loss 14.938577577042281
Epoch 5: loss 14.299468587312186
Epoch 6: loss 13.4967114680354
Epoch 7: loss 12.552219632788155
Epoch 8: loss 11.4916697497469
Epoch 9: loss 10.345215940812542
Epoch 10: loss 9.144278346098526
Epoch 11: loss 7.924181907056401
Epoch 12: loss 6.722477064111947
Epoch 13: loss 5.578913309316132
Epoch 14: loss 4.535086461679646
Epoch 15: loss 3.6308844162109692
Epoch 16: loss 2.8976576944095016
Epoch 17: loss 2.348750384719946
Epoch 18: loss 1.9690352266025388
Epoch 19: loss 1.717678557017888
-----------Time: 0:05:03.242455, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 150, rmse: 1.2847368717193604-------------


Epoch 0: loss 16.045875465305944
Epoch 1: loss 15.873444047682048
Epoch 2: loss 15.631774111763818
Epoch 3: loss 15.175346160855483
Epoch 4: loss 14.461052848179097
Epoch 5: loss 13.521368626215459
Epoch 6: loss 12.395650675401797
Epoch 7: loss 11.124557793302468
Epoch 8: loss 9.755408903384351
Epoch 9: loss 8.340120720824448
Epoch 10: loss 6.936324521988355
Epoch 11: loss 5.604681704600522
Epoch 12: loss 4.408799070842633
Epoch 13: loss 3.4073409694507757
Epoch 14: loss 2.6413424805560277
Epoch 15: loss 2.1142913920520243
Epoch 16: loss 1.7802086611556385
Epoch 17: loss 1.570161140989259
Epoch 18: loss 1.4302082306473978
Epoch 19: loss 1.3304548481516763
-----------Time: 0:05:43.394148, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 200, rmse: 1.1458438634872437-------------


Epoch 0: loss 16.01961616201852
Epoch 1: loss 15.799421062542084
Epoch 2: loss 15.58084502461295
Epoch 3: loss 15.363928921913699
Epoch 4: loss 15.148751117795495
Epoch 5: loss 14.93521802823397
Epoch 6: loss 14.723187648063771
Epoch 7: loss 14.51247758020066
Epoch 8: loss 14.302961147499188
Epoch 9: loss 14.094304465417826
Epoch 10: loss 13.885960888512566
Epoch 11: loss 13.67694056118358
Epoch 12: loss 13.466137874897827
Epoch 13: loss 13.251817662797588
Epoch 14: loss 13.031924223627586
Epoch 15: loss 12.803868198083626
Epoch 16: loss 12.564767180220338
Epoch 17: loss 12.311943615824713
Epoch 18: loss 12.042695557572
Epoch 19: loss 11.75493086259478
-----------Time: 0:04:16.275169, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 20, rmse: 3.410226583480835-------------


Epoch 0: loss 16.019358181707105
Epoch 1: loss 15.798938857270947
Epoch 2: loss 15.579490760670465
Epoch 3: loss 15.359401703658733
Epoch 4: loss 15.133715894055017
Epoch 5: loss 14.890367160483894
Epoch 6: loss 14.609849247354214
Epoch 7: loss 14.271120912034334
Epoch 8: loss 13.858757022413759
Epoch 9: loss 13.365770640225433
Epoch 10: loss 12.79139601114718
Epoch 11: loss 12.139667653337389
Epoch 12: loss 11.41859100304459
Epoch 13: loss 10.638665825468875
Epoch 14: loss 9.812399849935742
Epoch 15: loss 8.953416712326872
Epoch 16: loss 8.076344342125442
Epoch 17: loss 7.196788288316628
Epoch 18: loss 6.331225494252008
Epoch 19: loss 5.497805114282481
-----------Time: 0:04:40.348277, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 50, rmse: 2.2667832374572754-------------


Epoch 0: loss 16.019123950427222
Epoch 1: loss 15.7976013531563
Epoch 2: loss 15.565172858443061
Epoch 3: loss 15.270367448909441
Epoch 4: loss 14.831157854421427
Epoch 5: loss 14.204200962174516
Epoch 6: loss 13.390180480940437
Epoch 7: loss 12.409937996006065
Epoch 8: loss 11.292898230996062
Epoch 9: loss 10.07538895168792
Epoch 10: loss 8.797606886049014
Epoch 11: loss 7.503611443028494
Epoch 12: loss 6.242250330490934
Epoch 13: loss 5.062930511222578
Epoch 14: loss 4.016657212689365
Epoch 15: loss 3.149492602493531
Epoch 16: loss 2.489581552356919
Epoch 17: loss 2.033061131373121
Epoch 18: loss 1.738343272927405
Epoch 19: loss 1.5484498767440509
-----------Time: 0:03:34.846217, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 100, rmse: 1.2254481315612793-------------


Epoch 0: loss 16.019067368603324
Epoch 1: loss 15.790770401314202
Epoch 2: loss 15.46583991701542
Epoch 3: loss 14.859987830130414
Epoch 4: loss 13.933542694975463
Epoch 5: loss 12.73725954873073
Epoch 6: loss 11.331253251930889
Epoch 7: loss 9.783988444684574
Epoch 8: loss 8.173577621230747
Epoch 9: loss 6.586165723624341
Epoch 10: loss 5.1128147257742125
Epoch 11: loss 3.8459042663999456
Epoch 12: loss 2.8622213589101464
Epoch 13: loss 2.1916675120024918
Epoch 14: loss 1.786022564909781
Epoch 15: loss 1.547828181403172
Epoch 16: loss 1.3978425014466291
Epoch 17: loss 1.2949706675923345
Epoch 18: loss 1.2207343977971723
Epoch 19: loss 1.1657802715711194
-----------Time: 0:03:50.924903, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 150, rmse: 1.078918218612671-------------


Epoch 0: loss 16.018761367497074
Epoch 1: loss 15.762521380506954
Epoch 2: loss 15.224469381418482
Epoch 3: loss 14.226952039657435
Epoch 4: loss 12.843708634182057
Epoch 5: loss 11.178424910658919
Epoch 6: loss 9.344119792142726
Epoch 7: loss 7.465891603544525
Epoch 8: loss 5.681451268141655
Epoch 9: loss 4.132457166861554
Epoch 10: loss 2.943696896073091
Epoch 11: loss 2.1678679761126873
Epoch 12: loss 1.731705815232014
Epoch 13: loss 1.4914013044285734
Epoch 14: loss 1.3445853188091028
Epoch 15: loss 1.2462502989255584
Epoch 16: loss 1.1771690868825742
Epoch 17: loss 1.1276187233266266
Epoch 18: loss 1.0917187688012302
Epoch 19: loss 1.0655111755412061
-----------Time: 0:04:28.091354, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 200, rmse: 1.036303162574768-------------


Epoch 0: loss 15.98408879087696
Epoch 1: loss 15.69361501546448
Epoch 2: loss 15.406270486625269
Epoch 3: loss 15.121642921721566
Epoch 4: loss 14.839692789941719
Epoch 5: loss 14.559746243371595
Epoch 6: loss 14.281056465306575
Epoch 7: loss 14.00217837260559
Epoch 8: loss 13.720834789099804
Epoch 9: loss 13.433952447446291
Epoch 10: loss 13.137279216202138
Epoch 11: loss 12.82637951359793
Epoch 12: loss 12.496338140322761
Epoch 13: loss 12.142421908583442
Epoch 14: loss 11.760608753476083
Epoch 15: loss 11.347969452410434
Epoch 16: loss 10.902816263212335
Epoch 17: loss 10.424892258812644
Epoch 18: loss 9.915630607127884
Epoch 19: loss 9.377155014067126
-----------Time: 0:02:47.080369, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 20, rmse: 3.02213454246521-------------


Epoch 0: loss 15.983868591418533
Epoch 1: loss 15.693185058736878
Epoch 2: loss 15.403408834375462
Epoch 3: loss 15.106354520008447
Epoch 4: loss 14.775771813654524
Epoch 5: loss 14.36413052394508
Epoch 6: loss 13.830160442547282
Epoch 7: loss 13.15852954116705
Epoch 8: loss 12.353589840464258
Epoch 9: loss 11.4291211756239
Epoch 10: loss 10.404580040818132
Epoch 11: loss 9.305735398272317
Epoch 12: loss 8.163754157750356
Epoch 13: loss 7.013361731840904
Epoch 14: loss 5.893322225366357
Epoch 15: loss 4.843980423324195
Epoch 16: loss 3.9059866000414543
Epoch 17: loss 3.1155639953623653
Epoch 18: loss 2.4977505135108364
Epoch 19: loss 2.0547362248038
-----------Time: 0:03:07.280908, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 50, rmse: 1.3843581676483154-------------


Epoch 0: loss 15.983887538889169
Epoch 1: loss 15.687593755675373
Epoch 2: loss 15.318781512272365
Epoch 3: loss 14.687261789113428
Epoch 4: loss 13.700702192992086
Epoch 5: loss 12.403008392804859
Epoch 6: loss 10.869413916995954
Epoch 7: loss 9.190332106107471
Epoch 8: loss 7.467783914563966
Epoch 9: loss 5.812581938981621
Epoch 10: loss 4.340582339059665
Epoch 11: loss 3.158606606180607
Epoch 12: loss 2.3325488066919604
Epoch 13: loss 1.8358911232432313
Epoch 14: loss 1.558155793966062
Epoch 15: loss 1.3927011028005092
Epoch 16: loss 1.283648041132159
Epoch 17: loss 1.2072104100885437
Epoch 18: loss 1.151890620840445
Epoch 19: loss 1.1112916508143593
-----------Time: 0:04:04.427226, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 100, rmse: 1.055708408355713-------------


Epoch 0: loss 15.983713473620266
Epoch 1: loss 15.65272104720696
Epoch 2: loss 14.961205686545359
Epoch 3: loss 13.668251619447892
Epoch 4: loss 11.913216775756222
Epoch 5: loss 9.868790541089314
Epoch 6: loss 7.721149054087006
Epoch 7: loss 5.672879545114299
Epoch 8: loss 3.9339262583516357
Epoch 9: loss 2.6780311541106143
Epoch 10: loss 1.9431136377224656
Epoch 11: loss 1.5724859531642692
Epoch 12: loss 1.3748580426739376
Epoch 13: loss 1.2539697528825369
Epoch 14: loss 1.1740505141107331
Epoch 15: loss 1.119463848348154
Epoch 16: loss 1.081742552893521
Epoch 17: loss 1.0553047536766484
Epoch 18: loss 1.0365795924846344
Epoch 19: loss 1.023169455060238
-----------Time: 0:04:36.927353, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 150, rmse: 1.0176364183425903-------------


Epoch 0: loss 15.982576709392603
Epoch 1: loss 15.543856118800136
Epoch 2: loss 14.385639368807123
Epoch 3: loss 12.488631282441315
Epoch 4: loss 10.146368600545596
Epoch 5: loss 7.649636243594088
Epoch 6: loss 5.311470651056146
Epoch 7: loss 3.447224860932919
Epoch 8: loss 2.2656358959494627
Epoch 9: loss 1.6841451728907924
Epoch 10: loss 1.4127032272902567
Epoch 11: loss 1.263215718611613
Epoch 12: loss 1.170668177156749
Epoch 13: loss 1.110971033151802
Epoch 14: loss 1.0717442237922457
Epoch 15: loss 1.0455716260915997
Epoch 16: loss 1.0278497271689466
Epoch 17: loss 1.0156415741102665
Epoch 18: loss 1.0071497762281003
Epoch 19: loss 1.0011317885686937
-----------Time: 0:05:15.740070, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 200, rmse: 1.007690191268921-------------


Epoch 0: loss 15.937667736740071
Epoch 1: loss 15.55528666468792
Epoch 2: loss 15.177966006363521
Epoch 3: loss 14.804877478509834
Epoch 4: loss 14.434708890608952
Epoch 5: loss 14.064636043778835
Epoch 6: loss 13.68936247356305
Epoch 7: loss 13.299794874352045
Epoch 8: loss 12.883027533074317
Epoch 9: loss 12.424394711519856
Epoch 10: loss 11.910001091752251
Epoch 11: loss 11.330061369320827
Epoch 12: loss 10.680959576300268
Epoch 13: loss 9.965543564666284
Epoch 14: loss 9.19232410539292
Epoch 15: loss 8.373845127628444
Epoch 16: loss 7.525763832401361
Epoch 17: loss 6.666759458425708
Epoch 18: loss 5.816973115128626
Epoch 19: loss 4.997597000921726
-----------Time: 0:03:48.025057, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 20, rmse: 2.154756546020508-------------


Epoch 0: loss 15.937063669367546
Epoch 1: loss 15.553576035885918
Epoch 2: loss 15.164381565770226
Epoch 3: loss 14.721733124117414
Epoch 4: loss 14.122342435350362
Epoch 5: loss 13.271834185487748
Epoch 6: loss 12.154719529818296
Epoch 7: loss 10.820533161775257
Epoch 8: loss 9.339059320981896
Epoch 9: loss 7.791123592574808
Epoch 10: loss 6.262215437482012
Epoch 11: loss 4.844587329161303
Epoch 12: loss 3.630901099353332
Epoch 13: loss 2.695967872159646
Epoch 14: loss 2.067581382764948
Epoch 15: loss 1.6949829736632327
Epoch 16: loss 1.4798786484916422
Epoch 17: loss 1.3458399959333958
Epoch 18: loss 1.254604124727944
Epoch 19: loss 1.189168824846165
-----------Time: 0:03:40.193888, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 50, rmse: 1.0879658460617065-------------


Epoch 0: loss 15.93729633582293
Epoch 1: loss 15.527598443272453
Epoch 2: loss 14.828975009555204
Epoch 3: loss 13.500154478126015
Epoch 4: loss 11.621418688723807
Epoch 5: loss 9.417041554277263
Epoch 6: loss 7.1300523475566076
Epoch 7: loss 5.018048836978769
Epoch 8: loss 3.334553972996727
Epoch 9: loss 2.2462321669916667
Epoch 10: loss 1.6897591884010443
Epoch 11: loss 1.4230804247594255
Epoch 12: loss 1.2749839529775937
Epoch 13: loss 1.1821748603453384
Epoch 14: loss 1.1211896298079467
Epoch 15: loss 1.0802811474628977
Epoch 16: loss 1.0524569478548371
Epoch 17: loss 1.033303208361507
Epoch 18: loss 1.0199123842681213
Epoch 19: loss 1.0104294159348857
-----------Time: 0:04:56.097821, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 100, rmse: 1.0119143724441528-------------


Epoch 0: loss 15.935883102499966
Epoch 1: loss 15.377578155316368
Epoch 2: loss 13.907077950585984
Epoch 3: loss 11.535801540839406
Epoch 4: loss 8.733772517028484
Epoch 5: loss 5.960642090564581
Epoch 6: loss 3.6930751274175266
Epoch 7: loss 2.2769519092273036
Epoch 8: loss 1.6336073176960637
Epoch 9: loss 1.3599971763753191
Epoch 10: loss 1.216871867220709
Epoch 11: loss 1.1324342173555093
Epoch 12: loss 1.0807385457605692
Epoch 13: loss 1.0483440680792955
Epoch 14: loss 1.0276902810816793
Epoch 15: loss 1.0142350039642878
Epoch 16: loss 1.005299516949983
Epoch 17: loss 0.999273924188422
Epoch 18: loss 0.9951477541166391
Epoch 19: loss 0.9922421420638233
-----------Time: 0:05:32.766411, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 150, rmse: 1.0035359859466553-------------


Epoch 0: loss 15.929941170626584
Epoch 1: loss 15.036127311889084
Epoch 2: loss 12.713970842667415
Epoch 3: loss 9.541045992189026
Epoch 4: loss 6.271944047576257
Epoch 5: loss 3.634155647335913
Epoch 6: loss 2.1228042368074678
Epoch 7: loss 1.525205722349114
Epoch 8: loss 1.284704754749934
Epoch 9: loss 1.1612200272057354
Epoch 10: loss 1.0918179191669217
Epoch 11: loss 1.0515555525811358
Epoch 12: loss 1.0275433687265312
Epoch 13: loss 1.0129004122771408
Epoch 14: loss 1.0036933440563145
Epoch 15: loss 0.9977964458256846
Epoch 16: loss 0.9939047645427015
Epoch 17: loss 0.991343533558454
Epoch 18: loss 0.9895730091217356
Epoch 19: loss 0.988355254562994
-----------Time: 0:05:38.115609, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 200, rmse: 1.0015833377838135-------------


Epoch 0: loss 15.876258305844695
Epoch 1: loss 15.373247183944429
Epoch 2: loss 14.878467217290318
Epoch 3: loss 14.389418934401512
Epoch 4: loss 13.898798963309241
Epoch 5: loss 13.390173405911236
Epoch 6: loss 12.835703762411228
Epoch 7: loss 12.203802816111994
Epoch 8: loss 11.470731814054123
Epoch 9: loss 10.625719588499084
Epoch 10: loss 9.672250170756968
Epoch 11: loss 8.62915789089234
Epoch 12: loss 7.52583998010624
Epoch 13: loss 6.4031414880643664
Epoch 14: loss 5.306780821347509
Epoch 15: loss 4.286100109677525
Epoch 16: loss 3.390993255451362
Epoch 17: loss 2.662815978632082
Epoch 18: loss 2.1246732941070543
Epoch 19: loss 1.7648886803896158
-----------Time: 0:02:26.368715, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 20, rmse: 1.2891989946365356-------------


Epoch 0: loss 15.876121514513805
Epoch 1: loss 15.367531760092856
Epoch 2: loss 14.803916212655462
Epoch 3: loss 13.973838235451645
Epoch 4: loss 12.678098088182011
Epoch 5: loss 10.958996743206876
Epoch 6: loss 8.975331059741611
Epoch 7: loss 6.919727252059944
Epoch 8: loss 4.999711556302393
Epoch 9: loss 3.423073812824932
Epoch 10: loss 2.342838890011898
Epoch 11: loss 1.7485869207934492
Epoch 12: loss 1.4575840427604558
Epoch 13: loss 1.2999160642562968
Epoch 14: loss 1.2019878294300683
Epoch 15: loss 1.1371066576440156
Epoch 16: loss 1.0929201331036191
Epoch 17: loss 1.062340226459918
Epoch 18: loss 1.0409671170287575
Epoch 19: loss 1.025793850340877
-----------Time: 0:02:45.895720, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 50, rmse: 1.0187568664550781-------------


Epoch 0: loss 15.874491343101514
Epoch 1: loss 15.20504076601436
Epoch 2: loss 13.579651569918228
Epoch 3: loss 10.9536387034899
Epoch 4: loss 7.914910134833555
Epoch 5: loss 5.061405086271007
Epoch 6: loss 2.960980205285672
Epoch 7: loss 1.8681245808889193
Epoch 8: loss 1.4399533131123885
Epoch 9: loss 1.2496420282621368
Epoch 10: loss 1.1459447711203785
Epoch 11: loss 1.085420430192745
Epoch 12: loss 1.0491347461047544
Epoch 13: loss 1.0268478943738943
Epoch 14: loss 1.0128140878813496
Epoch 15: loss 1.0038234709947895
Epoch 16: loss 0.9979211392935493
Epoch 17: loss 0.9939604398995524
Epoch 18: loss 0.9912666140884858
Epoch 19: loss 0.989384502123853
-----------Time: 0:03:41.769419, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 100, rmse: 1.0022258758544922-------------


Epoch 0: loss 15.867386528044177
Epoch 1: loss 14.716080506124076
Epoch 2: loss 11.799495712060395
Epoch 3: loss 8.036525572428825
Epoch 4: loss 4.594989395906512
Epoch 5: loss 2.4196430317016318
Epoch 6: loss 1.5697803941027115
Epoch 7: loss 1.2792902411553704
Epoch 8: loss 1.1458661098594314
Epoch 9: loss 1.0765291534732904
Epoch 10: loss 1.0390390709828528
Epoch 11: loss 1.0180795152366258
Epoch 12: loss 1.0059504090864546
Epoch 13: loss 0.998713729270066
Epoch 14: loss 0.9942594003489382
Epoch 15: loss 0.9914407067450575
Epoch 16: loss 0.9896226613951999
Epoch 17: loss 0.9883971600476784
Epoch 18: loss 0.9875584926988976
Epoch 19: loss 0.9869767027811358
-----------Time: 0:04:20.499467, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 150, rmse: 1.0007151365280151-------------


Epoch 0: loss 15.840906365919917
Epoch 1: loss 13.963083455976161
Epoch 2: loss 9.923969116465043
Epoch 3: loss 5.640185951732824
Epoch 4: loss 2.718109516069382
Epoch 5: loss 1.5956560377843876
Epoch 6: loss 1.2632083696408918
Epoch 7: loss 1.1258722692484957
Epoch 8: loss 1.0601542599017106
Epoch 9: loss 1.0272596673092937
Epoch 10: loss 1.010046351468064
Epoch 11: loss 1.0006123807133378
Epoch 12: loss 0.9952705520498422
Epoch 13: loss 0.9921200680888302
Epoch 14: loss 0.9901823185266782
Epoch 15: loss 0.9889575215122375
Epoch 16: loss 0.9881686358234039
Epoch 17: loss 0.9876090165971089
Epoch 18: loss 0.9872429269108194
Epoch 19: loss 0.9869442637229886
-----------Time: 0:04:57.707211, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 200, rmse: 1.0003573894500732-------------


Epoch 0: loss 15.795808414845053
Epoch 1: loss 15.135789298959372
Epoch 2: loss 14.488561963697956
Epoch 3: loss 13.844497778676267
Epoch 4: loss 13.173458808009038
Epoch 5: loss 12.419980130550847
Epoch 6: loss 11.523243441278355
Epoch 7: loss 10.45138188430056
Epoch 8: loss 9.210784415047476
Epoch 9: loss 7.841639329973545
Epoch 10: loss 6.412273736671626
Epoch 11: loss 5.017543324119957
Epoch 12: loss 3.765693121723924
Epoch 13: loss 2.7596907951768292
Epoch 14: loss 2.061176190822263
Epoch 15: loss 1.6484378603180185
Epoch 16: loss 1.4235216495846847
Epoch 17: loss 1.2927901806748388
Epoch 18: loss 1.2082608152887884
Epoch 19: loss 1.1497169910176024
-----------Time: 0:03:18.508255, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 20, rmse: 1.0710828304290771-------------


Epoch 0: loss 15.795474237563365
Epoch 1: loss 15.10815290888734
Epoch 2: loss 14.1273114018754
Epoch 3: loss 12.383184914747096
Epoch 4: loss 9.96064513840709
Epoch 5: loss 7.261935724260497
Epoch 6: loss 4.751341999322581
Epoch 7: loss 2.880003174243251
Epoch 8: loss 1.863720241514738
Epoch 9: loss 1.444953002234138
Epoch 10: loss 1.2576657467364227
Epoch 11: loss 1.154722187605288
Epoch 12: loss 1.0931680732735347
Epoch 13: loss 1.0552430612649393
Epoch 14: loss 1.0314116585248188
Epoch 15: loss 1.0160834202466937
Epoch 16: loss 1.0060378519201356
Epoch 17: loss 0.9993090732970141
Epoch 18: loss 0.9947292795804871
Epoch 19: loss 0.9915481888450572
-----------Time: 0:03:23.911686, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 50, rmse: 1.0035072565078735-------------


Epoch 0: loss 15.789958648671268
Epoch 1: loss 14.578108767830463
Epoch 2: loss 11.489207977442199
Epoch 3: loss 7.453167721523806
Epoch 4: loss 3.964745186605552
Epoch 5: loss 2.0497952069906904
Epoch 6: loss 1.4191242069281464
Epoch 7: loss 1.2013570556114264
Epoch 8: loss 1.100294461014998
Epoch 9: loss 1.0492543744904765
Epoch 10: loss 1.0224036275854314
Epoch 11: loss 1.0076810494754027
Epoch 12: loss 0.9993107710408412
Epoch 13: loss 0.9943779294843451
Epoch 14: loss 0.9914058688511467
Epoch 15: loss 0.9895154568343917
Epoch 16: loss 0.9882918094214439
Epoch 17: loss 0.9874663464299618
Epoch 18: loss 0.9868855438295948
Epoch 19: loss 0.9864959912399938
-----------Time: 0:04:31.575183, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 100, rmse: 1.0004020929336548-------------


Epoch 0: loss 15.746129099216844
Epoch 1: loss 13.308853613805226
Epoch 2: loss 8.445066842260148
Epoch 3: loss 4.021085134597496
Epoch 4: loss 1.8774164844427892
Epoch 5: loss 1.3153808513790968
Epoch 6: loss 1.1347342763762553
Epoch 7: loss 1.0582955553682036
Epoch 8: loss 1.02351516289125
Epoch 9: loss 1.0067995409073551
Epoch 10: loss 0.9983426769217958
Epoch 11: loss 0.9937704391716744
Epoch 12: loss 0.9912359662811804
Epoch 13: loss 0.9897570419305041
Epoch 14: loss 0.9887938332531749
Epoch 15: loss 0.9882099173067703
Epoch 16: loss 0.9878302354278482
Epoch 17: loss 0.9875156399735684
Epoch 18: loss 0.9872953883489233
Epoch 19: loss 0.987095612572742
-----------Time: 0:05:00.817449, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 150, rmse: 1.0001846551895142-------------


Epoch 0: loss 15.624364119628774
Epoch 1: loss 11.7523280216858
Epoch 2: loss 5.9233630799028525
Epoch 3: loss 2.342441706245136
Epoch 4: loss 1.377856506025616
Epoch 5: loss 1.141467289846186
Epoch 6: loss 1.0546814540341343
Epoch 7: loss 1.01945019732551
Epoch 8: loss 1.0040046782225225
Epoch 9: loss 0.9967853396466532
Epoch 10: loss 0.9931545716676458
Epoch 11: loss 0.991237298585981
Epoch 12: loss 0.9901219871525144
Epoch 13: loss 0.9894838968394177
Epoch 14: loss 0.9890750099323962
Epoch 15: loss 0.98878979809327
Epoch 16: loss 0.9885944405316399
Epoch 17: loss 0.988430016046636
Epoch 18: loss 0.9883132845633311
Epoch 19: loss 0.9881945804871575
-----------Time: 0:05:43.673721, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 200, rmse: 1.0003576278686523-------------


Epoch 0: loss 15.68976541262985
Epoch 1: loss 14.823518665670505
Epoch 2: loss 13.971103038393718
Epoch 3: loss 13.062923869080619
Epoch 4: loss 11.934068438149847
Epoch 5: loss 10.442297037559207
Epoch 6: loss 8.615127457037381
Epoch 7: loss 6.627282620254193
Epoch 8: loss 4.720691454222567
Epoch 9: loss 3.153537251237555
Epoch 10: loss 2.1153729806910913
Epoch 11: loss 1.5848024735767081
Epoch 12: loss 1.3426420429013488
Epoch 13: loss 1.2158793921604176
Epoch 14: loss 1.1392656744596037
Epoch 15: loss 1.0899794078378848
Epoch 16: loss 1.057332376590431
Epoch 17: loss 1.0352960901457438
Epoch 18: loss 1.0201774183306245
Epoch 19: loss 1.0096617329859616
-----------Time: 0:04:15.809603, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 20, rmse: 1.0119643211364746-------------


Epoch 0: loss 15.688778639228659
Epoch 1: loss 14.747673543397987
Epoch 2: loss 13.009219262702365
Epoch 3: loss 9.921075506920786
Epoch 4: loss 6.353936108899804
Epoch 5: loss 3.433844519964439
Epoch 6: loss 1.8938076671146062
Epoch 7: loss 1.3787586629844477
Epoch 8: loss 1.1893655266562124
Epoch 9: loss 1.097607563700995
Epoch 10: loss 1.0493601731563016
Epoch 11: loss 1.0230687803195313
Epoch 12: loss 1.0082121345904809
Epoch 13: loss 0.9995077907909448
Epoch 14: loss 0.9942360021712794
Epoch 15: loss 0.9909621599318218
Epoch 16: loss 0.9888275619140457
Epoch 17: loss 0.9873971608814304
Epoch 18: loss 0.9864432086044839
Epoch 19: loss 0.9857353436486627
-----------Time: 0:04:29.062178, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 50, rmse: 1.0005632638931274-------------


Epoch 0: loss 15.655918518572543
Epoch 1: loss 13.114498404201571
Epoch 2: loss 7.895319211995102
Epoch 3: loss 3.4465958119864823
Epoch 4: loss 1.6440924687707081
Epoch 5: loss 1.229774270711093
Epoch 6: loss 1.092770112021064
Epoch 7: loss 1.0366859131367587
Epoch 8: loss 1.0120692643184517
Epoch 9: loss 1.0004989517940783
Epoch 10: loss 0.9947680076092984
Epoch 11: loss 0.9917177989827738
Epoch 12: loss 0.989992285014625
Epoch 13: loss 0.9889854085127772
Epoch 14: loss 0.9883426184431245
Epoch 15: loss 0.9878879096632486
Epoch 16: loss 0.9875796479236308
Epoch 17: loss 0.9873371967451283
Epoch 18: loss 0.9871128234220238
Epoch 19: loss 0.9869289412454395
-----------Time: 0:03:28.750137, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 100, rmse: 1.0001834630966187-------------


Epoch 0: loss 15.46510706497574
Epoch 1: loss 10.683510725883247
Epoch 2: loss 4.405240242088405
Epoch 3: loss 1.7101709192184211
Epoch 4: loss 1.204999086550878
Epoch 5: loss 1.0696185150828421
Epoch 6: loss 1.022490337401904
Epoch 7: loss 1.0043672484635917
Epoch 8: loss 0.9967302447463456
Epoch 9: loss 0.9932709855836264
Epoch 10: loss 0.9915074834751006
Epoch 11: loss 0.9905652231078488
Epoch 12: loss 0.9900375489244777
Epoch 13: loss 0.9896463224944893
Epoch 14: loss 0.9893929894492962
Epoch 15: loss 0.9891772243301139
Epoch 16: loss 0.9890022157423518
Epoch 17: loss 0.9888827792647872
Epoch 18: loss 0.9887372219996844
Epoch 19: loss 0.9886249859998121
-----------Time: 0:03:49.943170, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 150, rmse: 1.000200629234314-------------


Epoch 0: loss 15.115355570722105
Epoch 1: loss 8.453384008985813
Epoch 2: loss 2.6637113006494824
Epoch 3: loss 1.3196926489999075
Epoch 4: loss 1.0919937579251944
Epoch 5: loss 1.0267476418570631
Epoch 6: loss 1.0053068236075646
Epoch 7: loss 0.9973229069856278
Epoch 8: loss 0.9940451957759422
Epoch 9: loss 0.9924632027429235
Epoch 10: loss 0.9917232975517686
Epoch 11: loss 0.9913034990567627
Epoch 12: loss 0.9909867437849101
Epoch 13: loss 0.9908125429713512
Epoch 14: loss 0.9906291734951874
Epoch 15: loss 0.9905540931153518
Epoch 16: loss 0.9903746184899536
Epoch 17: loss 0.9902476794924536
Epoch 18: loss 0.9901647701608286
Epoch 19: loss 0.989973226956144
-----------Time: 0:04:33.040799, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 200, rmse: 1.0003831386566162-------------


Epoch 0: loss 15.55037975259421
Epoch 1: loss 14.414536235253404
Epoch 2: loss 13.242945190873595
Epoch 3: loss 11.708892238341058
Epoch 4: loss 9.483009692571162
Epoch 5: loss 6.814789377132662
Epoch 6: loss 4.296861910119921
Epoch 7: loss 2.5014666666668224
Epoch 8: loss 1.6308478351972102
Epoch 9: loss 1.3109796716020832
Epoch 10: loss 1.171522174032568
Epoch 11: loss 1.096674549978281
Epoch 12: loss 1.053543145952178
Epoch 13: loss 1.0278569583402761
Epoch 14: loss 1.0121341975342002
Epoch 15: loss 1.002271152339985
Epoch 16: loss 0.9959071694248847
Epoch 17: loss 0.9917212800852562
Epoch 18: loss 0.9888798280569961
Epoch 19: loss 0.9868953706875905
-----------Time: 0:02:45.087992, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 20, rmse: 1.001910924911499-------------


Epoch 0: loss 15.54717479807453
Epoch 1: loss 13.921783154006883
Epoch 2: loss 10.047476175753172
Epoch 3: loss 5.3279386071033485
Epoch 4: loss 2.325915681369412
Epoch 5: loss 1.401078184283642
Epoch 6: loss 1.1613881745177679
Epoch 7: loss 1.0679690739167262
Epoch 8: loss 1.0269131422399373
Epoch 9: loss 1.007724297263818
Epoch 10: loss 0.9981677959378872
Epoch 11: loss 0.9931817657601386
Epoch 12: loss 0.9903862136190776
Epoch 13: loss 0.9887604020807391
Epoch 14: loss 0.9877593477275334
Epoch 15: loss 0.9870793060027366
Epoch 16: loss 0.9866131735178619
Epoch 17: loss 0.9862077433885343
Epoch 18: loss 0.9859261963599528
Epoch 19: loss 0.9856391240877584
-----------Time: 0:03:06.151698, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 50, rmse: 1.0000858306884766-------------


Epoch 0: loss 15.372833512022549
Epoch 1: loss 10.32206213143677
Epoch 2: loss 3.8401532044483306
Epoch 3: loss 1.5247164860959284
Epoch 4: loss 1.1471594612095393
Epoch 5: loss 1.0457667065515928
Epoch 6: loss 1.0121802945802107
Epoch 7: loss 0.9997287135917639
Epoch 8: loss 0.9945938108147584
Epoch 9: loss 0.9922350719125457
Epoch 10: loss 0.9910469276707998
Epoch 11: loss 0.990353729379378
Epoch 12: loss 0.9899586462793044
Epoch 13: loss 0.9897303382393586
Epoch 14: loss 0.9894182754801044
Epoch 15: loss 0.9892391248719079
Epoch 16: loss 0.9890013047214952
Epoch 17: loss 0.9888833961193817
Epoch 18: loss 0.9886641784684045
Epoch 19: loss 0.9885190753038322
-----------Time: 0:04:08.614986, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 100, rmse: 1.0001542568206787-------------


Epoch 0: loss 14.83012056169204
Epoch 1: loss 7.031427362096122
Epoch 2: loss 1.9279877098764395
Epoch 3: loss 1.175416937546603
Epoch 4: loss 1.044163672933117
Epoch 5: loss 1.009637833727385
Epoch 6: loss 0.9989223587992659
Epoch 7: loss 0.9951142517200561
Epoch 8: loss 0.9934208370895085
Epoch 9: loss 0.992686152798511
Epoch 10: loss 0.9922115783480083
Epoch 11: loss 0.991920932795186
Epoch 12: loss 0.9916527393461895
Epoch 13: loss 0.991597941059208
Epoch 14: loss 0.9913244811557699
Epoch 15: loss 0.991141748029684
Epoch 16: loss 0.9908899530533412
Epoch 17: loss 0.9907271823136297
Epoch 18: loss 0.9904729397223008
Epoch 19: loss 0.9902539282729277
-----------Time: 0:04:36.319830, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 150, rmse: 1.0002788305282593-------------


Epoch 0: loss 14.136141227081199
Epoch 1: loss 4.890371345546726
Epoch 2: loss 1.422435736724253
Epoch 3: loss 1.0820729309219974
Epoch 4: loss 1.0177170305293561
Epoch 5: loss 1.0017658814650117
Epoch 6: loss 0.9969587348413701
Epoch 7: loss 0.9952162468543579
Epoch 8: loss 0.994392263445276
Epoch 9: loss 0.9940250862578195
Epoch 10: loss 0.9936916509064856
Epoch 11: loss 0.9935299208496108
Epoch 12: loss 0.9933457735798292
Epoch 13: loss 0.9931981535046045
Epoch 14: loss 0.9930051206537664
Epoch 15: loss 0.992774276953148
Epoch 16: loss 0.9926100171668948
Epoch 17: loss 0.9923264303079048
Epoch 18: loss 0.9921084414395772
Epoch 19: loss 0.9917880643185485
-----------Time: 0:05:19.459562, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 200, rmse: 1.0005388259887695-------------


Epoch 0: loss 15.368987694066655
Epoch 1: loss 13.881280299565791
Epoch 2: loss 12.201337653051711
Epoch 3: loss 9.65597469058615
Epoch 4: loss 6.342516810820114
Epoch 5: loss 3.4023405270708715
Epoch 6: loss 1.8123849512054067
Epoch 7: loss 1.310819397580695
Epoch 8: loss 1.1446170365927075
Epoch 9: loss 1.0688604144151603
Epoch 10: loss 1.030669107457877
Epoch 11: loss 1.0103962902078945
Epoch 12: loss 0.9991745368626663
Epoch 13: loss 0.992609119904696
Epoch 14: loss 0.9886077593337459
Epoch 15: loss 0.9859902702510973
Epoch 16: loss 0.9841570518242398
Epoch 17: loss 0.9828181446506122
Epoch 18: loss 0.9816963340906815
Epoch 19: loss 0.9807410138401407
-----------Time: 0:03:45.992227, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 20, rmse: 0.9997355937957764-------------


Epoch 0: loss 15.341109381608822
Epoch 1: loss 12.079330522382694
Epoch 2: loss 5.850732005180516
Epoch 3: loss 2.064379755506832
Epoch 4: loss 1.254555061354204
Epoch 5: loss 1.0825737440566643
Epoch 6: loss 1.025833665255038
Epoch 7: loss 1.0047785184382096
Epoch 8: loss 0.9962560459120627
Epoch 9: loss 0.9922987295759315
Epoch 10: loss 0.9904027295994201
Epoch 11: loss 0.9892334595136243
Epoch 12: loss 0.9885832120407658
Epoch 13: loss 0.9881359311048592
Epoch 14: loss 0.9877200005105036
Epoch 15: loss 0.9873591257930774
Epoch 16: loss 0.9870329270095783
Epoch 17: loss 0.9866758872627758
Epoch 18: loss 0.9863334073036374
Epoch 19: loss 0.9859633181910595
-----------Time: 0:03:57.340271, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 50, rmse: 0.9998279213905334-------------


Epoch 0: loss 14.777832913619141
Epoch 1: loss 6.706451608409954
Epoch 2: loss 1.735406680153141
Epoch 3: loss 1.1314957510556392
Epoch 4: loss 1.029899864351704
Epoch 5: loss 1.004680765427086
Epoch 6: loss 0.9971419018594515
Epoch 7: loss 0.9945053164728185
Epoch 8: loss 0.9932996934850038
Epoch 9: loss 0.9926697839175054
Epoch 10: loss 0.9922864033418741
Epoch 11: loss 0.991955647233908
Epoch 12: loss 0.9916384821355569
Epoch 13: loss 0.9913810225625995
Epoch 14: loss 0.9911020994348458
Epoch 15: loss 0.9907350336131096
Epoch 16: loss 0.9903835642428551
Epoch 17: loss 0.9900226282678005
Epoch 18: loss 0.9895714338283425
Epoch 19: loss 0.9891011985570857
-----------Time: 0:05:02.682091, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 100, rmse: 0.9999091029167175-------------


Epoch 0: loss 13.614645302328096
Epoch 1: loss 3.672951506978471
Epoch 2: loss 1.234674173749096
Epoch 3: loss 1.0411760856626862
Epoch 4: loss 1.0070646161161863
Epoch 5: loss 0.9990974086792072
Epoch 6: loss 0.9967941342001703
Epoch 7: loss 0.9958519066170936
Epoch 8: loss 0.9954014496838288
Epoch 9: loss 0.9950994106658845
Epoch 10: loss 0.994900241307813
Epoch 11: loss 0.9946368848940418
Epoch 12: loss 0.9944060726163179
Epoch 13: loss 0.9942275481323888
Epoch 14: loss 0.9938671302866715
Epoch 15: loss 0.9935212552936392
Epoch 16: loss 0.9931835883528464
Epoch 17: loss 0.9928290948004357
Epoch 18: loss 0.992284907099992
Epoch 19: loss 0.991819273492057
-----------Time: 0:05:31.017418, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 150, rmse: 1.0003407001495361-------------


Epoch 0: loss 12.476534109779386
Epoch 1: loss 2.4140301023798836
Epoch 2: loss 1.117402170941258
Epoch 3: loss 1.01871018268091
Epoch 4: loss 1.0028803680596499
Epoch 5: loss 0.9992255297544665
Epoch 6: loss 0.9981743344604612
Epoch 7: loss 0.9977232689861373
Epoch 8: loss 0.9971953644222251
Epoch 9: loss 0.9969812159858494
Epoch 10: loss 0.9968552948859414
Epoch 11: loss 0.9964837013800032
Epoch 12: loss 0.9961636985949422
Epoch 13: loss 0.995810102969165
Epoch 14: loss 0.9953727502904552
Epoch 15: loss 0.9947826283466823
Epoch 16: loss 0.9941795139030116
Epoch 17: loss 0.9934202698730398
Epoch 18: loss 0.9924539165726299
Epoch 19: loss 0.9913172500006462
-----------Time: 0:05:12.223877, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 200, rmse: 1.0003303289413452-------------


Epoch 0: loss 15.127557957541883
Epoch 1: loss 12.994514819005703
Epoch 2: loss 9.465613745813334
Epoch 3: loss 4.896388331011885
Epoch 4: loss 2.044671419426564
Epoch 5: loss 1.2861101615033506
Epoch 6: loss 1.105514002598259
Epoch 7: loss 1.0388049289316765
Epoch 8: loss 1.0109697262705377
Epoch 9: loss 0.9983259436065183
Epoch 10: loss 0.992189170777506
Epoch 11: loss 0.9889332924996335
Epoch 12: loss 0.9870277585025194
Epoch 13: loss 0.9858180286282752
Epoch 14: loss 0.9849093594001388
Epoch 15: loss 0.9842480033587476
Epoch 16: loss 0.9836194512473299
Epoch 17: loss 0.9830794385621702
Epoch 18: loss 0.9825112019108974
Epoch 19: loss 0.9819762127077923
-----------Time: 0:02:26.341899, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 20, rmse: 0.9996559619903564-------------


Epoch 0: loss 14.986838461070557
Epoch 1: loss 8.821457445913193
Epoch 2: loss 2.439521883187442
Epoch 3: loss 1.217457577416014
Epoch 4: loss 1.0515475755414605
Epoch 5: loss 1.0104562015945722
Epoch 6: loss 0.998228365260664
Epoch 7: loss 0.9939000923844898
Epoch 8: loss 0.9920724049542766
Epoch 9: loss 0.9911395638711171
Epoch 10: loss 0.9904264097242267
Epoch 11: loss 0.989937392037611
Epoch 12: loss 0.9894684484758216
Epoch 13: loss 0.9889602523546235
Epoch 14: loss 0.9884295295184562
Epoch 15: loss 0.9878824361645054
Epoch 16: loss 0.987240373991053
Epoch 17: loss 0.9865340792724397
Epoch 18: loss 0.985709325184052
Epoch 19: loss 0.984801008187279
-----------Time: 0:02:47.650343, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 50, rmse: 0.99921715259552-------------


Epoch 0: loss 13.455753951179002
Epoch 1: loss 3.282020416189757
Epoch 2: loss 1.1758264783707049
Epoch 3: loss 1.0280599223005182
Epoch 4: loss 1.003884988449526
Epoch 5: loss 0.9984427798598146
Epoch 6: loss 0.9968218161497121
Epoch 7: loss 0.9961632505715574
Epoch 8: loss 0.9956243923009901
Epoch 9: loss 0.995370863069329
Epoch 10: loss 0.9948316518705522
Epoch 11: loss 0.9945411899933807
Epoch 12: loss 0.9941143207798969
Epoch 13: loss 0.9935682085689609
Epoch 14: loss 0.9930436204249863
Epoch 15: loss 0.9922317579882893
Epoch 16: loss 0.9915528738544064
Epoch 17: loss 0.9905061237250676
Epoch 18: loss 0.9893395088171427
Epoch 19: loss 0.9879234769442861
-----------Time: 0:03:41.512350, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 100, rmse: 0.9994683861732483-------------


Epoch 0: loss 11.71211529199425
Epoch 1: loss 1.8466755702743198
Epoch 2: loss 1.0648363229857638
Epoch 3: loss 1.0101406055342055
Epoch 4: loss 1.0018562945806182
Epoch 5: loss 1.0001017269458117
Epoch 6: loss 0.9992404916547056
Epoch 7: loss 0.9988237297068218
Epoch 8: loss 0.9983751454095338
Epoch 9: loss 0.997787749689516
Epoch 10: loss 0.9971061337084923
Epoch 11: loss 0.9962724840077063
Epoch 12: loss 0.9952271134228471
Epoch 13: loss 0.9939965592285029
Epoch 14: loss 0.9924045472510682
Epoch 15: loss 0.9905655675118484
Epoch 16: loss 0.988385643289425
Epoch 17: loss 0.9860038361904607
Epoch 18: loss 0.9831949823281245
Epoch 19: loss 0.9802434907587019
-----------Time: 0:04:11.269491, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 150, rmse: 0.9972530603408813-------------


Epoch 0: loss 10.196129446252653
Epoch 1: loss 1.3911784474716167
Epoch 2: loss 1.0329004505881154
Epoch 3: loss 1.0070106206120455
Epoch 4: loss 1.0033372577083313
Epoch 5: loss 1.0025903836678656
Epoch 6: loss 1.002037905152561
Epoch 7: loss 1.0017235992621183
Epoch 8: loss 1.001368469343424
Epoch 9: loss 1.0010729431295733
Epoch 10: loss 1.0004589103077208
Epoch 11: loss 1.0001543593218691
Epoch 12: loss 0.9995975045810386
Epoch 13: loss 0.9989994547009273
Epoch 14: loss 0.9981201304048867
Epoch 15: loss 0.9970373279896685
Epoch 16: loss 0.9957465167568066
Epoch 17: loss 0.9940930330733879
Epoch 18: loss 0.9921825304343819
Epoch 19: loss 0.9895834215086656
-----------Time: 0:04:56.831439, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 200, rmse: 0.9999774098396301-------------


Epoch 0: loss 14.817113986800972
Epoch 1: loss 11.709280126052034
Epoch 2: loss 6.041334696604286
Epoch 3: loss 2.0058214957134566
Epoch 4: loss 1.1992535543480667
Epoch 5: loss 1.0561271715643874
Epoch 6: loss 1.0131454006558596
Epoch 7: loss 0.9981561612830077
Epoch 8: loss 0.9921576398069798
Epoch 9: loss 0.9893893732397112
Epoch 10: loss 0.9878391164713025
Epoch 11: loss 0.9867885938768092
Epoch 12: loss 0.986077722484097
Epoch 13: loss 0.9853440164514959
Epoch 14: loss 0.9846753628925502
Epoch 15: loss 0.9839687900513394
Epoch 16: loss 0.983227743847724
Epoch 17: loss 0.9824918116189139
Epoch 18: loss 0.9816416872623759
Epoch 19: loss 0.9807405260479145
-----------Time: 0:03:20.326948, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 20, rmse: 0.9995452761650085-------------


Epoch 0: loss 14.24400258466172
Epoch 1: loss 4.982518571009125
Epoch 2: loss 1.3060076352757821
Epoch 3: loss 1.0474181690249773
Epoch 4: loss 1.0069017451440854
Epoch 5: loss 0.9978785471562266
Epoch 6: loss 0.9951956742253234
Epoch 7: loss 0.9938816001811969
Epoch 8: loss 0.9930521130497005
Epoch 9: loss 0.9922437364961739
Epoch 10: loss 0.9913358687382149
Epoch 11: loss 0.9902669356334721
Epoch 12: loss 0.9891587274271616
Epoch 13: loss 0.9876547536005982
Epoch 14: loss 0.9859914552620923
Epoch 15: loss 0.9838211062065993
Epoch 16: loss 0.9817681095728478
Epoch 17: loss 0.9792271128907808
Epoch 18: loss 0.9765826508751247
Epoch 19: loss 0.9738899154336379
-----------Time: 0:03:17.929215, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 50, rmse: 0.9961205720901489-------------


Epoch 0: loss 11.52854378706998
Epoch 1: loss 1.677351303321632
Epoch 2: loss 1.047528418965026
Epoch 3: loss 1.0076488523200648
Epoch 4: loss 1.0018494326080427
Epoch 5: loss 1.0005465248410244
Epoch 6: loss 0.9996735700256738
Epoch 7: loss 0.9991332496748857
Epoch 8: loss 0.998286090317208
Epoch 9: loss 0.9975742628517327
Epoch 10: loss 0.996423542224045
Epoch 11: loss 0.9950656104891633
Epoch 12: loss 0.9933701734493581
Epoch 13: loss 0.9911689691209093
Epoch 14: loss 0.9887443449167923
Epoch 15: loss 0.9857396435452389
Epoch 16: loss 0.9824441178717517
Epoch 17: loss 0.978585242738926
Epoch 18: loss 0.9746685730756269
Epoch 19: loss 0.9703678524221915
-----------Time: 0:04:33.186795, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 100, rmse: 0.9957383871078491-------------


Epoch 0: loss 9.339828947110304
Epoch 1: loss 1.2266883164109452
Epoch 2: loss 1.0204384408935767
Epoch 3: loss 1.007120859840112
Epoch 4: loss 1.0052291967092226
Epoch 5: loss 1.0043635135470945
Epoch 6: loss 1.0039548674246874
Epoch 7: loss 1.0034098088125485
Epoch 8: loss 1.0028636834911835
Epoch 9: loss 1.0021582011167869
Epoch 10: loss 1.0012504663915982
Epoch 11: loss 1.0000622554795129
Epoch 12: loss 0.9986737770448242
Epoch 13: loss 0.9966479117712163
Epoch 14: loss 0.9942099760251566
Epoch 15: loss 0.9911074071814665
Epoch 16: loss 0.9870316234051112
Epoch 17: loss 0.9824200943801117
Epoch 18: loss 0.9768115212212055
Epoch 19: loss 0.9705547739656935
-----------Time: 0:05:02.645877, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 150, rmse: 0.9964680671691895-------------


Epoch 0: loss 8.174943429383978
Epoch 1: loss 1.125654421126862
Epoch 2: loss 1.0157914190852428
Epoch 3: loss 1.0091379339471986
Epoch 4: loss 1.00827278568733
Epoch 5: loss 1.0077267177666316
Epoch 6: loss 1.007008618994729
Epoch 7: loss 1.006394035016278
Epoch 8: loss 1.00573223561875
Epoch 9: loss 1.004795156577283
Epoch 10: loss 1.0036138922154352
Epoch 11: loss 1.0021683480864054
Epoch 12: loss 1.0000689562213623
Epoch 13: loss 0.9974273654544659
Epoch 14: loss 0.9937256496195044
Epoch 15: loss 0.9889090594518049
Epoch 16: loss 0.9831051107591232
Epoch 17: loss 0.9762915888838692
Epoch 18: loss 0.9680761404339551
Epoch 19: loss 0.9589706113670363
-----------Time: 0:05:44.298473, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 200, rmse: 0.9955987930297852-------------


Epoch 0: loss 14.408230869714302
Epoch 1: loss 9.629375312171467
Epoch 2: loss 2.9426533900699647
Epoch 3: loss 1.2149119049456536
Epoch 4: loss 1.0420291929092014
Epoch 5: loss 1.0049834863980092
Epoch 6: loss 0.994644036851803
Epoch 7: loss 0.9909131405404108
Epoch 8: loss 0.9890231006394398
Epoch 9: loss 0.9876128928923231
Epoch 10: loss 0.9864858192649466
Epoch 11: loss 0.9852634525253696
Epoch 12: loss 0.9839491907217504
Epoch 13: loss 0.9825520490323025
Epoch 14: loss 0.9808939885289336
Epoch 15: loss 0.979011179215886
Epoch 16: loss 0.976905290233888
Epoch 17: loss 0.9745141114794476
Epoch 18: loss 0.9717741256469355
Epoch 19: loss 0.9687538431545131
-----------Time: 0:04:18.176474, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 20, rmse: 0.9971795678138733-------------


Epoch 0: loss 12.7150557952578
Epoch 1: loss 2.2988606307933095
Epoch 2: loss 1.074097281713859
Epoch 3: loss 1.009511308834694
Epoch 4: loss 1.0005781826375295
Epoch 5: loss 0.9982554658947592
Epoch 6: loss 0.9971436895612402
Epoch 7: loss 0.9962182895297133
Epoch 8: loss 0.9950206908374587
Epoch 9: loss 0.9935400151992505
Epoch 10: loss 0.9917527032067038
Epoch 11: loss 0.9894390633211245
Epoch 12: loss 0.9869101847592354
Epoch 13: loss 0.9836074741655487
Epoch 14: loss 0.980270262157353
Epoch 15: loss 0.9764759099438114
Epoch 16: loss 0.9723585406161314
Epoch 17: loss 0.9681507336859991
Epoch 18: loss 0.9636644629630918
Epoch 19: loss 0.9589804396349298
-----------Time: 0:04:01.177027, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 50, rmse: 0.9940891265869141-------------


Epoch 0: loss 9.213436494319577
Epoch 1: loss 1.1809990911816954
Epoch 2: loss 1.0166662079507465
Epoch 3: loss 1.007133372261707
Epoch 4: loss 1.0052785395182235
Epoch 5: loss 1.004176584062789
Epoch 6: loss 1.0030663854049042
Epoch 7: loss 1.0011035487695905
Epoch 8: loss 0.9990324779267282
Epoch 9: loss 0.9956304429016922
Epoch 10: loss 0.9919057886940166
Epoch 11: loss 0.987293648032665
Epoch 12: loss 0.9822462518912674
Epoch 13: loss 0.9769502251837939
Epoch 14: loss 0.9716937058382932
Epoch 15: loss 0.9662391429462143
Epoch 16: loss 0.9602773656755638
Epoch 17: loss 0.9539237709846621
Epoch 18: loss 0.9463030699966652
Epoch 19: loss 0.9376443299247623
-----------Time: 0:03:39.932922, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 100, rmse: 0.9933109283447266-------------


Epoch 0: loss 7.513914285696609
Epoch 1: loss 1.0778404280113876
Epoch 2: loss 1.0146254035297328
Epoch 3: loss 1.011355861754052
Epoch 4: loss 1.010087317134324
Epoch 5: loss 1.0093002278011347
Epoch 6: loss 1.0078201966482767
Epoch 7: loss 1.0057815361632285
Epoch 8: loss 1.0030811608907255
Epoch 9: loss 0.999355657535639
Epoch 10: loss 0.9946144884244588
Epoch 11: loss 0.9892125609064958
Epoch 12: loss 0.9827701561699879
Epoch 13: loss 0.9762040300658372
Epoch 14: loss 0.968801948057043
Epoch 15: loss 0.9600116519774996
Epoch 16: loss 0.9499742140129508
Epoch 17: loss 0.9377317058936353
Epoch 18: loss 0.9224959768277916
Epoch 19: loss 0.9041431018714998
-----------Time: 0:03:48.778291, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 150, rmse: 0.9912912845611572-------------


Epoch 0: loss 6.50590677177731
Epoch 1: loss 1.049277731035636
Epoch 2: loss 1.0175476045696679
Epoch 3: loss 1.0154789935284947
Epoch 4: loss 1.014491727943327
Epoch 5: loss 1.0134276357307714
Epoch 6: loss 1.0123943333848784
Epoch 7: loss 1.0106351582526902
Epoch 8: loss 1.0082031376274985
Epoch 9: loss 1.0045506344209747
Epoch 10: loss 0.9997359574521735
Epoch 11: loss 0.9931244336275253
Epoch 12: loss 0.984612371478799
Epoch 13: loss 0.9748466210951294
Epoch 14: loss 0.9636554444951165
Epoch 15: loss 0.9503003726710827
Epoch 16: loss 0.9336289319575902
Epoch 17: loss 0.9137435415602689
Epoch 18: loss 0.8889432534891474
Epoch 19: loss 0.8593511450964838
-----------Time: 0:04:26.225461, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 200, rmse: 0.9901309013366699-------------


Epoch 0: loss 13.705809618870546
Epoch 1: loss 5.240367090883561
Epoch 2: loss 1.2703326019079157
Epoch 3: loss 1.0334473444172194
Epoch 4: loss 1.0010348366226043
Epoch 5: loss 0.9933738512244803
Epoch 6: loss 0.9901310849637036
Epoch 7: loss 0.9874817623172913
Epoch 8: loss 0.9849787110735243
Epoch 9: loss 0.98234560490951
Epoch 10: loss 0.9795432763296732
Epoch 11: loss 0.9764922778524738
Epoch 12: loss 0.97342720285844
Epoch 13: loss 0.970016749285044
Epoch 14: loss 0.9664156607566416
Epoch 15: loss 0.9625398567216821
Epoch 16: loss 0.9582910115722731
Epoch 17: loss 0.9536983991946262
Epoch 18: loss 0.9486581000249757
Epoch 19: loss 0.9433086075622015
-----------Time: 0:02:46.751380, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 20, rmse: 0.996571958065033-------------


Epoch 0: loss 10.2997448757461
Epoch 1: loss 1.2827510341410406
Epoch 2: loss 1.020084303429491
Epoch 3: loss 1.0055208620070892
Epoch 4: loss 1.0032714715530329
Epoch 5: loss 1.0021064271578133
Epoch 6: loss 1.0012513169651125
Epoch 7: loss 0.9999377946934018
Epoch 8: loss 0.9984833008410685
Epoch 9: loss 0.9965673034738237
Epoch 10: loss 0.994164807988354
Epoch 11: loss 0.9910144868624347
Epoch 12: loss 0.9871077516793297
Epoch 13: loss 0.9821588533640296
Epoch 14: loss 0.9762153049212814
Epoch 15: loss 0.9691826022144502
Epoch 16: loss 0.9613383745064613
Epoch 17: loss 0.9528285276312878
Epoch 18: loss 0.9433544323844972
Epoch 19: loss 0.932912139107573
-----------Time: 0:03:07.744887, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 50, rmse: 0.993554413318634-------------


Epoch 0: loss 7.26788618820332
Epoch 1: loss 1.06149030832119
Epoch 2: loss 1.014881658012806
Epoch 3: loss 1.012501531981981
Epoch 4: loss 1.0110548676324318
Epoch 5: loss 1.0095708164882244
Epoch 6: loss 1.0073427761489118
Epoch 7: loss 1.0046231635502332
Epoch 8: loss 0.9999567795668431
Epoch 9: loss 0.9946356149002415
Epoch 10: loss 0.9876286081155401
Epoch 11: loss 0.9795166071405095
Epoch 12: loss 0.9714998761359086
Epoch 13: loss 0.9621558932424434
Epoch 14: loss 0.9513116807572021
Epoch 15: loss 0.9379378024685182
Epoch 16: loss 0.9212816579144827
Epoch 17: loss 0.9005160869593981
Epoch 18: loss 0.8755667637780414
Epoch 19: loss 0.846458385242205
-----------Time: 0:04:02.153986, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 100, rmse: 0.9912878274917603-------------


Epoch 0: loss 6.0192188120458745
Epoch 1: loss 1.0373834805880118
Epoch 2: loss 1.0197555902472264
Epoch 3: loss 1.0182529640884876
Epoch 4: loss 1.0165811991840683
Epoch 5: loss 1.0142868880897582
Epoch 6: loss 1.0108443827911717
Epoch 7: loss 1.005575682884847
Epoch 8: loss 0.998731297244839
Epoch 9: loss 0.9898909020028469
Epoch 10: loss 0.9802538470738346
Epoch 11: loss 0.9694056924819427
Epoch 12: loss 0.9559641089401795
Epoch 13: loss 0.9389944862029421
Epoch 14: loss 0.9164036156497227
Epoch 15: loss 0.8881693032204035
Epoch 16: loss 0.853357715166542
Epoch 17: loss 0.812432624257862
Epoch 18: loss 0.7647784226059978
Epoch 19: loss 0.7120381891808476
-----------Time: 0:04:36.074379, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 150, rmse: 0.9935706853866577-------------


Epoch 0: loss 5.241583063833866
Epoch 1: loss 1.0335677725066952
Epoch 2: loss 1.0249961738744593
Epoch 3: loss 1.0231829515580835
Epoch 4: loss 1.0206588007562376
Epoch 5: loss 1.0174249923378311
Epoch 6: loss 1.0116639284740134
Epoch 7: loss 1.0054719968293269
Epoch 8: loss 0.9970194882287352
Epoch 9: loss 0.9870111848946302
Epoch 10: loss 0.975080081047475
Epoch 11: loss 0.9589617125945223
Epoch 12: loss 0.9372760018879723
Epoch 13: loss 0.9089300274654469
Epoch 14: loss 0.8720781056371443
Epoch 15: loss 0.8253432060629858
Epoch 16: loss 0.7691196845626623
Epoch 17: loss 0.7042045653384946
Epoch 18: loss 0.6335579939530944
Epoch 19: loss 0.5604113003136737
-----------Time: 0:05:14.378389, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 200, rmse: 1.0036282539367676-------------


Epoch 0: loss 12.494448272866876
Epoch 1: loss 2.4538667491105928
Epoch 2: loss 1.0562300984790753
Epoch 3: loss 1.004762266400458
Epoch 4: loss 0.9981705115955118
Epoch 5: loss 0.9956578925746236
Epoch 6: loss 0.9938359055556701
Epoch 7: loss 0.9917676261807992
Epoch 8: loss 0.9893813294162563
Epoch 9: loss 0.986303685118544
Epoch 10: loss 0.9827312698274803
Epoch 11: loss 0.9781856119081727
Epoch 12: loss 0.973021043909186
Epoch 13: loss 0.9670043212684228
Epoch 14: loss 0.9601564111756267
Epoch 15: loss 0.9526400978212579
Epoch 16: loss 0.9448789901030719
Epoch 17: loss 0.9365239792086626
Epoch 18: loss 0.9275340584712939
Epoch 19: loss 0.9182761858474178
-----------Time: 0:03:48.856433, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 20, rmse: 0.9950449466705322-------------


Epoch 0: loss 8.323325425791571
Epoch 1: loss 1.086443338042825
Epoch 2: loss 1.01318806795532
Epoch 3: loss 1.0090161765671866
Epoch 4: loss 1.0071598016385996
Epoch 5: loss 1.0053288563559275
Epoch 6: loss 1.002625207187301
Epoch 7: loss 0.9986773155641789
Epoch 8: loss 0.9930677943381361
Epoch 9: loss 0.9865070449197467
Epoch 10: loss 0.9785111473914785
Epoch 11: loss 0.9696990462377578
Epoch 12: loss 0.9595998816188098
Epoch 13: loss 0.9486595603387727
Epoch 14: loss 0.9351680729621257
Epoch 15: loss 0.9195138247943949
Epoch 16: loss 0.9013111807361383
Epoch 17: loss 0.87990090409785
Epoch 18: loss 0.8560380659037274
Epoch 19: loss 0.8300238111020432
-----------Time: 0:03:39.533508, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 50, rmse: 0.9949411153793335-------------


Epoch 0: loss 5.843552035812841
Epoch 1: loss 1.0342704202086463
Epoch 2: loss 1.021223692562737
Epoch 3: loss 1.0195367708518623
Epoch 4: loss 1.017140744787769
Epoch 5: loss 1.0141639340468371
Epoch 6: loss 1.0088515044587536
Epoch 7: loss 1.0010325006164242
Epoch 8: loss 0.9908068842574136
Epoch 9: loss 0.979221049455147
Epoch 10: loss 0.9645906432515062
Epoch 11: loss 0.946889001270427
Epoch 12: loss 0.9241380562368717
Epoch 13: loss 0.8953494081327357
Epoch 14: loss 0.8595255146764296
Epoch 15: loss 0.817244643958254
Epoch 16: loss 0.7683721735563014
Epoch 17: loss 0.7155976048982163
Epoch 18: loss 0.6603406008865731
Epoch 19: loss 0.6051414610738013
-----------Time: 0:04:52.917957, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 100, rmse: 1.012852668762207-------------


Epoch 0: loss 4.911194397613625
Epoch 1: loss 1.0340676384493344
Epoch 2: loss 1.0279532022421745
Epoch 3: loss 1.024026404488923
Epoch 4: loss 1.017905535598758
Epoch 5: loss 1.0085669957048677
Epoch 6: loss 0.9982999266814512
Epoch 7: loss 0.9877824845231052
Epoch 8: loss 0.9744876006363915
Epoch 9: loss 0.9560748243733811
Epoch 10: loss 0.9300355043276424
Epoch 11: loss 0.894827398562185
Epoch 12: loss 0.8501548224036106
Epoch 13: loss 0.7949942581967337
Epoch 14: loss 0.7301941663756067
Epoch 15: loss 0.6582650360431018
Epoch 16: loss 0.5834279727715393
Epoch 17: loss 0.5097189738074355
Epoch 18: loss 0.44173690511390784
Epoch 19: loss 0.3811950592031422
-----------Time: 0:05:31.648850, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 150, rmse: 1.0512183904647827-------------


Epoch 0: loss 4.349327409491065
Epoch 1: loss 1.038724958021528
Epoch 2: loss 1.0344265917677669
Epoch 3: loss 1.0302780344636626
Epoch 4: loss 1.0222656293009986
Epoch 5: loss 1.0121630461336544
Epoch 6: loss 0.9996481025556042
Epoch 7: loss 0.9855323183173521
Epoch 8: loss 0.9647372513442275
Epoch 9: loss 0.9339943857313564
Epoch 10: loss 0.8895019366567455
Epoch 11: loss 0.8306483760224924
Epoch 12: loss 0.7570792139548851
Epoch 13: loss 0.671316121448117
Epoch 14: loss 0.5790942945173735
Epoch 15: loss 0.4879582936573184
Epoch 16: loss 0.40365321532726417
Epoch 17: loss 0.33061880240335356
Epoch 18: loss 0.2692918020253469
Epoch 19: loss 0.21919037666234586
-----------Time: 0:05:44.231585, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 200, rmse: 1.0806927680969238-------------


Epoch 0: loss 10.379116951479348
Epoch 1: loss 1.2658103902648492
Epoch 2: loss 1.0145505059342335
Epoch 3: loss 1.003723148615092
Epoch 4: loss 1.0013058130315882
Epoch 5: loss 0.9991722350991764
Epoch 6: loss 0.9968110523417049
Epoch 7: loss 0.9937960357041123
Epoch 8: loss 0.989728572139797
Epoch 9: loss 0.9845757529163828
Epoch 10: loss 0.9778778521083761
Epoch 11: loss 0.9700852497366863
Epoch 12: loss 0.9609579929292947
Epoch 13: loss 0.9508486160154638
Epoch 14: loss 0.9397035934731908
Epoch 15: loss 0.927649200772902
Epoch 16: loss 0.9149909175045144
Epoch 17: loss 0.9013144851962531
Epoch 18: loss 0.8872816031429027
Epoch 19: loss 0.8727193685443976
-----------Time: 0:02:26.046081, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 20, rmse: 1.0001453161239624-------------


Epoch 0: loss 6.490119142075995
Epoch 1: loss 1.0363726551731103
Epoch 2: loss 1.0179692444560189
Epoch 3: loss 1.0153769309783902
Epoch 4: loss 1.0128808447269722
Epoch 5: loss 1.0094536672700287
Epoch 6: loss 1.0034690198041356
Epoch 7: loss 0.995213120187217
Epoch 8: loss 0.9843600125271319
Epoch 9: loss 0.9708751947773482
Epoch 10: loss 0.9547399154754357
Epoch 11: loss 0.9357709674293285
Epoch 12: loss 0.9129051810992419
Epoch 13: loss 0.885385327490858
Epoch 14: loss 0.853775668769767
Epoch 15: loss 0.818327243445424
Epoch 16: loss 0.7809379409356504
Epoch 17: loss 0.7425036140114409
Epoch 18: loss 0.7052308565948631
Epoch 19: loss 0.6705767304807204
-----------Time: 0:02:46.705307, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 50, rmse: 1.0266011953353882-------------


Epoch 0: loss 4.759572211104025
Epoch 1: loss 1.0351516448848594
Epoch 2: loss 1.0292062391691585
Epoch 3: loss 1.0229522146207604
Epoch 4: loss 1.0136731085471318
Epoch 5: loss 1.0026490231484677
Epoch 6: loss 0.9902928170300879
Epoch 7: loss 0.9752172427601371
Epoch 8: loss 0.9529160165864531
Epoch 9: loss 0.922220574668596
Epoch 10: loss 0.8808972334621911
Epoch 11: loss 0.8298622326821072
Epoch 12: loss 0.7695334537999017
Epoch 13: loss 0.7024916254871237
Epoch 14: loss 0.6328824831860554
Epoch 15: loss 0.5652321478703413
Epoch 16: loss 0.5034735120788354
Epoch 17: loss 0.44968995511888354
Epoch 18: loss 0.4041257000864038
Epoch 19: loss 0.3658605401179854
-----------Time: 0:03:35.166065, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 100, rmse: 1.1014859676361084-------------


Epoch 0: loss 4.0671068981103495
Epoch 1: loss 1.0415945545430414
Epoch 2: loss 1.0308814464443854
Epoch 3: loss 1.0142111515564527
Epoch 4: loss 0.9969733021809006
Epoch 5: loss 0.9809872872353638
Epoch 6: loss 0.96335514802982
Epoch 7: loss 0.9365411438068143
Epoch 8: loss 0.8963654798421605
Epoch 9: loss 0.8380908939162955
Epoch 10: loss 0.7620232863844965
Epoch 11: loss 0.6721280374123131
Epoch 12: loss 0.5768056023681598
Epoch 13: loss 0.4848990117064502
Epoch 14: loss 0.4035734400242292
Epoch 15: loss 0.33526875840752196
Epoch 16: loss 0.2798662027825215
Epoch 17: loss 0.23589579575472774
Epoch 18: loss 0.20084241058934701
Epoch 19: loss 0.17327725876359723
-----------Time: 0:04:12.641242, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 150, rmse: 1.1532739400863647-------------


Epoch 0: loss 3.654380527104678
Epoch 1: loss 1.0511610067331512
Epoch 2: loss 1.0441620102584719
Epoch 3: loss 1.031258779935956
Epoch 4: loss 1.0154424060000358
Epoch 5: loss 0.9964188715081425
Epoch 6: loss 0.9663458448541495
Epoch 7: loss 0.9174345450728013
Epoch 8: loss 0.8461631714908762
Epoch 9: loss 0.7500528136046443
Epoch 10: loss 0.635565899640662
Epoch 11: loss 0.5166755332092412
Epoch 12: loss 0.4076543184662721
Epoch 13: loss 0.31686668037201415
Epoch 14: loss 0.2451696862558297
Epoch 15: loss 0.19047472333254992
Epoch 16: loss 0.14942301514235898
Epoch 17: loss 0.1187332094226148
Epoch 18: loss 0.09568297526434007
Epoch 19: loss 0.07844379267241561
-----------Time: 0:04:50.668152, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 200, rmse: 1.1651791334152222-------------


Epoch 0: loss 8.37505008271364
Epoch 1: loss 1.0714254404553387
Epoch 2: loss 1.0106087279034544
Epoch 3: loss 1.0059698805528987
Epoch 4: loss 1.0020748698367834
Epoch 5: loss 0.9967145770707164
Epoch 6: loss 0.9893991078547384
Epoch 7: loss 0.9798895439063161
Epoch 8: loss 0.9681970303865021
Epoch 9: loss 0.9549875615601179
Epoch 10: loss 0.9405482612983762
Epoch 11: loss 0.9246452691731342
Epoch 12: loss 0.9081175720938008
Epoch 13: loss 0.8915276897543472
Epoch 14: loss 0.8749523225395883
Epoch 15: loss 0.8588201006577675
Epoch 16: loss 0.8436134047421378
Epoch 17: loss 0.8292801881724561
Epoch 18: loss 0.8157529223866279
Epoch 19: loss 0.8035162953075473
-----------Time: 0:03:20.049368, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 20, rmse: 1.0207644701004028-------------


Epoch 0: loss 5.2977863748220075
Epoch 1: loss 1.0311185108247563
Epoch 2: loss 1.0237800331463174
Epoch 3: loss 1.0168171677821483
Epoch 4: loss 1.006518084258084
Epoch 5: loss 0.9942321518862487
Epoch 6: loss 0.9804023963203243
Epoch 7: loss 0.9623200988043515
Epoch 8: loss 0.9380807709830037
Epoch 9: loss 0.9077705119277162
Epoch 10: loss 0.8714972255896329
Epoch 11: loss 0.8305851919389147
Epoch 12: loss 0.7867963500584254
Epoch 13: loss 0.7418281129273206
Epoch 14: loss 0.6980336384808259
Epoch 15: loss 0.6583388699517424
Epoch 16: loss 0.6230113126536049
Epoch 17: loss 0.5930752478146954
Epoch 18: loss 0.5682638456975072
Epoch 19: loss 0.5472106687428836
-----------Time: 0:03:18.081768, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 50, rmse: 1.0973256826400757-------------


Epoch 0: loss 3.9830912197978035
Epoch 1: loss 1.044270674095911
Epoch 2: loss 1.0336350122342102
Epoch 3: loss 1.019203504979967
Epoch 4: loss 1.0041564832633445
Epoch 5: loss 0.9846749523205884
Epoch 6: loss 0.9552860478300839
Epoch 7: loss 0.9108040535527769
Epoch 8: loss 0.8483960890231929
Epoch 9: loss 0.7694071194216763
Epoch 10: loss 0.6805766192287644
Epoch 11: loss 0.5911940280884878
Epoch 12: loss 0.5108339587118135
Epoch 13: loss 0.44367623856658844
Epoch 14: loss 0.3900161131389962
Epoch 15: loss 0.3473121695759311
Epoch 16: loss 0.3138330492022376
Epoch 17: loss 0.2868275802142922
Epoch 18: loss 0.265051308316404
Epoch 19: loss 0.24696278137364033
-----------Time: 0:04:26.813585, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 100, rmse: 1.2065787315368652-------------


Epoch 0: loss 3.4297996607242944
Epoch 1: loss 1.0569286787069123
Epoch 2: loss 1.0446531845863387
Epoch 3: loss 1.0267908193774689
Epoch 4: loss 1.004390255516155
Epoch 5: loss 0.96828868489281
Epoch 6: loss 0.907333789505733
Epoch 7: loss 0.8198640879695347
Epoch 8: loss 0.7093943502022689
Epoch 9: loss 0.5872821514861878
Epoch 10: loss 0.47133654180170725
Epoch 11: loss 0.3746237877337034
Epoch 12: loss 0.29941592179542653
Epoch 13: loss 0.2430186955629969
Epoch 14: loss 0.20095823024758636
Epoch 15: loss 0.16961465335778725
Epoch 16: loss 0.14565021666440386
Epoch 17: loss 0.12723463248054317
Epoch 18: loss 0.11250340461714751
Epoch 19: loss 0.10064732515602057
-----------Time: 0:05:02.876956, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 150, rmse: 1.244436264038086-------------


Epoch 0: loss 3.163585572559721
Epoch 1: loss 1.0677810689883105
Epoch 2: loss 1.0518483404007342
Epoch 3: loss 1.0292297397021564
Epoch 4: loss 0.9953798459333334
Epoch 5: loss 0.934345070031624
Epoch 6: loss 0.8339870838350159
Epoch 7: loss 0.6974459411642226
Epoch 8: loss 0.5470691811558732
Epoch 9: loss 0.4094293943204719
Epoch 10: loss 0.3000544883744298
Epoch 11: loss 0.21996933854650452
Epoch 12: loss 0.1639965895427641
Epoch 13: loss 0.12524844880675154
Epoch 14: loss 0.09801769256794368
Epoch 15: loss 0.07892779469725034
Epoch 16: loss 0.06519921525680805
Epoch 17: loss 0.05504394167058814
Epoch 18: loss 0.04749139999214189
Epoch 19: loss 0.04165784473408899
-----------Time: 0:05:38.297850, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 200, rmse: 1.2281384468078613-------------


Epoch 0: loss 6.725858194209623
Epoch 1: loss 1.0312251152714806
Epoch 2: loss 1.0116221566379167
Epoch 3: loss 1.0015891357697242
Epoch 4: loss 0.9893161850237471
Epoch 5: loss 0.9765644441032099
Epoch 6: loss 0.9632778250697905
Epoch 7: loss 0.9488866809368652
Epoch 8: loss 0.9319134223162966
Epoch 9: loss 0.9138053143685638
Epoch 10: loss 0.8946195164732857
Epoch 11: loss 0.8747984855281328
Epoch 12: loss 0.8559180996449114
Epoch 13: loss 0.8374078917788578
Epoch 14: loss 0.8204747576179422
Epoch 15: loss 0.8050557929189909
Epoch 16: loss 0.7919525467973484
Epoch 17: loss 0.7808228848808936
Epoch 18: loss 0.7713510851421843
Epoch 19: loss 0.7629016418700766
-----------Time: 0:04:15.691889, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 20, rmse: 1.051849603652954-------------


Epoch 0: loss 4.313089919210193
Epoch 1: loss 1.0393032852465074
Epoch 2: loss 1.030352609062921
Epoch 3: loss 1.0156158407691251
Epoch 4: loss 0.999959024658924
Epoch 5: loss 0.9799791944292721
Epoch 6: loss 0.9519681113773614
Epoch 7: loss 0.9117170204735113
Epoch 8: loss 0.8601710528637677
Epoch 9: loss 0.8013522474742442
Epoch 10: loss 0.7412528119183157
Epoch 11: loss 0.6857583867978505
Epoch 12: loss 0.6389937445302577
Epoch 13: loss 0.6012406451278176
Epoch 14: loss 0.5715093949668884
Epoch 15: loss 0.5480108605723073
Epoch 16: loss 0.5289005149781801
Epoch 17: loss 0.5132789326514674
Epoch 18: loss 0.5002595065581922
Epoch 19: loss 0.4891545375998478
-----------Time: 0:04:11.017781, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 50, rmse: 1.1707172393798828-------------


Epoch 0: loss 3.38531324291696
Epoch 1: loss 1.0498722512771281
Epoch 2: loss 1.023699998320802
Epoch 3: loss 0.9919531381344134
Epoch 4: loss 0.9498845401537555
Epoch 5: loss 0.8952304661565659
Epoch 6: loss 0.8230737621139871
Epoch 7: loss 0.7304920296003924
Epoch 8: loss 0.6270472115394925
Epoch 9: loss 0.529293672546801
Epoch 10: loss 0.4483725043290202
Epoch 11: loss 0.38618512863274046
Epoch 12: loss 0.3392890767681009
Epoch 13: loss 0.3040635899163057
Epoch 14: loss 0.2767006979162373
Epoch 15: loss 0.25511003626776885
Epoch 16: loss 0.23779841685437716
Epoch 17: loss 0.22343675444604522
Epoch 18: loss 0.211280853030246
Epoch 19: loss 0.2010389242307073
-----------Time: 0:04:07.812648, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 100, rmse: 1.3042879104614258-------------


Epoch 0: loss 3.008926279603819
Epoch 1: loss 1.0690999332655378
Epoch 2: loss 1.0421471960554958
Epoch 3: loss 1.0028829697262276
Epoch 4: loss 0.9343943620194813
Epoch 5: loss 0.8287077861969208
Epoch 6: loss 0.6900386880797624
Epoch 7: loss 0.5420501532763357
Epoch 8: loss 0.4136844328348892
Epoch 9: loss 0.31640267852225856
Epoch 10: loss 0.2476135608357797
Epoch 11: loss 0.1999072058449432
Epoch 12: loss 0.16632564127031976
Epoch 13: loss 0.14206911420906412
Epoch 14: loss 0.12363530604179915
Epoch 15: loss 0.10965023124960209
Epoch 16: loss 0.09845295968471078
Epoch 17: loss 0.08944308244668316
Epoch 18: loss 0.08201868880931096
Epoch 19: loss 0.07591432707344727
-----------Time: 0:03:48.704401, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 150, rmse: 1.3203026056289673-------------


Epoch 0: loss 2.7929507456699616
Epoch 1: loss 1.0757417211135876
Epoch 2: loss 1.036728774596972
Epoch 3: loss 0.9807829906137953
Epoch 4: loss 0.8792046642193267
Epoch 5: loss 0.7201909310406741
Epoch 6: loss 0.5351816040381198
Epoch 7: loss 0.37370688522101875
Epoch 8: loss 0.2577551982126008
Epoch 9: loss 0.18206756537483398
Epoch 10: loss 0.134244183478487
Epoch 11: loss 0.10376773147057294
Epoch 12: loss 0.08376024552945568
Epoch 13: loss 0.06994378442970743
Epoch 14: loss 0.06044510398677926
Epoch 15: loss 0.0534216178314737
Epoch 16: loss 0.048232355866583226
Epoch 17: loss 0.04445066679797787
Epoch 18: loss 0.041356016891892514
Epoch 19: loss 0.039045938162529124
-----------Time: 0:04:19.542652, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 200, rmse: 1.2547684907913208-------------


Epoch 0: loss 5.475721719992297
Epoch 1: loss 1.0248060729628092
Epoch 2: loss 1.0039714190090008
Epoch 3: loss 0.9792112014973015
Epoch 4: loss 0.9577793492578566
Epoch 5: loss 0.9389704672651099
Epoch 6: loss 0.9210098166061265
Epoch 7: loss 0.9013894378587952
Epoch 8: loss 0.8803262357772725
Epoch 9: loss 0.8583973646488054
Epoch 10: loss 0.8368450191890888
Epoch 11: loss 0.8175497840220932
Epoch 12: loss 0.8009295989275627
Epoch 13: loss 0.7871103980672172
Epoch 14: loss 0.7760296619379242
Epoch 15: loss 0.7667754615630191
Epoch 16: loss 0.7595187007343723
Epoch 17: loss 0.7531712202597338
Epoch 18: loss 0.748011636524955
Epoch 19: loss 0.7434259027955712
-----------Time: 0:02:46.069330, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0735211372375488-------------


Epoch 0: loss 3.602039344920744
Epoch 1: loss 1.0493359593446647
Epoch 2: loss 1.0305992074638946
Epoch 3: loss 1.0110072867743278
Epoch 4: loss 0.9828854509512324
Epoch 5: loss 0.9368599506258122
Epoch 6: loss 0.8724322257449019
Epoch 7: loss 0.7955635839211286
Epoch 8: loss 0.7210693311730177
Epoch 9: loss 0.6591970055258228
Epoch 10: loss 0.61184527361568
Epoch 11: loss 0.5772288531761833
Epoch 12: loss 0.5512065632697225
Epoch 13: loss 0.5310633906840371
Epoch 14: loss 0.5148081631764826
Epoch 15: loss 0.5015009078336449
Epoch 16: loss 0.4901614933152637
Epoch 17: loss 0.48061268448894473
Epoch 18: loss 0.4723139344560121
Epoch 19: loss 0.4649023600267029
-----------Time: 0:03:07.308163, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.2312203645706177-------------


Epoch 0: loss 2.9341050406685985
Epoch 1: loss 1.0765652486557153
Epoch 2: loss 1.0469360212517407
Epoch 3: loss 1.000413795928063
Epoch 4: loss 0.9149940188907294
Epoch 5: loss 0.7887805788785365
Epoch 6: loss 0.6457589362677574
Epoch 7: loss 0.5213529124912715
Epoch 8: loss 0.42974380409023955
Epoch 9: loss 0.36633648648412415
Epoch 10: loss 0.3221640652283629
Epoch 11: loss 0.28949218355585143
Epoch 12: loss 0.2653550786402253
Epoch 13: loss 0.24625814374680102
Epoch 14: loss 0.23102763796740736
Epoch 15: loss 0.21821655313595278
Epoch 16: loss 0.2074691207675327
Epoch 17: loss 0.1981956072398798
Epoch 18: loss 0.1904956787609905
Epoch 19: loss 0.18328491910327835
-----------Time: 0:04:00.705388, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.3742961883544922-------------


Epoch 0: loss 2.671140797375466
Epoch 1: loss 1.080607772926846
Epoch 2: loss 1.0251974873305534
Epoch 3: loss 0.9286739221404855
Epoch 4: loss 0.7763745096988688
Epoch 5: loss 0.5927011851158526
Epoch 6: loss 0.43155667011571874
Epoch 7: loss 0.3168468963377109
Epoch 8: loss 0.24241034454823576
Epoch 9: loss 0.19496905139043255
Epoch 10: loss 0.16322231347824062
Epoch 11: loss 0.140757025074155
Epoch 12: loss 0.12431523211142367
Epoch 13: loss 0.11189005576892512
Epoch 14: loss 0.10199037774416854
Epoch 15: loss 0.09427304095535449
Epoch 16: loss 0.08757425306740664
Epoch 17: loss 0.08235730673325749
Epoch 18: loss 0.07777478641517918
Epoch 19: loss 0.07381329086749465
-----------Time: 0:04:34.892083, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.3596680164337158-------------


Epoch 0: loss 2.5424278498408976
Epoch 1: loss 1.0815426515878186
Epoch 2: loss 0.9896904937011383
Epoch 3: loss 0.8298216531657862
Epoch 4: loss 0.6225352590177421
Epoch 5: loss 0.4231117012028852
Epoch 6: loss 0.27838475475530383
Epoch 7: loss 0.19026257357620363
Epoch 8: loss 0.14010303948619496
Epoch 9: loss 0.1105226128189427
Epoch 10: loss 0.09226798230924964
Epoch 11: loss 0.08008953058801474
Epoch 12: loss 0.07165439339995773
Epoch 13: loss 0.06588708409607054
Epoch 14: loss 0.06148010508834408
Epoch 15: loss 0.05791992756430224
Epoch 16: loss 0.055255278764680525
Epoch 17: loss 0.05308315322074539
Epoch 18: loss 0.051243141963024576
Epoch 19: loss 0.04967723802682124
-----------Time: 0:05:08.358815, Loss: regression, n_iter: 20, l2: 1e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.262505054473877-------------


Epoch 0: loss 16.129191126105187
Epoch 1: loss 16.129191645984744
Epoch 2: loss 16.129186052288162
Epoch 3: loss 16.12918778487429
Epoch 4: loss 16.129189488419765
Epoch 5: loss 16.129186677180794
Epoch 6: loss 16.12918759896225
Epoch 7: loss 16.129192396374485
Epoch 8: loss 16.12919333604706
Epoch 9: loss 16.12918471927031
Epoch 10: loss 16.12918356931229
Epoch 11: loss 16.129191729735915
Epoch 12: loss 16.12918868591242
Epoch 13: loss 16.129195711416937
Epoch 14: loss 16.12919658756308
Epoch 15: loss 16.129192471828325
Epoch 16: loss 16.129183073287557
Epoch 17: loss 16.129187074156157
Epoch 18: loss 16.129193034231694
Epoch 19: loss 16.129188624201035
-----------Time: 0:03:47.039378, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 20, rmse: 4.017178535461426-------------


Epoch 0: loss 16.129197612542573
Epoch 1: loss 16.129205768039654
Epoch 2: loss 16.12920578048565
Epoch 3: loss 16.129204704944307
Epoch 4: loss 16.129198680045878
Epoch 5: loss 16.12920270139851
Epoch 6: loss 16.129206627331847
Epoch 7: loss 16.12920473398496
Epoch 8: loss 16.129200505976964
Epoch 9: loss 16.129205833121834
Epoch 10: loss 16.1291985260267
Epoch 11: loss 16.129202574086356
Epoch 12: loss 16.12919652714815
Epoch 13: loss 16.129207266485516
Epoch 14: loss 16.129200663885516
Epoch 15: loss 16.129207363460555
Epoch 16: loss 16.12920305014564
Epoch 17: loss 16.129203407190104
Epoch 18: loss 16.129198937263094
Epoch 19: loss 16.129202638649954
-----------Time: 0:03:51.703807, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 50, rmse: 4.017184257507324-------------


Epoch 0: loss 16.129186754449677
Epoch 1: loss 16.129182347789804
Epoch 2: loss 16.129186221864835
Epoch 3: loss 16.129189502680802
Epoch 4: loss 16.129181669223822
Epoch 5: loss 16.129186702072783
Epoch 6: loss 16.12919065056449
Epoch 7: loss 16.129186628693276
Epoch 8: loss 16.129183486857578
Epoch 9: loss 16.129185552114762
Epoch 10: loss 16.12918057916215
Epoch 11: loss 16.12918894053672
Epoch 12: loss 16.129181731713086
Epoch 13: loss 16.12917667163851
Epoch 14: loss 16.129183146407772
Epoch 15: loss 16.129181441825132
Epoch 16: loss 16.12918378659861
Epoch 17: loss 16.12918488184611
Epoch 18: loss 16.12918274528375
Epoch 19: loss 16.129188842005934
-----------Time: 0:04:50.784220, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129168227549933
Epoch 1: loss 16.129172121849702
Epoch 2: loss 16.12917377224041
Epoch 3: loss 16.129165306889927
Epoch 4: loss 16.129168457282244
Epoch 5: loss 16.129167400928473
Epoch 6: loss 16.12916823377293
Epoch 7: loss 16.129174406726833
Epoch 8: loss 16.129164745783015
Epoch 9: loss 16.129167907843453
Epoch 10: loss 16.12916877284006
Epoch 11: loss 16.129169354949585
Epoch 12: loss 16.12916763766166
Epoch 13: loss 16.12916749427343
Epoch 14: loss 16.12917504328758
Epoch 15: loss 16.129167742156152
Epoch 16: loss 16.129167323918885
Epoch 17: loss 16.129174966537285
Epoch 18: loss 16.129165982603702
Epoch 19: loss 16.129172726258304
-----------Time: 0:05:33.377091, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12916938606457
Epoch 1: loss 16.129170970335934
Epoch 2: loss 16.12917029980799
Epoch 3: loss 16.129179957640307
Epoch 4: loss 16.12916498925778
Epoch 5: loss 16.129172261348558
Epoch 6: loss 16.129169600757976
Epoch 7: loss 16.129173252101562
Epoch 8: loss 16.129170766273486
Epoch 9: loss 16.129175914247895
Epoch 10: loss 16.129174926606385
Epoch 11: loss 16.129166576122056
Epoch 12: loss 16.129176887369077
Epoch 13: loss 16.129172155298313
Epoch 14: loss 16.129173990304604
Epoch 15: loss 16.129175876132038
Epoch 16: loss 16.129173609146026
Epoch 17: loss 16.1291764110505
Epoch 18: loss 16.1291700418129
Epoch 19: loss 16.12917110672329
-----------Time: 0:05:57.615077, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12920607530014
Epoch 1: loss 16.129206487314413
Epoch 2: loss 16.129202571752735
Epoch 3: loss 16.12920891946914
Epoch 4: loss 16.129201654638525
Epoch 5: loss 16.12920100744682
Epoch 6: loss 16.129203089298667
Epoch 7: loss 16.129205425256227
Epoch 8: loss 16.129204395609488
Epoch 9: loss 16.12920922206238
Epoch 10: loss 16.12920316838259
Epoch 11: loss 16.129207477289544
Epoch 12: loss 16.129207129060994
Epoch 13: loss 16.129197859128833
Epoch 14: loss 16.12921044954857
Epoch 15: loss 16.129203071148257
Epoch 16: loss 16.129208488785878
Epoch 17: loss 16.129199148845
Epoch 18: loss 16.129204922230624
Epoch 19: loss 16.129203757233697
-----------Time: 0:02:26.933965, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 20, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12916144163081
Epoch 1: loss 16.129171500846443
Epoch 2: loss 16.12917053783763
Epoch 3: loss 16.12917046238379
Epoch 4: loss 16.129172669473455
Epoch 5: loss 16.129164265056485
Epoch 6: loss 16.129166505854048
Epoch 7: loss 16.129167658923567
Epoch 8: loss 16.12916417041507
Epoch 9: loss 16.12916499599936
Epoch 10: loss 16.12916201933238
Epoch 11: loss 16.12916361630903
Epoch 12: loss 16.129160050013066
Epoch 13: loss 16.12917343179061
Epoch 14: loss 16.129166296346476
Epoch 15: loss 16.12916750101501
Epoch 16: loss 16.12916743722929
Epoch 17: loss 16.129164692368956
Epoch 18: loss 16.12916228743984
Epoch 19: loss 16.129164537312608
-----------Time: 0:02:45.627728, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 50, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129179736205323
Epoch 1: loss 16.129179377605112
Epoch 2: loss 16.129172596093944
Epoch 3: loss 16.129172161002728
Epoch 4: loss 16.12918462566606
Epoch 5: loss 16.129173361004014
Epoch 6: loss 16.129179982532296
Epoch 7: loss 16.129180857381982
Epoch 8: loss 16.129179202583316
Epoch 9: loss 16.129180321944933
Epoch 10: loss 16.129185612270405
Epoch 11: loss 16.129179304484897
Epoch 12: loss 16.129177540783783
Epoch 13: loss 16.129180015721616
Epoch 14: loss 16.129179216844353
Epoch 15: loss 16.129176173798736
Epoch 16: loss 16.129179485211104
Epoch 17: loss 16.129181929293242
Epoch 18: loss 16.12918284225879
Epoch 19: loss 16.12918041140052
-----------Time: 0:03:39.686168, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129165604038043
Epoch 1: loss 16.129167943625685
Epoch 2: loss 16.129172581055037
Epoch 3: loss 16.129170744233704
Epoch 4: loss 16.129171447950966
Epoch 5: loss 16.12915741249943
Epoch 6: loss 16.12917150006857
Epoch 7: loss 16.129175107332596
Epoch 8: loss 16.12917278226528
Epoch 9: loss 16.129169768260315
Epoch 10: loss 16.129164810605904
Epoch 11: loss 16.129169960913934
Epoch 12: loss 16.12917023291077
Epoch 13: loss 16.12917010767295
Epoch 14: loss 16.129168968345887
Epoch 15: loss 16.129165383380933
Epoch 16: loss 16.129164178193815
Epoch 17: loss 16.1291678788028
Epoch 18: loss 16.12916406151262
Epoch 19: loss 16.129171094536588
-----------Time: 0:04:23.295990, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917227690605
Epoch 1: loss 16.129171848037828
Epoch 2: loss 16.129171818219298
Epoch 3: loss 16.129177239227708
Epoch 4: loss 16.12917775314356
Epoch 5: loss 16.12916974907274
Epoch 6: loss 16.129176617965157
Epoch 7: loss 16.129178006989985
Epoch 8: loss 16.129175843720596
Epoch 9: loss 16.129178961442175
Epoch 10: loss 16.129175776564082
Epoch 11: loss 16.12917107664547
Epoch 12: loss 16.129174999985896
Epoch 13: loss 16.129176301370176
Epoch 14: loss 16.12918339688341
Epoch 15: loss 16.12917668486238
Epoch 16: loss 16.129171469472166
Epoch 17: loss 16.129177838709772
Epoch 18: loss 16.12917416740073
Epoch 19: loss 16.129181579768236
-----------Time: 0:04:49.227376, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 200, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129272531724094
Epoch 1: loss 16.129280759044935
Epoch 2: loss 16.12927411962554
Epoch 3: loss 16.129272778310355
Epoch 4: loss 16.129277144520746
Epoch 5: loss 16.129276796810778
Epoch 6: loss 16.129278977971286
Epoch 7: loss 16.129279160253247
Epoch 8: loss 16.129278776501753
Epoch 9: loss 16.129275488166332
Epoch 10: loss 16.129283079185715
Epoch 11: loss 16.12927854313936
Epoch 12: loss 16.129275272435763
Epoch 13: loss 16.129278777798213
Epoch 14: loss 16.129273590152195
Epoch 15: loss 16.12928192456045
Epoch 16: loss 16.12927373509617
Epoch 17: loss 16.129276178919017
Epoch 18: loss 16.129279302604306
Epoch 19: loss 16.12928025809366
-----------Time: 0:03:22.088568, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 20, rmse: 4.01721715927124-------------


Epoch 0: loss 16.129153778528377
Epoch 1: loss 16.12914992052942
Epoch 2: loss 16.129153436782115
Epoch 3: loss 16.129157969976262
Epoch 4: loss 16.129161935321918
Epoch 5: loss 16.12915030842958
Epoch 6: loss 16.129157633934415
Epoch 7: loss 16.129154445944824
Epoch 8: loss 16.129155837821855
Epoch 9: loss 16.1291502648686
Epoch 10: loss 16.129153968848374
Epoch 11: loss 16.12915601284365
Epoch 12: loss 16.129158569458323
Epoch 13: loss 16.129152988726318
Epoch 14: loss 16.12915351301383
Epoch 15: loss 16.129159850358572
Epoch 16: loss 16.129154234103627
Epoch 17: loss 16.129155039203887
Epoch 18: loss 16.12915121958008
Epoch 19: loss 16.129159587436945
-----------Time: 0:03:33.693290, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129180263604336
Epoch 1: loss 16.129184728864097
Epoch 2: loss 16.12918430595958
Epoch 3: loss 16.129182121428283
Epoch 4: loss 16.129184306996745
Epoch 5: loss 16.129183067583142
Epoch 6: loss 16.12917876982572
Epoch 7: loss 16.129187457129774
Epoch 8: loss 16.12918587285841
Epoch 9: loss 16.129180019351697
Epoch 10: loss 16.12918570198528
Epoch 11: loss 16.129184106823672
Epoch 12: loss 16.12918447812917
Epoch 13: loss 16.12918125850601
Epoch 14: loss 16.12918946767644
Epoch 15: loss 16.12918570198528
Epoch 16: loss 16.129185446583104
Epoch 17: loss 16.129182143986647
Epoch 18: loss 16.129183034653117
Epoch 19: loss 16.129177842599145
-----------Time: 0:04:28.046235, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 100, rmse: 4.017192363739014-------------


Epoch 0: loss 16.129181959371063
Epoch 1: loss 16.129181057295767
Epoch 2: loss 16.12918508098202
Epoch 3: loss 16.129181105264703
Epoch 4: loss 16.129182247443975
Epoch 5: loss 16.129183611576813
Epoch 6: loss 16.129178673369267
Epoch 7: loss 16.129183323763193
Epoch 8: loss 16.12917944087225
Epoch 9: loss 16.129182922379876
Epoch 10: loss 16.129180696621223
Epoch 11: loss 16.1291770769112
Epoch 12: loss 16.129178489531558
Epoch 13: loss 16.129176987455615
Epoch 14: loss 16.12918699636869
Epoch 15: loss 16.129177338536373
Epoch 16: loss 16.129176070082117
Epoch 17: loss 16.129181256690966
Epoch 18: loss 16.129187872774125
Epoch 19: loss 16.12918040025098
-----------Time: 0:05:03.438388, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917477855091
Epoch 1: loss 16.1291777150277
Epoch 2: loss 16.129179128685227
Epoch 3: loss 16.129178951589097
Epoch 4: loss 16.129174933607256
Epoch 5: loss 16.12917693481943
Epoch 6: loss 16.1291681549483
Epoch 7: loss 16.129169732996665
Epoch 8: loss 16.129179482358897
Epoch 9: loss 16.129176072156447
Epoch 10: loss 16.129177648908357
Epoch 11: loss 16.129172776301573
Epoch 12: loss 16.129185835261136
Epoch 13: loss 16.129179919005868
Epoch 14: loss 16.129182081756674
Epoch 15: loss 16.129175928508932
Epoch 16: loss 16.129179898521834
Epoch 17: loss 16.129177693506502
Epoch 18: loss 16.12917280949089
Epoch 19: loss 16.129168877593845
-----------Time: 0:05:40.152041, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12930001144243
Epoch 1: loss 16.129291406333802
Epoch 2: loss 16.12930114921375
Epoch 3: loss 16.12929363572254
Epoch 4: loss 16.129296606944397
Epoch 5: loss 16.12929441411577
Epoch 6: loss 16.129295953788986
Epoch 7: loss 16.129296072803808
Epoch 8: loss 16.129298558891175
Epoch 9: loss 16.129295230106273
Epoch 10: loss 16.129294003397955
Epoch 11: loss 16.129296926650877
Epoch 12: loss 16.129290482218725
Epoch 13: loss 16.12929921075013
Epoch 14: loss 16.129296497782654
Epoch 15: loss 16.129295598818857
Epoch 16: loss 16.12929431714073
Epoch 17: loss 16.12930061014662
Epoch 18: loss 16.129290387318015
Epoch 19: loss 16.12929692691017
-----------Time: 0:04:14.738119, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 20, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129213477295984
Epoch 1: loss 16.12921457487711
Epoch 2: loss 16.129210630015486
Epoch 3: loss 16.129212287407068
Epoch 4: loss 16.12921708067064
Epoch 5: loss 16.129216353876426
Epoch 6: loss 16.129208856461293
Epoch 7: loss 16.129215962086896
Epoch 8: loss 16.129211895617537
Epoch 9: loss 16.129213653095654
Epoch 10: loss 16.129214108930196
Epoch 11: loss 16.1292099576725
Epoch 12: loss 16.129208857239167
Epoch 13: loss 16.12921155153765
Epoch 14: loss 16.12921262811616
Epoch 15: loss 16.129218923974257
Epoch 16: loss 16.129214541947082
Epoch 17: loss 16.129212840994523
Epoch 18: loss 16.129211810051324
Epoch 19: loss 16.129211818089363
-----------Time: 0:04:16.357427, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 50, rmse: 4.01719331741333-------------


Epoch 0: loss 16.129175162561694
Epoch 1: loss 16.129181150900013
Epoch 2: loss 16.12917647276189
Epoch 3: loss 16.129179574148107
Epoch 4: loss 16.1291822466661
Epoch 5: loss 16.12917706031654
Epoch 6: loss 16.129180801115716
Epoch 7: loss 16.12917781200274
Epoch 8: loss 16.1291874501289
Epoch 9: loss 16.129184171646557
Epoch 10: loss 16.129187091010106
Epoch 11: loss 16.12917362159202
Epoch 12: loss 16.12917817008437
Epoch 13: loss 16.12917843663608
Epoch 14: loss 16.129180725661875
Epoch 15: loss 16.129173928593215
Epoch 16: loss 16.129178839056564
Epoch 17: loss 16.12918339169758
Epoch 18: loss 16.129176403012465
Epoch 19: loss 16.12918218443613
-----------Time: 0:03:52.088119, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12918556300501
Epoch 1: loss 16.129179612782547
Epoch 2: loss 16.1291798228087
Epoch 3: loss 16.129180725402584
Epoch 4: loss 16.12918359809365
Epoch 5: loss 16.12918500371314
Epoch 6: loss 16.129182190659126
Epoch 7: loss 16.129176381491266
Epoch 8: loss 16.129184810281643
Epoch 9: loss 16.129180196447823
Epoch 10: loss 16.1291874501289
Epoch 11: loss 16.129183830159587
Epoch 12: loss 16.12918084545457
Epoch 13: loss 16.12918245617367
Epoch 14: loss 16.129188188591232
Epoch 15: loss 16.129177368354902
Epoch 16: loss 16.129180885904052
Epoch 17: loss 16.129180539749836
Epoch 18: loss 16.12918437441255
Epoch 19: loss 16.129186360845104
-----------Time: 0:03:48.895262, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917824035238
Epoch 1: loss 16.12918182505804
Epoch 2: loss 16.129184751941043
Epoch 3: loss 16.129169535675796
Epoch 4: loss 16.129172841643044
Epoch 5: loss 16.129180394546566
Epoch 6: loss 16.1291757913437
Epoch 7: loss 16.129178333438045
Epoch 8: loss 16.129171147172773
Epoch 9: loss 16.129179326524678
Epoch 10: loss 16.129183526788477
Epoch 11: loss 16.129182262482885
Epoch 12: loss 16.129178081925243
Epoch 13: loss 16.12918101088258
Epoch 14: loss 16.12918092583495
Epoch 15: loss 16.129180880199637
Epoch 16: loss 16.129180074840086
Epoch 17: loss 16.129181040182523
Epoch 18: loss 16.129178036549224
Epoch 19: loss 16.1291758312746
-----------Time: 0:04:23.781703, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12914197272487
Epoch 1: loss 16.129152356832815
Epoch 2: loss 16.129148396413697
Epoch 3: loss 16.129149580598202
Epoch 4: loss 16.12914821905828
Epoch 5: loss 16.129150041099994
Epoch 6: loss 16.129151794170152
Epoch 7: loss 16.129154197024935
Epoch 8: loss 16.129148601254023
Epoch 9: loss 16.129153783195623
Epoch 10: loss 16.12915043392669
Epoch 11: loss 16.129148109118663
Epoch 12: loss 16.129142602025457
Epoch 13: loss 16.12915463807986
Epoch 14: loss 16.12915162588994
Epoch 15: loss 16.12914865622383
Epoch 16: loss 16.129148779646606
Epoch 17: loss 16.12915036573301
Epoch 18: loss 16.12914846357021
Epoch 19: loss 16.129146309376022
-----------Time: 0:02:50.312943, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 20, rmse: 4.017212867736816-------------


Epoch 0: loss 16.129200912286823
Epoch 1: loss 16.12920062914045
Epoch 2: loss 16.129197975810033
Epoch 3: loss 16.129195441753726
Epoch 4: loss 16.129193193436706
Epoch 5: loss 16.129202193187073
Epoch 6: loss 16.12919386889119
Epoch 7: loss 16.129198158869865
Epoch 8: loss 16.129194882721148
Epoch 9: loss 16.129196131209955
Epoch 10: loss 16.129200571059144
Epoch 11: loss 16.129194579868617
Epoch 12: loss 16.129196906232394
Epoch 13: loss 16.129202789557635
Epoch 14: loss 16.1291985045055
Epoch 15: loss 16.12919443103527
Epoch 16: loss 16.129199864489674
Epoch 17: loss 16.129196873043075
Epoch 18: loss 16.129200850316142
Epoch 19: loss 16.129195401822827
-----------Time: 0:03:06.965849, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 50, rmse: 4.017194747924805-------------


Epoch 0: loss 16.129177789185086
Epoch 1: loss 16.129176473799056
Epoch 2: loss 16.12918316118739
Epoch 3: loss 16.12917693507872
Epoch 4: loss 16.12917917976566
Epoch 5: loss 16.129174561005303
Epoch 6: loss 16.129175997999067
Epoch 7: loss 16.129177490999805
Epoch 8: loss 16.129179113387025
Epoch 9: loss 16.12917740335926
Epoch 10: loss 16.12917386895616
Epoch 11: loss 16.129177191777355
Epoch 12: loss 16.12918027942112
Epoch 13: loss 16.129180618315175
Epoch 14: loss 16.129170218909028
Epoch 15: loss 16.1291748954914
Epoch 16: loss 16.12917338045088
Epoch 17: loss 16.129179651676278
Epoch 18: loss 16.129176314594048
Epoch 19: loss 16.129179055565007
-----------Time: 0:04:00.708814, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12916956808724
Epoch 1: loss 16.1291687041278
Epoch 2: loss 16.12917571251907
Epoch 3: loss 16.129171475695163
Epoch 4: loss 16.129173498947118
Epoch 5: loss 16.12917410543005
Epoch 6: loss 16.12916626004566
Epoch 7: loss 16.129174780106663
Epoch 8: loss 16.129176725830444
Epoch 9: loss 16.12917656169889
Epoch 10: loss 16.12917794112993
Epoch 11: loss 16.12916939280615
Epoch 12: loss 16.129168764024147
Epoch 13: loss 16.12917252141798
Epoch 14: loss 16.129173922110926
Epoch 15: loss 16.129177171552616
Epoch 16: loss 16.12917195253232
Epoch 17: loss 16.12917694778401
Epoch 18: loss 16.1291684985096
Epoch 19: loss 16.129169429884843
-----------Time: 0:04:35.973873, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129177894716744
Epoch 1: loss 16.129175356771064
Epoch 2: loss 16.129170787535394
Epoch 3: loss 16.129181318661647
Epoch 4: loss 16.129173754867878
Epoch 5: loss 16.129176822027606
Epoch 6: loss 16.129172349766975
Epoch 7: loss 16.12917428589697
Epoch 8: loss 16.129176765242757
Epoch 9: loss 16.129179118313562
Epoch 10: loss 16.12917303377808
Epoch 11: loss 16.129178351329163
Epoch 12: loss 16.129181967409103
Epoch 13: loss 16.12917684225235
Epoch 14: loss 16.129171539740177
Epoch 15: loss 16.129174045792993
Epoch 16: loss 16.129178512867796
Epoch 17: loss 16.129176953747713
Epoch 18: loss 16.129172349766975
Epoch 19: loss 16.12917088762193
-----------Time: 0:05:11.809307, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129264159718566
Epoch 1: loss 16.129263936987126
Epoch 2: loss 16.12926070673301
Epoch 3: loss 16.129260781408977
Epoch 4: loss 16.12926245954388
Epoch 5: loss 16.129258713818167
Epoch 6: loss 16.1292606579862
Epoch 7: loss 16.12926037665487
Epoch 8: loss 16.129265520999198
Epoch 9: loss 16.129264902329563
Epoch 10: loss 16.129264019441838
Epoch 11: loss 16.129257614940585
Epoch 12: loss 16.12926423906178
Epoch 13: loss 16.129266520308825
Epoch 14: loss 16.129269707520542
Epoch 15: loss 16.12926710734489
Epoch 16: loss 16.129256816841195
Epoch 17: loss 16.129258037326515
Epoch 18: loss 16.1292611519366
Epoch 19: loss 16.12925839125948
-----------Time: 0:03:47.547839, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 20, rmse: 4.017223358154297-------------


Epoch 0: loss 16.129165639560984
Epoch 1: loss 16.129169248121467
Epoch 2: loss 16.129166637314864
Epoch 3: loss 16.12916622296697
Epoch 4: loss 16.129169603091597
Epoch 5: loss 16.129168717351668
Epoch 6: loss 16.12916858200148
Epoch 7: loss 16.129163770068917
Epoch 8: loss 16.12916447897201
Epoch 9: loss 16.12916154068018
Epoch 10: loss 16.129160005933503
Epoch 11: loss 16.129167448378826
Epoch 12: loss 16.12916995650598
Epoch 13: loss 16.12916897586534
Epoch 14: loss 16.129167401965642
Epoch 15: loss 16.129166528412412
Epoch 16: loss 16.12916625978637
Epoch 17: loss 16.1291656605636
Epoch 18: loss 16.129166619423746
Epoch 19: loss 16.129158444220504
-----------Time: 0:03:51.579007, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 50, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129156164269915
Epoch 1: loss 16.129156148971717
Epoch 2: loss 16.129160420022107
Epoch 3: loss 16.129158330132224
Epoch 4: loss 16.129158917686873
Epoch 5: loss 16.12915794093561
Epoch 6: loss 16.129160573263412
Epoch 7: loss 16.129149889933018
Epoch 8: loss 16.12915770653605
Epoch 9: loss 16.129151225543787
Epoch 10: loss 16.129151710418984
Epoch 11: loss 16.1291539771457
Epoch 12: loss 16.129155619239082
Epoch 13: loss 16.129147060284346
Epoch 14: loss 16.129157241626302
Epoch 15: loss 16.129151526581275
Epoch 16: loss 16.129155034018055
Epoch 17: loss 16.129148457606505
Epoch 18: loss 16.129149799180976
Epoch 19: loss 16.12915044792843
-----------Time: 0:04:50.770758, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129181343294345
Epoch 1: loss 16.12918218495471
Epoch 2: loss 16.129186871390164
Epoch 3: loss 16.12918359264853
Epoch 4: loss 16.129181013994078
Epoch 5: loss 16.129179815029957
Epoch 6: loss 16.129181261358216
Epoch 7: loss 16.12918023145218
Epoch 8: loss 16.129188450475695
Epoch 9: loss 16.129185472252967
Epoch 10: loss 16.129184492390205
Epoch 11: loss 16.129178004915655
Epoch 12: loss 16.12918754088094
Epoch 13: loss 16.12918373629605
Epoch 14: loss 16.129179565072903
Epoch 15: loss 16.129176639486356
Epoch 16: loss 16.129183378733003
Epoch 17: loss 16.129175163598862
Epoch 18: loss 16.129175595578584
Epoch 19: loss 16.129175081662734
-----------Time: 0:05:30.265656, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129178343291123
Epoch 1: loss 16.129174303788087
Epoch 2: loss 16.129174647349387
Epoch 3: loss 16.129167891248795
Epoch 4: loss 16.129169539305877
Epoch 5: loss 16.129171814848508
Epoch 6: loss 16.129174927124968
Epoch 7: loss 16.129165991419615
Epoch 8: loss 16.129168838959405
Epoch 9: loss 16.12917076368057
Epoch 10: loss 16.129172887019063
Epoch 11: loss 16.129159538690132
Epoch 12: loss 16.129168918302618
Epoch 13: loss 16.129171032565907
Epoch 14: loss 16.129176484948594
Epoch 15: loss 16.129166844748102
Epoch 16: loss 16.129176471465435
Epoch 17: loss 16.129171444061594
Epoch 18: loss 16.12917181977505
Epoch 19: loss 16.129169527119174
-----------Time: 0:05:44.854965, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12937498351943
Epoch 1: loss 16.12938141576488
Epoch 2: loss 16.12937579095331
Epoch 3: loss 16.129376147479192
Epoch 4: loss 16.129378970904867
Epoch 5: loss 16.129379766411336
Epoch 6: loss 16.129383703753508
Epoch 7: loss 16.12937828663447
Epoch 8: loss 16.129382480934563
Epoch 9: loss 16.12937591256105
Epoch 10: loss 16.129373005384203
Epoch 11: loss 16.129379304094506
Epoch 12: loss 16.129375426389394
Epoch 13: loss 16.1293828356454
Epoch 14: loss 16.129377654481672
Epoch 15: loss 16.12937038005727
Epoch 16: loss 16.129374722672132
Epoch 17: loss 16.12938229605969
Epoch 18: loss 16.129373263379296
Epoch 19: loss 16.129378107982593
-----------Time: 0:02:26.347458, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 20, rmse: 4.01725959777832-------------


Epoch 0: loss 16.129207786365072
Epoch 1: loss 16.12921452120376
Epoch 2: loss 16.129212295445104
Epoch 3: loss 16.129217116712162
Epoch 4: loss 16.12921060045625
Epoch 5: loss 16.12920769094578
Epoch 6: loss 16.12920726726339
Epoch 7: loss 16.129207060348733
Epoch 8: loss 16.12920834902773
Epoch 9: loss 16.12921015032612
Epoch 10: loss 16.129212121719767
Epoch 11: loss 16.12921007176078
Epoch 12: loss 16.12921106718104
Epoch 13: loss 16.12921169829667
Epoch 14: loss 16.1292110101369
Epoch 15: loss 16.12920739613129
Epoch 16: loss 16.129216553012334
Epoch 17: loss 16.12921565845649
Epoch 18: loss 16.129203750751405
Epoch 19: loss 16.129210174699526
-----------Time: 0:02:45.719998, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 50, rmse: 4.017199993133545-------------


Epoch 0: loss 16.1291918129685
Epoch 1: loss 16.129183074843308
Epoch 2: loss 16.129179178728496
Epoch 3: loss 16.129181845282783
Epoch 4: loss 16.129179065936672
Epoch 5: loss 16.129185112615588
Epoch 6: loss 16.129179706386797
Epoch 7: loss 16.129182777954483
Epoch 8: loss 16.129180282273325
Epoch 9: loss 16.129180305868857
Epoch 10: loss 16.129183012354044
Epoch 11: loss 16.129183417367443
Epoch 12: loss 16.129180446145586
Epoch 13: loss 16.129184810800226
Epoch 14: loss 16.129179170690456
Epoch 15: loss 16.129178131968512
Epoch 16: loss 16.129181161271678
Epoch 17: loss 16.129176215285383
Epoch 18: loss 16.129179059713675
Epoch 19: loss 16.129181726527253
-----------Time: 0:03:39.050700, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.12916870957292
Epoch 1: loss 16.129170866100733
Epoch 2: loss 16.129170337664554
Epoch 3: loss 16.129174708801486
Epoch 4: loss 16.129173627037144
Epoch 5: loss 16.129173091340803
Epoch 6: loss 16.129175261611067
Epoch 7: loss 16.12917333196336
Epoch 8: loss 16.12917723611621
Epoch 9: loss 16.129175484342507
Epoch 10: loss 16.129174065758445
Epoch 11: loss 16.12917406809207
Epoch 12: loss 16.129169213894983
Epoch 13: loss 16.12917301873917
Epoch 14: loss 16.129175295059678
Epoch 15: loss 16.129169300757653
Epoch 16: loss 16.12917006048189
Epoch 17: loss 16.129169103177492
Epoch 18: loss 16.12917528053935
Epoch 19: loss 16.129175033953086
-----------Time: 0:04:21.227331, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129175131706003
Epoch 1: loss 16.129176336115247
Epoch 2: loss 16.12916821173315
Epoch 3: loss 16.12917265728675
Epoch 4: loss 16.12917194527216
Epoch 5: loss 16.12916900231308
Epoch 6: loss 16.12917143317135
Epoch 7: loss 16.12917105486498
Epoch 8: loss 16.129171678461155
Epoch 9: loss 16.129164481046345
Epoch 10: loss 16.12917945617045
Epoch 11: loss 16.129174543373477
Epoch 12: loss 16.12917060447556
Epoch 13: loss 16.129174953313417
Epoch 14: loss 16.129177213039263
Epoch 15: loss 16.129176460056605
Epoch 16: loss 16.129168688829598
Epoch 17: loss 16.1291783471805
Epoch 18: loss 16.129167705596043
Epoch 19: loss 16.129173192464506
-----------Time: 0:04:57.122987, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12938310453074
Epoch 1: loss 16.12937455854058
Epoch 2: loss 16.12937398602484
Epoch 3: loss 16.129371615840792
Epoch 4: loss 16.129376692250737
Epoch 5: loss 16.129380337371327
Epoch 6: loss 16.129379320429873
Epoch 7: loss 16.129377652666633
Epoch 8: loss 16.129375813511675
Epoch 9: loss 16.129375470468958
Epoch 10: loss 16.12937614021903
Epoch 11: loss 16.12937767081704
Epoch 12: loss 16.129369553176524
Epoch 13: loss 16.1293815563009
Epoch 14: loss 16.12937736407514
Epoch 15: loss 16.12937948093134
Epoch 16: loss 16.129370151362124
Epoch 17: loss 16.129382069438876
Epoch 18: loss 16.1293778162796
Epoch 19: loss 16.1293790588047
-----------Time: 0:03:18.007435, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 20, rmse: 4.017195701599121-------------


Epoch 0: loss 16.129203733378873
Epoch 1: loss 16.12920474098583
Epoch 2: loss 16.12919861159291
Epoch 3: loss 16.129202354207123
Epoch 4: loss 16.129200533202578
Epoch 5: loss 16.12920004936455
Epoch 6: loss 16.129195056705775
Epoch 7: loss 16.129199258525322
Epoch 8: loss 16.12920545429688
Epoch 9: loss 16.129204823699833
Epoch 10: loss 16.129209585589127
Epoch 11: loss 16.1291906041513
Epoch 12: loss 16.12919972499082
Epoch 13: loss 16.129202429660964
Epoch 14: loss 16.129200204420894
Epoch 15: loss 16.129201505286595
Epoch 16: loss 16.129198665006967
Epoch 17: loss 16.129203120413653
Epoch 18: loss 16.129199510038127
Epoch 19: loss 16.129202831303573
-----------Time: 0:03:26.005170, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12916499599936
Epoch 1: loss 16.129168469468947
Epoch 2: loss 16.12916931735231
Epoch 3: loss 16.12917142824481
Epoch 4: loss 16.129169496782062
Epoch 5: loss 16.12917079194335
Epoch 6: loss 16.129166655465273
Epoch 7: loss 16.12916549150551
Epoch 8: loss 16.129170831614957
Epoch 9: loss 16.129167663590813
Epoch 10: loss 16.129173973969237
Epoch 11: loss 16.12916512694159
Epoch 12: loss 16.129166672319222
Epoch 13: loss 16.12917237284392
Epoch 14: loss 16.129164266871523
Epoch 15: loss 16.129165732905943
Epoch 16: loss 16.129173896700355
Epoch 17: loss 16.12916913766327
Epoch 18: loss 16.12917114354269
Epoch 19: loss 16.129168571111233
-----------Time: 0:04:31.290821, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12917950621372
Epoch 1: loss 16.129176668527013
Epoch 2: loss 16.129181102931078
Epoch 3: loss 16.129176203876554
Epoch 4: loss 16.129174136026453
Epoch 5: loss 16.129175391775426
Epoch 6: loss 16.12917868088872
Epoch 7: loss 16.129177208372017
Epoch 8: loss 16.129178137413636
Epoch 9: loss 16.129172830752797
Epoch 10: loss 16.1291772947161
Epoch 11: loss 16.12917507310611
Epoch 12: loss 16.129168582520062
Epoch 13: loss 16.12917748088743
Epoch 14: loss 16.1291754814903
Epoch 15: loss 16.129179486507564
Epoch 16: loss 16.129183259977474
Epoch 17: loss 16.129177907940615
Epoch 18: loss 16.129176418310667
Epoch 19: loss 16.129174633347645
-----------Time: 0:05:03.516829, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.1291816238478
Epoch 1: loss 16.12917810992873
Epoch 2: loss 16.129184259546392
Epoch 3: loss 16.12917602029814
Epoch 4: loss 16.129166594272466
Epoch 5: loss 16.129177159625204
Epoch 6: loss 16.129168585890852
Epoch 7: loss 16.129175450116023
Epoch 8: loss 16.12917355625055
Epoch 9: loss 16.129175951067296
Epoch 10: loss 16.129177821077946
Epoch 11: loss 16.129179766801727
Epoch 12: loss 16.12917415236182
Epoch 13: loss 16.12917895677493
Epoch 14: loss 16.129177361094737
Epoch 15: loss 16.129174417617076
Epoch 16: loss 16.129174709060777
Epoch 17: loss 16.12917233706169
Epoch 18: loss 16.129176530065322
Epoch 19: loss 16.129180737589287
-----------Time: 0:05:44.637462, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129069695205647
Epoch 1: loss 16.12906791957712
Epoch 2: loss 16.129069723468426
Epoch 3: loss 16.129073262538775
Epoch 4: loss 16.129067667286446
Epoch 5: loss 16.129065841096065
Epoch 6: loss 16.129067542048627
Epoch 7: loss 16.129060786985196
Epoch 8: loss 16.129069864004446
Epoch 9: loss 16.129067633319252
Epoch 10: loss 16.129067428478926
Epoch 11: loss 16.12906494965172
Epoch 12: loss 16.129066313265977
Epoch 13: loss 16.12906687489147
Epoch 14: loss 16.12907096132628
Epoch 15: loss 16.12906535310937
Epoch 16: loss 16.129068305402946
Epoch 17: loss 16.129067427701052
Epoch 18: loss 16.129062955181126
Epoch 19: loss 16.129065028735642
-----------Time: 0:04:15.302965, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 20, rmse: 4.017258644104004-------------


Epoch 0: loss 16.129190687643177
Epoch 1: loss 16.12918988772875
Epoch 2: loss 16.129190566035444
Epoch 3: loss 16.1291921044122
Epoch 4: loss 16.129186970439534
Epoch 5: loss 16.129193963273316
Epoch 6: loss 16.129188487035805
Epoch 7: loss 16.129193662235828
Epoch 8: loss 16.12918130803069
Epoch 9: loss 16.12919395238307
Epoch 10: loss 16.129189320917426
Epoch 11: loss 16.129188374762563
Epoch 12: loss 16.12918812324976
Epoch 13: loss 16.129184825061262
Epoch 14: loss 16.129181399819903
Epoch 15: loss 16.129192958777853
Epoch 16: loss 16.12919357900324
Epoch 17: loss 16.129195017812044
Epoch 18: loss 16.129187356783945
Epoch 19: loss 16.129198823693397
-----------Time: 0:04:14.302363, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 50, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129167076295456
Epoch 1: loss 16.129172449594222
Epoch 2: loss 16.129174156510487
Epoch 3: loss 16.12917118502934
Epoch 4: loss 16.12917194553145
Epoch 5: loss 16.129178683481637
Epoch 6: loss 16.129173059966526
Epoch 7: loss 16.129172090475425
Epoch 8: loss 16.129173640261012
Epoch 9: loss 16.129173700935237
Epoch 10: loss 16.12917399600902
Epoch 11: loss 16.129171384424538
Epoch 12: loss 16.12917594821509
Epoch 13: loss 16.12916582651019
Epoch 14: loss 16.129174779069494
Epoch 15: loss 16.1291716341223
Epoch 16: loss 16.129167064627335
Epoch 17: loss 16.12917281778822
Epoch 18: loss 16.12916948770686
Epoch 19: loss 16.12917438468705
-----------Time: 0:03:39.898786, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12917873093199
Epoch 1: loss 16.129177109063352
Epoch 2: loss 16.129188279602566
Epoch 3: loss 16.129182655828163
Epoch 4: loss 16.129174980020444
Epoch 5: loss 16.1291764940238
Epoch 6: loss 16.129182250555473
Epoch 7: loss 16.129172788747567
Epoch 8: loss 16.129180208375235
Epoch 9: loss 16.12918354467959
Epoch 10: loss 16.12917118891871
Epoch 11: loss 16.12916773697032
Epoch 12: loss 16.129177518744
Epoch 13: loss 16.129182331195146
Epoch 14: loss 16.129177439660076
Epoch 15: loss 16.129177685209175
Epoch 16: loss 16.129173479500253
Epoch 17: loss 16.129185197922506
Epoch 18: loss 16.129177604569502
Epoch 19: loss 16.12917467068563
-----------Time: 0:03:48.446358, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12916834423113
Epoch 1: loss 16.129174878118867
Epoch 2: loss 16.12917711554564
Epoch 3: loss 16.12918151313031
Epoch 4: loss 16.12917620128364
Epoch 5: loss 16.129173125048705
Epoch 6: loss 16.129177263082532
Epoch 7: loss 16.12917946239345
Epoch 8: loss 16.129177646574732
Epoch 9: loss 16.12917648728222
Epoch 10: loss 16.129172765929912
Epoch 11: loss 16.12918205219744
Epoch 12: loss 16.129173924963133
Epoch 13: loss 16.129173829803136
Epoch 14: loss 16.129176529028157
Epoch 15: loss 16.129174626346774
Epoch 16: loss 16.129178219090473
Epoch 17: loss 16.129168586150143
Epoch 18: loss 16.12917596195754
Epoch 19: loss 16.129170415970602
-----------Time: 0:04:26.792905, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12928509880759
Epoch 1: loss 16.129277732593977
Epoch 2: loss 16.129282440291338
Epoch 3: loss 16.12928194634094
Epoch 4: loss 16.12927926319199
Epoch 5: loss 16.129281930264863
Epoch 6: loss 16.1292870865366
Epoch 7: loss 16.12928165100786
Epoch 8: loss 16.129286013847462
Epoch 9: loss 16.12927611072534
Epoch 10: loss 16.129280482380853
Epoch 11: loss 16.129277551349187
Epoch 12: loss 16.12928097710913
Epoch 13: loss 16.129291470119526
Epoch 14: loss 16.12927929301052
Epoch 15: loss 16.1292860618164
Epoch 16: loss 16.12928297650626
Epoch 17: loss 16.129290246263412
Epoch 18: loss 16.129279319198965
Epoch 19: loss 16.12928671134173
-----------Time: 0:02:50.521992, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 20, rmse: 4.01722526550293-------------


Epoch 0: loss 16.129171521589768
Epoch 1: loss 16.129163305677753
Epoch 2: loss 16.129166473961188
Epoch 3: loss 16.129161749409878
Epoch 4: loss 16.129170575694197
Epoch 5: loss 16.12916407318074
Epoch 6: loss 16.129164261944986
Epoch 7: loss 16.129160147247397
Epoch 8: loss 16.129168422018594
Epoch 9: loss 16.129164092109022
Epoch 10: loss 16.129171578115326
Epoch 11: loss 16.129170320292022
Epoch 12: loss 16.12917341882603
Epoch 13: loss 16.12916738848248
Epoch 14: loss 16.129170758754032
Epoch 15: loss 16.129166609570667
Epoch 16: loss 16.129170917440458
Epoch 17: loss 16.12916400343131
Epoch 18: loss 16.129166729881945
Epoch 19: loss 16.12917050257398
-----------Time: 0:03:12.393981, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 50, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129185014344092
Epoch 1: loss 16.12917951632609
Epoch 2: loss 16.12917435953577
Epoch 3: loss 16.129179893595296
Epoch 4: loss 16.12917497198241
Epoch 5: loss 16.129178923067027
Epoch 6: loss 16.12918402125746
Epoch 7: loss 16.129181525317012
Epoch 8: loss 16.129182452025006
Epoch 9: loss 16.12917779670454
Epoch 10: loss 16.129185178994224
Epoch 11: loss 16.129178716670953
Epoch 12: loss 16.129181375705787
Epoch 13: loss 16.129179128425935
Epoch 14: loss 16.129190932155108
Epoch 15: loss 16.129184964819405
Epoch 16: loss 16.12918002194461
Epoch 17: loss 16.129178590914552
Epoch 18: loss 16.1291812188344
Epoch 19: loss 16.12918184320845
-----------Time: 0:04:06.784897, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12917128770879
Epoch 1: loss 16.129166275862445
Epoch 2: loss 16.129169070247464
Epoch 3: loss 16.129165406457883
Epoch 4: loss 16.129170376039706
Epoch 5: loss 16.129171921158044
Epoch 6: loss 16.12916464751152
Epoch 7: loss 16.12918114986285
Epoch 8: loss 16.129176473799056
Epoch 9: loss 16.129170234725812
Epoch 10: loss 16.12916698346908
Epoch 11: loss 16.129173696008696
Epoch 12: loss 16.129169652875575
Epoch 13: loss 16.129172913726094
Epoch 14: loss 16.129168067307756
Epoch 15: loss 16.12916936220975
Epoch 16: loss 16.129175815717108
Epoch 17: loss 16.129173850027875
Epoch 18: loss 16.129174340348193
Epoch 19: loss 16.129173394971208
-----------Time: 0:04:37.282028, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 150, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129173278808594
Epoch 1: loss 16.129177196963187
Epoch 2: loss 16.129164864538545
Epoch 3: loss 16.129181473977283
Epoch 4: loss 16.129173366449137
Epoch 5: loss 16.129179793508758
Epoch 6: loss 16.129173989267436
Epoch 7: loss 16.129175899208985
Epoch 8: loss 16.129174837928677
Epoch 9: loss 16.129174167919313
Epoch 10: loss 16.129177642166777
Epoch 11: loss 16.129175558759183
Epoch 12: loss 16.129177055908585
Epoch 13: loss 16.12917148191816
Epoch 14: loss 16.129181508203768
Epoch 15: loss 16.129175565500763
Epoch 16: loss 16.12917581260561
Epoch 17: loss 16.129173842767713
Epoch 18: loss 16.129178517535046
Epoch 19: loss 16.12917690759382
-----------Time: 0:05:13.353616, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129384489147608
Epoch 1: loss 16.129380417492417
Epoch 2: loss 16.129387011276503
Epoch 3: loss 16.129381578859263
Epoch 4: loss 16.129379630023983
Epoch 5: loss 16.129382922248777
Epoch 6: loss 16.129384910496373
Epoch 7: loss 16.129379514379952
Epoch 8: loss 16.12938166183256
Epoch 9: loss 16.129376970211275
Epoch 10: loss 16.12937959968687
Epoch 11: loss 16.129382361141868
Epoch 12: loss 16.12938683703258
Epoch 13: loss 16.129378242295616
Epoch 14: loss 16.12938651214027
Epoch 15: loss 16.129378087239267
Epoch 16: loss 16.12938060055225
Epoch 17: loss 16.12937704436866
Epoch 18: loss 16.12937480771976
Epoch 19: loss 16.129381644460025
-----------Time: 0:03:49.300315, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 20, rmse: 4.017230033874512-------------


Epoch 0: loss 16.12915942252752
Epoch 1: loss 16.129164889949116
Epoch 2: loss 16.129160997723677
Epoch 3: loss 16.129160160211974
Epoch 4: loss 16.129161442667975
Epoch 5: loss 16.129162136272868
Epoch 6: loss 16.129169534120045
Epoch 7: loss 16.12915591690578
Epoch 8: loss 16.129165407235757
Epoch 9: loss 16.129161043618282
Epoch 10: loss 16.129156716301623
Epoch 11: loss 16.12915913315815
Epoch 12: loss 16.12916420412297
Epoch 13: loss 16.129167862986016
Epoch 14: loss 16.1291535384244
Epoch 15: loss 16.12916166747375
Epoch 16: loss 16.129158355802087
Epoch 17: loss 16.12916130757708
Epoch 18: loss 16.129157820105746
Epoch 19: loss 16.129168351232
-----------Time: 0:03:51.250069, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 50, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129166530227455
Epoch 1: loss 16.129166234116504
Epoch 2: loss 16.129171842333413
Epoch 3: loss 16.129168166097834
Epoch 4: loss 16.129174156510487
Epoch 5: loss 16.129168174654456
Epoch 6: loss 16.12916944622021
Epoch 7: loss 16.129164318729835
Epoch 8: loss 16.129162026851834
Epoch 9: loss 16.129173576475292
Epoch 10: loss 16.129169804561133
Epoch 11: loss 16.12916452045866
Epoch 12: loss 16.129172130924907
Epoch 13: loss 16.12917116687893
Epoch 14: loss 16.12916741155943
Epoch 15: loss 16.129172044062237
Epoch 16: loss 16.129173415714533
Epoch 17: loss 16.129169724699334
Epoch 18: loss 16.129170392634364
Epoch 19: loss 16.12915987836206
-----------Time: 0:04:55.425909, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129178826869865
Epoch 1: loss 16.129175777082665
Epoch 2: loss 16.12917416558569
Epoch 3: loss 16.12917489393565
Epoch 4: loss 16.129172900761514
Epoch 5: loss 16.12917976420881
Epoch 6: loss 16.129178763343432
Epoch 7: loss 16.12917694596897
Epoch 8: loss 16.129173451756056
Epoch 9: loss 16.12917754571032
Epoch 10: loss 16.12918053689763
Epoch 11: loss 16.129175695146536
Epoch 12: loss 16.129174129025582
Epoch 13: loss 16.129171193326666
Epoch 14: loss 16.129172883907565
Epoch 15: loss 16.12917396307899
Epoch 16: loss 16.12918015599834
Epoch 17: loss 16.1291842133925
Epoch 18: loss 16.12918045185
Epoch 19: loss 16.129177690654295
-----------Time: 0:05:31.353796, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129175899986862
Epoch 1: loss 16.129177631535825
Epoch 2: loss 16.129175260573902
Epoch 3: loss 16.12917613412713
Epoch 4: loss 16.12917844415554
Epoch 5: loss 16.129181850727903
Epoch 6: loss 16.129178738192152
Epoch 7: loss 16.129170133602106
Epoch 8: loss 16.129175616840488
Epoch 9: loss 16.129175524532698
Epoch 10: loss 16.12917604570871
Epoch 11: loss 16.129178945884682
Epoch 12: loss 16.129176407939003
Epoch 13: loss 16.129182341566807
Epoch 14: loss 16.12917355106472
Epoch 15: loss 16.129173606812405
Epoch 16: loss 16.129178730154116
Epoch 17: loss 16.129186392737964
Epoch 18: loss 16.129173022628542
Epoch 19: loss 16.12918062635321
-----------Time: 0:05:32.700067, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12935361452502
Epoch 1: loss 16.12935635679244
Epoch 2: loss 16.12935632515887
Epoch 3: loss 16.129355358260685
Epoch 4: loss 16.129352755232826
Epoch 5: loss 16.129363070109928
Epoch 6: loss 16.12935964512786
Epoch 7: loss 16.129365485929288
Epoch 8: loss 16.129361323003472
Epoch 9: loss 16.12935666820159
Epoch 10: loss 16.12935155730587
Epoch 11: loss 16.129355158865483
Epoch 12: loss 16.129356757916465
Epoch 13: loss 16.129353512104856
Epoch 14: loss 16.129359128619097
Epoch 15: loss 16.129364142021192
Epoch 16: loss 16.129358144348377
Epoch 17: loss 16.12935391919259
Epoch 18: loss 16.129352683149776
Epoch 19: loss 16.12935995809276
-----------Time: 0:02:26.072359, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 20, rmse: 4.017234802246094-------------


Epoch 0: loss 16.129153187084352
Epoch 1: loss 16.129158005758494
Epoch 2: loss 16.129152460549435
Epoch 3: loss 16.129151191835884
Epoch 4: loss 16.129163423136823
Epoch 5: loss 16.129155539377283
Epoch 6: loss 16.12916130939212
Epoch 7: loss 16.129151382415174
Epoch 8: loss 16.129154096160523
Epoch 9: loss 16.129155914312864
Epoch 10: loss 16.129161477413042
Epoch 11: loss 16.12915395173513
Epoch 12: loss 16.12915356824293
Epoch 13: loss 16.129156501867513
Epoch 14: loss 16.12915837498966
Epoch 15: loss 16.12914602234028
Epoch 16: loss 16.129152584490793
Epoch 17: loss 16.129151941707043
Epoch 18: loss 16.129155489852597
Epoch 19: loss 16.129158721143877
-----------Time: 0:02:45.437530, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129161080437683
Epoch 1: loss 16.129159282769375
Epoch 2: loss 16.12916035001339
Epoch 3: loss 16.129164461599476
Epoch 4: loss 16.129163862376707
Epoch 5: loss 16.129166470331107
Epoch 6: loss 16.129162619851606
Epoch 7: loss 16.12915616012125
Epoch 8: loss 16.129166871714425
Epoch 9: loss 16.12916326185748
Epoch 10: loss 16.129163709135405
Epoch 11: loss 16.129160617342976
Epoch 12: loss 16.129158630132544
Epoch 13: loss 16.129159737826043
Epoch 14: loss 16.12916040394603
Epoch 15: loss 16.129161069806727
Epoch 16: loss 16.129164410778333
Epoch 17: loss 16.129165695308668
Epoch 18: loss 16.129161853126497
Epoch 19: loss 16.129157411202975
-----------Time: 0:03:43.324593, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 100, rmse: 4.017184257507324-------------


Epoch 0: loss 16.1291696183898
Epoch 1: loss 16.12917330966429
Epoch 2: loss 16.1291725514958
Epoch 3: loss 16.129175900246153
Epoch 4: loss 16.12916761302896
Epoch 5: loss 16.12917650802554
Epoch 6: loss 16.129179065158795
Epoch 7: loss 16.12917483844726
Epoch 8: loss 16.12917336359693
Epoch 9: loss 16.12917218174605
Epoch 10: loss 16.129169801708926
Epoch 11: loss 16.129170539652673
Epoch 12: loss 16.129170753049618
Epoch 13: loss 16.129170433343138
Epoch 14: loss 16.12917714043763
Epoch 15: loss 16.129173704565318
Epoch 16: loss 16.129173831877466
Epoch 17: loss 16.129175336027743
Epoch 18: loss 16.129170762643405
Epoch 19: loss 16.129171120725033
-----------Time: 0:04:21.291502, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12917261191073
Epoch 1: loss 16.129177730585194
Epoch 2: loss 16.129175282872975
Epoch 3: loss 16.129175355474608
Epoch 4: loss 16.129180059541884
Epoch 5: loss 16.129174340088902
Epoch 6: loss 16.129169863420312
Epoch 7: loss 16.129170503092563
Epoch 8: loss 16.129179092125117
Epoch 9: loss 16.129180977174677
Epoch 10: loss 16.129172020726
Epoch 11: loss 16.129172317096238
Epoch 12: loss 16.129178785123923
Epoch 13: loss 16.129174079241604
Epoch 14: loss 16.12916977370544
Epoch 15: loss 16.1291796602329
Epoch 16: loss 16.129166175775907
Epoch 17: loss 16.129180298349404
Epoch 18: loss 16.129177513039586
Epoch 19: loss 16.129178266022244
-----------Time: 0:04:59.860019, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129273417982606
Epoch 1: loss 16.129271704843344
Epoch 2: loss 16.12927997831808
Epoch 3: loss 16.1292754492726
Epoch 4: loss 16.129276177881852
Epoch 5: loss 16.129274885832064
Epoch 6: loss 16.129273963013443
Epoch 7: loss 16.129275046333532
Epoch 8: loss 16.129269868281305
Epoch 9: loss 16.129272918846375
Epoch 10: loss 16.12928224089614
Epoch 11: loss 16.129277209084343
Epoch 12: loss 16.1292730070055
Epoch 13: loss 16.129278046855337
Epoch 14: loss 16.129271641576207
Epoch 15: loss 16.129276521443153
Epoch 16: loss 16.129276056274115
Epoch 17: loss 16.129274261717306
Epoch 18: loss 16.12928907685853
Epoch 19: loss 16.12927739421851
-----------Time: 0:03:17.770663, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 20, rmse: 4.017207622528076-------------


Epoch 0: loss 16.129212149204672
Epoch 1: loss 16.129217878251442
Epoch 2: loss 16.129217105562628
Epoch 3: loss 16.129216225267818
Epoch 4: loss 16.12921566597595
Epoch 5: loss 16.129208775043747
Epoch 6: loss 16.129210508667043
Epoch 7: loss 16.129215817402212
Epoch 8: loss 16.129217072373308
Epoch 9: loss 16.1292223129148
Epoch 10: loss 16.129215978940845
Epoch 11: loss 16.129215061048765
Epoch 12: loss 16.129214383260656
Epoch 13: loss 16.129211109964142
Epoch 14: loss 16.129221131841796
Epoch 15: loss 16.129221470217267
Epoch 16: loss 16.129210705469326
Epoch 17: loss 16.12921831671345
Epoch 18: loss 16.129222567539102
Epoch 19: loss 16.129218691389738
-----------Time: 0:03:29.942070, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 50, rmse: 4.01719331741333-------------


Epoch 0: loss 16.12916330334413
Epoch 1: loss 16.129166076467243
Epoch 2: loss 16.129161486747538
Epoch 3: loss 16.12916138017871
Epoch 4: loss 16.129158448109877
Epoch 5: loss 16.12916327326631
Epoch 6: loss 16.129157884669343
Epoch 7: loss 16.129169585459774
Epoch 8: loss 16.129163372574972
Epoch 9: loss 16.129162919333346
Epoch 10: loss 16.129169327723975
Epoch 11: loss 16.129154474207603
Epoch 12: loss 16.129160329010773
Epoch 13: loss 16.12916132287528
Epoch 14: loss 16.129165166353907
Epoch 15: loss 16.129166724955407
Epoch 16: loss 16.129161551570427
Epoch 17: loss 16.129168731871996
Epoch 18: loss 16.12916001241579
Epoch 19: loss 16.129165974306375
-----------Time: 0:04:33.965811, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 100, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12918256974337
Epoch 1: loss 16.129184954188453
Epoch 2: loss 16.129178432228127
Epoch 3: loss 16.12918071321588
Epoch 4: loss 16.12918434589048
Epoch 5: loss 16.12918120405478
Epoch 6: loss 16.129174905603772
Epoch 7: loss 16.129181495757773
Epoch 8: loss 16.129178687111718
Epoch 9: loss 16.129182222292695
Epoch 10: loss 16.129180748738822
Epoch 11: loss 16.12917789730966
Epoch 12: loss 16.12918488029036
Epoch 13: loss 16.129178527388124
Epoch 14: loss 16.12918176127232
Epoch 15: loss 16.129180129550605
Epoch 16: loss 16.12917974398407
Epoch 17: loss 16.12918584589209
Epoch 18: loss 16.129187544511023
Epoch 19: loss 16.129176334559496
-----------Time: 0:05:04.960195, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129169686324186
Epoch 1: loss 16.129178978036837
Epoch 2: loss 16.12917365815213
Epoch 3: loss 16.129169085286374
Epoch 4: loss 16.12917170802039
Epoch 5: loss 16.129169153480053
Epoch 6: loss 16.1291766812323
Epoch 7: loss 16.1291647800095
Epoch 8: loss 16.12916875443036
Epoch 9: loss 16.12917245815084
Epoch 10: loss 16.129169019944904
Epoch 11: loss 16.12917432245708
Epoch 12: loss 16.129174493070916
Epoch 13: loss 16.129170691338228
Epoch 14: loss 16.129174036977084
Epoch 15: loss 16.1291778654168
Epoch 16: loss 16.129174273710266
Epoch 17: loss 16.129169894016716
Epoch 18: loss 16.12916927716212
Epoch 19: loss 16.129173553916928
-----------Time: 0:05:49.575818, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129278250917785
Epoch 1: loss 16.12928677149737
Epoch 2: loss 16.129272226019356
Epoch 3: loss 16.129273041231986
Epoch 4: loss 16.129276553595307
Epoch 5: loss 16.12927771599932
Epoch 6: loss 16.129279641757652
Epoch 7: loss 16.12927601037951
Epoch 8: loss 16.129279976503042
Epoch 9: loss 16.12927355955579
Epoch 10: loss 16.12927358600353
Epoch 11: loss 16.129278018333267
Epoch 12: loss 16.12928068929551
Epoch 13: loss 16.12927847805718
Epoch 14: loss 16.12927934668387
Epoch 15: loss 16.129288570721425
Epoch 16: loss 16.12927846664835
Epoch 17: loss 16.129279685577924
Epoch 18: loss 16.12928174227849
Epoch 19: loss 16.12927704936075
-----------Time: 0:04:16.542670, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 20, rmse: 4.017263412475586-------------


Epoch 0: loss 16.129150103589257
Epoch 1: loss 16.129149259595263
Epoch 2: loss 16.129147546974586
Epoch 3: loss 16.129152130730585
Epoch 4: loss 16.129154617336535
Epoch 5: loss 16.129148370225252
Epoch 6: loss 16.12915254404131
Epoch 7: loss 16.12914592536524
Epoch 8: loss 16.12914916547243
Epoch 9: loss 16.12916282287689
Epoch 10: loss 16.129154230473546
Epoch 11: loss 16.12915916012447
Epoch 12: loss 16.129154799618494
Epoch 13: loss 16.1291535384244
Epoch 14: loss 16.12914722830527
Epoch 15: loss 16.129156321141302
Epoch 16: loss 16.129148321997025
Epoch 17: loss 16.129145021993484
Epoch 18: loss 16.12915683427928
Epoch 19: loss 16.129147747666245
-----------Time: 0:04:01.333254, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12917092288558
Epoch 1: loss 16.129173699120194
Epoch 2: loss 16.129173546656762
Epoch 3: loss 16.12917374527409
Epoch 4: loss 16.129172381141252
Epoch 5: loss 16.129176817878943
Epoch 6: loss 16.129173517097527
Epoch 7: loss 16.129171440950095
Epoch 8: loss 16.129178033956308
Epoch 9: loss 16.129174984946985
Epoch 10: loss 16.129166142327296
Epoch 11: loss 16.129169291163866
Epoch 12: loss 16.12916629193852
Epoch 13: loss 16.12917239410583
Epoch 14: loss 16.12917032521856
Epoch 15: loss 16.129174976908946
Epoch 16: loss 16.129171397907697
Epoch 17: loss 16.12917794890868
Epoch 18: loss 16.12917089436351
Epoch 19: loss 16.129169840343366
-----------Time: 0:03:29.149071, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.12917995815889
Epoch 1: loss 16.129178978036837
Epoch 2: loss 16.129180667580567
Epoch 3: loss 16.129180980286176
Epoch 4: loss 16.129181834133245
Epoch 5: loss 16.12918219973433
Epoch 6: loss 16.129181204573364
Epoch 7: loss 16.12918102825511
Epoch 8: loss 16.129182911230338
Epoch 9: loss 16.129184231802196
Epoch 10: loss 16.12917920491694
Epoch 11: loss 16.1291860940341
Epoch 12: loss 16.12918677363725
Epoch 13: loss 16.129189379776605
Epoch 14: loss 16.129182592820317
Epoch 15: loss 16.12918342203469
Epoch 16: loss 16.129182527478847
Epoch 17: loss 16.12918403136983
Epoch 18: loss 16.12918461192361
Epoch 19: loss 16.1291838389755
-----------Time: 0:03:46.723777, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129163850190004
Epoch 1: loss 16.12917187733777
Epoch 2: loss 16.129169661172906
Epoch 3: loss 16.129168506029057
Epoch 4: loss 16.129163381909468
Epoch 5: loss 16.129173541470934
Epoch 6: loss 16.129171022194246
Epoch 7: loss 16.12917161415685
Epoch 8: loss 16.129170551320794
Epoch 9: loss 16.129172742852962
Epoch 10: loss 16.129171477769496
Epoch 11: loss 16.12916727776499
Epoch 12: loss 16.129172120812537
Epoch 13: loss 16.129168012078654
Epoch 14: loss 16.129166910089573
Epoch 15: loss 16.129172466966754
Epoch 16: loss 16.129168717092377
Epoch 17: loss 16.12917586990904
Epoch 18: loss 16.129171894191725
Epoch 19: loss 16.12917136782988
-----------Time: 0:04:28.523668, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129281541846122
Epoch 1: loss 16.129284184286295
Epoch 2: loss 16.12928824686628
Epoch 3: loss 16.129282499669102
Epoch 4: loss 16.129275338555107
Epoch 5: loss 16.129274582979534
Epoch 6: loss 16.129278231470916
Epoch 7: loss 16.129285855161037
Epoch 8: loss 16.129275346333856
Epoch 9: loss 16.129281096642533
Epoch 10: loss 16.12927721893742
Epoch 11: loss 16.1292769127141
Epoch 12: loss 16.129279635534655
Epoch 13: loss 16.129283968815017
Epoch 14: loss 16.129281111940735
Epoch 15: loss 16.12927980225912
Epoch 16: loss 16.1292878229246
Epoch 17: loss 16.12928482784792
Epoch 18: loss 16.129280443227827
Epoch 19: loss 16.12929039276314
-----------Time: 0:02:44.012692, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 20, rmse: 4.017230033874512-------------


Epoch 0: loss 16.129216126996322
Epoch 1: loss 16.129216960100067
Epoch 2: loss 16.129215406165816
Epoch 3: loss 16.129220162091404
Epoch 4: loss 16.12921982190089
Epoch 5: loss 16.129221329162664
Epoch 6: loss 16.129217282918045
Epoch 7: loss 16.129214969000262
Epoch 8: loss 16.12920982828602
Epoch 9: loss 16.129215653270663
Epoch 10: loss 16.12921277669022
Epoch 11: loss 16.12922179381312
Epoch 12: loss 16.129214953183478
Epoch 13: loss 16.12921854955726
Epoch 14: loss 16.129209015666305
Epoch 15: loss 16.12921857081917
Epoch 16: loss 16.129213441773043
Epoch 17: loss 16.129223033745305
Epoch 18: loss 16.129216353617135
Epoch 19: loss 16.129216919909876
-----------Time: 0:03:05.314339, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 50, rmse: 4.017192840576172-------------


Epoch 0: loss 16.12917787889996
Epoch 1: loss 16.129175224273084
Epoch 2: loss 16.129174413468412
Epoch 3: loss 16.129172289611336
Epoch 4: loss 16.129174915716142
Epoch 5: loss 16.129180464814578
Epoch 6: loss 16.129179122980812
Epoch 7: loss 16.129171157803725
Epoch 8: loss 16.12917715547654
Epoch 9: loss 16.12918333802423
Epoch 10: loss 16.12918238564637
Epoch 11: loss 16.129174338273863
Epoch 12: loss 16.129177568527975
Epoch 13: loss 16.129179945194313
Epoch 14: loss 16.12918008495246
Epoch 15: loss 16.129178328511507
Epoch 16: loss 16.129169896868923
Epoch 17: loss 16.12917760923675
Epoch 18: loss 16.1291805807179
Epoch 19: loss 16.129175982700865
-----------Time: 0:04:04.380560, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 100, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12917116195239
Epoch 1: loss 16.129172669473455
Epoch 2: loss 16.12917525046153
Epoch 3: loss 16.129176101715686
Epoch 4: loss 16.12917377690766
Epoch 5: loss 16.129176761094094
Epoch 6: loss 16.12917618157748
Epoch 7: loss 16.129171775436195
Epoch 8: loss 16.129170802833595
Epoch 9: loss 16.129174767401377
Epoch 10: loss 16.129172715108766
Epoch 11: loss 16.129172414849155
Epoch 12: loss 16.1291747451023
Epoch 13: loss 16.12917246618888
Epoch 14: loss 16.12917720733485
Epoch 15: loss 16.129176530324614
Epoch 16: loss 16.129174991169982
Epoch 17: loss 16.12917072349038
Epoch 18: loss 16.129176049598083
Epoch 19: loss 16.129170662816158
-----------Time: 0:04:35.021319, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917315771944
Epoch 1: loss 16.129173691082155
Epoch 2: loss 16.129174620123777
Epoch 3: loss 16.12917971883279
Epoch 4: loss 16.129173451237474
Epoch 5: loss 16.129181282101538
Epoch 6: loss 16.12917063507196
Epoch 7: loss 16.129180746145906
Epoch 8: loss 16.12917892514136
Epoch 9: loss 16.129185998614812
Epoch 10: loss 16.129176007333562
Epoch 11: loss 16.12917792920252
Epoch 12: loss 16.129177730844486
Epoch 13: loss 16.129179230327512
Epoch 14: loss 16.12918015547976
Epoch 15: loss 16.129179165245333
Epoch 16: loss 16.129179834995405
Epoch 17: loss 16.12917521649434
Epoch 18: loss 16.12918183672616
Epoch 19: loss 16.12917091795904
-----------Time: 0:05:17.408744, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129350905965502
Epoch 1: loss 16.129352435785638
Epoch 2: loss 16.129350403717773
Epoch 3: loss 16.129355591104495
Epoch 4: loss 16.129351071652803
Epoch 5: loss 16.129346575796635
Epoch 6: loss 16.129349704667757
Epoch 7: loss 16.129355455495016
Epoch 8: loss 16.12934857519377
Epoch 9: loss 16.12935570571136
Epoch 10: loss 16.129348980985043
Epoch 11: loss 16.1293552535069
Epoch 12: loss 16.12935158219786
Epoch 13: loss 16.12935001037249
Epoch 14: loss 16.129355922479096
Epoch 15: loss 16.129347571735476
Epoch 16: loss 16.12935020432257
Epoch 17: loss 16.129354127403705
Epoch 18: loss 16.12934928202253
Epoch 19: loss 16.129361601741888
-----------Time: 0:03:46.592011, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 20, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129188501815424
Epoch 1: loss 16.129188907347405
Epoch 2: loss 16.12918507475902
Epoch 3: loss 16.12918939351906
Epoch 4: loss 16.1291867720815
Epoch 5: loss 16.129180699214135
Epoch 6: loss 16.129189578134643
Epoch 7: loss 16.129197242533532
Epoch 8: loss 16.12918969144505
Epoch 9: loss 16.129185543558144
Epoch 10: loss 16.129188480812807
Epoch 11: loss 16.12919144736742
Epoch 12: loss 16.12918269705552
Epoch 13: loss 16.129191839934823
Epoch 14: loss 16.129190549440782
Epoch 15: loss 16.129185033272375
Epoch 16: loss 16.129189456526905
Epoch 17: loss 16.12919322766319
Epoch 18: loss 16.12919107813625
Epoch 19: loss 16.129188053759627
-----------Time: 0:03:53.064947, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 50, rmse: 4.017181873321533-------------


Epoch 0: loss 16.129159816909965
Epoch 1: loss 16.129161904984805
Epoch 2: loss 16.129164938695926
Epoch 3: loss 16.129171380794457
Epoch 4: loss 16.129166232560756
Epoch 5: loss 16.129165220027257
Epoch 6: loss 16.12916141284945
Epoch 7: loss 16.129170375002538
Epoch 8: loss 16.12916845494862
Epoch 9: loss 16.129167549761824
Epoch 10: loss 16.129170481830656
Epoch 11: loss 16.12916932876114
Epoch 12: loss 16.12916318251427
Epoch 13: loss 16.129164623656695
Epoch 14: loss 16.129158723477502
Epoch 15: loss 16.129165247512162
Epoch 16: loss 16.12917117206476
Epoch 17: loss 16.1291642523512
Epoch 18: loss 16.129163499887124
Epoch 19: loss 16.129161560127045
-----------Time: 0:04:59.314245, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129180144589515
Epoch 1: loss 16.129174191774137
Epoch 2: loss 16.129180405436813
Epoch 3: loss 16.129174755992548
Epoch 4: loss 16.129183429294855
Epoch 5: loss 16.129178323844258
Epoch 6: loss 16.129176312779006
Epoch 7: loss 16.129180273198124
Epoch 8: loss 16.129175926175307
Epoch 9: loss 16.12918219506708
Epoch 10: loss 16.129175708370404
Epoch 11: loss 16.129179802843254
Epoch 12: loss 16.129183769485365
Epoch 13: loss 16.129181385558866
Epoch 14: loss 16.129180200337196
Epoch 15: loss 16.12918009195333
Epoch 16: loss 16.1291814335278
Epoch 17: loss 16.12918017907529
Epoch 18: loss 16.12917879782921
Epoch 19: loss 16.129182821774755
-----------Time: 0:05:30.780658, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917774329048
Epoch 1: loss 16.129170749678828
Epoch 2: loss 16.129169861345982
Epoch 3: loss 16.12917905245351
Epoch 4: loss 16.129177052019212
Epoch 5: loss 16.129171159618764
Epoch 6: loss 16.129178700076295
Epoch 7: loss 16.12917389255169
Epoch 8: loss 16.12917731260722
Epoch 9: loss 16.129179297743317
Epoch 10: loss 16.12917365141055
Epoch 11: loss 16.129176187022605
Epoch 12: loss 16.129179652713447
Epoch 13: loss 16.129178905694495
Epoch 14: loss 16.129172048988778
Epoch 15: loss 16.129174672500667
Epoch 16: loss 16.12917173654246
Epoch 17: loss 16.12917243611106
Epoch 18: loss 16.129167122708644
Epoch 19: loss 16.12918145634546
-----------Time: 0:05:24.298516, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129255431446452
Epoch 1: loss 16.12925673205286
Epoch 2: loss 16.12925920336061
Epoch 3: loss 16.129258398778934
Epoch 4: loss 16.129251956939697
Epoch 5: loss 16.129262844851123
Epoch 6: loss 16.12925876438002
Epoch 7: loss 16.129261468013
Epoch 8: loss 16.129254683649624
Epoch 9: loss 16.129257998692076
Epoch 10: loss 16.12926314433286
Epoch 11: loss 16.129263929208378
Epoch 12: loss 16.129260136810185
Epoch 13: loss 16.12925765798298
Epoch 14: loss 16.129259310966603
Epoch 15: loss 16.129256228767964
Epoch 16: loss 16.12925758045481
Epoch 17: loss 16.12926122038957
Epoch 18: loss 16.129252735332926
Epoch 19: loss 16.129257592382217
-----------Time: 0:02:26.160291, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 20, rmse: 4.017192363739014-------------


Epoch 0: loss 16.12916408692319
Epoch 1: loss 16.129168123833313
Epoch 2: loss 16.129177320645255
Epoch 3: loss 16.12916139003179
Epoch 4: loss 16.129167445008036
Epoch 5: loss 16.129164860908464
Epoch 6: loss 16.129168076901543
Epoch 7: loss 16.129161384068084
Epoch 8: loss 16.129166504557592
Epoch 9: loss 16.129165469206438
Epoch 10: loss 16.129166903866576
Epoch 11: loss 16.129166416657757
Epoch 12: loss 16.12916900438741
Epoch 13: loss 16.129166459959443
Epoch 14: loss 16.129170205944448
Epoch 15: loss 16.1291640620312
Epoch 16: loss 16.129166005680652
Epoch 17: loss 16.12916870283134
Epoch 18: loss 16.129155702212376
Epoch 19: loss 16.129170394190112
-----------Time: 0:02:45.947401, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 50, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129177529374953
Epoch 1: loss 16.12917939731127
Epoch 2: loss 16.129179612004673
Epoch 3: loss 16.12917751692896
Epoch 4: loss 16.12918120198045
Epoch 5: loss 16.12917352435769
Epoch 6: loss 16.129177332313375
Epoch 7: loss 16.12918178123777
Epoch 8: loss 16.12918093568803
Epoch 9: loss 16.129177508372337
Epoch 10: loss 16.12917934856446
Epoch 11: loss 16.129178125226932
Epoch 12: loss 16.12917822168339
Epoch 13: loss 16.12918215824768
Epoch 14: loss 16.129179625747124
Epoch 15: loss 16.12917552842207
Epoch 16: loss 16.129183177263467
Epoch 17: loss 16.129172766707786
Epoch 18: loss 16.12917335192881
Epoch 19: loss 16.129179876741343
-----------Time: 0:03:48.801373, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129176786763956
Epoch 1: loss 16.129170506204062
Epoch 2: loss 16.12916989505388
Epoch 3: loss 16.129174040866456
Epoch 4: loss 16.12917063792417
Epoch 5: loss 16.129177613385416
Epoch 6: loss 16.129169038354604
Epoch 7: loss 16.12917274259367
Epoch 8: loss 16.129173568696544
Epoch 9: loss 16.12917561191395
Epoch 10: loss 16.12916778390209
Epoch 11: loss 16.129173531877147
Epoch 12: loss 16.129176344931157
Epoch 13: loss 16.12917664830227
Epoch 14: loss 16.129176702494203
Epoch 15: loss 16.129171228331025
Epoch 16: loss 16.129169674137483
Epoch 17: loss 16.129184785908237
Epoch 18: loss 16.129174104133593
Epoch 19: loss 16.129172910873887
-----------Time: 0:04:21.743191, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129167032993767
Epoch 1: loss 16.129172860830618
Epoch 2: loss 16.129175374662182
Epoch 3: loss 16.12916689297633
Epoch 4: loss 16.129174157806943
Epoch 5: loss 16.129167291248148
Epoch 6: loss 16.129172256422017
Epoch 7: loss 16.12916131794874
Epoch 8: loss 16.129163132989582
Epoch 9: loss 16.12916880447363
Epoch 10: loss 16.12916712374581
Epoch 11: loss 16.12917173654246
Epoch 12: loss 16.129161958139573
Epoch 13: loss 16.129170316921233
Epoch 14: loss 16.129173756423626
Epoch 15: loss 16.12917793775914
Epoch 16: loss 16.129176741387937
Epoch 17: loss 16.129166722621783
Epoch 18: loss 16.12917880794158
Epoch 19: loss 16.129161887612273
-----------Time: 0:05:01.907209, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12916835252846
Epoch 1: loss 16.12917096774302
Epoch 2: loss 16.129174542854894
Epoch 3: loss 16.129175309580003
Epoch 4: loss 16.1291730975638
Epoch 5: loss 16.129165529102785
Epoch 6: loss 16.129165749759892
Epoch 7: loss 16.12916864241641
Epoch 8: loss 16.129166644834317
Epoch 9: loss 16.129169532305006
Epoch 10: loss 16.129163946905752
Epoch 11: loss 16.129171319083067
Epoch 12: loss 16.12916025277906
Epoch 13: loss 16.129165076120447
Epoch 14: loss 16.129166764627012
Epoch 15: loss 16.129171136023235
Epoch 16: loss 16.1291670135469
Epoch 17: loss 16.129172053656024
Epoch 18: loss 16.12916881847537
Epoch 19: loss 16.129173311997913
-----------Time: 0:03:17.581277, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 20, rmse: 4.017207145690918-------------


Epoch 0: loss 16.129198734756397
Epoch 1: loss 16.129196616603732
Epoch 2: loss 16.12919920562985
Epoch 3: loss 16.129197055843616
Epoch 4: loss 16.1291931353554
Epoch 5: loss 16.129202053428926
Epoch 6: loss 16.129196543483516
Epoch 7: loss 16.12920152110338
Epoch 8: loss 16.129194079954512
Epoch 9: loss 16.129192292139283
Epoch 10: loss 16.12919525869389
Epoch 11: loss 16.129199071057535
Epoch 12: loss 16.12919334953022
Epoch 13: loss 16.129196554892346
Epoch 14: loss 16.129204229662896
Epoch 15: loss 16.129196539594144
Epoch 16: loss 16.129199822743733
Epoch 17: loss 16.129199871749837
Epoch 18: loss 16.129194527232432
Epoch 19: loss 16.12919723060612
-----------Time: 0:03:20.224489, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12919159723793
Epoch 1: loss 16.129193662235828
Epoch 2: loss 16.129196611158612
Epoch 3: loss 16.129191911499287
Epoch 4: loss 16.129194926022834
Epoch 5: loss 16.12918735082024
Epoch 6: loss 16.1291883122733
Epoch 7: loss 16.129193922305248
Epoch 8: loss 16.12919305990156
Epoch 9: loss 16.129198831731433
Epoch 10: loss 16.129195674597536
Epoch 11: loss 16.129186009764346
Epoch 12: loss 16.129194073212933
Epoch 13: loss 16.129197095774515
Epoch 14: loss 16.12919613250641
Epoch 15: loss 16.1292001888634
Epoch 16: loss 16.129193860853153
Epoch 17: loss 16.12919189957188
Epoch 18: loss 16.1291890885922
Epoch 19: loss 16.12919261806876
-----------Time: 0:04:34.806929, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129171867225402
Epoch 1: loss 16.129178294285023
Epoch 2: loss 16.129178152452546
Epoch 3: loss 16.12917367345033
Epoch 4: loss 16.12917536584627
Epoch 5: loss 16.129177961354674
Epoch 6: loss 16.129174700763446
Epoch 7: loss 16.129173029629417
Epoch 8: loss 16.129172026948996
Epoch 9: loss 16.129172768263533
Epoch 10: loss 16.129175960661083
Epoch 11: loss 16.129172297649372
Epoch 12: loss 16.12916834578688
Epoch 13: loss 16.129173049076282
Epoch 14: loss 16.1291778623053
Epoch 15: loss 16.12916770870754
Epoch 16: loss 16.129166463330233
Epoch 17: loss 16.129180101547117
Epoch 18: loss 16.129169217525064
Epoch 19: loss 16.129170220205484
-----------Time: 0:05:01.659834, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129181569396575
Epoch 1: loss 16.129176573626303
Epoch 2: loss 16.12917986248031
Epoch 3: loss 16.129171000413756
Epoch 4: loss 16.12916476652634
Epoch 5: loss 16.129178431190958
Epoch 6: loss 16.129170074742927
Epoch 7: loss 16.12917585512942
Epoch 8: loss 16.129178755564688
Epoch 9: loss 16.129171577856035
Epoch 10: loss 16.12917724467283
Epoch 11: loss 16.129172090475425
Epoch 12: loss 16.12917025469126
Epoch 13: loss 16.12918035824575
Epoch 14: loss 16.12917510836976
Epoch 15: loss 16.12917073178771
Epoch 16: loss 16.12917498598415
Epoch 17: loss 16.12917200620567
Epoch 18: loss 16.129177031016596
Epoch 19: loss 16.129176406642546
-----------Time: 0:05:47.207704, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 200, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129234529695413
Epoch 1: loss 16.129223537030203
Epoch 2: loss 16.129229140579863
Epoch 3: loss 16.1292286147366
Epoch 4: loss 16.12923447057694
Epoch 5: loss 16.129233262796905
Epoch 6: loss 16.129232858042798
Epoch 7: loss 16.129238795819266
Epoch 8: loss 16.129233283799522
Epoch 9: loss 16.129234217249095
Epoch 10: loss 16.12922959226574
Epoch 11: loss 16.129238800745803
Epoch 12: loss 16.12922776711253
Epoch 13: loss 16.1292379847553
Epoch 14: loss 16.12922956218792
Epoch 15: loss 16.129236567467693
Epoch 16: loss 16.129231454497642
Epoch 17: loss 16.129234534103368
Epoch 18: loss 16.129234942228265
Epoch 19: loss 16.1292325756743
-----------Time: 0:04:16.330509, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 20, rmse: 4.017233848571777-------------


Epoch 0: loss 16.12920731549162
Epoch 1: loss 16.12921027400819
Epoch 2: loss 16.129207197773255
Epoch 3: loss 16.129199853080845
Epoch 4: loss 16.12920127036845
Epoch 5: loss 16.129206307106784
Epoch 6: loss 16.129203370629995
Epoch 7: loss 16.129205665100912
Epoch 8: loss 16.12920678783332
Epoch 9: loss 16.129203469679368
Epoch 10: loss 16.12920484288741
Epoch 11: loss 16.129199958353215
Epoch 12: loss 16.129201714794164
Epoch 13: loss 16.12920078445609
Epoch 14: loss 16.12919848402147
Epoch 15: loss 16.129206494833866
Epoch 16: loss 16.129206127417742
Epoch 17: loss 16.12919848765155
Epoch 18: loss 16.12920533320773
Epoch 19: loss 16.129202841156655
-----------Time: 0:04:18.028029, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 50, rmse: 4.017195224761963-------------


Epoch 0: loss 16.12917463075473
Epoch 1: loss 16.12917280430506
Epoch 2: loss 16.12917894199531
Epoch 3: loss 16.129169037058148
Epoch 4: loss 16.12917359410712
Epoch 5: loss 16.12918419705713
Epoch 6: loss 16.129177502667922
Epoch 7: loss 16.129172865238573
Epoch 8: loss 16.129184109157293
Epoch 9: loss 16.12917737354073
Epoch 10: loss 16.1291709866713
Epoch 11: loss 16.129171895747472
Epoch 12: loss 16.129176570774096
Epoch 13: loss 16.129178184345406
Epoch 14: loss 16.12917497198241
Epoch 15: loss 16.129177789703668
Epoch 16: loss 16.12917145469255
Epoch 17: loss 16.12917427448814
Epoch 18: loss 16.129174820815436
Epoch 19: loss 16.12917205598965
-----------Time: 0:03:17.017321, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12918024571322
Epoch 1: loss 16.129165887702996
Epoch 2: loss 16.129175633175855
Epoch 3: loss 16.129169023315693
Epoch 4: loss 16.129173165238896
Epoch 5: loss 16.1291757177049
Epoch 6: loss 16.12917314086549
Epoch 7: loss 16.129176757982595
Epoch 8: loss 16.129166461515194
Epoch 9: loss 16.129177646834023
Epoch 10: loss 16.129172914503968
Epoch 11: loss 16.129171773621152
Epoch 12: loss 16.12917278252457
Epoch 13: loss 16.129174191514846
Epoch 14: loss 16.12918319411742
Epoch 15: loss 16.129174393762252
Epoch 16: loss 16.129176930670766
Epoch 17: loss 16.12917939860773
Epoch 18: loss 16.129171774917612
Epoch 19: loss 16.1291772947161
-----------Time: 0:03:46.606597, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917545296823
Epoch 1: loss 16.12916982634162
Epoch 2: loss 16.129175413296622
Epoch 3: loss 16.129176957118506
Epoch 4: loss 16.129170424527224
Epoch 5: loss 16.129179697570883
Epoch 6: loss 16.12917145832263
Epoch 7: loss 16.129169201189697
Epoch 8: loss 16.12917583283035
Epoch 9: loss 16.12917077897877
Epoch 10: loss 16.12917108157201
Epoch 11: loss 16.12917540940725
Epoch 12: loss 16.129175716149152
Epoch 13: loss 16.129175614506867
Epoch 14: loss 16.129172622800976
Epoch 15: loss 16.12917951736326
Epoch 16: loss 16.12917644761061
Epoch 17: loss 16.129169081915585
Epoch 18: loss 16.129167625474956
Epoch 19: loss 16.129173763165205
-----------Time: 0:04:29.439308, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129476723559463
Epoch 1: loss 16.129472569968144
Epoch 2: loss 16.12948070887057
Epoch 3: loss 16.12948004949216
Epoch 4: loss 16.129480026674504
Epoch 5: loss 16.129485285107112
Epoch 6: loss 16.129479107745254
Epoch 7: loss 16.129493286066435
Epoch 8: loss 16.12948439418135
Epoch 9: loss 16.129479630217727
Epoch 10: loss 16.129477022781913
Epoch 11: loss 16.129480350529647
Epoch 12: loss 16.129480819847352
Epoch 13: loss 16.129475109728865
Epoch 14: loss 16.12947173219715
Epoch 15: loss 16.12948514897905
Epoch 16: loss 16.129480813883646
Epoch 17: loss 16.12948236989223
Epoch 18: loss 16.129484087958033
Epoch 19: loss 16.129476732375377
-----------Time: 0:02:44.084354, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 20, rmse: 4.0172119140625-------------


Epoch 0: loss 16.12921333624138
Epoch 1: loss 16.129219645323346
Epoch 2: loss 16.129209260437527
Epoch 3: loss 16.129209042373333
Epoch 4: loss 16.129209788873702
Epoch 5: loss 16.129209181612897
Epoch 6: loss 16.129204388090034
Epoch 7: loss 16.129208677290833
Epoch 8: loss 16.129201465096404
Epoch 9: loss 16.129208748077424
Epoch 10: loss 16.129213375135112
Epoch 11: loss 16.129213178073538
Epoch 12: loss 16.12921055196873
Epoch 13: loss 16.129212033819933
Epoch 14: loss 16.129209750757845
Epoch 15: loss 16.129206947297618
Epoch 16: loss 16.129208572537046
Epoch 17: loss 16.1292113137673
Epoch 18: loss 16.129207591377828
Epoch 19: loss 16.129216579978657
-----------Time: 0:03:05.959517, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129186351251317
Epoch 1: loss 16.129176802062158
Epoch 2: loss 16.129183556347712
Epoch 3: loss 16.129185850559335
Epoch 4: loss 16.1291762884056
Epoch 5: loss 16.129176300851594
Epoch 6: loss 16.12917943879792
Epoch 7: loss 16.129177939574184
Epoch 8: loss 16.129180753665363
Epoch 9: loss 16.129174327383616
Epoch 10: loss 16.12918142963843
Epoch 11: loss 16.129182017970955
Epoch 12: loss 16.129177569305853
Epoch 13: loss 16.129173068004565
Epoch 14: loss 16.12917794761222
Epoch 15: loss 16.129181525835595
Epoch 16: loss 16.12918679515845
Epoch 17: loss 16.129179781062764
Epoch 18: loss 16.129178706817875
Epoch 19: loss 16.129176380713393
-----------Time: 0:04:06.693164, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129176500246796
Epoch 1: loss 16.129173665930875
Epoch 2: loss 16.129169487447566
Epoch 3: loss 16.129176475095516
Epoch 4: loss 16.129172245531773
Epoch 5: loss 16.12918184009695
Epoch 6: loss 16.129178313731888
Epoch 7: loss 16.129174822371183
Epoch 8: loss 16.12917778166563
Epoch 9: loss 16.129171006896044
Epoch 10: loss 16.129171028157952
Epoch 11: loss 16.129176650895186
Epoch 12: loss 16.129171224960235
Epoch 13: loss 16.12917827172666
Epoch 14: loss 16.12916776315877
Epoch 15: loss 16.12917463568127
Epoch 16: loss 16.12917253490114
Epoch 17: loss 16.129174720728898
Epoch 18: loss 16.12917086921223
Epoch 19: loss 16.129177669133096
-----------Time: 0:04:35.156298, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129178733265615
Epoch 1: loss 16.129171241295605
Epoch 2: loss 16.12917216852218
Epoch 3: loss 16.129175338361367
Epoch 4: loss 16.129171053568523
Epoch 5: loss 16.1291764909123
Epoch 6: loss 16.129167638439533
Epoch 7: loss 16.129176717792404
Epoch 8: loss 16.12916923541618
Epoch 9: loss 16.129176269736607
Epoch 10: loss 16.12917300499672
Epoch 11: loss 16.12916951622893
Epoch 12: loss 16.129172424702233
Epoch 13: loss 16.129175973107078
Epoch 14: loss 16.129180449257085
Epoch 15: loss 16.129177794111623
Epoch 16: loss 16.12917173757963
Epoch 17: loss 16.1291771658482
Epoch 18: loss 16.129176483392847
Epoch 19: loss 16.12918498737777
-----------Time: 0:05:18.518744, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 200, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129257504482382
Epoch 1: loss 16.129269381850357
Epoch 2: loss 16.12926335980414
Epoch 3: loss 16.129267196281894
Epoch 4: loss 16.129255907765025
Epoch 5: loss 16.129260353059337
Epoch 6: loss 16.129263893944728
Epoch 7: loss 16.129263234047738
Epoch 8: loss 16.129259247958757
Epoch 9: loss 16.129265310973043
Epoch 10: loss 16.1292592282526
Epoch 11: loss 16.12926411667617
Epoch 12: loss 16.129269208643603
Epoch 13: loss 16.12925557535326
Epoch 14: loss 16.129265945459462
Epoch 15: loss 16.129262700944313
Epoch 16: loss 16.129256026001972
Epoch 17: loss 16.129261971816476
Epoch 18: loss 16.129261198090497
Epoch 19: loss 16.12926460077349
-----------Time: 0:03:48.939431, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 20, rmse: 4.017214298248291-------------


Epoch 0: loss 16.129193760248032
Epoch 1: loss 16.1291849067381
Epoch 2: loss 16.129185892564568
Epoch 3: loss 16.1291926535917
Epoch 4: loss 16.12918295323557
Epoch 5: loss 16.129188969577378
Epoch 6: loss 16.129188770700758
Epoch 7: loss 16.129189596025757
Epoch 8: loss 16.129189068108165
Epoch 9: loss 16.129193715909178
Epoch 10: loss 16.129196129394913
Epoch 11: loss 16.129190033969184
Epoch 12: loss 16.129192991707882
Epoch 13: loss 16.129187368711353
Epoch 14: loss 16.12919047683915
Epoch 15: loss 16.129189681073388
Epoch 16: loss 16.12918826378578
Epoch 17: loss 16.12918511417134
Epoch 18: loss 16.12919524132136
Epoch 19: loss 16.129190648749447
-----------Time: 0:04:01.344123, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 50, rmse: 4.017192840576172-------------


Epoch 0: loss 16.12918657113055
Epoch 1: loss 16.129186285131972
Epoch 2: loss 16.129187174761277
Epoch 3: loss 16.129191107176904
Epoch 4: loss 16.129180430847384
Epoch 5: loss 16.129189524720584
Epoch 6: loss 16.12919201703095
Epoch 7: loss 16.12918679386199
Epoch 8: loss 16.12918837087319
Epoch 9: loss 16.129189258687454
Epoch 10: loss 16.1291848483975
Epoch 11: loss 16.129186587725208
Epoch 12: loss 16.12918908392495
Epoch 13: loss 16.129190921783447
Epoch 14: loss 16.129181780200604
Epoch 15: loss 16.129191433365673
Epoch 16: loss 16.12918937044211
Epoch 17: loss 16.129180406473978
Epoch 18: loss 16.129184480462794
Epoch 19: loss 16.129189195420317
-----------Time: 0:05:08.746344, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 100, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129172424961524
Epoch 1: loss 16.129179518659715
Epoch 2: loss 16.129179486248272
Epoch 3: loss 16.12917235391564
Epoch 4: loss 16.129173152015028
Epoch 5: loss 16.1291764172735
Epoch 6: loss 16.129173217356495
Epoch 7: loss 16.12918119342383
Epoch 8: loss 16.1291672007554
Epoch 9: loss 16.12917525875886
Epoch 10: loss 16.129178674665724
Epoch 11: loss 16.12917396437545
Epoch 12: loss 16.129176780022377
Epoch 13: loss 16.129172688401738
Epoch 14: loss 16.129175371809975
Epoch 15: loss 16.12917439091005
Epoch 16: loss 16.12917757060231
Epoch 17: loss 16.12917284449525
Epoch 18: loss 16.129175053659246
Epoch 19: loss 16.12918024960259
-----------Time: 0:05:35.714196, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129167964887593
Epoch 1: loss 16.129167329104714
Epoch 2: loss 16.12917163593734
Epoch 3: loss 16.129172962991486
Epoch 4: loss 16.12916932539035
Epoch 5: loss 16.129164556500186
Epoch 6: loss 16.12917285642266
Epoch 7: loss 16.129167380444443
Epoch 8: loss 16.12916910239962
Epoch 9: loss 16.129173315109412
Epoch 10: loss 16.129169435329967
Epoch 11: loss 16.1291676700731
Epoch 12: loss 16.1291737724997
Epoch 13: loss 16.12916944077509
Epoch 14: loss 16.12916581458278
Epoch 15: loss 16.129171000673047
Epoch 16: loss 16.129169573791653
Epoch 17: loss 16.12917384198984
Epoch 18: loss 16.129171499031404
Epoch 19: loss 16.129167951404433
-----------Time: 0:05:13.073938, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129458149468658
Epoch 1: loss 16.129459787413374
Epoch 2: loss 16.129461473586314
Epoch 3: loss 16.12946297306934
Epoch 4: loss 16.129459906687487
Epoch 5: loss 16.1294526379675
Epoch 6: loss 16.129464869009144
Epoch 7: loss 16.129456797781817
Epoch 8: loss 16.129459667102093
Epoch 9: loss 16.12945954912444
Epoch 10: loss 16.129467275234717
Epoch 11: loss 16.129459608761497
Epoch 12: loss 16.129466454317676
Epoch 13: loss 16.12946116658512
Epoch 14: loss 16.129455118350453
Epoch 15: loss 16.129455420165815
Epoch 16: loss 16.129453590345356
Epoch 17: loss 16.12945980711953
Epoch 18: loss 16.12946141809792
Epoch 19: loss 16.129461880674047
-----------Time: 0:02:28.938137, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 20, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129162685452368
Epoch 1: loss 16.12915741224014
Epoch 2: loss 16.12916558718409
Epoch 3: loss 16.129163296083966
Epoch 4: loss 16.12916707707333
Epoch 5: loss 16.129158551048622
Epoch 6: loss 16.1291638595245
Epoch 7: loss 16.129161455632552
Epoch 8: loss 16.129164391072177
Epoch 9: loss 16.129164010691476
Epoch 10: loss 16.12916255710305
Epoch 11: loss 16.129160528146684
Epoch 12: loss 16.12916192909892
Epoch 13: loss 16.129164971885245
Epoch 14: loss 16.129165274737776
Epoch 15: loss 16.129165656155642
Epoch 16: loss 16.129160852001828
Epoch 17: loss 16.129158050875226
Epoch 18: loss 16.12916328000789
Epoch 19: loss 16.129163873007663
-----------Time: 0:02:51.236290, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 50, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129164576984216
Epoch 1: loss 16.12916017939955
Epoch 2: loss 16.129161866868948
Epoch 3: loss 16.12915956021133
Epoch 4: loss 16.129155003940237
Epoch 5: loss 16.129160617342976
Epoch 6: loss 16.129163771365373
Epoch 7: loss 16.12916580654474
Epoch 8: loss 16.129160205587997
Epoch 9: loss 16.1291641019621
Epoch 10: loss 16.129160139727944
Epoch 11: loss 16.12916767474035
Epoch 12: loss 16.129158916649708
Epoch 13: loss 16.12916904535548
Epoch 14: loss 16.129157099793826
Epoch 15: loss 16.12916277283362
Epoch 16: loss 16.129158157962635
Epoch 17: loss 16.129166831783525
Epoch 18: loss 16.129167778716262
Epoch 19: loss 16.129164735670646
-----------Time: 0:03:46.582670, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129165437054283
Epoch 1: loss 16.12916664639007
Epoch 2: loss 16.129172752187458
Epoch 3: loss 16.12917142798552
Epoch 4: loss 16.12916768433414
Epoch 5: loss 16.12916438329343
Epoch 6: loss 16.12917177310257
Epoch 7: loss 16.129173323925322
Epoch 8: loss 16.129174189699803
Epoch 9: loss 16.129176617187284
Epoch 10: loss 16.129168752096735
Epoch 11: loss 16.12916857033336
Epoch 12: loss 16.129171472324373
Epoch 13: loss 16.129169544751
Epoch 14: loss 16.12916786557893
Epoch 15: loss 16.12917126074247
Epoch 16: loss 16.129176688751752
Epoch 17: loss 16.129172284166213
Epoch 18: loss 16.12917424622536
Epoch 19: loss 16.129173188315843
-----------Time: 0:04:21.376056, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129176289183476
Epoch 1: loss 16.12918185487657
Epoch 2: loss 16.129177037239593
Epoch 3: loss 16.129174378982636
Epoch 4: loss 16.129173010701134
Epoch 5: loss 16.12917285071825
Epoch 6: loss 16.129173217356495
Epoch 7: loss 16.129175052362786
Epoch 8: loss 16.129177335424874
Epoch 9: loss 16.129173350632353
Epoch 10: loss 16.12917414199016
Epoch 11: loss 16.129169049763433
Epoch 12: loss 16.129173013812633
Epoch 13: loss 16.12917743343708
Epoch 14: loss 16.129177835598274
Epoch 15: loss 16.129174909493145
Epoch 16: loss 16.12917421225817
Epoch 17: loss 16.129178308286765
Epoch 18: loss 16.129170698079808
Epoch 19: loss 16.129173350632353
-----------Time: 0:05:01.423217, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12941191415543
Epoch 1: loss 16.12941096151828
Epoch 2: loss 16.12940354163132
Epoch 3: loss 16.129407397037358
Epoch 4: loss 16.12941047119796
Epoch 5: loss 16.129409547601465
Epoch 6: loss 16.129412209747795
Epoch 7: loss 16.129408025560075
Epoch 8: loss 16.12941305970549
Epoch 9: loss 16.12941501943102
Epoch 10: loss 16.129411049158822
Epoch 11: loss 16.129407550537955
Epoch 12: loss 16.129411278891137
Epoch 13: loss 16.129409043797985
Epoch 14: loss 16.129404354510324
Epoch 15: loss 16.129408538179465
Epoch 16: loss 16.12940774915528
Epoch 17: loss 16.129410140601237
Epoch 18: loss 16.12941565106523
Epoch 19: loss 16.129408251403014
-----------Time: 0:03:17.915505, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 20, rmse: 4.017210006713867-------------


Epoch 0: loss 16.129182333528767
Epoch 1: loss 16.129180809413047
Epoch 2: loss 16.129183162743143
Epoch 3: loss 16.12918332298532
Epoch 4: loss 16.12917930681852
Epoch 5: loss 16.12918185824736
Epoch 6: loss 16.129182194029916
Epoch 7: loss 16.12918544269373
Epoch 8: loss 16.12918345211251
Epoch 9: loss 16.129190052638176
Epoch 10: loss 16.129182219181196
Epoch 11: loss 16.129179835513987
Epoch 12: loss 16.129185195329594
Epoch 13: loss 16.129178015028025
Epoch 14: loss 16.129173886847276
Epoch 15: loss 16.12917531684017
Epoch 16: loss 16.129181693597225
Epoch 17: loss 16.12918789040595
Epoch 18: loss 16.12918415531119
Epoch 19: loss 16.129183243901398
-----------Time: 0:03:29.341435, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 50, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129180301720194
Epoch 1: loss 16.129166952613385
Epoch 2: loss 16.129175131187417
Epoch 3: loss 16.129170118044613
Epoch 4: loss 16.129164967995873
Epoch 5: loss 16.129169519081138
Epoch 6: loss 16.12917580275253
Epoch 7: loss 16.129174022197464
Epoch 8: loss 16.129175965846915
Epoch 9: loss 16.129169462296286
Epoch 10: loss 16.129177678986178
Epoch 11: loss 16.129173089525764
Epoch 12: loss 16.129171053827815
Epoch 13: loss 16.129170273878835
Epoch 14: loss 16.129170048554478
Epoch 15: loss 16.12916372417431
Epoch 16: loss 16.129174213295336
Epoch 17: loss 16.12916961994555
Epoch 18: loss 16.129167089260033
Epoch 19: loss 16.129163676464668
-----------Time: 0:04:34.327540, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129177508372337
Epoch 1: loss 16.129174169215773
Epoch 2: loss 16.129173570770877
Epoch 3: loss 16.12916423083
Epoch 4: loss 16.129172831789965
Epoch 5: loss 16.129175698517326
Epoch 6: loss 16.129174292119966
Epoch 7: loss 16.129175247609325
Epoch 8: loss 16.129173193501675
Epoch 9: loss 16.129178637846323
Epoch 10: loss 16.12917340223137
Epoch 11: loss 16.12917578408354
Epoch 12: loss 16.129174857894125
Epoch 13: loss 16.129177890049498
Epoch 14: loss 16.12917478970045
Epoch 15: loss 16.12917301873917
Epoch 16: loss 16.12918220336441
Epoch 17: loss 16.12917171165047
Epoch 18: loss 16.129172023318915
Epoch 19: loss 16.129171622972763
-----------Time: 0:05:03.893828, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 150, rmse: 4.017190933227539-------------


Epoch 0: loss 16.12917202876404
Epoch 1: loss 16.129165682344087
Epoch 2: loss 16.12917353109927
Epoch 3: loss 16.129174758844755
Epoch 4: loss 16.129171765842408
Epoch 5: loss 16.12917350076216
Epoch 6: loss 16.129171952791612
Epoch 7: loss 16.12918169619014
Epoch 8: loss 16.129164064624117
Epoch 9: loss 16.129174499034622
Epoch 10: loss 16.129169633947292
Epoch 11: loss 16.129174983391234
Epoch 12: loss 16.129174363425143
Epoch 13: loss 16.129172180190302
Epoch 14: loss 16.129180044243686
Epoch 15: loss 16.129171507588023
Epoch 16: loss 16.12917420240509
Epoch 17: loss 16.129168717092377
Epoch 18: loss 16.129175096183058
Epoch 19: loss 16.12917585305509
-----------Time: 0:05:50.828356, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12916245960943
Epoch 1: loss 16.1291660930619
Epoch 2: loss 16.12916704180968
Epoch 3: loss 16.129156457269367
Epoch 4: loss 16.129161324171736
Epoch 5: loss 16.129167178456324
Epoch 6: loss 16.12916047758483
Epoch 7: loss 16.129166246562498
Epoch 8: loss 16.12916234941052
Epoch 9: loss 16.129164792974077
Epoch 10: loss 16.129163428063364
Epoch 11: loss 16.12915704197181
Epoch 12: loss 16.129168274222412
Epoch 13: loss 16.12917016212418
Epoch 14: loss 16.12916448882509
Epoch 15: loss 16.129159490461905
Epoch 16: loss 16.12916549695063
Epoch 17: loss 16.12915798968242
Epoch 18: loss 16.12916314932495
Epoch 19: loss 16.129163832817472
-----------Time: 0:04:17.743458, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 20, rmse: 4.017175197601318-------------


Epoch 0: loss 16.129192393003695
Epoch 1: loss 16.129190293001443
Epoch 2: loss 16.1291923652595
Epoch 3: loss 16.12919531340441
Epoch 4: loss 16.12918960458238
Epoch 5: loss 16.129194339764645
Epoch 6: loss 16.129193175286296
Epoch 7: loss 16.129195965781946
Epoch 8: loss 16.129191789113676
Epoch 9: loss 16.129188375021855
Epoch 10: loss 16.129192516945054
Epoch 11: loss 16.12919037623403
Epoch 12: loss 16.129191469147905
Epoch 13: loss 16.12919218271825
Epoch 14: loss 16.129192281249036
Epoch 15: loss 16.129195414787404
Epoch 16: loss 16.12919233829318
Epoch 17: loss 16.12919594140854
Epoch 18: loss 16.129192084446753
Epoch 19: loss 16.129201069676792
-----------Time: 0:03:59.966804, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 50, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129152592528833
Epoch 1: loss 16.129158724773962
Epoch 2: loss 16.12916106851027
Epoch 3: loss 16.12916033549306
Epoch 4: loss 16.129160471880418
Epoch 5: loss 16.129160399797367
Epoch 6: loss 16.129155791927253
Epoch 7: loss 16.12916477845375
Epoch 8: loss 16.129162509652698
Epoch 9: loss 16.129158434626717
Epoch 10: loss 16.129155018979144
Epoch 11: loss 16.1291592482836
Epoch 12: loss 16.12915995174157
Epoch 13: loss 16.12916758969272
Epoch 14: loss 16.129160965052943
Epoch 15: loss 16.129156918808324
Epoch 16: loss 16.129159438603594
Epoch 17: loss 16.129160863669945
Epoch 18: loss 16.129156884322548
Epoch 19: loss 16.12915738812603
-----------Time: 0:03:19.037183, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 100, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129185669833127
Epoch 1: loss 16.129181862655315
Epoch 2: loss 16.129181294547532
Epoch 3: loss 16.129179992385374
Epoch 4: loss 16.12917782574519
Epoch 5: loss 16.129170935590867
Epoch 6: loss 16.129183298871204
Epoch 7: loss 16.129178225572762
Epoch 8: loss 16.129183354359597
Epoch 9: loss 16.129179521771214
Epoch 10: loss 16.129171420206774
Epoch 11: loss 16.12917803784568
Epoch 12: loss 16.129182000339128
Epoch 13: loss 16.12917858702518
Epoch 14: loss 16.129182279336835
Epoch 15: loss 16.12917815297113
Epoch 16: loss 16.129183159372353
Epoch 17: loss 16.129176200246473
Epoch 18: loss 16.129185627827894
Epoch 19: loss 16.129172191858423
-----------Time: 0:03:46.956768, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129187722125735
Epoch 1: loss 16.12918177501477
Epoch 2: loss 16.12917998123584
Epoch 3: loss 16.12917571122261
Epoch 4: loss 16.129184154792608
Epoch 5: loss 16.129187337337076
Epoch 6: loss 16.129181971817058
Epoch 7: loss 16.12918108815146
Epoch 8: loss 16.12917852972175
Epoch 9: loss 16.129180563863947
Epoch 10: loss 16.129181013994078
Epoch 11: loss 16.129177111137686
Epoch 12: loss 16.129185877525657
Epoch 13: loss 16.129172848384624
Epoch 14: loss 16.129182334306645
Epoch 15: loss 16.129178273282406
Epoch 16: loss 16.129177436289286
Epoch 17: loss 16.129182282189042
Epoch 18: loss 16.129174345793317
Epoch 19: loss 16.129177783999253
-----------Time: 0:04:29.913504, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12933807647896
Epoch 1: loss 16.129347219617554
Epoch 2: loss 16.12934290500618
Epoch 3: loss 16.129343126441164
Epoch 4: loss 16.129345003452684
Epoch 5: loss 16.129342443207932
Epoch 6: loss 16.129343600944697
Epoch 7: loss 16.129346005614522
Epoch 8: loss 16.129343467928134
Epoch 9: loss 16.129335011912143
Epoch 10: loss 16.129342837071796
Epoch 11: loss 16.129336470167814
Epoch 12: loss 16.129340997657547
Epoch 13: loss 16.12933714432584
Epoch 14: loss 16.129338748562652
Epoch 15: loss 16.129342583743952
Epoch 16: loss 16.129341894806306
Epoch 17: loss 16.12935136335579
Epoch 18: loss 16.129343520305024
Epoch 19: loss 16.12934740164022
-----------Time: 0:02:45.163001, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 20, rmse: 4.017215251922607-------------


Epoch 0: loss 16.129152531595317
Epoch 1: loss 16.129151465647762
Epoch 2: loss 16.129154208174473
Epoch 3: loss 16.129157148540635
Epoch 4: loss 16.129151071783898
Epoch 5: loss 16.12915011422021
Epoch 6: loss 16.129154158909078
Epoch 7: loss 16.129152494516628
Epoch 8: loss 16.129154008260688
Epoch 9: loss 16.12915523082034
Epoch 10: loss 16.12915330895138
Epoch 11: loss 16.129151163573106
Epoch 12: loss 16.129157855110105
Epoch 13: loss 16.12915023323503
Epoch 14: loss 16.12914947817804
Epoch 15: loss 16.12916036738592
Epoch 16: loss 16.12914744403584
Epoch 17: loss 16.12915961362539
Epoch 18: loss 16.129151831508135
Epoch 19: loss 16.129150642656384
-----------Time: 0:03:07.923450, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 50, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129174338014572
Epoch 1: loss 16.12917535080736
Epoch 2: loss 16.1291783225478
Epoch 3: loss 16.129177544932446
Epoch 4: loss 16.12917441969141
Epoch 5: loss 16.12918475453396
Epoch 6: loss 16.129165564107144
Epoch 7: loss 16.12917686766292
Epoch 8: loss 16.129178617880875
Epoch 9: loss 16.129172505341902
Epoch 10: loss 16.129180635687707
Epoch 11: loss 16.12917966152936
Epoch 12: loss 16.12917142798552
Epoch 13: loss 16.129179655306363
Epoch 14: loss 16.129171789178645
Epoch 15: loss 16.129170719860298
Epoch 16: loss 16.129179035340268
Epoch 17: loss 16.129176413124835
Epoch 18: loss 16.12917718244286
Epoch 19: loss 16.12917951451105
-----------Time: 0:04:07.146267, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 100, rmse: 4.017190933227539-------------


Epoch 0: loss 16.12917024665322
Epoch 1: loss 16.129170864804273
Epoch 2: loss 16.129173076561187
Epoch 3: loss 16.12917018260821
Epoch 4: loss 16.129168296521485
Epoch 5: loss 16.12915971423051
Epoch 6: loss 16.129167443192998
Epoch 7: loss 16.129172465411006
Epoch 8: loss 16.12915945830975
Epoch 9: loss 16.129169913982164
Epoch 10: loss 16.129167337661336
Epoch 11: loss 16.12916360956745
Epoch 12: loss 16.129163165401025
Epoch 13: loss 16.12916333523699
Epoch 14: loss 16.12916483031206
Epoch 15: loss 16.129164249758283
Epoch 16: loss 16.1291688820018
Epoch 17: loss 16.129167841724108
Epoch 18: loss 16.12917008563317
Epoch 19: loss 16.1291611719676
-----------Time: 0:04:35.032056, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129174455473642
Epoch 1: loss 16.129181892733136
Epoch 2: loss 16.12916862530317
Epoch 3: loss 16.129170210352406
Epoch 4: loss 16.12917632963296
Epoch 5: loss 16.129167429191252
Epoch 6: loss 16.12917434657119
Epoch 7: loss 16.12916570101308
Epoch 8: loss 16.12917235443422
Epoch 9: loss 16.129171613378976
Epoch 10: loss 16.129172055730358
Epoch 11: loss 16.129174340348193
Epoch 12: loss 16.12917252478877
Epoch 13: loss 16.12917759264209
Epoch 14: loss 16.12917006488985
Epoch 15: loss 16.129174203442258
Epoch 16: loss 16.129170675262152
Epoch 17: loss 16.12917508114415
Epoch 18: loss 16.129174825482682
Epoch 19: loss 16.129175116667092
-----------Time: 0:05:20.553392, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12918995410739
Epoch 1: loss 16.12919875264751
Epoch 2: loss 16.12919695420133
Epoch 3: loss 16.129194158779143
Epoch 4: loss 16.12918575384359
Epoch 5: loss 16.129191778223433
Epoch 6: loss 16.12919233699672
Epoch 7: loss 16.129189496976387
Epoch 8: loss 16.12918674796739
Epoch 9: loss 16.129194864052156
Epoch 10: loss 16.129197929396845
Epoch 11: loss 16.129190800435
Epoch 12: loss 16.129189767936055
Epoch 13: loss 16.129187308815006
Epoch 14: loss 16.12919317710134
Epoch 15: loss 16.129193251258723
Epoch 16: loss 16.129189991704663
Epoch 17: loss 16.1291892143486
Epoch 18: loss 16.1291911779635
Epoch 19: loss 16.1291890823692
-----------Time: 0:03:48.525399, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 20, rmse: 4.017211437225342-------------


Epoch 0: loss 16.129180922204867
Epoch 1: loss 16.129177439400785
Epoch 2: loss 16.12917704994488
Epoch 3: loss 16.129176079935196
Epoch 4: loss 16.129178517794337
Epoch 5: loss 16.129170493758068
Epoch 6: loss 16.129177499815718
Epoch 7: loss 16.12917466783342
Epoch 8: loss 16.129179931711153
Epoch 9: loss 16.129180671729234
Epoch 10: loss 16.129178735080654
Epoch 11: loss 16.12918571028261
Epoch 12: loss 16.129176565328976
Epoch 13: loss 16.129181888065887
Epoch 14: loss 16.129179788322926
Epoch 15: loss 16.129181695412267
Epoch 16: loss 16.129175357808233
Epoch 17: loss 16.129178154786167
Epoch 18: loss 16.129182328083647
Epoch 19: loss 16.12917984381132
-----------Time: 0:03:37.981401, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129169544751
Epoch 1: loss 16.1291767455366
Epoch 2: loss 16.129174516147863
Epoch 3: loss 16.129175543979564
Epoch 4: loss 16.129179600336553
Epoch 5: loss 16.129173855213708
Epoch 6: loss 16.129179302669854
Epoch 7: loss 16.129177442252992
Epoch 8: loss 16.129181439491507
Epoch 9: loss 16.129181913217167
Epoch 10: loss 16.12917409350264
Epoch 11: loss 16.129176679157965
Epoch 12: loss 16.129178678814387
Epoch 13: loss 16.129177312088636
Epoch 14: loss 16.129177949686554
Epoch 15: loss 16.12917700456886
Epoch 16: loss 16.129179754615024
Epoch 17: loss 16.129178959108554
Epoch 18: loss 16.12917420577588
Epoch 19: loss 16.12917733309125
-----------Time: 0:04:59.556000, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 100, rmse: 4.017185211181641-------------


Epoch 0: loss 16.12917471320944
Epoch 1: loss 16.12916680403933
Epoch 2: loss 16.129177611311082
Epoch 3: loss 16.129179180024952
Epoch 4: loss 16.129172196007087
Epoch 5: loss 16.129167319770218
Epoch 6: loss 16.129168579667855
Epoch 7: loss 16.12917232694932
Epoch 8: loss 16.129168090643994
Epoch 9: loss 16.129175766451713
Epoch 10: loss 16.12917485011538
Epoch 11: loss 16.12916971899492
Epoch 12: loss 16.12916644129045
Epoch 13: loss 16.129170590733107
Epoch 14: loss 16.129176593332463
Epoch 15: loss 16.12917675876047
Epoch 16: loss 16.129174571895547
Epoch 17: loss 16.12917312582658
Epoch 18: loss 16.12917022979927
Epoch 19: loss 16.129172410441196
-----------Time: 0:05:32.051835, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12918384701354
Epoch 1: loss 16.12917836792382
Epoch 2: loss 16.12917546671068
Epoch 3: loss 16.12917884865035
Epoch 4: loss 16.129173024962167
Epoch 5: loss 16.129180784261763
Epoch 6: loss 16.12917896481297
Epoch 7: loss 16.129176148128874
Epoch 8: loss 16.129173931445422
Epoch 9: loss 16.129169057023596
Epoch 10: loss 16.129173007848927
Epoch 11: loss 16.129179756430066
Epoch 12: loss 16.12917401001076
Epoch 13: loss 16.12918235686501
Epoch 14: loss 16.129177454439695
Epoch 15: loss 16.12918200863646
Epoch 16: loss 16.12917591191427
Epoch 17: loss 16.12917530932071
Epoch 18: loss 16.12917414925032
Epoch 19: loss 16.1291749507205
-----------Time: 0:05:23.574049, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12931494144981
Epoch 1: loss 16.129314229175925
Epoch 2: loss 16.12930868189253
Epoch 3: loss 16.12931282511219
Epoch 4: loss 16.129314665822896
Epoch 5: loss 16.129311648447143
Epoch 6: loss 16.129317307744486
Epoch 7: loss 16.12931893868833
Epoch 8: loss 16.129310535308523
Epoch 9: loss 16.129314162537998
Epoch 10: loss 16.129315633239663
Epoch 11: loss 16.129314220619307
Epoch 12: loss 16.129316309990607
Epoch 13: loss 16.12931884223187
Epoch 14: loss 16.12931411768056
Epoch 15: loss 16.129307182150214
Epoch 16: loss 16.129314664526436
Epoch 17: loss 16.12931065354547
Epoch 18: loss 16.129312417505876
Epoch 19: loss 16.129318857789364
-----------Time: 0:02:26.220080, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 20, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129166784592464
Epoch 1: loss 16.129162879402447
Epoch 2: loss 16.12916519020873
Epoch 3: loss 16.129170665668365
Epoch 4: loss 16.12916804656443
Epoch 5: loss 16.12916916981542
Epoch 6: loss 16.12916953282359
Epoch 7: loss 16.129168857109814
Epoch 8: loss 16.129168164023504
Epoch 9: loss 16.129179195063863
Epoch 10: loss 16.129170826429124
Epoch 11: loss 16.129171663940827
Epoch 12: loss 16.129172649508003
Epoch 13: loss 16.129167965406175
Epoch 14: loss 16.1291652187308
Epoch 15: loss 16.129167648551903
Epoch 16: loss 16.129170291769952
Epoch 17: loss 16.12916582806594
Epoch 18: loss 16.129165074046117
Epoch 19: loss 16.12917085546978
-----------Time: 0:02:44.825655, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 50, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129172718738847
Epoch 1: loss 16.12917425789348
Epoch 2: loss 16.129180924538492
Epoch 3: loss 16.12917204613657
Epoch 4: loss 16.12917683551077
Epoch 5: loss 16.129175904135526
Epoch 6: loss 16.129173526432023
Epoch 7: loss 16.12917401001076
Epoch 8: loss 16.12917485037467
Epoch 9: loss 16.12917265339738
Epoch 10: loss 16.129176469909684
Epoch 11: loss 16.129182321342064
Epoch 12: loss 16.129177249858664
Epoch 13: loss 16.129176463427395
Epoch 14: loss 16.129179156948005
Epoch 15: loss 16.129176153314702
Epoch 16: loss 16.129175457116894
Epoch 17: loss 16.129173602145155
Epoch 18: loss 16.129173535507228
Epoch 19: loss 16.129173237062655
-----------Time: 0:03:40.425983, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129168812770956
Epoch 1: loss 16.129172371547465
Epoch 2: loss 16.129180990139254
Epoch 3: loss 16.129178624103872
Epoch 4: loss 16.129177033868803
Epoch 5: loss 16.129176004999938
Epoch 6: loss 16.129174211480294
Epoch 7: loss 16.129173211133498
Epoch 8: loss 16.129175955734542
Epoch 9: loss 16.12917314760707
Epoch 10: loss 16.12917721926226
Epoch 11: loss 16.12917641856996
Epoch 12: loss 16.129173277512137
Epoch 13: loss 16.129171671460284
Epoch 14: loss 16.1291779543538
Epoch 15: loss 16.129178022288187
Epoch 16: loss 16.129177809928407
Epoch 17: loss 16.129181927996786
Epoch 18: loss 16.129180433958883
Epoch 19: loss 16.129175760747298
-----------Time: 0:04:12.598754, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129175181230686
Epoch 1: loss 16.129181844504906
Epoch 2: loss 16.12918077855735
Epoch 3: loss 16.129179006299616
Epoch 4: loss 16.12917789056808
Epoch 5: loss 16.129172335246647
Epoch 6: loss 16.129175184082893
Epoch 7: loss 16.129175112259134
Epoch 8: loss 16.129179120387896
Epoch 9: loss 16.1291750979981
Epoch 10: loss 16.129170513204933
Epoch 11: loss 16.129183681066948
Epoch 12: loss 16.129175691775746
Epoch 13: loss 16.129173305515625
Epoch 14: loss 16.12917747388656
Epoch 15: loss 16.129177196963187
Epoch 16: loss 16.129180057726845
Epoch 17: loss 16.12917296999236
Epoch 18: loss 16.129173389526084
Epoch 19: loss 16.129183511749567
-----------Time: 0:04:50.658850, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129275058520236
Epoch 1: loss 16.129273536738136
Epoch 2: loss 16.129269672516184
Epoch 3: loss 16.129267144942165
Epoch 4: loss 16.12927244615788
Epoch 5: loss 16.12927679836653
Epoch 6: loss 16.129269761453184
Epoch 7: loss 16.129273424205604
Epoch 8: loss 16.129273320748275
Epoch 9: loss 16.129272868803106
Epoch 10: loss 16.12927189801555
Epoch 11: loss 16.129269833276943
Epoch 12: loss 16.129267278995897
Epoch 13: loss 16.12926953483237
Epoch 14: loss 16.12927280242447
Epoch 15: loss 16.12926454398864
Epoch 16: loss 16.129273698017478
Epoch 17: loss 16.12927240752344
Epoch 18: loss 16.12926035850446
Epoch 19: loss 16.12926225003631
-----------Time: 0:03:21.963158, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 20, rmse: 4.017210483551025-------------


Epoch 0: loss 16.129147938504822
Epoch 1: loss 16.129147808081175
Epoch 2: loss 16.12915146020264
Epoch 3: loss 16.129150959251366
Epoch 4: loss 16.12914958215395
Epoch 5: loss 16.12914743210843
Epoch 6: loss 16.12915254196698
Epoch 7: loss 16.129155134104593
Epoch 8: loss 16.12914789131376
Epoch 9: loss 16.12915105207774
Epoch 10: loss 16.129149196068834
Epoch 11: loss 16.12914736184042
Epoch 12: loss 16.129149085092052
Epoch 13: loss 16.129145870914012
Epoch 14: loss 16.129139874278366
Epoch 15: loss 16.129152532113903
Epoch 16: loss 16.1291458315017
Epoch 17: loss 16.129142997185777
Epoch 18: loss 16.129142637289107
Epoch 19: loss 16.129155000569444
-----------Time: 0:03:43.734972, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 50, rmse: 4.01718282699585-------------


Epoch 0: loss 16.12917499272573
Epoch 1: loss 16.129176044930837
Epoch 2: loss 16.12917856550398
Epoch 3: loss 16.12917894666256
Epoch 4: loss 16.12916473644852
Epoch 5: loss 16.129171803439682
Epoch 6: loss 16.129169276124955
Epoch 7: loss 16.129174938015215
Epoch 8: loss 16.129175450634605
Epoch 9: loss 16.12917136005113
Epoch 10: loss 16.12917136316263
Epoch 11: loss 16.12916704155039
Epoch 12: loss 16.129174713728023
Epoch 13: loss 16.129175992813234
Epoch 14: loss 16.129176179762442
Epoch 15: loss 16.129173580364665
Epoch 16: loss 16.12917552479199
Epoch 17: loss 16.129169570161572
Epoch 18: loss 16.129170707414303
Epoch 19: loss 16.129176515544998
-----------Time: 0:04:31.557045, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 100, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129176148388165
Epoch 1: loss 16.12916924993651
Epoch 2: loss 16.12917334855802
Epoch 3: loss 16.129174323494244
Epoch 4: loss 16.129175010098265
Epoch 5: loss 16.129172589870947
Epoch 6: loss 16.129172726776886
Epoch 7: loss 16.12917533447199
Epoch 8: loss 16.129167495310597
Epoch 9: loss 16.129172547347135
Epoch 10: loss 16.12917949247127
Epoch 11: loss 16.129169885719385
Epoch 12: loss 16.12916757569098
Epoch 13: loss 16.129170885029016
Epoch 14: loss 16.129176998345862
Epoch 15: loss 16.12917466731484
Epoch 16: loss 16.12917156411358
Epoch 17: loss 16.12916907958196
Epoch 18: loss 16.129175005171724
Epoch 19: loss 16.12917165771783
-----------Time: 0:05:02.354646, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129172093327632
Epoch 1: loss 16.129169589608438
Epoch 2: loss 16.129170481571364
Epoch 3: loss 16.129174266450104
Epoch 4: loss 16.129175216235048
Epoch 5: loss 16.12917380179965
Epoch 6: loss 16.129178340438916
Epoch 7: loss 16.129171383387373
Epoch 8: loss 16.129173505688698
Epoch 9: loss 16.12917207128785
Epoch 10: loss 16.129175429891283
Epoch 11: loss 16.129167344402916
Epoch 12: loss 16.12917431830841
Epoch 13: loss 16.12917327699355
Epoch 14: loss 16.12917215789123
Epoch 15: loss 16.129171188140838
Epoch 16: loss 16.129174499553205
Epoch 17: loss 16.129179495323477
Epoch 18: loss 16.129172884426147
Epoch 19: loss 16.12917187681919
-----------Time: 0:05:43.699720, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129266368104688
Epoch 1: loss 16.12926842454596
Epoch 2: loss 16.129278203726724
Epoch 3: loss 16.12926944122812
Epoch 4: loss 16.129264083486852
Epoch 5: loss 16.129268833708025
Epoch 6: loss 16.12926289800589
Epoch 7: loss 16.12926698988582
Epoch 8: loss 16.12926168944798
Epoch 9: loss 16.129264251507774
Epoch 10: loss 16.12925925703396
Epoch 11: loss 16.129266786082663
Epoch 12: loss 16.12926749887513
Epoch 13: loss 16.129262068791515
Epoch 14: loss 16.129263361359886
Epoch 15: loss 16.12927072342483
Epoch 16: loss 16.12925593161985
Epoch 17: loss 16.12926189532547
Epoch 18: loss 16.129266715555364
Epoch 19: loss 16.1292678987027
-----------Time: 0:04:15.657288, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 20, rmse: 4.017218112945557-------------


Epoch 0: loss 16.129165278367857
Epoch 1: loss 16.12917712565801
Epoch 2: loss 16.129178951848388
Epoch 3: loss 16.12917212962845
Epoch 4: loss 16.129178226091344
Epoch 5: loss 16.12916924967722
Epoch 6: loss 16.129173979414357
Epoch 7: loss 16.12917763464732
Epoch 8: loss 16.129177990654618
Epoch 9: loss 16.129166484073558
Epoch 10: loss 16.129175224013792
Epoch 11: loss 16.129176502839712
Epoch 12: loss 16.129171949939405
Epoch 13: loss 16.129171275262795
Epoch 14: loss 16.129170834726455
Epoch 15: loss 16.1291620647084
Epoch 16: loss 16.12917684614172
Epoch 17: loss 16.129176317186964
Epoch 18: loss 16.129175968180537
Epoch 19: loss 16.12916530248197
-----------Time: 0:04:14.211449, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 50, rmse: 4.017193794250488-------------


Epoch 0: loss 16.129167398854143
Epoch 1: loss 16.12916973766391
Epoch 2: loss 16.129168332044426
Epoch 3: loss 16.129176289442768
Epoch 4: loss 16.1291722538291
Epoch 5: loss 16.129167180530658
Epoch 6: loss 16.129175845017052
Epoch 7: loss 16.129168191249114
Epoch 8: loss 16.12916582106507
Epoch 9: loss 16.12917373205022
Epoch 10: loss 16.129174300417297
Epoch 11: loss 16.129172498600322
Epoch 12: loss 16.12917498079832
Epoch 13: loss 16.129172162299184
Epoch 14: loss 16.1291663233128
Epoch 15: loss 16.12916675425535
Epoch 16: loss 16.12917056973049
Epoch 17: loss 16.129170320551314
Epoch 18: loss 16.129175169043986
Epoch 19: loss 16.129168624784583
-----------Time: 0:03:38.564184, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129172477338418
Epoch 1: loss 16.129179136723263
Epoch 2: loss 16.12917729938335
Epoch 3: loss 16.12917845789799
Epoch 4: loss 16.12917850845984
Epoch 5: loss 16.12916707862908
Epoch 6: loss 16.129179284000863
Epoch 7: loss 16.129178693334715
Epoch 8: loss 16.129173965671907
Epoch 9: loss 16.12917461286361
Epoch 10: loss 16.12916895512202
Epoch 11: loss 16.129178407076846
Epoch 12: loss 16.12917189859968
Epoch 13: loss 16.129175314506544
Epoch 14: loss 16.129173163423854
Epoch 15: loss 16.12917506454949
Epoch 16: loss 16.129167414670928
Epoch 17: loss 16.1291773193488
Epoch 18: loss 16.12917226834943
Epoch 19: loss 16.129173010701134
-----------Time: 0:03:49.691943, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917360421949
Epoch 1: loss 16.129172637321304
Epoch 2: loss 16.12917360966461
Epoch 3: loss 16.129176022891055
Epoch 4: loss 16.12917674035077
Epoch 5: loss 16.129179247181465
Epoch 6: loss 16.129177325312504
Epoch 7: loss 16.12917465253522
Epoch 8: loss 16.129173665412292
Epoch 9: loss 16.12917275763258
Epoch 10: loss 16.12916867119777
Epoch 11: loss 16.12917045071567
Epoch 12: loss 16.129166699026253
Epoch 13: loss 16.129171843111287
Epoch 14: loss 16.129177235597627
Epoch 15: loss 16.129175572501634
Epoch 16: loss 16.129173890477357
Epoch 17: loss 16.129168887706214
Epoch 18: loss 16.129169281051496
Epoch 19: loss 16.129172937062332
-----------Time: 0:04:26.837599, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12934572065311
Epoch 1: loss 16.12935306534552
Epoch 2: loss 16.129343064470483
Epoch 3: loss 16.129346435001324
Epoch 4: loss 16.129347241657335
Epoch 5: loss 16.12935362956393
Epoch 6: loss 16.129338760230773
Epoch 7: loss 16.129348793776543
Epoch 8: loss 16.129346908726987
Epoch 9: loss 16.129349099740573
Epoch 10: loss 16.12934710993723
Epoch 11: loss 16.129348616421126
Epoch 12: loss 16.1293443770043
Epoch 13: loss 16.129348244856335
Epoch 14: loss 16.129341090483923
Epoch 15: loss 16.129351980210387
Epoch 16: loss 16.1293451963656
Epoch 17: loss 16.12935094408136
Epoch 18: loss 16.12934512480113
Epoch 19: loss 16.12935064174741
-----------Time: 0:02:45.902693, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 20, rmse: 4.017192840576172-------------


Epoch 0: loss 16.12915577636976
Epoch 1: loss 16.129163397726252
Epoch 2: loss 16.129149855447242
Epoch 3: loss 16.129160677757906
Epoch 4: loss 16.129163941719924
Epoch 5: loss 16.129160634196925
Epoch 6: loss 16.129163503257914
Epoch 7: loss 16.129157702646676
Epoch 8: loss 16.12916085615049
Epoch 9: loss 16.12915991155138
Epoch 10: loss 16.129159735233127
Epoch 11: loss 16.129159855025822
Epoch 12: loss 16.129157956493103
Epoch 13: loss 16.12915756937082
Epoch 14: loss 16.129156155194714
Epoch 15: loss 16.129160217515405
Epoch 16: loss 16.129161930913963
Epoch 17: loss 16.129164144485912
Epoch 18: loss 16.129157627452127
Epoch 19: loss 16.129152955796293
-----------Time: 0:03:06.857451, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 50, rmse: 4.017192363739014-------------


Epoch 0: loss 16.129178847613186
Epoch 1: loss 16.12918009091616
Epoch 2: loss 16.129171472842955
Epoch 3: loss 16.129172587537326
Epoch 4: loss 16.129179447354538
Epoch 5: loss 16.12917921165852
Epoch 6: loss 16.12917924173634
Epoch 7: loss 16.12917708209703
Epoch 8: loss 16.12916618459182
Epoch 9: loss 16.12918111485849
Epoch 10: loss 16.129176262735736
Epoch 11: loss 16.129177142511963
Epoch 12: loss 16.12917436835168
Epoch 13: loss 16.129175852277214
Epoch 14: loss 16.12917642220004
Epoch 15: loss 16.129176312519714
Epoch 16: loss 16.129171299895493
Epoch 17: loss 16.129172001019842
Epoch 18: loss 16.129177109581935
Epoch 19: loss 16.129176683047337
-----------Time: 0:04:03.622907, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129168185803994
Epoch 1: loss 16.12916669695192
Epoch 2: loss 16.129163924088097
Epoch 3: loss 16.12916641406484
Epoch 4: loss 16.129160266002927
Epoch 5: loss 16.129163464104888
Epoch 6: loss 16.12916673351203
Epoch 7: loss 16.129162797207027
Epoch 8: loss 16.12916685952772
Epoch 9: loss 16.12916666479977
Epoch 10: loss 16.1291610646209
Epoch 11: loss 16.129164384589888
Epoch 12: loss 16.1291619205423
Epoch 13: loss 16.129165631263653
Epoch 14: loss 16.129161345433644
Epoch 15: loss 16.129167482864602
Epoch 16: loss 16.1291692164879
Epoch 17: loss 16.129155811114828
Epoch 18: loss 16.12916069668619
Epoch 19: loss 16.12915989677176
-----------Time: 0:04:34.366849, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129164481564928
Epoch 1: loss 16.129169841639822
Epoch 2: loss 16.129166575344183
Epoch 3: loss 16.129172803267892
Epoch 4: loss 16.12916873965074
Epoch 5: loss 16.129171380794457
Epoch 6: loss 16.129174068610652
Epoch 7: loss 16.129168653047362
Epoch 8: loss 16.129164379663347
Epoch 9: loss 16.129165509396625
Epoch 10: loss 16.12916954345454
Epoch 11: loss 16.12916949989356
Epoch 12: loss 16.129166515447835
Epoch 13: loss 16.12917040559894
Epoch 14: loss 16.129163542410936
Epoch 15: loss 16.129172396439454
Epoch 16: loss 16.129171750803497
Epoch 17: loss 16.129171310267154
Epoch 18: loss 16.12916737162853
Epoch 19: loss 16.129171139134733
-----------Time: 0:05:14.681713, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129221955351753
Epoch 1: loss 16.12923293375593
Epoch 2: loss 16.129225208423524
Epoch 3: loss 16.129231998750605
Epoch 4: loss 16.129222244721124
Epoch 5: loss 16.129229328047654
Epoch 6: loss 16.12922089484932
Epoch 7: loss 16.129227917760918
Epoch 8: loss 16.1292274121424
Epoch 9: loss 16.129220758202674
Epoch 10: loss 16.12922334100579
Epoch 11: loss 16.129221595455085
Epoch 12: loss 16.129228060111977
Epoch 13: loss 16.129228783016817
Epoch 14: loss 16.129222899950868
Epoch 15: loss 16.129227888979557
Epoch 16: loss 16.129222094591317
Epoch 17: loss 16.129231496762166
Epoch 18: loss 16.1292304004775
Epoch 19: loss 16.129232456140898
-----------Time: 0:03:47.264451, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 20, rmse: 4.017246246337891-------------


Epoch 0: loss 16.129156817166038
Epoch 1: loss 16.129152318457667
Epoch 2: loss 16.129152174810148
Epoch 3: loss 16.12915341370517
Epoch 4: loss 16.129157354158835
Epoch 5: loss 16.129157529958505
Epoch 6: loss 16.129148273250212
Epoch 7: loss 16.12916158424116
Epoch 8: loss 16.129153169193238
Epoch 9: loss 16.129155931426105
Epoch 10: loss 16.129153433929908
Epoch 11: loss 16.12915003876637
Epoch 12: loss 16.129160324084232
Epoch 13: loss 16.129151037557413
Epoch 14: loss 16.129160009045002
Epoch 15: loss 16.12915793082324
Epoch 16: loss 16.129155146032005
Epoch 17: loss 16.129154089937526
Epoch 18: loss 16.129153516125328
Epoch 19: loss 16.129158963322187
-----------Time: 0:03:50.774621, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 50, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129182868706526
Epoch 1: loss 16.129171183473588
Epoch 2: loss 16.129180960320728
Epoch 3: loss 16.12917288261111
Epoch 4: loss 16.129177362391196
Epoch 5: loss 16.129179650898404
Epoch 6: loss 16.12917784052481
Epoch 7: loss 16.129180843120945
Epoch 8: loss 16.129170549765043
Epoch 9: loss 16.129170287102703
Epoch 10: loss 16.12917787215838
Epoch 11: loss 16.12917411346809
Epoch 12: loss 16.12917374475551
Epoch 13: loss 16.129169582348275
Epoch 14: loss 16.129172161002728
Epoch 15: loss 16.12918167389107
Epoch 16: loss 16.12917180940339
Epoch 17: loss 16.129172793155522
Epoch 18: loss 16.12917257172054
Epoch 19: loss 16.129171261001762
-----------Time: 0:04:56.475150, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12918212635482
Epoch 1: loss 16.129185395761958
Epoch 2: loss 16.12917350387366
Epoch 3: loss 16.129180692213264
Epoch 4: loss 16.129172107329378
Epoch 5: loss 16.129172771893614
Epoch 6: loss 16.129179599040096
Epoch 7: loss 16.129177505260838
Epoch 8: loss 16.129172373621795
Epoch 9: loss 16.129173645446844
Epoch 10: loss 16.129183185042216
Epoch 11: loss 16.12917150188361
Epoch 12: loss 16.129168091940453
Epoch 13: loss 16.129175410963
Epoch 14: loss 16.12917437664901
Epoch 15: loss 16.129171268780507
Epoch 16: loss 16.12917319609459
Epoch 17: loss 16.129172560571003
Epoch 18: loss 16.129171398426283
Epoch 19: loss 16.129173630407934
-----------Time: 0:05:31.714448, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917792246094
Epoch 1: loss 16.129171221848736
Epoch 2: loss 16.129178071553582
Epoch 3: loss 16.129175721853567
Epoch 4: loss 16.129179597225054
Epoch 5: loss 16.12917400638068
Epoch 6: loss 16.129170566618992
Epoch 7: loss 16.12917231398474
Epoch 8: loss 16.12918330327916
Epoch 9: loss 16.129173947002915
Epoch 10: loss 16.129170418044936
Epoch 11: loss 16.129180426439426
Epoch 12: loss 16.129178064034125
Epoch 13: loss 16.129174993244316
Epoch 14: loss 16.129174660573256
Epoch 15: loss 16.12917729964264
Epoch 16: loss 16.129172266534386
Epoch 17: loss 16.129168116054565
Epoch 18: loss 16.129176595925376
Epoch 19: loss 16.129171462730586
-----------Time: 0:05:33.700643, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.1293491173724
Epoch 1: loss 16.12935250942444
Epoch 2: loss 16.129348672687392
Epoch 3: loss 16.129351602681893
Epoch 4: loss 16.12935409836305
Epoch 5: loss 16.129357144260872
Epoch 6: loss 16.129351039241357
Epoch 7: loss 16.1293498138295
Epoch 8: loss 16.129352016251914
Epoch 9: loss 16.129353717463765
Epoch 10: loss 16.129348198961733
Epoch 11: loss 16.129351010459995
Epoch 12: loss 16.129351681247233
Epoch 13: loss 16.12934609714444
Epoch 14: loss 16.12934931832335
Epoch 15: loss 16.129348593862762
Epoch 16: loss 16.1293457914397
Epoch 17: loss 16.12934715272033
Epoch 18: loss 16.12935066793586
Epoch 19: loss 16.129357360510024
-----------Time: 0:02:26.812519, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 20, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129180330760846
Epoch 1: loss 16.12918482350551
Epoch 2: loss 16.129183920393046
Epoch 3: loss 16.12917331977666
Epoch 4: loss 16.129181719785674
Epoch 5: loss 16.129182935863035
Epoch 6: loss 16.129180726180458
Epoch 7: loss 16.129181430416303
Epoch 8: loss 16.12918252929389
Epoch 9: loss 16.129181445973796
Epoch 10: loss 16.129171467916418
Epoch 11: loss 16.129178043550095
Epoch 12: loss 16.129175247868616
Epoch 13: loss 16.129178183048946
Epoch 14: loss 16.129183231714695
Epoch 15: loss 16.12917206428698
Epoch 16: loss 16.129183328949026
Epoch 17: loss 16.12917973724249
Epoch 18: loss 16.129183518750438
Epoch 19: loss 16.129175103443224
-----------Time: 0:02:45.323871, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129161840680503
Epoch 1: loss 16.12917063170117
Epoch 2: loss 16.12916755442907
Epoch 3: loss 16.129165050450585
Epoch 4: loss 16.129165792543
Epoch 5: loss 16.129162002737722
Epoch 6: loss 16.129163284156554
Epoch 7: loss 16.129167976555713
Epoch 8: loss 16.12916497447816
Epoch 9: loss 16.129164641807105
Epoch 10: loss 16.129161822530094
Epoch 11: loss 16.12916344050936
Epoch 12: loss 16.12916019495704
Epoch 13: loss 16.129160857187657
Epoch 14: loss 16.129163017604842
Epoch 15: loss 16.12916905468997
Epoch 16: loss 16.129168817697497
Epoch 17: loss 16.129164631954026
Epoch 18: loss 16.12916636039149
Epoch 19: loss 16.129163671278835
-----------Time: 0:03:38.355243, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917821027456
Epoch 1: loss 16.129174250374028
Epoch 2: loss 16.129179729204452
Epoch 3: loss 16.129178464120987
Epoch 4: loss 16.12918127924933
Epoch 5: loss 16.129176404827504
Epoch 6: loss 16.129167125301556
Epoch 7: loss 16.129178906213077
Epoch 8: loss 16.12916868234731
Epoch 9: loss 16.129174435767485
Epoch 10: loss 16.129171506032275
Epoch 11: loss 16.12917986248031
Epoch 12: loss 16.12917059280744
Epoch 13: loss 16.129170180015294
Epoch 14: loss 16.129176428163746
Epoch 15: loss 16.12917754545103
Epoch 16: loss 16.129169022019237
Epoch 17: loss 16.129167019510607
Epoch 18: loss 16.129168857369105
Epoch 19: loss 16.129169609055303
-----------Time: 0:04:12.682941, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129173354521726
Epoch 1: loss 16.12917722937463
Epoch 2: loss 16.129177510965253
Epoch 3: loss 16.12917802980764
Epoch 4: loss 16.129178292988563
Epoch 5: loss 16.129176447869902
Epoch 6: loss 16.129176622113825
Epoch 7: loss 16.129177802149663
Epoch 8: loss 16.129173961523243
Epoch 9: loss 16.129182263001468
Epoch 10: loss 16.129174348904815
Epoch 11: loss 16.129172104736462
Epoch 12: loss 16.129172445445555
Epoch 13: loss 16.12917255642234
Epoch 14: loss 16.129172665843374
Epoch 15: loss 16.129175388404633
Epoch 16: loss 16.129172259014933
Epoch 17: loss 16.129174665240505
Epoch 18: loss 16.129176329114372
Epoch 19: loss 16.129174158584817
-----------Time: 0:04:49.171532, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129269721262997
Epoch 1: loss 16.12927106828259
Epoch 2: loss 16.129269491789977
Epoch 3: loss 16.12926882437353
Epoch 4: loss 16.12926713197759
Epoch 5: loss 16.129265942866546
Epoch 6: loss 16.12926696473454
Epoch 7: loss 16.12927108280292
Epoch 8: loss 16.12926314537003
Epoch 9: loss 16.12926797571229
Epoch 10: loss 16.1292668278286
Epoch 11: loss 16.12927091270766
Epoch 12: loss 16.12926965332861
Epoch 13: loss 16.129264099303636
Epoch 14: loss 16.12926482843147
Epoch 15: loss 16.12925975202153
Epoch 16: loss 16.129267804579868
Epoch 17: loss 16.12926385427312
Epoch 18: loss 16.12926146127142
Epoch 19: loss 16.129256016667476
-----------Time: 0:03:19.930919, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 20, rmse: 4.017233848571777-------------


Epoch 0: loss 16.12917334518723
Epoch 1: loss 16.129173343112896
Epoch 2: loss 16.129174605344158
Epoch 3: loss 16.12917603170697
Epoch 4: loss 16.129173799725315
Epoch 5: loss 16.12917310223105
Epoch 6: loss 16.129177638536696
Epoch 7: loss 16.12917773991969
Epoch 8: loss 16.129176888665537
Epoch 9: loss 16.12917690318586
Epoch 10: loss 16.129179441390832
Epoch 11: loss 16.129175268352647
Epoch 12: loss 16.12917364803976
Epoch 13: loss 16.12917948080315
Epoch 14: loss 16.12917530206055
Epoch 15: loss 16.129176973194582
Epoch 16: loss 16.12917635841432
Epoch 17: loss 16.12917420707234
Epoch 18: loss 16.129171010785416
Epoch 19: loss 16.129176091862607
-----------Time: 0:03:16.907138, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 50, rmse: 4.01719331741333-------------


Epoch 0: loss 16.12917666178543
Epoch 1: loss 16.129179671123147
Epoch 2: loss 16.129184354965684
Epoch 3: loss 16.129189387814645
Epoch 4: loss 16.129172498859614
Epoch 5: loss 16.129175372069266
Epoch 6: loss 16.129177425658334
Epoch 7: loss 16.129179869740472
Epoch 8: loss 16.12918127587854
Epoch 9: loss 16.129171611563937
Epoch 10: loss 16.12917837440611
Epoch 11: loss 16.129178915806865
Epoch 12: loss 16.129179697311592
Epoch 13: loss 16.129178780456677
Epoch 14: loss 16.129178582357934
Epoch 15: loss 16.12918694425109
Epoch 16: loss 16.129180661098278
Epoch 17: loss 16.12917725219229
Epoch 18: loss 16.129182679164405
Epoch 19: loss 16.129179821252954
-----------Time: 0:04:26.981112, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129175818828607
Epoch 1: loss 16.12917445184356
Epoch 2: loss 16.129174126432666
Epoch 3: loss 16.1291758833922
Epoch 4: loss 16.129180600164766
Epoch 5: loss 16.12917537258785
Epoch 6: loss 16.129182372422502
Epoch 7: loss 16.12918027812466
Epoch 8: loss 16.129178232573633
Epoch 9: loss 16.129180156257632
Epoch 10: loss 16.129167438007165
Epoch 11: loss 16.129173643891093
Epoch 12: loss 16.129174551411516
Epoch 13: loss 16.129174852967587
Epoch 14: loss 16.12916999306609
Epoch 15: loss 16.129175535941524
Epoch 16: loss 16.129165937486974
Epoch 17: loss 16.129176739313603
Epoch 18: loss 16.129172111996624
Epoch 19: loss 16.129173969301988
-----------Time: 0:05:04.566458, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12918342877627
Epoch 1: loss 16.129182055308938
Epoch 2: loss 16.12917711502706
Epoch 3: loss 16.129179307855686
Epoch 4: loss 16.129185647534054
Epoch 5: loss 16.12917887820959
Epoch 6: loss 16.129171922454503
Epoch 7: loss 16.129179055824302
Epoch 8: loss 16.129185294119672
Epoch 9: loss 16.129173875697738
Epoch 10: loss 16.12918006550559
Epoch 11: loss 16.129174585119415
Epoch 12: loss 16.12917336333764
Epoch 13: loss 16.129179246403588
Epoch 14: loss 16.12917532021096
Epoch 15: loss 16.129173825135886
Epoch 16: loss 16.12918354208668
Epoch 17: loss 16.12917502773009
Epoch 18: loss 16.129178347958373
Epoch 19: loss 16.129169487188275
-----------Time: 0:05:37.071754, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.1293726312265
Epoch 1: loss 16.129367800365653
Epoch 2: loss 16.12936214936564
Epoch 3: loss 16.129365855679037
Epoch 4: loss 16.129365118772455
Epoch 5: loss 16.129371764414852
Epoch 6: loss 16.129365893794894
Epoch 7: loss 16.1293730883575
Epoch 8: loss 16.129369625000283
Epoch 9: loss 16.12936660295728
Epoch 10: loss 16.12936201712695
Epoch 11: loss 16.12936189707496
Epoch 12: loss 16.129369919814774
Epoch 13: loss 16.129365207190872
Epoch 14: loss 16.129359005974194
Epoch 15: loss 16.129365625687434
Epoch 16: loss 16.129358771833925
Epoch 17: loss 16.129362061984388
Epoch 18: loss 16.129362627758546
Epoch 19: loss 16.12936918238961
-----------Time: 0:04:17.186849, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 20, rmse: 4.0172014236450195-------------


Epoch 0: loss 16.12921256433044
Epoch 1: loss 16.12920621920695
Epoch 2: loss 16.12920284426815
Epoch 3: loss 16.129206297253706
Epoch 4: loss 16.12921136744065
Epoch 5: loss 16.129208426037323
Epoch 6: loss 16.129200708224374
Epoch 7: loss 16.129207376684423
Epoch 8: loss 16.129205973398562
Epoch 9: loss 16.129205357321844
Epoch 10: loss 16.129206454125093
Epoch 11: loss 16.12920465567891
Epoch 12: loss 16.129199518854037
Epoch 13: loss 16.12920657106558
Epoch 14: loss 16.129209357153275
Epoch 15: loss 16.12920884790467
Epoch 16: loss 16.12920841359133
Epoch 17: loss 16.12920069759342
Epoch 18: loss 16.129207191031675
Epoch 19: loss 16.129200685925298
-----------Time: 0:04:02.150482, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 50, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129172557459505
Epoch 1: loss 16.129174556856636
Epoch 2: loss 16.129176326262165
Epoch 3: loss 16.12918192877466
Epoch 4: loss 16.129176865588587
Epoch 5: loss 16.129171234554022
Epoch 6: loss 16.129169212339235
Epoch 7: loss 16.129172966621567
Epoch 8: loss 16.129177968874128
Epoch 9: loss 16.129168353824916
Epoch 10: loss 16.129177757292226
Epoch 11: loss 16.129177975356416
Epoch 12: loss 16.12917337837655
Epoch 13: loss 16.12917284631029
Epoch 14: loss 16.129174385464925
Epoch 15: loss 16.129175861093128
Epoch 16: loss 16.129176685640253
Epoch 17: loss 16.129170009401456
Epoch 18: loss 16.129170716489508
Epoch 19: loss 16.12917387725349
-----------Time: 0:04:13.782391, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129177110878395
Epoch 1: loss 16.12917974216903
Epoch 2: loss 16.129178629548996
Epoch 3: loss 16.129177045796215
Epoch 4: loss 16.12917206091619
Epoch 5: loss 16.129169211042775
Epoch 6: loss 16.129173861177414
Epoch 7: loss 16.129177171034033
Epoch 8: loss 16.129177061353708
Epoch 9: loss 16.129173841471253
Epoch 10: loss 16.129177457551194
Epoch 11: loss 16.12917491053031
Epoch 12: loss 16.129173562473547
Epoch 13: loss 16.129173884772943
Epoch 14: loss 16.129174676390043
Epoch 15: loss 16.12917299358789
Epoch 16: loss 16.12917620958097
Epoch 17: loss 16.129171741987584
Epoch 18: loss 16.129173329889028
Epoch 19: loss 16.12916630982964
-----------Time: 0:03:47.137243, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12917482470481
Epoch 1: loss 16.12917979610167
Epoch 2: loss 16.12918224148027
Epoch 3: loss 16.129172518565774
Epoch 4: loss 16.129180426439426
Epoch 5: loss 16.129175401887796
Epoch 6: loss 16.12916893334153
Epoch 7: loss 16.12917334492794
Epoch 8: loss 16.129170378632622
Epoch 9: loss 16.129176301110885
Epoch 10: loss 16.129177476220185
Epoch 11: loss 16.12918218806621
Epoch 12: loss 16.129175113296302
Epoch 13: loss 16.129175436373572
Epoch 14: loss 16.129168266443664
Epoch 15: loss 16.129169065580218
Epoch 16: loss 16.129182083571717
Epoch 17: loss 16.129175210789924
Epoch 18: loss 16.12917409505839
Epoch 19: loss 16.129179960492515
-----------Time: 0:04:17.150062, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.1291912238581
Epoch 1: loss 16.129196258522104
Epoch 2: loss 16.12919385566732
Epoch 3: loss 16.129195865436117
Epoch 4: loss 16.12920023553588
Epoch 5: loss 16.12919582394947
Epoch 6: loss 16.129197062585195
Epoch 7: loss 16.129201141759843
Epoch 8: loss 16.129196494736703
Epoch 9: loss 16.12919557814108
Epoch 10: loss 16.129197749448508
Epoch 11: loss 16.129198541843483
Epoch 12: loss 16.12919551072528
Epoch 13: loss 16.129195195686044
Epoch 14: loss 16.129186483490006
Epoch 15: loss 16.12919171910496
Epoch 16: loss 16.129195430085606
Epoch 17: loss 16.129195512021735
Epoch 18: loss 16.129189347365163
Epoch 19: loss 16.129191169666168
-----------Time: 0:02:45.470476, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 20, rmse: 4.017226219177246-------------


Epoch 0: loss 16.12922374938998
Epoch 1: loss 16.129215751023576
Epoch 2: loss 16.12921423650164
Epoch 3: loss 16.129224007903655
Epoch 4: loss 16.12922166494522
Epoch 5: loss 16.129226209807488
Epoch 6: loss 16.12921767911553
Epoch 7: loss 16.1292203187035
Epoch 8: loss 16.1292259792973
Epoch 9: loss 16.12922226805736
Epoch 10: loss 16.12922173184244
Epoch 11: loss 16.1292168333065
Epoch 12: loss 16.129214832094327
Epoch 13: loss 16.129216425440895
Epoch 14: loss 16.129221132101087
Epoch 15: loss 16.12921379985467
Epoch 16: loss 16.129210949462674
Epoch 17: loss 16.129209442460194
Epoch 18: loss 16.129214694151223
Epoch 19: loss 16.12921800193351
-----------Time: 0:03:07.398914, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 50, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129168748985236
Epoch 1: loss 16.129160310860364
Epoch 2: loss 16.12916420593801
Epoch 3: loss 16.129164489343673
Epoch 4: loss 16.129165922707354
Epoch 5: loss 16.129160847334578
Epoch 6: loss 16.12916168925424
Epoch 7: loss 16.12916164932334
Epoch 8: loss 16.129166690728923
Epoch 9: loss 16.129161187525092
Epoch 10: loss 16.129161392365415
Epoch 11: loss 16.129159343184305
Epoch 12: loss 16.129161104811086
Epoch 13: loss 16.129162900923646
Epoch 14: loss 16.12916630205089
Epoch 15: loss 16.129157091755786
Epoch 16: loss 16.129162033593413
Epoch 17: loss 16.129167352440955
Epoch 18: loss 16.12915662347525
Epoch 19: loss 16.129160197290666
-----------Time: 0:03:59.960790, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12917739817343
Epoch 1: loss 16.129174499553205
Epoch 2: loss 16.12917903897035
Epoch 3: loss 16.129172256162725
Epoch 4: loss 16.129178108891566
Epoch 5: loss 16.129172584166536
Epoch 6: loss 16.12917892021482
Epoch 7: loss 16.129175212604963
Epoch 8: loss 16.12916938917607
Epoch 9: loss 16.129172755039665
Epoch 10: loss 16.129182237850188
Epoch 11: loss 16.12916996324756
Epoch 12: loss 16.12917288157394
Epoch 13: loss 16.129173860658828
Epoch 14: loss 16.12916853040246
Epoch 15: loss 16.129179587631267
Epoch 16: loss 16.12917279600773
Epoch 17: loss 16.12917071467447
Epoch 18: loss 16.129167894878876
Epoch 19: loss 16.129172564201085
-----------Time: 0:04:35.392303, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129171825998046
Epoch 1: loss 16.12916520550693
Epoch 2: loss 16.12917422911212
Epoch 3: loss 16.129170820206127
Epoch 4: loss 16.12917140802007
Epoch 5: loss 16.12916923515689
Epoch 6: loss 16.129166038351386
Epoch 7: loss 16.129167965406175
Epoch 8: loss 16.129167104039652
Epoch 9: loss 16.129173185463635
Epoch 10: loss 16.129165599370793
Epoch 11: loss 16.129168099200616
Epoch 12: loss 16.12917763905528
Epoch 13: loss 16.129165169724697
Epoch 14: loss 16.129171029195117
Epoch 15: loss 16.12916912314294
Epoch 16: loss 16.12916246764747
Epoch 17: loss 16.12916130446558
Epoch 18: loss 16.129171008711083
Epoch 19: loss 16.129164749153805
-----------Time: 0:05:05.634349, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129334136025292
Epoch 1: loss 16.12933658684901
Epoch 2: loss 16.129334695317162
Epoch 3: loss 16.12933216359448
Epoch 4: loss 16.12933239125246
Epoch 5: loss 16.129334345014282
Epoch 6: loss 16.12933051294448
Epoch 7: loss 16.12933075045554
Epoch 8: loss 16.129334124875754
Epoch 9: loss 16.129339407163187
Epoch 10: loss 16.12932730187794
Epoch 11: loss 16.12933126955722
Epoch 12: loss 16.129336593072008
Epoch 13: loss 16.12933259531491
Epoch 14: loss 16.129322813800524
Epoch 15: loss 16.129334176734066
Epoch 16: loss 16.129336072155287
Epoch 17: loss 16.129327226164808
Epoch 18: loss 16.129327185456034
Epoch 19: loss 16.12932797836959
-----------Time: 0:03:47.112483, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 20, rmse: 4.017197608947754-------------


Epoch 0: loss 16.129173113639876
Epoch 1: loss 16.129172727036178
Epoch 2: loss 16.12917835262562
Epoch 3: loss 16.129176382787723
Epoch 4: loss 16.129176938449515
Epoch 5: loss 16.129170511130603
Epoch 6: loss 16.129173214244997
Epoch 7: loss 16.129171819256467
Epoch 8: loss 16.129173655559214
Epoch 9: loss 16.129169269383375
Epoch 10: loss 16.129171000413756
Epoch 11: loss 16.12916942599547
Epoch 12: loss 16.129167649848362
Epoch 13: loss 16.12917064959229
Epoch 14: loss 16.129168732131287
Epoch 15: loss 16.129167844317024
Epoch 16: loss 16.129170693671853
Epoch 17: loss 16.12917177647336
Epoch 18: loss 16.129172192117714
Epoch 19: loss 16.12916989297955
-----------Time: 0:03:50.583999, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 50, rmse: 4.01719856262207-------------


Epoch 0: loss 16.129178781753133
Epoch 1: loss 16.129177580455387
Epoch 2: loss 16.12917795565026
Epoch 3: loss 16.129171266706177
Epoch 4: loss 16.129181894029593
Epoch 5: loss 16.129177220040134
Epoch 6: loss 16.129179927562486
Epoch 7: loss 16.129176585813006
Epoch 8: loss 16.129172403440325
Epoch 9: loss 16.129172374658964
Epoch 10: loss 16.129178370257446
Epoch 11: loss 16.1291761626492
Epoch 12: loss 16.12917623836233
Epoch 13: loss 16.129183195413876
Epoch 14: loss 16.12918041477131
Epoch 15: loss 16.129171427207645
Epoch 16: loss 16.129171766879573
Epoch 17: loss 16.129175764118088
Epoch 18: loss 16.129178003878486
Epoch 19: loss 16.129170545097796
-----------Time: 0:04:48.424504, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129171922454503
Epoch 1: loss 16.129164989776363
Epoch 2: loss 16.12917271770168
Epoch 3: loss 16.129159147678475
Epoch 4: loss 16.129177228856047
Epoch 5: loss 16.129169871717643
Epoch 6: loss 16.129173488316166
Epoch 7: loss 16.129175369994936
Epoch 8: loss 16.12916763299441
Epoch 9: loss 16.12916002382462
Epoch 10: loss 16.129169520636886
Epoch 11: loss 16.129163236965493
Epoch 12: loss 16.12916529599968
Epoch 13: loss 16.12916290999885
Epoch 14: loss 16.129171956421693
Epoch 15: loss 16.129165751834226
Epoch 16: loss 16.12916476860067
Epoch 17: loss 16.12916729954548
Epoch 18: loss 16.129158078619422
Epoch 19: loss 16.129163152436448
-----------Time: 0:05:33.899417, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129162321666325
Epoch 1: loss 16.129168225216308
Epoch 2: loss 16.12916495451271
Epoch 3: loss 16.1291711554701
Epoch 4: loss 16.129168246218924
Epoch 5: loss 16.129167283469403
Epoch 6: loss 16.129161988476685
Epoch 7: loss 16.12917048079349
Epoch 8: loss 16.12917606697062
Epoch 9: loss 16.12916932564964
Epoch 10: loss 16.129176022631764
Epoch 11: loss 16.12916404336221
Epoch 12: loss 16.12916445226498
Epoch 13: loss 16.129169068432425
Epoch 14: loss 16.129158658395323
Epoch 15: loss 16.129169404733563
Epoch 16: loss 16.129165838956183
Epoch 17: loss 16.129167746045525
Epoch 18: loss 16.129164739300727
Epoch 19: loss 16.12916479012187
-----------Time: 0:06:00.282098, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12921351022601
Epoch 1: loss 16.129214241946762
Epoch 2: loss 16.1292134982986
Epoch 3: loss 16.129210734509982
Epoch 4: loss 16.12920962266782
Epoch 5: loss 16.129222772379425
Epoch 6: loss 16.12921616018564
Epoch 7: loss 16.129212112644563
Epoch 8: loss 16.129213519041922
Epoch 9: loss 16.129207480401043
Epoch 10: loss 16.129213916535868
Epoch 11: loss 16.129209233211913
Epoch 12: loss 16.129211477120975
Epoch 13: loss 16.129205192931
Epoch 14: loss 16.12920665766896
Epoch 15: loss 16.12921078455325
Epoch 16: loss 16.129208121629045
Epoch 17: loss 16.129207783772156
Epoch 18: loss 16.129207618084855
Epoch 19: loss 16.129209622149236
-----------Time: 0:02:31.096179, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 20, rmse: 4.017219543457031-------------


Epoch 0: loss 16.129179098088823
Epoch 1: loss 16.129176950376923
Epoch 2: loss 16.12917521986513
Epoch 3: loss 16.129183350729516
Epoch 4: loss 16.129181518056846
Epoch 5: loss 16.129184877697444
Epoch 6: loss 16.12916853040246
Epoch 7: loss 16.129171987018097
Epoch 8: loss 16.129180985472008
Epoch 9: loss 16.129172130406324
Epoch 10: loss 16.12917099756155
Epoch 11: loss 16.12917995634385
Epoch 12: loss 16.129169788744345
Epoch 13: loss 16.129175095923767
Epoch 14: loss 16.12917819004982
Epoch 15: loss 16.129181565247908
Epoch 16: loss 16.129175179674938
Epoch 17: loss 16.129174154176862
Epoch 18: loss 16.12917306722669
Epoch 19: loss 16.1291719960933
-----------Time: 0:02:44.712835, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 50, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129172114070958
Epoch 1: loss 16.129165872145503
Epoch 2: loss 16.129170506204062
Epoch 3: loss 16.129171258668137
Epoch 4: loss 16.129160530739597
Epoch 5: loss 16.129166540599115
Epoch 6: loss 16.129159666520867
Epoch 7: loss 16.129169634984457
Epoch 8: loss 16.12916355304189
Epoch 9: loss 16.12916300723318
Epoch 10: loss 16.12916557525668
Epoch 11: loss 16.129168166097834
Epoch 12: loss 16.12916964146675
Epoch 13: loss 16.129165293406768
Epoch 14: loss 16.12916730784281
Epoch 15: loss 16.129162326592866
Epoch 16: loss 16.129167128413055
Epoch 17: loss 16.129152890973405
Epoch 18: loss 16.1291577049803
Epoch 19: loss 16.12915699711437
-----------Time: 0:03:31.743038, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129179023412856
Epoch 1: loss 16.129176257549908
Epoch 2: loss 16.12917508944148
Epoch 3: loss 16.129175154523658
Epoch 4: loss 16.12917421718471
Epoch 5: loss 16.129183515120356
Epoch 6: loss 16.12918158728769
Epoch 7: loss 16.129173788057194
Epoch 8: loss 16.129179447095247
Epoch 9: loss 16.1291681824332
Epoch 10: loss 16.129176500246796
Epoch 11: loss 16.129176768872842
Epoch 12: loss 16.12917384043409
Epoch 13: loss 16.12917612401476
Epoch 14: loss 16.12916843809467
Epoch 15: loss 16.12917262383814
Epoch 16: loss 16.129165538696572
Epoch 17: loss 16.12917949610135
Epoch 18: loss 16.12917839281581
Epoch 19: loss 16.12916910239962
-----------Time: 0:04:11.538303, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917229401929
Epoch 1: loss 16.12917370871398
Epoch 2: loss 16.12917272859193
Epoch 3: loss 16.12917337345001
Epoch 4: loss 16.129170625478174
Epoch 5: loss 16.129173580364665
Epoch 6: loss 16.129170088485377
Epoch 7: loss 16.129178287024857
Epoch 8: loss 16.12916740170635
Epoch 9: loss 16.129170176903795
Epoch 10: loss 16.12916526073603
Epoch 11: loss 16.129174557375222
Epoch 12: loss 16.129176576997093
Epoch 13: loss 16.129166488481513
Epoch 14: loss 16.12916631657122
Epoch 15: loss 16.12917035866717
Epoch 16: loss 16.129165831955312
Epoch 17: loss 16.12916630697743
Epoch 18: loss 16.12916211967821
Epoch 19: loss 16.12917010896941
-----------Time: 0:04:44.615066, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12918245254359
Epoch 1: loss 16.129176791171915
Epoch 2: loss 16.129174884341865
Epoch 3: loss 16.12917101156329
Epoch 4: loss 16.129170767829237
Epoch 5: loss 16.129172743112253
Epoch 6: loss 16.12917786878759
Epoch 7: loss 16.129174543632768
Epoch 8: loss 16.129169523229802
Epoch 9: loss 16.129173631963685
Epoch 10: loss 16.129177255044493
Epoch 11: loss 16.129175872761248
Epoch 12: loss 16.129168261776417
Epoch 13: loss 16.129166239561627
Epoch 14: loss 16.129176501543252
Epoch 15: loss 16.129171097388795
Epoch 16: loss 16.12916850654764
Epoch 17: loss 16.129171785807856
Epoch 18: loss 16.12916668554309
Epoch 19: loss 16.129170868175066
-----------Time: 0:03:19.791701, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 20, rmse: 4.017239093780518-------------


Epoch 0: loss 16.129205076509095
Epoch 1: loss 16.129200984888456
Epoch 2: loss 16.129203221537356
Epoch 3: loss 16.129201448501746
Epoch 4: loss 16.12920216284996
Epoch 5: loss 16.129204700017766
Epoch 6: loss 16.129202656022485
Epoch 7: loss 16.12920602629404
Epoch 8: loss 16.129206188091963
Epoch 9: loss 16.129194217638325
Epoch 10: loss 16.12919684685463
Epoch 11: loss 16.129207046606282
Epoch 12: loss 16.129206380486295
Epoch 13: loss 16.129196181253224
Epoch 14: loss 16.129196849447546
Epoch 15: loss 16.129195816689304
Epoch 16: loss 16.12919416344639
Epoch 17: loss 16.129197107961218
Epoch 18: loss 16.129196769326455
Epoch 19: loss 16.12918750172792
-----------Time: 0:03:23.208977, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 50, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129173656337088
Epoch 1: loss 16.129174427470154
Epoch 2: loss 16.129175052362786
Epoch 3: loss 16.129169589089855
Epoch 4: loss 16.129173689267116
Epoch 5: loss 16.129164250276865
Epoch 6: loss 16.129179850034316
Epoch 7: loss 16.129166598421133
Epoch 8: loss 16.12916710715115
Epoch 9: loss 16.129165222620173
Epoch 10: loss 16.1291714285041
Epoch 11: loss 16.129172970510943
Epoch 12: loss 16.129162567734006
Epoch 13: loss 16.129166173960865
Epoch 14: loss 16.129164438003947
Epoch 15: loss 16.12917234069177
Epoch 16: loss 16.129159730565878
Epoch 17: loss 16.129156811980206
Epoch 18: loss 16.129168772062183
Epoch 19: loss 16.12916226332573
-----------Time: 0:04:20.050336, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129178432228127
Epoch 1: loss 16.12917496524083
Epoch 2: loss 16.129177085727115
Epoch 3: loss 16.129184450384972
Epoch 4: loss 16.129185446064522
Epoch 5: loss 16.129167520980463
Epoch 6: loss 16.12917869411259
Epoch 7: loss 16.12916856022099
Epoch 8: loss 16.129167940773478
Epoch 9: loss 16.129172225825613
Epoch 10: loss 16.129168611042132
Epoch 11: loss 16.129170198424994
Epoch 12: loss 16.129173684340575
Epoch 13: loss 16.129173429197692
Epoch 14: loss 16.12917692263273
Epoch 15: loss 16.129173555991258
Epoch 16: loss 16.129165760131553
Epoch 17: loss 16.129160771880738
Epoch 18: loss 16.12916924993651
Epoch 19: loss 16.12916440351817
-----------Time: 0:05:03.388406, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12916954812179
Epoch 1: loss 16.129168480618482
Epoch 2: loss 16.129170478719157
Epoch 3: loss 16.129167050107007
Epoch 4: loss 16.129166058316834
Epoch 5: loss 16.129164953216254
Epoch 6: loss 16.129165385195975
Epoch 7: loss 16.129171355124594
Epoch 8: loss 16.129163972057032
Epoch 9: loss 16.129164958402086
Epoch 10: loss 16.129158160814843
Epoch 11: loss 16.129164084070982
Epoch 12: loss 16.129167182864283
Epoch 13: loss 16.129163556671973
Epoch 14: loss 16.129162490983706
Epoch 15: loss 16.129158169371465
Epoch 16: loss 16.129156693224676
Epoch 17: loss 16.12915249477592
Epoch 18: loss 16.12915998856097
Epoch 19: loss 16.129162530655314
-----------Time: 0:05:32.348178, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 200, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129186543645645
Epoch 1: loss 16.129186059029742
Epoch 2: loss 16.129191804930464
Epoch 3: loss 16.12918511002267
Epoch 4: loss 16.12918717164978
Epoch 5: loss 16.12918328772167
Epoch 6: loss 16.12918592290168
Epoch 7: loss 16.12918451468928
Epoch 8: loss 16.12917931537514
Epoch 9: loss 16.129176996012237
Epoch 10: loss 16.129188195332812
Epoch 11: loss 16.129183966287652
Epoch 12: loss 16.129177791000124
Epoch 13: loss 16.129176478725597
Epoch 14: loss 16.12917616109345
Epoch 15: loss 16.129170375521124
Epoch 16: loss 16.12917526031461
Epoch 17: loss 16.12916999176963
Epoch 18: loss 16.129174957980663
Epoch 19: loss 16.129177186072944
-----------Time: 0:04:16.080970, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 20, rmse: 4.017182350158691-------------


Epoch 0: loss 16.12914814386373
Epoch 1: loss 16.129147997623296
Epoch 2: loss 16.129143844031976
Epoch 3: loss 16.129140848696004
Epoch 4: loss 16.12914354792103
Epoch 5: loss 16.12914174895626
Epoch 6: loss 16.129143789840043
Epoch 7: loss 16.12914282294186
Epoch 8: loss 16.129138954052657
Epoch 9: loss 16.129146000041203
Epoch 10: loss 16.12913609173325
Epoch 11: loss 16.129128639434846
Epoch 12: loss 16.129136733479836
Epoch 13: loss 16.12913794800145
Epoch 14: loss 16.129139650509757
Epoch 15: loss 16.12913134021562
Epoch 16: loss 16.129140724236063
Epoch 17: loss 16.12914453089529
Epoch 18: loss 16.129134824834743
Epoch 19: loss 16.129130874528
-----------Time: 0:04:12.190690, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129178021510313
Epoch 1: loss 16.129172450890678
Epoch 2: loss 16.129182265853675
Epoch 3: loss 16.12917953940304
Epoch 4: loss 16.129173812949183
Epoch 5: loss 16.129178364812322
Epoch 6: loss 16.12917666308189
Epoch 7: loss 16.12917572600223
Epoch 8: loss 16.129177204741932
Epoch 9: loss 16.129177678986178
Epoch 10: loss 16.129171845963494
Epoch 11: loss 16.12916959660931
Epoch 12: loss 16.129173003181677
Epoch 13: loss 16.12917287924032
Epoch 14: loss 16.12917314086549
Epoch 15: loss 16.129166724955407
Epoch 16: loss 16.129175033175212
Epoch 17: loss 16.12917055909954
Epoch 18: loss 16.129162250879734
Epoch 19: loss 16.129161521751897
-----------Time: 0:04:24.335568, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129188188591232
Epoch 1: loss 16.129187600258707
Epoch 2: loss 16.129188326015754
Epoch 3: loss 16.129187417458166
Epoch 4: loss 16.129180429291633
Epoch 5: loss 16.12917944761383
Epoch 6: loss 16.12918459403249
Epoch 7: loss 16.12918602195105
Epoch 8: loss 16.129174807591564
Epoch 9: loss 16.12918201589662
Epoch 10: loss 16.129182071903596
Epoch 11: loss 16.12917702609006
Epoch 12: loss 16.12917941053514
Epoch 13: loss 16.129174403615334
Epoch 14: loss 16.129175023062842
Epoch 15: loss 16.129177807594786
Epoch 16: loss 16.129177025571472
Epoch 17: loss 16.129177726955113
Epoch 18: loss 16.12916978770718
Epoch 19: loss 16.129170613810054
-----------Time: 0:03:49.004181, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12917232720861
Epoch 1: loss 16.129172119516078
Epoch 2: loss 16.129178735599236
Epoch 3: loss 16.129169602054432
Epoch 4: loss 16.129170953481985
Epoch 5: loss 16.129175515457494
Epoch 6: loss 16.129167287877358
Epoch 7: loss 16.129170503351855
Epoch 8: loss 16.129172525048062
Epoch 9: loss 16.129164010950767
Epoch 10: loss 16.129174305862417
Epoch 11: loss 16.129160203772955
Epoch 12: loss 16.129169531267838
Epoch 13: loss 16.129169724699334
Epoch 14: loss 16.129170491683738
Epoch 15: loss 16.129167937143396
Epoch 16: loss 16.1291560206224
Epoch 17: loss 16.129167308620683
Epoch 18: loss 16.129166041462884
Epoch 19: loss 16.129159569545827
-----------Time: 0:04:17.642783, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 200, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129202655503903
Epoch 1: loss 16.129205531825054
Epoch 2: loss 16.129200939512433
Epoch 3: loss 16.129197913839352
Epoch 4: loss 16.129205640208923
Epoch 5: loss 16.12920131885597
Epoch 6: loss 16.12920223441443
Epoch 7: loss 16.12919552291198
Epoch 8: loss 16.129197031729504
Epoch 9: loss 16.129199849969346
Epoch 10: loss 16.12919561158969
Epoch 11: loss 16.129191204411235
Epoch 12: loss 16.12919145462758
Epoch 13: loss 16.129194467595376
Epoch 14: loss 16.12919758687271
Epoch 15: loss 16.12918920942206
Epoch 16: loss 16.129194277534673
Epoch 17: loss 16.129185138804036
Epoch 18: loss 16.129183229899652
Epoch 19: loss 16.12918261978664
-----------Time: 0:02:43.415980, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 20, rmse: 4.017175197601318-------------


Epoch 0: loss 16.129159565137872
Epoch 1: loss 16.129159323218854
Epoch 2: loss 16.12916016591639
Epoch 3: loss 16.129159242838476
Epoch 4: loss 16.12915280281428
Epoch 5: loss 16.129163515444617
Epoch 6: loss 16.129155919239402
Epoch 7: loss 16.129159019069867
Epoch 8: loss 16.12915115527578
Epoch 9: loss 16.129151682415497
Epoch 10: loss 16.12914995216299
Epoch 11: loss 16.129155148884212
Epoch 12: loss 16.12915200627064
Epoch 13: loss 16.129147304018403
Epoch 14: loss 16.129150591057368
Epoch 15: loss 16.129145527871295
Epoch 16: loss 16.12914476399839
Epoch 17: loss 16.12914146529131
Epoch 18: loss 16.129139273240554
Epoch 19: loss 16.129141045239
-----------Time: 0:03:05.534817, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 50, rmse: 4.017195701599121-------------


Epoch 0: loss 16.129190964825845
Epoch 1: loss 16.12918807631799
Epoch 2: loss 16.129184899477934
Epoch 3: loss 16.129184418492112
Epoch 4: loss 16.129193542961715
Epoch 5: loss 16.12918648867584
Epoch 6: loss 16.129184135605033
Epoch 7: loss 16.129181847357113
Epoch 8: loss 16.129181940702072
Epoch 9: loss 16.129176696530497
Epoch 10: loss 16.129173406639328
Epoch 11: loss 16.129176625484615
Epoch 12: loss 16.129173002144512
Epoch 13: loss 16.129172857459828
Epoch 14: loss 16.129170255469134
Epoch 15: loss 16.129173156682274
Epoch 16: loss 16.129172797822772
Epoch 17: loss 16.129173837841172
Epoch 18: loss 16.12916873939145
Epoch 19: loss 16.129162078710145
-----------Time: 0:03:59.021022, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 100, rmse: 4.017181873321533-------------


Epoch 0: loss 16.12917295132337
Epoch 1: loss 16.129174852189713
Epoch 2: loss 16.12916823714372
Epoch 3: loss 16.129171193585957
Epoch 4: loss 16.1291619542502
Epoch 5: loss 16.129171206031952
Epoch 6: loss 16.12916521743434
Epoch 7: loss 16.12916403324984
Epoch 8: loss 16.129165578108886
Epoch 9: loss 16.12916693316652
Epoch 10: loss 16.12916396116679
Epoch 11: loss 16.129161186228632
Epoch 12: loss 16.129156287433403
Epoch 13: loss 16.12915472831332
Epoch 14: loss 16.129162588218037
Epoch 15: loss 16.12916148830329
Epoch 16: loss 16.12915589201379
Epoch 17: loss 16.129154709125746
Epoch 18: loss 16.12915424629033
Epoch 19: loss 16.1291540354863
-----------Time: 0:04:34.367302, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12918394735937
Epoch 1: loss 16.12917279419269
Epoch 2: loss 16.129169968951974
Epoch 3: loss 16.12917037707687
Epoch 4: loss 16.129174054608907
Epoch 5: loss 16.12916765814569
Epoch 6: loss 16.129173727123682
Epoch 7: loss 16.129172624356723
Epoch 8: loss 16.12916832711789
Epoch 9: loss 16.129164273353812
Epoch 10: loss 16.129169887015845
Epoch 11: loss 16.12916678718538
Epoch 12: loss 16.1291609142318
Epoch 13: loss 16.129161837828295
Epoch 14: loss 16.129158998067254
Epoch 15: loss 16.12916681778178
Epoch 16: loss 16.12915589823679
Epoch 17: loss 16.129155917424363
Epoch 18: loss 16.129155040241052
Epoch 19: loss 16.129156043958638
-----------Time: 0:05:03.262124, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12928100926128
Epoch 1: loss 16.129277813752232
Epoch 2: loss 16.129279804074162
Epoch 3: loss 16.12927662256686
Epoch 4: loss 16.129269506828884
Epoch 5: loss 16.129271571308195
Epoch 6: loss 16.129267747276433
Epoch 7: loss 16.129270990495126
Epoch 8: loss 16.12927114503289
Epoch 9: loss 16.129274015649628
Epoch 10: loss 16.12926856689702
Epoch 11: loss 16.129258800680837
Epoch 12: loss 16.129259588667853
Epoch 13: loss 16.129259979160924
Epoch 14: loss 16.129250477940705
Epoch 15: loss 16.129260429809637
Epoch 16: loss 16.129255869130585
Epoch 17: loss 16.129253793501736
Epoch 18: loss 16.12926014225531
Epoch 19: loss 16.12925218356051
-----------Time: 0:03:47.439628, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 20, rmse: 4.01722526550293-------------


Epoch 0: loss 16.12914141576662
Epoch 1: loss 16.129141039793875
Epoch 2: loss 16.12914260254404
Epoch 3: loss 16.129138373239588
Epoch 4: loss 16.129142426225787
Epoch 5: loss 16.12913579691876
Epoch 6: loss 16.129136955174108
Epoch 7: loss 16.129136366582294
Epoch 8: loss 16.12913685482828
Epoch 9: loss 16.129130214631008
Epoch 10: loss 16.12912850927049
Epoch 11: loss 16.129126505983983
Epoch 12: loss 16.129125302611907
Epoch 13: loss 16.12913072750969
Epoch 14: loss 16.12912372637858
Epoch 15: loss 16.129124689905975
Epoch 16: loss 16.129122416437678
Epoch 17: loss 16.12911928990018
Epoch 18: loss 16.12912277529718
Epoch 19: loss 16.12911875212951
-----------Time: 0:03:43.662145, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 50, rmse: 4.017185211181641-------------


Epoch 0: loss 16.12917030110445
Epoch 1: loss 16.129164490899424
Epoch 2: loss 16.12916148882187
Epoch 3: loss 16.129159848802825
Epoch 4: loss 16.129157870667598
Epoch 5: loss 16.12915306547662
Epoch 6: loss 16.129150923469133
Epoch 7: loss 16.12915743427992
Epoch 8: loss 16.12915613496997
Epoch 9: loss 16.129156093224033
Epoch 10: loss 16.129157361418997
Epoch 11: loss 16.129150427703692
Epoch 12: loss 16.129152340238154
Epoch 13: loss 16.12914468284014
Epoch 14: loss 16.129145452158163
Epoch 15: loss 16.12914757808957
Epoch 16: loss 16.129138279376047
Epoch 17: loss 16.129131575911636
Epoch 18: loss 16.12913961057886
Epoch 19: loss 16.12913573624454
-----------Time: 0:04:44.266560, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129169383990238
Epoch 1: loss 16.129171523145516
Epoch 2: loss 16.12916844302121
Epoch 3: loss 16.129174216666126
Epoch 4: loss 16.12917548823188
Epoch 5: loss 16.12917500257881
Epoch 6: loss 16.129169679582606
Epoch 7: loss 16.12917097111381
Epoch 8: loss 16.12916457776209
Epoch 9: loss 16.129167252354417
Epoch 10: loss 16.129166249933288
Epoch 11: loss 16.12916179660094
Epoch 12: loss 16.129162026592542
Epoch 13: loss 16.129162875513074
Epoch 14: loss 16.129150672474914
Epoch 15: loss 16.12915393488118
Epoch 16: loss 16.1291568213147
Epoch 17: loss 16.12914994542141
Epoch 18: loss 16.12914986167024
Epoch 19: loss 16.129145939366982
-----------Time: 0:05:32.580179, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 150, rmse: 4.017185211181641-------------


Epoch 0: loss 16.12917717051545
Epoch 1: loss 16.1291668141517
Epoch 2: loss 16.129173929889674
Epoch 3: loss 16.12917403957
Epoch 4: loss 16.12916826903658
Epoch 5: loss 16.12917243066594
Epoch 6: loss 16.12916466773626
Epoch 7: loss 16.129169208968445
Epoch 8: loss 16.129169793152304
Epoch 9: loss 16.12916064301284
Epoch 10: loss 16.129155363577613
Epoch 11: loss 16.12916203618633
Epoch 12: loss 16.12915508509849
Epoch 13: loss 16.12915474490798
Epoch 14: loss 16.129151450608852
Epoch 15: loss 16.12915448224564
Epoch 16: loss 16.12915145735043
Epoch 17: loss 16.129153548277483
Epoch 18: loss 16.12914659874539
Epoch 19: loss 16.129145294249607
-----------Time: 0:06:04.766482, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 200, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129309626491644
Epoch 1: loss 16.12930884706125
Epoch 2: loss 16.129294189309995
Epoch 3: loss 16.12929960228037
Epoch 4: loss 16.12930026891894
Epoch 5: loss 16.129299336765822
Epoch 6: loss 16.129298043160286
Epoch 7: loss 16.129293010570617
Epoch 8: loss 16.129295404609486
Epoch 9: loss 16.129295296225617
Epoch 10: loss 16.129291915841698
Epoch 11: loss 16.129296526564016
Epoch 12: loss 16.129280917731364
Epoch 13: loss 16.129284981089224
Epoch 14: loss 16.129284830440834
Epoch 15: loss 16.12927899145445
Epoch 16: loss 16.12928210036012
Epoch 17: loss 16.129275405970912
Epoch 18: loss 16.129275264138435
Epoch 19: loss 16.12927261521597
-----------Time: 0:02:47.982503, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 20, rmse: 4.017209053039551-------------


Epoch 0: loss 16.129159457531877
Epoch 1: loss 16.129161947767912
Epoch 2: loss 16.12915740549856
Epoch 3: loss 16.129164347770487
Epoch 4: loss 16.12915090791164
Epoch 5: loss 16.12915309762877
Epoch 6: loss 16.129148081374467
Epoch 7: loss 16.1291525168157
Epoch 8: loss 16.129149984833727
Epoch 9: loss 16.129147235565434
Epoch 10: loss 16.12914323547471
Epoch 11: loss 16.129133709362502
Epoch 12: loss 16.129143193728773
Epoch 13: loss 16.129136658025992
Epoch 14: loss 16.12914137194635
Epoch 15: loss 16.129134716191587
Epoch 16: loss 16.129131985592284
Epoch 17: loss 16.129135489658275
Epoch 18: loss 16.129133353873787
Epoch 19: loss 16.129130500370294
-----------Time: 0:02:50.239499, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 50, rmse: 4.017175674438477-------------


Epoch 0: loss 16.129182663088326
Epoch 1: loss 16.12917980569546
Epoch 2: loss 16.129187691788623
Epoch 3: loss 16.129179128425935
Epoch 4: loss 16.129183111662705
Epoch 5: loss 16.129169318908062
Epoch 6: loss 16.12917861347292
Epoch 7: loss 16.129172005168506
Epoch 8: loss 16.12917188511652
Epoch 9: loss 16.129167089260033
Epoch 10: loss 16.129161767560287
Epoch 11: loss 16.129163524779113
Epoch 12: loss 16.129160282597585
Epoch 13: loss 16.129166767997805
Epoch 14: loss 16.129164077847985
Epoch 15: loss 16.12915152761844
Epoch 16: loss 16.12916203748279
Epoch 17: loss 16.129158926762077
Epoch 18: loss 16.129153138596834
Epoch 19: loss 16.12916206600486
-----------Time: 0:03:25.010351, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12918460881211
Epoch 1: loss 16.12916876091265
Epoch 2: loss 16.129175197306765
Epoch 3: loss 16.129169881830013
Epoch 4: loss 16.129163766179545
Epoch 5: loss 16.129166309311053
Epoch 6: loss 16.12916053255464
Epoch 7: loss 16.129159862026693
Epoch 8: loss 16.129160935493704
Epoch 9: loss 16.129164097813433
Epoch 10: loss 16.129156808090833
Epoch 11: loss 16.129153118631386
Epoch 12: loss 16.1291547749858
Epoch 13: loss 16.12914257428126
Epoch 14: loss 16.12914804299932
Epoch 15: loss 16.129141115507007
Epoch 16: loss 16.129142711965073
Epoch 17: loss 16.129140062005447
Epoch 18: loss 16.129140768056335
Epoch 19: loss 16.12913830141583
-----------Time: 0:04:11.695273, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 150, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12917170568677
Epoch 1: loss 16.12917232357853
Epoch 2: loss 16.12917333040761
Epoch 3: loss 16.12916716263954
Epoch 4: loss 16.12916956964299
Epoch 5: loss 16.129164260129944
Epoch 6: loss 16.129153814829195
Epoch 7: loss 16.12916039357437
Epoch 8: loss 16.1291586806944
Epoch 9: loss 16.12915760307872
Epoch 10: loss 16.12915555260115
Epoch 11: loss 16.129151665302253
Epoch 12: loss 16.12914994179133
Epoch 13: loss 16.12914717074255
Epoch 14: loss 16.129147293906033
Epoch 15: loss 16.12914668923814
Epoch 16: loss 16.12914816382918
Epoch 17: loss 16.129144723808203
Epoch 18: loss 16.129143938154808
Epoch 19: loss 16.129143215768554
-----------Time: 0:04:41.758565, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 200, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12929523269919
Epoch 1: loss 16.12928845404023
Epoch 2: loss 16.129286821281347
Epoch 3: loss 16.129287296044172
Epoch 4: loss 16.129287654644383
Epoch 5: loss 16.129285168038432
Epoch 6: loss 16.129280263020203
Epoch 7: loss 16.129274465520464
Epoch 8: loss 16.129269501643055
Epoch 9: loss 16.129269964478468
Epoch 10: loss 16.129267938633596
Epoch 11: loss 16.129263538715307
Epoch 12: loss 16.129265625752982
Epoch 13: loss 16.12925768339355
Epoch 14: loss 16.129261412524606
Epoch 15: loss 16.12925546852514
Epoch 16: loss 16.129253607848987
Epoch 17: loss 16.12925501735785
Epoch 18: loss 16.129248031784233
Epoch 19: loss 16.129242797465736
-----------Time: 0:03:16.123790, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 20, rmse: 4.017170429229736-------------


Epoch 0: loss 16.129184002847758
Epoch 1: loss 16.129179627821458
Epoch 2: loss 16.129190439241874
Epoch 3: loss 16.12917525201728
Epoch 4: loss 16.12917712876951
Epoch 5: loss 16.129173415973824
Epoch 6: loss 16.12917201268796
Epoch 7: loss 16.129165351747364
Epoch 8: loss 16.12917099782084
Epoch 9: loss 16.129161607836693
Epoch 10: loss 16.129163384502384
Epoch 11: loss 16.129164292541386
Epoch 12: loss 16.129153125113675
Epoch 13: loss 16.12915344248653
Epoch 14: loss 16.129152202554344
Epoch 15: loss 16.129151419234574
Epoch 16: loss 16.129146330119347
Epoch 17: loss 16.129143520176832
Epoch 18: loss 16.129149643087466
Epoch 19: loss 16.12914158300967
-----------Time: 0:03:18.082623, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 50, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129168822624038
Epoch 1: loss 16.129163749584883
Epoch 2: loss 16.129166490555846
Epoch 3: loss 16.129161218380787
Epoch 4: loss 16.129161074473977
Epoch 5: loss 16.129164756932553
Epoch 6: loss 16.12914957204158
Epoch 7: loss 16.129152008863556
Epoch 8: loss 16.129154494950924
Epoch 9: loss 16.129144056651047
Epoch 10: loss 16.129141163475946
Epoch 11: loss 16.129144449996325
Epoch 12: loss 16.12915008440168
Epoch 13: loss 16.12913665206229
Epoch 14: loss 16.12913933754486
Epoch 15: loss 16.12913416312271
Epoch 16: loss 16.129134338403798
Epoch 17: loss 16.12912591065059
Epoch 18: loss 16.129126326554232
Epoch 19: loss 16.12912936648835
-----------Time: 0:04:14.635473, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 100, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129177683653424
Epoch 1: loss 16.129173902923352
Epoch 2: loss 16.129175735596018
Epoch 3: loss 16.129168845441693
Epoch 4: loss 16.12916894708398
Epoch 5: loss 16.129159142492647
Epoch 6: loss 16.129162082599517
Epoch 7: loss 16.129160826591256
Epoch 8: loss 16.12915453306678
Epoch 9: loss 16.12915152735915
Epoch 10: loss 16.129153888727284
Epoch 11: loss 16.129143703495956
Epoch 12: loss 16.129150547237096
Epoch 13: loss 16.129145169789666
Epoch 14: loss 16.12914718967083
Epoch 15: loss 16.129142868317878
Epoch 16: loss 16.12913740556353
Epoch 17: loss 16.129136916021086
Epoch 18: loss 16.129130087059565
Epoch 19: loss 16.12913217902378
-----------Time: 0:05:03.120661, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 150, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129176696530497
Epoch 1: loss 16.12917391614722
Epoch 2: loss 16.12916204422437
Epoch 3: loss 16.12916508000982
Epoch 4: loss 16.129160355199218
Epoch 5: loss 16.12916343376778
Epoch 6: loss 16.129159862545276
Epoch 7: loss 16.129159838431164
Epoch 8: loss 16.12915489607495
Epoch 9: loss 16.12914610323924
Epoch 10: loss 16.129149886302937
Epoch 11: loss 16.129148010847164
Epoch 12: loss 16.129134012733612
Epoch 13: loss 16.129142515681373
Epoch 14: loss 16.129142390443555
Epoch 15: loss 16.129134607548426
Epoch 16: loss 16.12913160339654
Epoch 17: loss 16.129127833816007
Epoch 18: loss 16.129124895524175
Epoch 19: loss 16.12913320374398
-----------Time: 0:05:32.888788, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 200, rmse: 4.017181873321533-------------


Epoch 0: loss 16.12937868490629
Epoch 1: loss 16.129375202361498
Epoch 2: loss 16.129365848418875
Epoch 3: loss 16.12936666155717
Epoch 4: loss 16.12936208921
Epoch 5: loss 16.129362140549727
Epoch 6: loss 16.12936210787899
Epoch 7: loss 16.129353268370803
Epoch 8: loss 16.129361594741017
Epoch 9: loss 16.129347748831602
Epoch 10: loss 16.12934202574854
Epoch 11: loss 16.129342644936756
Epoch 12: loss 16.129343801636356
Epoch 13: loss 16.129341225315528
Epoch 14: loss 16.129332791857905
Epoch 15: loss 16.129328232216018
Epoch 16: loss 16.1293256862323
Epoch 17: loss 16.12932455598044
Epoch 18: loss 16.12932035675381
Epoch 19: loss 16.12931766815974
-----------Time: 0:04:10.068733, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 20, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129207736321803
Epoch 1: loss 16.12921092664502
Epoch 2: loss 16.12920179750817
Epoch 3: loss 16.129198028186924
Epoch 4: loss 16.12919828332981
Epoch 5: loss 16.12919145125679
Epoch 6: loss 16.129187212617843
Epoch 7: loss 16.129187687380668
Epoch 8: loss 16.129180744330867
Epoch 9: loss 16.129177193333106
Epoch 10: loss 16.129174176994518
Epoch 11: loss 16.129174247781112
Epoch 12: loss 16.12916807327146
Epoch 13: loss 16.129173881920735
Epoch 14: loss 16.129159866175357
Epoch 15: loss 16.129163560302054
Epoch 16: loss 16.129159686745606
Epoch 17: loss 16.129153616211866
Epoch 18: loss 16.129145372555655
Epoch 19: loss 16.129150876537363
-----------Time: 0:04:34.291360, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 50, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129176649598726
Epoch 1: loss 16.129163413283745
Epoch 2: loss 16.129164891245573
Epoch 3: loss 16.12916782824095
Epoch 4: loss 16.129154912410318
Epoch 5: loss 16.129159391412532
Epoch 6: loss 16.12915431448401
Epoch 7: loss 16.129151421308908
Epoch 8: loss 16.129142401074507
Epoch 9: loss 16.129143722942825
Epoch 10: loss 16.129141009456767
Epoch 11: loss 16.12913785802728
Epoch 12: loss 16.1291377960566
Epoch 13: loss 16.129128726038225
Epoch 14: loss 16.129130149289537
Epoch 15: loss 16.129128944880293
Epoch 16: loss 16.12912400407983
Epoch 17: loss 16.129117312802123
Epoch 18: loss 16.129117562759177
Epoch 19: loss 16.12911446474375
-----------Time: 0:04:40.991010, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 100, rmse: 4.017184257507324-------------


Epoch 0: loss 16.129181015549825
Epoch 1: loss 16.129174008973596
Epoch 2: loss 16.129174973019573
Epoch 3: loss 16.129167153045753
Epoch 4: loss 16.129167204644773
Epoch 5: loss 16.129160270670173
Epoch 6: loss 16.129159215872154
Epoch 7: loss 16.12916069824194
Epoch 8: loss 16.12915170445528
Epoch 9: loss 16.129152024939632
Epoch 10: loss 16.129145207646232
Epoch 11: loss 16.12914272985619
Epoch 12: loss 16.129134306510938
Epoch 13: loss 16.12913451575922
Epoch 14: loss 16.12913356182561
Epoch 15: loss 16.129133669690894
Epoch 16: loss 16.129131620769076
Epoch 17: loss 16.129122310906016
Epoch 18: loss 16.1291186512651
Epoch 19: loss 16.12911679862698
-----------Time: 0:03:47.139120, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 150, rmse: 4.0171799659729-------------


Epoch 0: loss 16.1291768222869
Epoch 1: loss 16.129176107938683
Epoch 2: loss 16.12917620958097
Epoch 3: loss 16.129167747601276
Epoch 4: loss 16.129164698591953
Epoch 5: loss 16.129165746648393
Epoch 6: loss 16.129165184763607
Epoch 7: loss 16.1291599048098
Epoch 8: loss 16.12915170445528
Epoch 9: loss 16.12914928552442
Epoch 10: loss 16.12914738206516
Epoch 11: loss 16.129147216118568
Epoch 12: loss 16.129144593384552
Epoch 13: loss 16.129142504531835
Epoch 14: loss 16.129131476862266
Epoch 15: loss 16.129132389568518
Epoch 16: loss 16.129130533818902
Epoch 17: loss 16.129120362070736
Epoch 18: loss 16.129124407796773
Epoch 19: loss 16.12912155118178
-----------Time: 0:04:16.295763, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 200, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129202710473713
Epoch 1: loss 16.12919517338697
Epoch 2: loss 16.129187776058377
Epoch 3: loss 16.1291860909226
Epoch 4: loss 16.12918512609875
Epoch 5: loss 16.1291872722549
Epoch 6: loss 16.129172316577655
Epoch 7: loss 16.12917945617045
Epoch 8: loss 16.129170059185434
Epoch 9: loss 16.12916996376614
Epoch 10: loss 16.129157660641443
Epoch 11: loss 16.129151731162306
Epoch 12: loss 16.129147211710613
Epoch 13: loss 16.12914974421117
Epoch 14: loss 16.129145638848076
Epoch 15: loss 16.129141898567486
Epoch 16: loss 16.12914798802951
Epoch 17: loss 16.129137460014753
Epoch 18: loss 16.12912818256314
Epoch 19: loss 16.1291227716671
-----------Time: 0:02:40.926502, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 20, rmse: 4.01721715927124-------------


Epoch 0: loss 16.129214046700223
Epoch 1: loss 16.12920274521878
Epoch 2: loss 16.12919474970458
Epoch 3: loss 16.129193434059264
Epoch 4: loss 16.129188468626104
Epoch 5: loss 16.12918875436539
Epoch 6: loss 16.129184345631188
Epoch 7: loss 16.129174635162684
Epoch 8: loss 16.129175285984473
Epoch 9: loss 16.129167927290318
Epoch 10: loss 16.129161843014128
Epoch 11: loss 16.12916287343874
Epoch 12: loss 16.129161934803335
Epoch 13: loss 16.129154774207922
Epoch 14: loss 16.12914961897335
Epoch 15: loss 16.129145223722308
Epoch 16: loss 16.129139991737436
Epoch 17: loss 16.12914079450407
Epoch 18: loss 16.12913755958271
Epoch 19: loss 16.129130948426088
-----------Time: 0:03:07.500673, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 50, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129191304238482
Epoch 1: loss 16.129183086511425
Epoch 2: loss 16.12917666697126
Epoch 3: loss 16.12917580793836
Epoch 4: loss 16.129168679754393
Epoch 5: loss 16.129163488478294
Epoch 6: loss 16.129158643615707
Epoch 7: loss 16.129158233416476
Epoch 8: loss 16.129157232032515
Epoch 9: loss 16.129149968239066
Epoch 10: loss 16.12915069555186
Epoch 11: loss 16.129135770989606
Epoch 12: loss 16.129131350846574
Epoch 13: loss 16.129136643505667
Epoch 14: loss 16.129131595099214
Epoch 15: loss 16.129131246870664
Epoch 16: loss 16.129118623780194
Epoch 17: loss 16.129116027493914
Epoch 18: loss 16.129114541234756
Epoch 19: loss 16.12910641659337
-----------Time: 0:03:49.295725, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 100, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129172016318044
Epoch 1: loss 16.1291796081153
Epoch 2: loss 16.129169883126472
Epoch 3: loss 16.129164723483942
Epoch 4: loss 16.129159112414825
Epoch 5: loss 16.129154379047602
Epoch 6: loss 16.12915322338517
Epoch 7: loss 16.129153908692732
Epoch 8: loss 16.12914875812541
Epoch 9: loss 16.129143556477647
Epoch 10: loss 16.129136631318964
Epoch 11: loss 16.129134803572835
Epoch 12: loss 16.1291310555135
Epoch 13: loss 16.12912507002739
Epoch 14: loss 16.129125542197297
Epoch 15: loss 16.12912006803412
Epoch 16: loss 16.129120644698524
Epoch 17: loss 16.129108110545058
Epoch 18: loss 16.129104652892252
Epoch 19: loss 16.129109461713316
-----------Time: 0:04:35.055884, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 150, rmse: 4.017179012298584-------------


Epoch 0: loss 16.129173768351038
Epoch 1: loss 16.129164407407544
Epoch 2: loss 16.12915587178905
Epoch 3: loss 16.129154330300793
Epoch 4: loss 16.129153561760642
Epoch 5: loss 16.12914837826329
Epoch 6: loss 16.129138868745738
Epoch 7: loss 16.12913533019397
Epoch 8: loss 16.129140982490444
Epoch 9: loss 16.129134953961934
Epoch 10: loss 16.129129963377494
Epoch 11: loss 16.129119747031183
Epoch 12: loss 16.12911501003388
Epoch 13: loss 16.129116226889117
Epoch 14: loss 16.129111559122656
Epoch 15: loss 16.129100731107577
Epoch 16: loss 16.129100951764688
Epoch 17: loss 16.12909334155773
Epoch 18: loss 16.12909020412999
Epoch 19: loss 16.129087689261258
-----------Time: 0:05:04.074923, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 200, rmse: 4.017178058624268-------------


Epoch 0: loss 16.129286668817915
Epoch 1: loss 16.129277311504502
Epoch 2: loss 16.129273960679818
Epoch 3: loss 16.129271346243133
Epoch 4: loss 16.129264448828643
Epoch 5: loss 16.129256329632376
Epoch 6: loss 16.129249141811353
Epoch 7: loss 16.12924993161341
Epoch 8: loss 16.129242224949998
Epoch 9: loss 16.129232058646952
Epoch 10: loss 16.129234823732027
Epoch 11: loss 16.129227446887466
Epoch 12: loss 16.129217376003712
Epoch 13: loss 16.129217273324258
Epoch 14: loss 16.12920510269754
Epoch 15: loss 16.12919964461044
Epoch 16: loss 16.129199717471366
Epoch 17: loss 16.129192265950834
Epoch 18: loss 16.12918342047894
Epoch 19: loss 16.12918480120644
-----------Time: 0:03:39.275575, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 20, rmse: 4.017180442810059-------------


Epoch 0: loss 16.12916565537777
Epoch 1: loss 16.1291572999669
Epoch 2: loss 16.129159851655032
Epoch 3: loss 16.12915054801497
Epoch 4: loss 16.129148876362354
Epoch 5: loss 16.129143320003756
Epoch 6: loss 16.129133066838044
Epoch 7: loss 16.12913765707633
Epoch 8: loss 16.129123742195365
Epoch 9: loss 16.129124683682978
Epoch 10: loss 16.129109814609116
Epoch 11: loss 16.129112803462796
Epoch 12: loss 16.129115432160518
Epoch 13: loss 16.129102854964653
Epoch 14: loss 16.129093960227365
Epoch 15: loss 16.12909323654465
Epoch 16: loss 16.129082085711598
Epoch 17: loss 16.129075243266914
Epoch 18: loss 16.129077036008685
Epoch 19: loss 16.129068422602725
-----------Time: 0:03:51.215516, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 50, rmse: 4.01716947555542-------------


Epoch 0: loss 16.129180875791683
Epoch 1: loss 16.12917715210575
Epoch 2: loss 16.12917228572196
Epoch 3: loss 16.129168648120825
Epoch 4: loss 16.12916724639071
Epoch 5: loss 16.129148086301004
Epoch 6: loss 16.129147275496333
Epoch 7: loss 16.129154066082705
Epoch 8: loss 16.12913568257119
Epoch 9: loss 16.129132894668455
Epoch 10: loss 16.129125952396528
Epoch 11: loss 16.129123735972367
Epoch 12: loss 16.12912035118049
Epoch 13: loss 16.129114659212412
Epoch 14: loss 16.129111867420306
Epoch 15: loss 16.12910154372729
Epoch 16: loss 16.129095353141565
Epoch 17: loss 16.12909238892058
Epoch 18: loss 16.12908517672615
Epoch 19: loss 16.129076823648905
-----------Time: 0:04:37.945870, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 100, rmse: 4.017175674438477-------------


Epoch 0: loss 16.12917463412552
Epoch 1: loss 16.129165688307793
Epoch 2: loss 16.12915859746181
Epoch 3: loss 16.129157243700636
Epoch 4: loss 16.129148310588196
Epoch 5: loss 16.12914903634524
Epoch 6: loss 16.129142474194726
Epoch 7: loss 16.12913858585866
Epoch 8: loss 16.12913433892238
Epoch 9: loss 16.129131227683086
Epoch 10: loss 16.129128303133708
Epoch 11: loss 16.129118155240363
Epoch 12: loss 16.129108751254474
Epoch 13: loss 16.129100279940282
Epoch 14: loss 16.12909054094971
Epoch 15: loss 16.129090283991786
Epoch 16: loss 16.129090181312332
Epoch 17: loss 16.1290818985031
Epoch 18: loss 16.129078692363098
Epoch 19: loss 16.129076205497853
-----------Time: 0:05:30.272870, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 150, rmse: 4.017175197601318-------------


Epoch 0: loss 16.129173312257205
Epoch 1: loss 16.129168012597237
Epoch 2: loss 16.129160269633008
Epoch 3: loss 16.129158909648837
Epoch 4: loss 16.129157006189576
Epoch 5: loss 16.129134803313544
Epoch 6: loss 16.12914882994917
Epoch 7: loss 16.129135989053797
Epoch 8: loss 16.129134900807166
Epoch 9: loss 16.129123377631448
Epoch 10: loss 16.129113241146932
Epoch 11: loss 16.12911314728339
Epoch 12: loss 16.12910493344571
Epoch 13: loss 16.12910484295296
Epoch 14: loss 16.129098332660753
Epoch 15: loss 16.129086388395557
Epoch 16: loss 16.12909020231495
Epoch 17: loss 16.129075457182445
Epoch 18: loss 16.129071116123335
Epoch 19: loss 16.129063442130658
-----------Time: 0:05:58.887098, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 200, rmse: 4.017175674438477-------------


Epoch 0: loss 16.12917367552466
Epoch 1: loss 16.12916913429248
Epoch 2: loss 16.129164414667706
Epoch 3: loss 16.129155812411284
Epoch 4: loss 16.129152028569713
Epoch 5: loss 16.12914249908671
Epoch 6: loss 16.129139365289053
Epoch 7: loss 16.129130687060208
Epoch 8: loss 16.129123171235374
Epoch 9: loss 16.129111815302704
Epoch 10: loss 16.129115489723244
Epoch 11: loss 16.129103425665352
Epoch 12: loss 16.129087620548997
Epoch 13: loss 16.12908309876368
Epoch 14: loss 16.12908179452719
Epoch 15: loss 16.129072311975957
Epoch 16: loss 16.12905591048904
Epoch 17: loss 16.129059660363417
Epoch 18: loss 16.129050651537845
Epoch 19: loss 16.129039826115687
-----------Time: 0:03:03.735625, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 20, rmse: 4.017149448394775-------------


Epoch 0: loss 16.129196210034586
Epoch 1: loss 16.129198003294935
Epoch 2: loss 16.12917998486592
Epoch 3: loss 16.129172750113124
Epoch 4: loss 16.12916783731615
Epoch 5: loss 16.129157227365265
Epoch 6: loss 16.12914819027692
Epoch 7: loss 16.129146883706802
Epoch 8: loss 16.12914404368647
Epoch 9: loss 16.129139389143877
Epoch 10: loss 16.129129172797562
Epoch 11: loss 16.129116201219254
Epoch 12: loss 16.12910747813297
Epoch 13: loss 16.129109207348307
Epoch 14: loss 16.129094886416777
Epoch 15: loss 16.129093187019965
Epoch 16: loss 16.129085549328106
Epoch 17: loss 16.12907256063655
Epoch 18: loss 16.129061387504425
Epoch 19: loss 16.12906073408972
-----------Time: 0:02:46.445242, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 50, rmse: 4.017165660858154-------------


Epoch 0: loss 16.12917095270411
Epoch 1: loss 16.129170332997308
Epoch 2: loss 16.12915732667393
Epoch 3: loss 16.129149507477983
Epoch 4: loss 16.129142127262632
Epoch 5: loss 16.12913522258798
Epoch 6: loss 16.129123483940983
Epoch 7: loss 16.129119462588353
Epoch 8: loss 16.129116046422197
Epoch 9: loss 16.12910496870936
Epoch 10: loss 16.12909423766932
Epoch 11: loss 16.129087738526653
Epoch 12: loss 16.129085718386197
Epoch 13: loss 16.129076316474638
Epoch 14: loss 16.12906936097884
Epoch 15: loss 16.129057906774673
Epoch 16: loss 16.129049474872797
Epoch 17: loss 16.129045135110147
Epoch 18: loss 16.129038834325513
Epoch 19: loss 16.12903168021239
-----------Time: 0:03:22.647809, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 100, rmse: 4.017175674438477-------------


Epoch 0: loss 16.129170672409945
Epoch 1: loss 16.129167066442378
Epoch 2: loss 16.129161173264055
Epoch 3: loss 16.129158097029123
Epoch 4: loss 16.129147369100583
Epoch 5: loss 16.129137361224675
Epoch 6: loss 16.129128890429065
Epoch 7: loss 16.12912120813906
Epoch 8: loss 16.12911599819397
Epoch 9: loss 16.12910396602894
Epoch 10: loss 16.129103523158975
Epoch 11: loss 16.129090213205192
Epoch 12: loss 16.12908582132494
Epoch 13: loss 16.129084034028295
Epoch 14: loss 16.129068136085564
Epoch 15: loss 16.12907146668551
Epoch 16: loss 16.129055167618752
Epoch 17: loss 16.129046868733443
Epoch 18: loss 16.129043974780465
Epoch 19: loss 16.129031718068955
-----------Time: 0:04:15.913543, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 150, rmse: 4.0171709060668945-------------


Epoch 0: loss 16.1291661114716
Epoch 1: loss 16.129172665843374
Epoch 2: loss 16.12914919192017
Epoch 3: loss 16.129147309463526
Epoch 4: loss 16.12914125474657
Epoch 5: loss 16.129133138921095
Epoch 6: loss 16.129124416871974
Epoch 7: loss 16.12912009344469
Epoch 8: loss 16.129108830079105
Epoch 9: loss 16.129104971042985
Epoch 10: loss 16.129098601805378
Epoch 11: loss 16.12909262513518
Epoch 12: loss 16.129080908787255
Epoch 13: loss 16.129070691144484
Epoch 14: loss 16.129063895890866
Epoch 15: loss 16.12906345950319
Epoch 16: loss 16.12905632224402
Epoch 17: loss 16.129045063286387
Epoch 18: loss 16.129042631390952
Epoch 19: loss 16.1290294002618
-----------Time: 0:04:51.829171, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 200, rmse: 4.0171709060668945-------------


Epoch 0: loss 16.129309958384827
Epoch 1: loss 16.129304587419686
Epoch 2: loss 16.129290240040415
Epoch 3: loss 16.129279002344692
Epoch 4: loss 16.12927135220684
Epoch 5: loss 16.129264671819374
Epoch 6: loss 16.129251082349302
Epoch 7: loss 16.129240151654773
Epoch 8: loss 16.129231079043482
Epoch 9: loss 16.12922269796275
Epoch 10: loss 16.129217944630078
Epoch 11: loss 16.129207776771285
Epoch 12: loss 16.129194949877657
Epoch 13: loss 16.129186446411314
Epoch 14: loss 16.129171628677177
Epoch 15: loss 16.1291625065412
Epoch 16: loss 16.129154790802584
Epoch 17: loss 16.129143556477647
Epoch 18: loss 16.1291363110939
Epoch 19: loss 16.129128511344824
-----------Time: 0:03:18.116515, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 20, rmse: 4.017153263092041-------------


Epoch 0: loss 16.129211110223434
Epoch 1: loss 16.12920024953762
Epoch 2: loss 16.129195753422167
Epoch 3: loss 16.129187817545024
Epoch 4: loss 16.129176825916982
Epoch 5: loss 16.129161206712666
Epoch 6: loss 16.129154509471253
Epoch 7: loss 16.129140479724132
Epoch 8: loss 16.12912708627847
Epoch 9: loss 16.12912493804799
Epoch 10: loss 16.129114624467345
Epoch 11: loss 16.129107598444246
Epoch 12: loss 16.129096148129452
Epoch 13: loss 16.12908792703161
Epoch 14: loss 16.12907442183129
Epoch 15: loss 16.12907044611397
Epoch 16: loss 16.12906141187783
Epoch 17: loss 16.129041823697776
Epoch 18: loss 16.12903300985945
Epoch 19: loss 16.12902814477212
-----------Time: 0:03:26.313140, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 50, rmse: 4.017158508300781-------------


Epoch 0: loss 16.129178465676734
Epoch 1: loss 16.12916597715858
Epoch 2: loss 16.129157608783135
Epoch 3: loss 16.129148708341432
Epoch 4: loss 16.129135038231688
Epoch 5: loss 16.129135000634413
Epoch 6: loss 16.129111845639816
Epoch 7: loss 16.129108872343625
Epoch 8: loss 16.129099046490385
Epoch 9: loss 16.129092813380844
Epoch 10: loss 16.129078825638953
Epoch 11: loss 16.129066680941392
Epoch 12: loss 16.129052953268925
Epoch 13: loss 16.12904959077612
Epoch 14: loss 16.12903719534363
Epoch 15: loss 16.12902827312144
Epoch 16: loss 16.129029057996956
Epoch 17: loss 16.129010052704306
Epoch 18: loss 16.12899641189451
Epoch 19: loss 16.128993370923222
-----------Time: 0:04:17.031444, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 100, rmse: 4.01716423034668-------------


Epoch 0: loss 16.129165230658213
Epoch 1: loss 16.129152510592704
Epoch 2: loss 16.12914826624934
Epoch 3: loss 16.129139847830626
Epoch 4: loss 16.12912722499945
Epoch 5: loss 16.129116129395495
Epoch 6: loss 16.12911212437823
Epoch 7: loss 16.129097215114175
Epoch 8: loss 16.129092857460407
Epoch 9: loss 16.129075503336338
Epoch 10: loss 16.12906596244451
Epoch 11: loss 16.12905209475461
Epoch 12: loss 16.129057135900897
Epoch 13: loss 16.129041918339194
Epoch 14: loss 16.129029288247853
Epoch 15: loss 16.129023389624408
Epoch 16: loss 16.129009599721968
Epoch 17: loss 16.129003888047734
Epoch 18: loss 16.12899166426625
Epoch 19: loss 16.128981453365057
-----------Time: 0:05:03.526909, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 150, rmse: 4.017165184020996-------------


Epoch 0: loss 16.12917954614462
Epoch 1: loss 16.1291663876171
Epoch 2: loss 16.129150414998406
Epoch 3: loss 16.129137875140522
Epoch 4: loss 16.129134554134367
Epoch 5: loss 16.129124856889735
Epoch 6: loss 16.129123164493794
Epoch 7: loss 16.12910376533728
Epoch 8: loss 16.12909467613133
Epoch 9: loss 16.12908698632187
Epoch 10: loss 16.1290775509617
Epoch 11: loss 16.12906418448236
Epoch 12: loss 16.12905731844215
Epoch 13: loss 16.129049509617865
Epoch 14: loss 16.129044975645844
Epoch 15: loss 16.12903064563911
Epoch 16: loss 16.12902340673765
Epoch 17: loss 16.129009452703663
Epoch 18: loss 16.12900205459719
Epoch 19: loss 16.128986885523005
-----------Time: 0:05:31.576795, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 200, rmse: 4.01716423034668-------------


Epoch 0: loss 16.12925919947124
Epoch 1: loss 16.129246632647035
Epoch 2: loss 16.129235362021284
Epoch 3: loss 16.129222506087004
Epoch 4: loss 16.129201900446915
Epoch 5: loss 16.129199039164675
Epoch 6: loss 16.12918467570933
Epoch 7: loss 16.12917303040729
Epoch 8: loss 16.129166845007394
Epoch 9: loss 16.129144029425433
Epoch 10: loss 16.129139388884585
Epoch 11: loss 16.12912578385702
Epoch 12: loss 16.1291109993122
Epoch 13: loss 16.12910052367434
Epoch 14: loss 16.129079137048105
Epoch 15: loss 16.129069140840315
Epoch 16: loss 16.129056783264392
Epoch 17: loss 16.129044034676813
Epoch 18: loss 16.12903359222827
Epoch 19: loss 16.129016421164035
-----------Time: 0:04:12.831771, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 20, rmse: 4.017155647277832-------------


Epoch 0: loss 16.129210481960012
Epoch 1: loss 16.129202073394378
Epoch 2: loss 16.12919096897451
Epoch 3: loss 16.129176137238627
Epoch 4: loss 16.129161994699682
Epoch 5: loss 16.12915114516341
Epoch 6: loss 16.129139427000442
Epoch 7: loss 16.129119722139194
Epoch 8: loss 16.129107623076944
Epoch 9: loss 16.12909766965226
Epoch 10: loss 16.1290857090517
Epoch 11: loss 16.12907164092943
Epoch 12: loss 16.129056887758885
Epoch 13: loss 16.129046671153283
Epoch 14: loss 16.12904052335066
Epoch 15: loss 16.12901955599886
Epoch 16: loss 16.129008231181174
Epoch 17: loss 16.128997296078687
Epoch 18: loss 16.12899116227781
Epoch 19: loss 16.128975234775844
-----------Time: 0:04:17.442070, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 50, rmse: 4.01716423034668-------------


Epoch 0: loss 16.12917203783924
Epoch 1: loss 16.12915132459316
Epoch 2: loss 16.129139353361644
Epoch 3: loss 16.129129261993857
Epoch 4: loss 16.129112607697678
Epoch 5: loss 16.12910175945786
Epoch 6: loss 16.129086820115987
Epoch 7: loss 16.12907104637391
Epoch 8: loss 16.129071199874506
Epoch 9: loss 16.12905272820386
Epoch 10: loss 16.129034729740294
Epoch 11: loss 16.12902647052659
Epoch 12: loss 16.129017889532076
Epoch 13: loss 16.12899751077209
Epoch 14: loss 16.128987593907517
Epoch 15: loss 16.128978174623423
Epoch 16: loss 16.128964658532862
Epoch 17: loss 16.128942210367025
Epoch 18: loss 16.128939147615252
Epoch 19: loss 16.128925445353357
-----------Time: 0:04:34.970227, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 100, rmse: 4.017155647277832-------------


Epoch 0: loss 16.129163916827935
Epoch 1: loss 16.12915613496997
Epoch 2: loss 16.12913881870247
Epoch 3: loss 16.129128312727495
Epoch 4: loss 16.12911163898445
Epoch 5: loss 16.12910141097002
Epoch 6: loss 16.129090901624256
Epoch 7: loss 16.129074913707356
Epoch 8: loss 16.129066106869903
Epoch 9: loss 16.129050527596487
Epoch 10: loss 16.12904021401584
Epoch 11: loss 16.129026297319836
Epoch 12: loss 16.12901199531659
Epoch 13: loss 16.12900687949433
Epoch 14: loss 16.12899070307248
Epoch 15: loss 16.12897143045024
Epoch 16: loss 16.128958107272588
Epoch 17: loss 16.128956633459428
Epoch 18: loss 16.12894038469523
Epoch 19: loss 16.128928247517123
-----------Time: 0:03:51.007786, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 150, rmse: 4.017158508300781-------------


Epoch 0: loss 16.129168719944584
Epoch 1: loss 16.12915870247489
Epoch 2: loss 16.129142190011187
Epoch 3: loss 16.129126055075982
Epoch 4: loss 16.129114842012953
Epoch 5: loss 16.129104553065005
Epoch 6: loss 16.129088875260802
Epoch 7: loss 16.12907598380358
Epoch 8: loss 16.129069091574923
Epoch 9: loss 16.1290547672726
Epoch 10: loss 16.12904025887328
Epoch 11: loss 16.129027027225547
Epoch 12: loss 16.129016808545607
Epoch 13: loss 16.129002444571682
Epoch 14: loss 16.128992326756155
Epoch 15: loss 16.128978829593876
Epoch 16: loss 16.128969466057466
Epoch 17: loss 16.12895614832494
Epoch 18: loss 16.128935235683656
Epoch 19: loss 16.128938299731885
-----------Time: 0:04:19.814187, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 200, rmse: 4.017157554626465-------------


Epoch 0: loss 16.12938640012632
Epoch 1: loss 16.129378684387706
Epoch 2: loss 16.129348288417315
Epoch 3: loss 16.129335212863094
Epoch 4: loss 16.129320565483503
Epoch 5: loss 16.12931108578448
Epoch 6: loss 16.129290770291632
Epoch 7: loss 16.12926997614659
Epoch 8: loss 16.129252305168247
Epoch 9: loss 16.129237791323803
Epoch 10: loss 16.129220416456413
Epoch 11: loss 16.129201301224146
Epoch 12: loss 16.12918352730706
Epoch 13: loss 16.129172719257433
Epoch 14: loss 16.129153649141895
Epoch 15: loss 16.129145872210472
Epoch 16: loss 16.129119002345853
Epoch 17: loss 16.129103235345355
Epoch 18: loss 16.129085779579
Epoch 19: loss 16.1290700403227
-----------Time: 0:02:42.176230, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 20, rmse: 4.017169952392578-------------


Epoch 0: loss 16.129173103008924
Epoch 1: loss 16.12915659806468
Epoch 2: loss 16.12914220245718
Epoch 3: loss 16.129126814281637
Epoch 4: loss 16.129108384616224
Epoch 5: loss 16.12909411891379
Epoch 6: loss 16.12908197551269
Epoch 7: loss 16.129063916893482
Epoch 8: loss 16.12904691022009
Epoch 9: loss 16.129024720308635
Epoch 10: loss 16.129001911208967
Epoch 11: loss 16.128988072041132
Epoch 12: loss 16.1289797830089
Epoch 13: loss 16.128961747725935
Epoch 14: loss 16.128941498352432
Epoch 15: loss 16.128923667650497
Epoch 16: loss 16.128905261321325
Epoch 17: loss 16.128886082562627
Epoch 18: loss 16.128868381506468
Epoch 19: loss 16.12884888304129
-----------Time: 0:03:07.429564, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 50, rmse: 4.017143726348877-------------


Epoch 0: loss 16.12914725967955
Epoch 1: loss 16.129125476855826
Epoch 2: loss 16.129107760760757
Epoch 3: loss 16.129098912177362
Epoch 4: loss 16.129082515098403
Epoch 5: loss 16.12905814584148
Epoch 6: loss 16.129043874693927
Epoch 7: loss 16.129022133356855
Epoch 8: loss 16.129016236807743
Epoch 9: loss 16.128994422869038
Epoch 10: loss 16.128979380329127
Epoch 11: loss 16.128962935281226
Epoch 12: loss 16.12894059938863
Epoch 13: loss 16.128931907676627
Epoch 14: loss 16.128908137642476
Epoch 15: loss 16.12889203745234
Epoch 16: loss 16.128876046942523
Epoch 17: loss 16.128861982968917
Epoch 18: loss 16.12884090386246
Epoch 19: loss 16.128822699521404
-----------Time: 0:03:51.428834, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 100, rmse: 4.017148494720459-------------


Epoch 0: loss 16.129178018658106
Epoch 1: loss 16.129155455626115
Epoch 2: loss 16.129145390446773
Epoch 3: loss 16.129143611188166
Epoch 4: loss 16.12911233959022
Epoch 5: loss 16.129097801372367
Epoch 6: loss 16.12907898743688
Epoch 7: loss 16.129065644812364
Epoch 8: loss 16.12904469068443
Epoch 9: loss 16.12903213604693
Epoch 10: loss 16.129009869125888
Epoch 11: loss 16.128999142493807
Epoch 12: loss 16.12897846036271
Epoch 13: loss 16.128967705727142
Epoch 14: loss 16.12894463163151
Epoch 15: loss 16.128926005941686
Epoch 16: loss 16.128915746812265
Epoch 17: loss 16.1288898264733
Epoch 18: loss 16.12887339283423
Epoch 19: loss 16.12885636645468
-----------Time: 0:04:36.126138, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 150, rmse: 4.017147541046143-------------


Epoch 0: loss 16.1291626937497
Epoch 1: loss 16.12915112934662
Epoch 2: loss 16.129135479027322
Epoch 3: loss 16.12911637338884
Epoch 4: loss 16.129099550034574
Epoch 5: loss 16.12907846340866
Epoch 6: loss 16.129064648614232
Epoch 7: loss 16.129040031733883
Epoch 8: loss 16.129026457562013
Epoch 9: loss 16.129014127470995
Epoch 10: loss 16.12899466789955
Epoch 11: loss 16.12898148370217
Epoch 12: loss 16.128963030441227
Epoch 13: loss 16.128948501557872
Epoch 14: loss 16.128925438093194
Epoch 15: loss 16.128913970146574
Epoch 16: loss 16.12889949467728
Epoch 17: loss 16.12887585895615
Epoch 18: loss 16.128866262057347
Epoch 19: loss 16.128840707060174
-----------Time: 0:05:05.488564, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 200, rmse: 4.017146110534668-------------


Epoch 0: loss 16.129241296945544
Epoch 1: loss 16.12921974126122
Epoch 2: loss 16.12919300804325
Epoch 3: loss 16.12917404294079
Epoch 4: loss 16.129158014574408
Epoch 5: loss 16.12911731124637
Epoch 6: loss 16.129108701729788
Epoch 7: loss 16.12909616627986
Epoch 8: loss 16.129063356823735
Epoch 9: loss 16.1290414026083
Epoch 10: loss 16.129019779767468
Epoch 11: loss 16.12899488907524
Epoch 12: loss 16.12897482898457
Epoch 13: loss 16.128945639497758
Epoch 14: loss 16.128931520554342
Epoch 15: loss 16.12890906590622
Epoch 16: loss 16.128883467348068
Epoch 17: loss 16.128865381243955
Epoch 18: loss 16.128841439299507
Epoch 19: loss 16.12882107013331
-----------Time: 0:03:44.018244, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 20, rmse: 4.017117500305176-------------


Epoch 0: loss 16.12918205893902
Epoch 1: loss 16.12916312806304
Epoch 2: loss 16.12914253175745
Epoch 3: loss 16.12912215870188
Epoch 4: loss 16.129100971470844
Epoch 5: loss 16.129073443264986
Epoch 6: loss 16.129047543410053
Epoch 7: loss 16.12903129205294
Epoch 8: loss 16.129004328324783
Epoch 9: loss 16.128984461665606
Epoch 10: loss 16.128959765960627
Epoch 11: loss 16.12893683680897
Epoch 12: loss 16.128915127364756
Epoch 13: loss 16.12889142708003
Epoch 14: loss 16.128873973128716
Epoch 15: loss 16.128848293412307
Epoch 16: loss 16.12883155069771
Epoch 17: loss 16.128808047215273
Epoch 18: loss 16.12878803846433
Epoch 19: loss 16.12875688043608
-----------Time: 0:03:39.348083, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 50, rmse: 4.017130374908447-------------


Epoch 0: loss 16.129161675771076
Epoch 1: loss 16.12914170202449
Epoch 2: loss 16.129123991115254
Epoch 3: loss 16.129099257294413
Epoch 4: loss 16.129077810771832
Epoch 5: loss 16.12905600098179
Epoch 6: loss 16.129029892915742
Epoch 7: loss 16.129017524708868
Epoch 8: loss 16.128988292698242
Epoch 9: loss 16.12895425705238
Epoch 10: loss 16.128942474844404
Epoch 11: loss 16.1289153192405
Epoch 12: loss 16.12889480201883
Epoch 13: loss 16.12887364071695
Epoch 14: loss 16.128850428937504
Epoch 15: loss 16.12883371681931
Epoch 16: loss 16.12880874470954
Epoch 17: loss 16.12878111434281
Epoch 18: loss 16.128764487272246
Epoch 19: loss 16.12874065993466
-----------Time: 0:04:40.185932, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 100, rmse: 4.017136096954346-------------


Epoch 0: loss 16.129174801887153
Epoch 1: loss 16.12914577134606
Epoch 2: loss 16.129123817908496
Epoch 3: loss 16.129103511490854
Epoch 4: loss 16.129074145167206
Epoch 5: loss 16.129051133560836
Epoch 6: loss 16.12903042990854
Epoch 7: loss 16.12901637552872
Epoch 8: loss 16.12899256530438
Epoch 9: loss 16.128966617739803
Epoch 10: loss 16.128947495765956
Epoch 11: loss 16.12892746834602
Epoch 12: loss 16.128900700642273
Epoch 13: loss 16.128884565188486
Epoch 14: loss 16.128857504225994
Epoch 15: loss 16.128831780689318
Epoch 16: loss 16.12881160417674
Epoch 17: loss 16.128789929477595
Epoch 18: loss 16.128766280273304
Epoch 19: loss 16.128750257092758
-----------Time: 0:05:32.475032, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 150, rmse: 4.017134189605713-------------


Epoch 0: loss 16.12916539090039
Epoch 1: loss 16.12914099986298
Epoch 2: loss 16.129114393697865
Epoch 3: loss 16.12910276732411
Epoch 4: loss 16.129080156841766
Epoch 5: loss 16.129057120343408
Epoch 6: loss 16.12903313043002
Epoch 7: loss 16.1290098442339
Epoch 8: loss 16.128988991229672
Epoch 9: loss 16.128956883157187
Epoch 10: loss 16.128933235249356
Epoch 11: loss 16.12892438459163
Epoch 12: loss 16.128895883783173
Epoch 13: loss 16.12887575342449
Epoch 14: loss 16.128847351665403
Epoch 15: loss 16.12883379408819
Epoch 16: loss 16.128807251708803
Epoch 17: loss 16.12878693725312
Epoch 18: loss 16.128762081565256
Epoch 19: loss 16.12873718542791
-----------Time: 0:05:59.910470, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 200, rmse: 4.017133712768555-------------


Epoch 0: loss 16.129330096522253
Epoch 1: loss 16.129302173674656
Epoch 2: loss 16.12927475929779
Epoch 3: loss 16.12924276609146
Epoch 4: loss 16.129214870988058
Epoch 5: loss 16.129186127742003
Epoch 6: loss 16.12915584041477
Epoch 7: loss 16.12912691073809
Epoch 8: loss 16.129101761013608
Epoch 9: loss 16.12907210169051
Epoch 10: loss 16.12903492343108
Epoch 11: loss 16.129004511125324
Epoch 12: loss 16.1289806451533
Epoch 13: loss 16.128955005367793
Epoch 14: loss 16.128928584077553
Epoch 15: loss 16.12889104903295
Epoch 16: loss 16.12885970301833
Epoch 17: loss 16.128828973080427
Epoch 18: loss 16.128796377539828
Epoch 19: loss 16.12877240162819
-----------Time: 0:03:02.754651, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 20, rmse: 4.017148494720459-------------


Epoch 0: loss 16.12920273718074
Epoch 1: loss 16.129171924788125
Epoch 2: loss 16.129136481448448
Epoch 3: loss 16.12911028600115
Epoch 4: loss 16.129084090813144
Epoch 5: loss 16.1290461735728
Epoch 6: loss 16.129027775540955
Epoch 7: loss 16.12899286789762
Epoch 8: loss 16.128966475648035
Epoch 9: loss 16.128929532047458
Epoch 10: loss 16.128904414734418
Epoch 11: loss 16.128875020925864
Epoch 12: loss 16.12884478338261
Epoch 13: loss 16.128817422160513
Epoch 14: loss 16.12878831979566
Epoch 15: loss 16.128755620797733
Epoch 16: loss 16.128730506596195
Epoch 17: loss 16.128698741307137
Epoch 18: loss 16.12867252122714
Epoch 19: loss 16.128646490948558
-----------Time: 0:02:46.080787, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 50, rmse: 4.017117023468018-------------


Epoch 0: loss 16.129162642669264
Epoch 1: loss 16.129131913509234
Epoch 2: loss 16.129107554105392
Epoch 3: loss 16.129073848019093
Epoch 4: loss 16.1290444861034
Epoch 5: loss 16.12901649558071
Epoch 6: loss 16.128990409813735
Epoch 7: loss 16.128956243744227
Epoch 8: loss 16.12892644518157
Epoch 9: loss 16.128905800907038
Epoch 10: loss 16.128871204932143
Epoch 11: loss 16.128844922881466
Epoch 12: loss 16.128812209622506
Epoch 13: loss 16.128785647277667
Epoch 14: loss 16.12875303669816
Epoch 15: loss 16.128720927069924
Epoch 16: loss 16.128696443465433
Epoch 17: loss 16.128668239286505
Epoch 18: loss 16.128633605195752
Epoch 19: loss 16.12860407240693
-----------Time: 0:03:24.945814, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 100, rmse: 4.017116546630859-------------


Epoch 0: loss 16.129159563841412
Epoch 1: loss 16.129137402711322
Epoch 2: loss 16.12911432550419
Epoch 3: loss 16.129076041107012
Epoch 4: loss 16.12905057167605
Epoch 5: loss 16.129015986591398
Epoch 6: loss 16.128988824245916
Epoch 7: loss 16.128963669854187
Epoch 8: loss 16.128932565239992
Epoch 9: loss 16.12890426927186
Epoch 10: loss 16.128874325505933
Epoch 11: loss 16.12884229003508
Epoch 12: loss 16.1288130689147
Epoch 13: loss 16.12878946456785
Epoch 14: loss 16.128757347160867
Epoch 15: loss 16.12872730823494
Epoch 16: loss 16.128694354612716
Epoch 17: loss 16.128657537027827
Epoch 18: loss 16.128632253249616
Epoch 19: loss 16.128614644501248
-----------Time: 0:04:16.320189, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 150, rmse: 4.017115592956543-------------


Epoch 0: loss 16.12915903566453
Epoch 1: loss 16.129130098986977
Epoch 2: loss 16.129106012098553
Epoch 3: loss 16.12907459296371
Epoch 4: loss 16.12904035740407
Epoch 5: loss 16.129013153312645
Epoch 6: loss 16.128984134439673
Epoch 7: loss 16.128956815222804
Epoch 8: loss 16.128922608444523
Epoch 9: loss 16.12889548680781
Epoch 10: loss 16.12886999585565
Epoch 11: loss 16.128840664536362
Epoch 12: loss 16.128806013073074
Epoch 13: loss 16.128785889455973
Epoch 14: loss 16.128747544384574
Epoch 15: loss 16.12872136008681
Epoch 16: loss 16.128692026174605
Epoch 17: loss 16.128657208505437
Epoch 18: loss 16.12863042628136
Epoch 19: loss 16.128592288902496
-----------Time: 0:04:41.602601, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 200, rmse: 4.017116069793701-------------


Epoch 0: loss 16.129143129943053
Epoch 1: loss 16.129099534995664
Epoch 2: loss 16.129063652675395
Epoch 3: loss 16.12902034268942
Epoch 4: loss 16.128987340320382
Epoch 5: loss 16.1289430180602
Epoch 6: loss 16.12890706391617
Epoch 7: loss 16.128868957392996
Epoch 8: loss 16.12882647117627
Epoch 9: loss 16.128794638730703
Epoch 10: loss 16.128748994342914
Epoch 11: loss 16.128709809166896
Epoch 12: loss 16.12867897136371
Epoch 13: loss 16.128633142619627
Epoch 14: loss 16.12859949539251
Epoch 15: loss 16.128556582381893
Epoch 16: loss 16.128515101179214
Epoch 17: loss 16.12847616466379
Epoch 18: loss 16.12844333731655
Epoch 19: loss 16.12841210279729
-----------Time: 0:03:14.329109, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 20, rmse: 4.017127990722656-------------


Epoch 0: loss 16.129173368782762
Epoch 1: loss 16.12914242830012
Epoch 2: loss 16.129097651761146
Epoch 3: loss 16.129067793042847
Epoch 4: loss 16.129021236208096
Epoch 5: loss 16.12898050072791
Epoch 6: loss 16.12893493075679
Epoch 7: loss 16.128901351464062
Epoch 8: loss 16.128859180027277
Epoch 9: loss 16.12882685440918
Epoch 10: loss 16.12878245332437
Epoch 11: loss 16.128742072814312
Epoch 12: loss 16.12870730052116
Epoch 13: loss 16.128668276105905
Epoch 14: loss 16.128632867770587
Epoch 15: loss 16.128586037123963
Epoch 16: loss 16.128544889888794
Epoch 17: loss 16.12851694422354
Epoch 18: loss 16.12847686786247
Epoch 19: loss 16.128433264358463
-----------Time: 0:03:23.843286, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 50, rmse: 4.017097473144531-------------


Epoch 0: loss 16.129157091496495
Epoch 1: loss 16.129117706406692
Epoch 2: loss 16.12907680912858
Epoch 3: loss 16.129045476337826
Epoch 4: loss 16.12901084198778
Epoch 5: loss 16.128968236756233
Epoch 6: loss 16.12893592280626
Epoch 7: loss 16.12888844341218
Epoch 8: loss 16.128850944668393
Epoch 9: loss 16.12880823701669
Epoch 10: loss 16.12877541226236
Epoch 11: loss 16.128735311527883
Epoch 12: loss 16.128698160234777
Epoch 13: loss 16.128663458468928
Epoch 14: loss 16.12862042229483
Epoch 15: loss 16.128584991401144
Epoch 16: loss 16.128539765250625
Epoch 17: loss 16.1285046486183
Epoch 18: loss 16.1284618792552
Epoch 19: loss 16.12842738077393
-----------Time: 0:04:11.168508, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 100, rmse: 4.017092704772949-------------


Epoch 0: loss 16.129153894172408
Epoch 1: loss 16.12912233450155
Epoch 2: loss 16.12907836461858
Epoch 3: loss 16.12903347502849
Epoch 4: loss 16.128999701526386
Epoch 5: loss 16.12895539378653
Epoch 6: loss 16.1289215352368
Epoch 7: loss 16.12888570088547
Epoch 8: loss 16.128846255380736
Epoch 9: loss 16.128809401235742
Epoch 10: loss 16.128771508628095
Epoch 11: loss 16.128731577211
Epoch 12: loss 16.12868005649884
Epoch 13: loss 16.128650062689644
Epoch 14: loss 16.128613704828673
Epoch 15: loss 16.128576042471924
Epoch 16: loss 16.128540180117103
Epoch 17: loss 16.12849468611841
Epoch 18: loss 16.12846027346264
Epoch 19: loss 16.128415124581
-----------Time: 0:05:02.803840, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 150, rmse: 4.017090797424316-------------


Epoch 0: loss 16.12915890575946
Epoch 1: loss 16.129124952827606
Epoch 2: loss 16.129088320376887
Epoch 3: loss 16.129049029669208
Epoch 4: loss 16.12900486505829
Epoch 5: loss 16.12895923207933
Epoch 6: loss 16.12892411155763
Epoch 7: loss 16.12889287677908
Epoch 8: loss 16.12885472254626
Epoch 9: loss 16.128816377734154
Epoch 10: loss 16.12877534718018
Epoch 11: loss 16.128732444281937
Epoch 12: loss 16.12870085764476
Epoch 13: loss 16.12865949571619
Epoch 14: loss 16.12861505340402
Epoch 15: loss 16.128587738076522
Epoch 16: loss 16.128554698628793
Epoch 17: loss 16.128504911799222
Epoch 18: loss 16.12845740569811
Epoch 19: loss 16.12842066538211
-----------Time: 0:05:33.997307, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 200, rmse: 4.017092227935791-------------


Epoch 0: loss 16.129095018914757
Epoch 1: loss 16.12904842500132
Epoch 2: loss 16.1289926843192
Epoch 3: loss 16.128932227123816
Epoch 4: loss 16.12888630244186
Epoch 5: loss 16.12884268052815
Epoch 6: loss 16.128789491274876
Epoch 7: loss 16.128739138930435
Epoch 8: loss 16.12867773272798
Epoch 9: loss 16.128633662758475
Epoch 10: loss 16.128579546278623
Epoch 11: loss 16.12852782928276
Epoch 12: loss 16.128480791202897
Epoch 13: loss 16.128422633664254
Epoch 14: loss 16.128373898780495
Epoch 15: loss 16.128331694672976
Epoch 16: loss 16.128271556924776
Epoch 17: loss 16.128229034407234
Epoch 18: loss 16.128174532101553
Epoch 19: loss 16.128113316478387
-----------Time: 0:04:07.077503, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 20, rmse: 4.017059803009033-------------


Epoch 0: loss 16.129133296570355
Epoch 1: loss 16.129086668689723
Epoch 2: loss 16.129030820142322
Epoch 3: loss 16.128976410403727
Epoch 4: loss 16.12893542392932
Epoch 5: loss 16.128876988430136
Epoch 6: loss 16.12882773988982
Epoch 7: loss 16.128771946052936
Epoch 8: loss 16.128730545749217
Epoch 9: loss 16.128671235659642
Epoch 10: loss 16.128619855742794
Epoch 11: loss 16.128570779112774
Epoch 12: loss 16.128522299371898
Epoch 13: loss 16.12846438012219
Epoch 14: loss 16.12841540409729
Epoch 15: loss 16.128360642759354
Epoch 16: loss 16.128320929925035
Epoch 17: loss 16.128257913781354
Epoch 18: loss 16.128211579418753
Epoch 19: loss 16.12816242655702
-----------Time: 0:04:07.369195, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 50, rmse: 4.017052173614502-------------


Epoch 0: loss 16.129166582604345
Epoch 1: loss 16.12910468789661
Epoch 2: loss 16.129054823279574
Epoch 3: loss 16.129002753128624
Epoch 4: loss 16.12895588695906
Epoch 5: loss 16.128898574970155
Epoch 6: loss 16.12884712685963
Epoch 7: loss 16.128801551702686
Epoch 8: loss 16.128738018790944
Epoch 9: loss 16.12869704242891
Epoch 10: loss 16.12864918135767
Epoch 11: loss 16.128591626931172
Epoch 12: loss 16.128542474328725
Epoch 13: loss 16.128485747042266
Epoch 14: loss 16.128436000402885
Epoch 15: loss 16.128380795157817
Epoch 16: loss 16.128336171341566
Epoch 17: loss 16.128290662822547
Epoch 18: loss 16.1282351531692
Epoch 19: loss 16.12818884577292
-----------Time: 0:05:02.006836, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 100, rmse: 4.017061710357666-------------


Epoch 0: loss 16.129149326492485
Epoch 1: loss 16.12909416325265
Epoch 2: loss 16.129041462245358
Epoch 3: loss 16.128999497204646
Epoch 4: loss 16.128942964905434
Epoch 5: loss 16.128890472887132
Epoch 6: loss 16.128839533765916
Epoch 7: loss 16.12878429507224
Epoch 8: loss 16.128735577561013
Epoch 9: loss 16.12868815910045
Epoch 10: loss 16.128630642789805
Epoch 11: loss 16.128581477482076
Epoch 12: loss 16.128526466964964
Epoch 13: loss 16.128474998629695
Epoch 14: loss 16.128425855102456
Epoch 15: loss 16.12837726101401
Epoch 16: loss 16.128322153262562
Epoch 17: loss 16.128271477062977
Epoch 18: loss 16.128228667509692
Epoch 19: loss 16.1281711097124
-----------Time: 0:03:46.217199, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 150, rmse: 4.017062187194824-------------


Epoch 0: loss 16.129144806522206
Epoch 1: loss 16.129095487195297
Epoch 2: loss 16.129047669425745
Epoch 3: loss 16.128992353463186
Epoch 4: loss 16.128941731714825
Epoch 5: loss 16.128887957240526
Epoch 6: loss 16.128838632987076
Epoch 7: loss 16.128788086692555
Epoch 8: loss 16.12874032622644
Epoch 9: loss 16.128684227721987
Epoch 10: loss 16.128637828277206
Epoch 11: loss 16.128583970051736
Epoch 12: loss 16.128532156340125
Epoch 13: loss 16.12847995161686
Epoch 14: loss 16.1284340977215
Epoch 15: loss 16.128378989970056
Epoch 16: loss 16.128320175905213
Epoch 17: loss 16.128280685024457
Epoch 18: loss 16.128222930684174
Epoch 19: loss 16.128174854919532
-----------Time: 0:04:15.549402, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 200, rmse: 4.017060279846191-------------


Epoch 0: loss 16.129243974390075
Epoch 1: loss 16.129174708282903
Epoch 2: loss 16.129103834308832
Epoch 3: loss 16.12903442740635
Epoch 4: loss 16.12897716572001
Epoch 5: loss 16.128900960711697
Epoch 6: loss 16.128837736875482
Epoch 7: loss 16.12876653541619
Epoch 8: loss 16.12869496291069
Epoch 9: loss 16.12863139629105
Epoch 10: loss 16.1285606351088
Epoch 11: loss 16.12849445145956
Epoch 12: loss 16.12842538526617
Epoch 13: loss 16.128359905852772
Epoch 14: loss 16.12828851692569
Epoch 15: loss 16.12823345221664
Epoch 16: loss 16.12815418990168
Epoch 17: loss 16.12809035569316
Epoch 18: loss 16.128014799691595
Epoch 19: loss 16.127952138777335
-----------Time: 0:02:36.816331, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 20, rmse: 4.017030715942383-------------


Epoch 0: loss 16.129143775060424
Epoch 1: loss 16.129073769194463
Epoch 2: loss 16.12900717093803
Epoch 3: loss 16.128932101367415
Epoch 4: loss 16.128876559561917
Epoch 5: loss 16.12880392992477
Epoch 6: loss 16.128735316454424
Epoch 7: loss 16.12866430713017
Epoch 8: loss 16.128601036102893
Epoch 9: loss 16.128532182787865
Epoch 10: loss 16.128464908299076
Epoch 11: loss 16.1283956206707
Epoch 12: loss 16.12832823027859
Epoch 13: loss 16.128255992171685
Epoch 14: loss 16.128193447160747
Epoch 15: loss 16.128123995919406
Epoch 16: loss 16.12805175807179
Epoch 17: loss 16.12798693103593
Epoch 18: loss 16.127919318690257
Epoch 19: loss 16.12785220211002
-----------Time: 0:03:06.736997, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 50, rmse: 4.017027854919434-------------


Epoch 0: loss 16.129156109040817
Epoch 1: loss 16.12908647266531
Epoch 2: loss 16.1290213189221
Epoch 3: loss 16.12895060882029
Epoch 4: loss 16.128883364668614
Epoch 5: loss 16.12881954160963
Epoch 6: loss 16.12874509796881
Epoch 7: loss 16.12867715995295
Epoch 8: loss 16.12861464527912
Epoch 9: loss 16.128541620222364
Epoch 10: loss 16.128474007876687
Epoch 11: loss 16.12840616242791
Epoch 12: loss 16.128345257436017
Epoch 13: loss 16.12826849521017
Epoch 14: loss 16.128201299027427
Epoch 15: loss 16.128132566023677
Epoch 16: loss 16.1280711665628
Epoch 17: loss 16.12800070356584
Epoch 18: loss 16.127934357081504
Epoch 19: loss 16.127867163232384
-----------Time: 0:03:44.065201, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 100, rmse: 4.017021179199219-------------


Epoch 0: loss 16.129138660275334
Epoch 1: loss 16.129070719407263
Epoch 2: loss 16.12900485339017
Epoch 3: loss 16.128934225483782
Epoch 4: loss 16.128861187203153
Epoch 5: loss 16.128791635875277
Epoch 6: loss 16.128722304945214
Epoch 7: loss 16.128665219317835
Epoch 8: loss 16.12859800394752
Epoch 9: loss 16.128523877938846
Epoch 10: loss 16.12845378287659
Epoch 11: loss 16.128390546853677
Epoch 12: loss 16.128312802950024
Epoch 13: loss 16.128254240916565
Epoch 14: loss 16.128188359082685
Epoch 15: loss 16.128119562811797
Epoch 16: loss 16.12804868235544
Epoch 17: loss 16.1279792311141
Epoch 18: loss 16.127911607100305
Epoch 19: loss 16.127848494500164
-----------Time: 0:04:36.046583, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 150, rmse: 4.017019748687744-------------


Epoch 0: loss 16.129142146709498
Epoch 1: loss 16.129074781727958
Epoch 2: loss 16.129010958668978
Epoch 3: loss 16.12894350500973
Epoch 4: loss 16.128885774005685
Epoch 5: loss 16.12880417288095
Epoch 6: loss 16.128733237195494
Epoch 7: loss 16.128666156916076
Epoch 8: loss 16.128605977421937
Epoch 9: loss 16.128533891000586
Epoch 10: loss 16.12846845981542
Epoch 11: loss 16.128400720676176
Epoch 12: loss 16.12833260271198
Epoch 13: loss 16.12826308612917
Epoch 14: loss 16.128189636612145
Epoch 15: loss 16.128125286672738
Epoch 16: loss 16.128062798705937
Epoch 17: loss 16.127995171580643
Epoch 18: loss 16.127920959227886
Epoch 19: loss 16.127852482404187
-----------Time: 0:05:06.623393, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 200, rmse: 4.0170207023620605-------------


Epoch 0: loss 16.12912702249275
Epoch 1: loss 16.129036521444895
Epoch 2: loss 16.12894420042966
Epoch 3: loss 16.12885316005539
Epoch 4: loss 16.128772657289655
Epoch 5: loss 16.12867427248226
Epoch 6: loss 16.128586464436435
Epoch 7: loss 16.12850461698473
Epoch 8: loss 16.12841223814748
Epoch 9: loss 16.128314974516744
Epoch 10: loss 16.128226508388966
Epoch 11: loss 16.128139742954456
Epoch 12: loss 16.128040047687573
Epoch 13: loss 16.127956090380536
Epoch 14: loss 16.12787259798325
Epoch 15: loss 16.127776815425836
Epoch 16: loss 16.127690633915897
Epoch 17: loss 16.127595538221797
Epoch 18: loss 16.127508671145
Epoch 19: loss 16.127417749007673
-----------Time: 0:03:37.317878, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 20, rmse: 4.01694917678833-------------


Epoch 0: loss 16.12914072371748
Epoch 1: loss 16.129051093889228
Epoch 2: loss 16.128959159218404
Epoch 3: loss 16.12887251746596
Epoch 4: loss 16.128780127479175
Epoch 5: loss 16.12869242055705
Epoch 6: loss 16.128608295747675
Epoch 7: loss 16.128516498242078
Epoch 8: loss 16.128422396412486
Epoch 9: loss 16.128339692002214
Epoch 10: loss 16.128240332777178
Epoch 11: loss 16.128155747725305
Epoch 12: loss 16.128065858346215
Epoch 13: loss 16.12797399757348
Epoch 14: loss 16.127885410097257
Epoch 15: loss 16.1277963081866
Epoch 16: loss 16.12770649789143
Epoch 17: loss 16.127614289927315
Epoch 18: loss 16.12752580772346
Epoch 19: loss 16.12743379059792
-----------Time: 0:03:36.084597, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 50, rmse: 4.01696252822876-------------


Epoch 0: loss 16.129126122751074
Epoch 1: loss 16.129033905971045
Epoch 2: loss 16.128945889454812
Epoch 3: loss 16.128859372421605
Epoch 4: loss 16.128771335939923
Epoch 5: loss 16.128673375592793
Epoch 6: loss 16.128583569446292
Epoch 7: loss 16.12849668499696
Epoch 8: loss 16.128403779797868
Epoch 9: loss 16.12831514824208
Epoch 10: loss 16.128227032417186
Epoch 11: loss 16.128131768442874
Epoch 12: loss 16.128052558764093
Epoch 13: loss 16.12795583445978
Epoch 14: loss 16.127867335661264
Epoch 15: loss 16.12778228232885
Epoch 16: loss 16.127687414292733
Epoch 17: loss 16.12760163909265
Epoch 18: loss 16.127512006412193
Epoch 19: loss 16.127420499572423
-----------Time: 0:04:35.398502, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 100, rmse: 4.016964912414551-------------


Epoch 0: loss 16.129123648072532
Epoch 1: loss 16.12904029284047
Epoch 2: loss 16.128946226533824
Epoch 3: loss 16.128851863597642
Epoch 4: loss 16.128760740250073
Epoch 5: loss 16.12867364473742
Epoch 6: loss 16.128583854666996
Epoch 7: loss 16.12849442993836
Epoch 8: loss 16.128396955503597
Epoch 9: loss 16.128319527417048
Epoch 10: loss 16.12822525186212
Epoch 11: loss 16.128134964729796
Epoch 12: loss 16.128044342074205
Epoch 13: loss 16.127957725473046
Epoch 14: loss 16.127861979475743
Epoch 15: loss 16.127774228474056
Epoch 16: loss 16.127685467272496
Epoch 17: loss 16.12760000348156
Epoch 18: loss 16.12750055765315
Epoch 19: loss 16.127414407776776
-----------Time: 0:05:29.836661, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 150, rmse: 4.016967296600342-------------


Epoch 0: loss 16.12913216372558
Epoch 1: loss 16.129045356026545
Epoch 2: loss 16.128957732855593
Epoch 3: loss 16.128869489718547
Epoch 4: loss 16.128778882879743
Epoch 5: loss 16.128680554857198
Epoch 6: loss 16.128594763062452
Epoch 7: loss 16.128508896591743
Epoch 8: loss 16.12841964299553
Epoch 9: loss 16.12833108559713
Epoch 10: loss 16.128237172791074
Epoch 11: loss 16.128147838036607
Epoch 12: loss 16.128056078128285
Epoch 13: loss 16.127962195918638
Epoch 14: loss 16.12787964967621
Epoch 15: loss 16.127790141455694
Epoch 16: loss 16.127697233663685
Epoch 17: loss 16.127609112393667
Epoch 18: loss 16.127513375212278
Epoch 19: loss 16.12742455878162
-----------Time: 0:06:01.508103, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 200, rmse: 4.016965866088867-------------


Epoch 0: loss 16.129314605926545
Epoch 1: loss 16.129198459907354
Epoch 2: loss 16.129075564788437
Epoch 3: loss 16.12896043960007
Epoch 4: loss 16.128839153644506
Epoch 5: loss 16.12871574798053
Epoch 6: loss 16.128597799107197
Epoch 7: loss 16.12848383943434
Epoch 8: loss 16.12836142711629
Epoch 9: loss 16.128246002446186
Epoch 10: loss 16.128129969478106
Epoch 11: loss 16.128004621201846
Epoch 12: loss 16.127896202070808
Epoch 13: loss 16.127774129683974
Epoch 14: loss 16.127654408552907
Epoch 15: loss 16.12753746287858
Epoch 16: loss 16.12741720605117
Epoch 17: loss 16.12729436616135
Epoch 18: loss 16.12717749075503
Epoch 19: loss 16.127063239379186
-----------Time: 0:03:32.803372, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 20, rmse: 4.016913414001465-------------


Epoch 0: loss 16.12912184781131
Epoch 1: loss 16.129018666110266
Epoch 2: loss 16.12889240460917
Epoch 3: loss 16.12876837223752
Epoch 4: loss 16.128653925615136
Epoch 5: loss 16.128537706216434
Epoch 6: loss 16.128416079033194
Epoch 7: loss 16.128297957212396
Epoch 8: loss 16.128165732524263
Epoch 9: loss 16.128057399996603
Epoch 10: loss 16.12794304231122
Epoch 11: loss 16.127816421691332
Epoch 12: loss 16.127700706873984
Epoch 13: loss 16.127582858087187
Epoch 14: loss 16.127460846115287
Epoch 15: loss 16.127349198545172
Epoch 16: loss 16.12722699054886
Epoch 17: loss 16.127109814623633
Epoch 18: loss 16.126988030050423
Epoch 19: loss 16.12687336043731
-----------Time: 0:02:45.043475, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 50, rmse: 4.01689338684082-------------


Epoch 0: loss 16.129114262755635
Epoch 1: loss 16.129001728667713
Epoch 2: loss 16.128883525688664
Epoch 3: loss 16.128759561510694
Epoch 4: loss 16.128644695354584
Epoch 5: loss 16.128527463422383
Epoch 6: loss 16.128403308405833
Epoch 7: loss 16.128287427382602
Epoch 8: loss 16.128167461739604
Epoch 9: loss 16.128045455731407
Epoch 10: loss 16.127927675916165
Epoch 11: loss 16.127814024022378
Epoch 12: loss 16.12769511162165
Epoch 13: loss 16.127574555312506
Epoch 14: loss 16.127452893124904
Epoch 15: loss 16.127334354363295
Epoch 16: loss 16.127218705665292
Epoch 17: loss 16.127100834320135
Epoch 18: loss 16.12698185320715
Epoch 19: loss 16.126861268894512
-----------Time: 0:03:19.483742, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 100, rmse: 4.01689338684082-------------


Epoch 0: loss 16.129111480816608
Epoch 1: loss 16.128997166951496
Epoch 2: loss 16.128874603725762
Epoch 3: loss 16.12875578103991
Epoch 4: loss 16.128641295005213
Epoch 5: loss 16.128521288912733
Epoch 6: loss 16.12839591159582
Epoch 7: loss 16.12828029349422
Epoch 8: loss 16.12816010019324
Epoch 9: loss 16.128041793756864
Epoch 10: loss 16.127927930540466
Epoch 11: loss 16.127806407333132
Epoch 12: loss 16.127696091484413
Epoch 13: loss 16.12757365764516
Epoch 14: loss 16.12746023392794
Epoch 15: loss 16.127335840363163
Epoch 16: loss 16.127224125117955
Epoch 17: loss 16.127103372525106
Epoch 18: loss 16.12697558561183
Epoch 19: loss 16.126855419795756
-----------Time: 0:04:17.974857, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 150, rmse: 4.016895771026611-------------


Epoch 0: loss 16.129129134940996
Epoch 1: loss 16.129003856154874
Epoch 2: loss 16.128883868731386
Epoch 3: loss 16.12876273005342
Epoch 4: loss 16.12863705869989
Epoch 5: loss 16.1285226245235
Epoch 6: loss 16.12840411350609
Epoch 7: loss 16.128283697473673
Epoch 8: loss 16.128166219214503
Epoch 9: loss 16.128046584168228
Epoch 10: loss 16.127933417149638
Epoch 11: loss 16.127818058080294
Epoch 12: loss 16.12768993305084
Epoch 13: loss 16.12757716145186
Epoch 14: loss 16.12745324239062
Epoch 15: loss 16.12733563422638
Epoch 16: loss 16.127225809735148
Epoch 17: loss 16.127104182551903
Epoch 18: loss 16.126982041712104
Epoch 19: loss 16.126867444959913
-----------Time: 0:04:46.901212, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 200, rmse: 4.016895294189453-------------


Epoch 0: loss 16.12942039402624
Epoch 1: loss 16.129260097657163
Epoch 2: loss 16.1291049184068
Epoch 3: loss 16.128949062664784
Epoch 4: loss 16.128783002244578
Epoch 5: loss 16.128632293439807
Epoch 6: loss 16.128472658523467
Epoch 7: loss 16.128321902268343
Epoch 8: loss 16.128152094566673
Epoch 9: loss 16.128006310226997
Epoch 10: loss 16.12784368204902
Epoch 11: loss 16.127689202626545
Epoch 12: loss 16.127526269262418
Epoch 13: loss 16.12737702623277
Epoch 14: loss 16.127218118110644
Epoch 15: loss 16.12706562097206
Epoch 16: loss 16.12690353471342
Epoch 17: loss 16.126751246045245
Epoch 18: loss 16.126598916668293
Epoch 19: loss 16.126436325309715
-----------Time: 0:03:08.271420, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 20, rmse: 4.016860485076904-------------


Epoch 0: loss 16.129159085189215
Epoch 1: loss 16.129003615013733
Epoch 2: loss 16.12883921172581
Epoch 3: loss 16.12868061553142
Epoch 4: loss 16.12852973170485
Epoch 5: loss 16.128368710615895
Epoch 6: loss 16.12821133749974
Epoch 7: loss 16.128049770083486
Epoch 8: loss 16.127902237340898
Epoch 9: loss 16.127744431726438
Epoch 10: loss 16.127589343487408
Epoch 11: loss 16.127432877113794
Epoch 12: loss 16.127269216436872
Epoch 13: loss 16.127119213423697
Epoch 14: loss 16.126958074875667
Epoch 15: loss 16.12680943443958
Epoch 16: loss 16.12664364990558
Epoch 17: loss 16.126490348963195
Epoch 18: loss 16.1263334648709
Epoch 19: loss 16.12617374257331
-----------Time: 0:03:17.132047, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 50, rmse: 4.016798496246338-------------


Epoch 0: loss 16.129089203523904
Epoch 1: loss 16.12893278511923
Epoch 2: loss 16.128767778719165
Epoch 3: loss 16.12861435202038
Epoch 4: loss 16.128463029472513
Epoch 5: loss 16.12830505454067
Epoch 6: loss 16.128151947548364
Epoch 7: loss 16.12799344832901
Epoch 8: loss 16.127840435978122
Epoch 9: loss 16.127681372021772
Epoch 10: loss 16.12752173425323
Epoch 11: loss 16.127365461570403
Epoch 12: loss 16.127206523370457
Epoch 13: loss 16.127051385866032
Epoch 14: loss 16.126895169190185
Epoch 15: loss 16.12673117869441
Epoch 16: loss 16.12658161881049
Epoch 17: loss 16.12641876893824
Epoch 18: loss 16.12626544854899
Epoch 19: loss 16.126109083558372
-----------Time: 0:04:04.804029, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 100, rmse: 4.0167999267578125-------------


Epoch 0: loss 16.129095768267334
Epoch 1: loss 16.12893743006803
Epoch 2: loss 16.128779723762236
Epoch 3: loss 16.12862530656973
Epoch 4: loss 16.12846159455308
Epoch 5: loss 16.128307405796434
Epoch 6: loss 16.128150382723867
Epoch 7: loss 16.127993330610646
Epoch 8: loss 16.127837782906994
Epoch 9: loss 16.127679402443167
Epoch 10: loss 16.127519603913864
Epoch 11: loss 16.12736287773012
Epoch 12: loss 16.127204450593815
Epoch 13: loss 16.12706233108089
Epoch 14: loss 16.12689422147957
Epoch 15: loss 16.126735659252372
Epoch 16: loss 16.126592238614453
Epoch 17: loss 16.1264360128634
Epoch 18: loss 16.126270054604063
Epoch 19: loss 16.126113056942067
-----------Time: 0:05:01.009658, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 150, rmse: 4.016800880432129-------------


Epoch 0: loss 16.1290917562492
Epoch 1: loss 16.128939035860594
Epoch 2: loss 16.128780236122335
Epoch 3: loss 16.12862753725493
Epoch 4: loss 16.128468680731817
Epoch 5: loss 16.128318215920395
Epoch 6: loss 16.128158856630975
Epoch 7: loss 16.127994332253902
Epoch 8: loss 16.127840448424116
Epoch 9: loss 16.127683445316997
Epoch 10: loss 16.1275316205214
Epoch 11: loss 16.12736731057844
Epoch 12: loss 16.12720727842745
Epoch 13: loss 16.127054410242657
Epoch 14: loss 16.126900590198595
Epoch 15: loss 16.126746230050234
Epoch 16: loss 16.126590971715945
Epoch 17: loss 16.12642319711931
Epoch 18: loss 16.12626882193204
Epoch 19: loss 16.126118328339253
-----------Time: 0:05:29.874802, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 200, rmse: 4.016798496246338-------------


Epoch 0: loss 16.129111367765493
Epoch 1: loss 16.12890128767834
Epoch 2: loss 16.128694522633637
Epoch 3: loss 16.128481624825223
Epoch 4: loss 16.128279350191566
Epoch 5: loss 16.12806808773495
Epoch 6: loss 16.127861040321754
Epoch 7: loss 16.12765439584762
Epoch 8: loss 16.127442611437118
Epoch 9: loss 16.127236460913387
Epoch 10: loss 16.127034644447896
Epoch 11: loss 16.12682022096801
Epoch 12: loss 16.126614314437624
Epoch 13: loss 16.12642017507631
Epoch 14: loss 16.126201196621786
Epoch 15: loss 16.125995716626
Epoch 16: loss 16.125786360480852
Epoch 17: loss 16.1255826620773
Epoch 18: loss 16.125376986316393
Epoch 19: loss 16.12516417174057
-----------Time: 0:04:02.181027, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 20, rmse: 4.016727924346924-------------


Epoch 0: loss 16.129053880495505
Epoch 1: loss 16.128852248386302
Epoch 2: loss 16.128651714895394
Epoch 3: loss 16.128442136018805
Epoch 4: loss 16.128233532856314
Epoch 5: loss 16.128019428564322
Epoch 6: loss 16.127813884523526
Epoch 7: loss 16.127609485773498
Epoch 8: loss 16.127403835682458
Epoch 9: loss 16.12718428730511
Epoch 10: loss 16.12698733981632
Epoch 11: loss 16.126781631384677
Epoch 12: loss 16.126576916558253
Epoch 13: loss 16.126361201547162
Epoch 14: loss 16.12614794358279
Epoch 15: loss 16.125944844661294
Epoch 16: loss 16.125736485751442
Epoch 17: loss 16.125536942754252
Epoch 18: loss 16.12532161097607
Epoch 19: loss 16.125114586639818
-----------Time: 0:04:02.049281, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 50, rmse: 4.0166754722595215-------------


Epoch 0: loss 16.129063472986353
Epoch 1: loss 16.128866724892763
Epoch 2: loss 16.12866080358276
Epoch 3: loss 16.12845265936631
Epoch 4: loss 16.128244199851338
Epoch 5: loss 16.128034747768318
Epoch 6: loss 16.127825526454775
Epoch 7: loss 16.127616588806184
Epoch 8: loss 16.127408725661777
Epoch 9: loss 16.127201438922476
Epoch 10: loss 16.12699181492916
Epoch 11: loss 16.126787013758648
Epoch 12: loss 16.126580945171046
Epoch 13: loss 16.12637297027198
Epoch 14: loss 16.126161937547675
Epoch 15: loss 16.125960780719883
Epoch 16: loss 16.125757078167666
Epoch 17: loss 16.12554382512983
Epoch 18: loss 16.125331340632144
Epoch 19: loss 16.125130331600534
-----------Time: 0:05:02.654160, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 100, rmse: 4.016674995422363-------------


Epoch 0: loss 16.12907840169727
Epoch 1: loss 16.128875766129777
Epoch 2: loss 16.128665940407632
Epoch 3: loss 16.128453692643134
Epoch 4: loss 16.128254913778132
Epoch 5: loss 16.128050790655024
Epoch 6: loss 16.127838518776407
Epoch 7: loss 16.127630977931393
Epoch 8: loss 16.12742008392807
Epoch 9: loss 16.12721059191415
Epoch 10: loss 16.12699760102007
Epoch 11: loss 16.12679230304695
Epoch 12: loss 16.126588708100726
Epoch 13: loss 16.12637889663962
Epoch 14: loss 16.126172249572573
Epoch 15: loss 16.125968889544488
Epoch 16: loss 16.12575702838369
Epoch 17: loss 16.125550482181055
Epoch 18: loss 16.12533582663523
Epoch 19: loss 16.1251429272061
-----------Time: 0:04:04.667495, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 150, rmse: 4.0166730880737305-------------


Epoch 0: loss 16.129066115685816
Epoch 1: loss 16.128867228177658
Epoch 2: loss 16.128649706682346
Epoch 3: loss 16.128443329797093
Epoch 4: loss 16.12823814435651
Epoch 5: loss 16.128030376890678
Epoch 6: loss 16.127819498185556
Epoch 7: loss 16.127613384221934
Epoch 8: loss 16.12740799653394
Epoch 9: loss 16.127204207119053
Epoch 10: loss 16.12699308597633
Epoch 11: loss 16.12678530995388
Epoch 12: loss 16.126583750446315
Epoch 13: loss 16.126370171479003
Epoch 14: loss 16.126158202193626
Epoch 15: loss 16.125961552630823
Epoch 16: loss 16.125741945394296
Epoch 17: loss 16.125537119072504
Epoch 18: loss 16.125338412031265
Epoch 19: loss 16.125125767550696
-----------Time: 0:04:16.448174, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 200, rmse: 4.016674995422363-------------


Epoch 0: loss 16.12905374229311
Epoch 1: loss 16.128772570686277
Epoch 2: loss 16.1285039827576
Epoch 3: loss 16.128231920840754
Epoch 4: loss 16.127952876980373
Epoch 5: loss 16.127681846525306
Epoch 6: loss 16.127406271467265
Epoch 7: loss 16.127129073503422
Epoch 8: loss 16.126851103887926
Epoch 9: loss 16.12658092702064
Epoch 10: loss 16.12631072837286
Epoch 11: loss 16.126033911567593
Epoch 12: loss 16.125759862440315
Epoch 13: loss 16.12548600078083
Epoch 14: loss 16.12521006297391
Epoch 15: loss 16.124937336752115
Epoch 16: loss 16.124664284600843
Epoch 17: loss 16.124383272458317
Epoch 18: loss 16.124109678387708
Epoch 19: loss 16.123837583281542
-----------Time: 0:02:32.830723, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 20, rmse: 4.016510963439941-------------


Epoch 0: loss 16.129052860442552
Epoch 1: loss 16.128776545625723
Epoch 2: loss 16.12850054740387
Epoch 3: loss 16.128229035185107
Epoch 4: loss 16.127953168424074
Epoch 5: loss 16.127677953521992
Epoch 6: loss 16.127401128160106
Epoch 7: loss 16.127134085090475
Epoch 8: loss 16.126860390155453
Epoch 9: loss 16.12658550403506
Epoch 10: loss 16.126302600619972
Epoch 11: loss 16.12603407543985
Epoch 12: loss 16.125759912742875
Epoch 13: loss 16.125491936742254
Epoch 14: loss 16.12520704767249
Epoch 15: loss 16.12493926251045
Epoch 16: loss 16.124663595663197
Epoch 17: loss 16.124388736768413
Epoch 18: loss 16.124114364045283
Epoch 19: loss 16.12384468294344
-----------Time: 0:03:05.162252, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 50, rmse: 4.016506195068359-------------


Epoch 0: loss 16.129058580154826
Epoch 1: loss 16.12877979351166
Epoch 2: loss 16.128505258990604
Epoch 3: loss 16.12822769620355
Epoch 4: loss 16.127956823138454
Epoch 5: loss 16.127679454042188
Epoch 6: loss 16.127403320988734
Epoch 7: loss 16.127129170737753
Epoch 8: loss 16.1268590817703
Epoch 9: loss 16.12658435770712
Epoch 10: loss 16.12631189881491
Epoch 11: loss 16.12603601079197
Epoch 12: loss 16.125762598225446
Epoch 13: loss 16.125486771913895
Epoch 14: loss 16.125211788818458
Epoch 15: loss 16.12494091341974
Epoch 16: loss 16.124661104389997
Epoch 17: loss 16.124389769267363
Epoch 18: loss 16.12411702800666
Epoch 19: loss 16.123839242228872
-----------Time: 0:03:43.754153, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 100, rmse: 4.016509056091309-------------


Epoch 0: loss 16.1290322628405
Epoch 1: loss 16.128761815791417
Epoch 2: loss 16.128485683256546
Epoch 3: loss 16.12821616239688
Epoch 4: loss 16.127941023985805
Epoch 5: loss 16.127659541488406
Epoch 6: loss 16.127389302391148
Epoch 7: loss 16.127115075130575
Epoch 8: loss 16.126841259884277
Epoch 9: loss 16.126569605314454
Epoch 10: loss 16.12629493440604
Epoch 11: loss 16.126015209905347
Epoch 12: loss 16.1257420281083
Epoch 13: loss 16.125463410263933
Epoch 14: loss 16.125193506430644
Epoch 15: loss 16.12491830760464
Epoch 16: loss 16.124647731169077
Epoch 17: loss 16.12436993501963
Epoch 18: loss 16.124096988140728
Epoch 19: loss 16.12382028568303
-----------Time: 0:04:36.236375, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 150, rmse: 4.016509056091309-------------


Epoch 0: loss 16.129033033973567
Epoch 1: loss 16.128762759353364
Epoch 2: loss 16.128487677208557
Epoch 3: loss 16.128210397827164
Epoch 4: loss 16.127937861925368
Epoch 5: loss 16.12766328747341
Epoch 6: loss 16.127388025898853
Epoch 7: loss 16.127113837013432
Epoch 8: loss 16.126847219959817
Epoch 9: loss 16.126569611796743
Epoch 10: loss 16.126292488768154
Epoch 11: loss 16.126026075776988
Epoch 12: loss 16.125744428629456
Epoch 13: loss 16.125465169816383
Epoch 14: loss 16.12519878923666
Epoch 15: loss 16.124919327398302
Epoch 16: loss 16.124647324081348
Epoch 17: loss 16.124373986190786
Epoch 18: loss 16.12409926083115
Epoch 19: loss 16.123822139358314
-----------Time: 0:05:01.854149, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 200, rmse: 4.016510009765625-------------


Epoch 0: loss 16.12914758794265
Epoch 1: loss 16.12878563431309
Epoch 2: loss 16.128426956054373
Epoch 3: loss 16.128068984105834
Epoch 4: loss 16.127704885875875
Epoch 5: loss 16.12734133630683
Epoch 6: loss 16.12697530609554
Epoch 7: loss 16.12661106032869
Epoch 8: loss 16.126257333242094
Epoch 9: loss 16.125887552897137
Epoch 10: loss 16.125529838425106
Epoch 11: loss 16.12516391996847
Epoch 12: loss 16.124799232628114
Epoch 13: loss 16.124432917714703
Epoch 14: loss 16.12407697031458
Epoch 15: loss 16.12371205531624
Epoch 16: loss 16.12335767248132
Epoch 17: loss 16.1229923057971
Epoch 18: loss 16.12262543703694
Epoch 19: loss 16.122260293084167
-----------Time: 0:03:09.802326, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 20, rmse: 4.016328811645508-------------


Epoch 0: loss 16.12895402394928
Epoch 1: loss 16.1285925863099
Epoch 2: loss 16.128235326375957
Epoch 3: loss 16.1278724460384
Epoch 4: loss 16.127514737270786
Epoch 5: loss 16.12715469487923
Epoch 6: loss 16.12678797028517
Epoch 7: loss 16.126419055714692
Epoch 8: loss 16.12604497735302
Epoch 9: loss 16.125690290369114
Epoch 10: loss 16.125335207447012
Epoch 11: loss 16.12496872969851
Epoch 12: loss 16.124610657663432
Epoch 13: loss 16.12423896322826
Epoch 14: loss 16.12388127364822
Epoch 15: loss 16.123519993139517
Epoch 16: loss 16.12315452040506
Epoch 17: loss 16.1227970185521
Epoch 18: loss 16.122438410820685
Epoch 19: loss 16.12206414473193
-----------Time: 0:03:36.177360, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 50, rmse: 4.016289710998535-------------


Epoch 0: loss 16.129003416137113
Epoch 1: loss 16.12865300175935
Epoch 2: loss 16.128277974909192
Epoch 3: loss 16.12791699025215
Epoch 4: loss 16.1275512564111
Epoch 5: loss 16.12718917624726
Epoch 6: loss 16.126829685368833
Epoch 7: loss 16.126470511085383
Epoch 8: loss 16.126106091593194
Epoch 9: loss 16.125741494745583
Epoch 10: loss 16.125382304126767
Epoch 11: loss 16.12501373182113
Epoch 12: loss 16.124653448029147
Epoch 13: loss 16.124285979009052
Epoch 14: loss 16.123929207580385
Epoch 15: loss 16.123567783424164
Epoch 16: loss 16.123206238438087
Epoch 17: loss 16.122846784379057
Epoch 18: loss 16.122486508106523
Epoch 19: loss 16.122106873904837
-----------Time: 0:04:29.763412, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 100, rmse: 4.016290187835693-------------


Epoch 0: loss 16.128991911111804
Epoch 1: loss 16.128631810898234
Epoch 2: loss 16.128262283362535
Epoch 3: loss 16.127901167503968
Epoch 4: loss 16.127537910934375
Epoch 5: loss 16.127175485912776
Epoch 6: loss 16.12681722148337
Epoch 7: loss 16.12645700873727
Epoch 8: loss 16.126081910063352
Epoch 9: loss 16.125734234322927
Epoch 10: loss 16.125367453721893
Epoch 11: loss 16.1250063754606
Epoch 12: loss 16.124641889071192
Epoch 13: loss 16.124279659814714
Epoch 14: loss 16.123914828048964
Epoch 15: loss 16.123553647886094
Epoch 16: loss 16.123189571695914
Epoch 17: loss 16.12282595159957
Epoch 18: loss 16.122465731852596
Epoch 19: loss 16.12209981495171
-----------Time: 0:05:16.635315, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 150, rmse: 4.016289234161377-------------


Epoch 0: loss 16.1289925310779
Epoch 1: loss 16.12863881825234
Epoch 2: loss 16.128271330822546
Epoch 3: loss 16.127897382365937
Epoch 4: loss 16.12754162009998
Epoch 5: loss 16.1271750464136
Epoch 6: loss 16.126822899449706
Epoch 7: loss 16.126455722132604
Epoch 8: loss 16.12608813409769
Epoch 9: loss 16.1257313388142
Epoch 10: loss 16.12537448181932
Epoch 11: loss 16.125000680640316
Epoch 12: loss 16.124645597458922
Epoch 13: loss 16.12427658876561
Epoch 14: loss 16.123912023551572
Epoch 15: loss 16.123553980816443
Epoch 16: loss 16.123199556754166
Epoch 17: loss 16.12283595999406
Epoch 18: loss 16.12247097472771
Epoch 19: loss 16.12210790795953
-----------Time: 0:05:56.313093, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 200, rmse: 4.016290187835693-------------


Epoch 0: loss 16.128991099269964
Epoch 1: loss 16.12850682044431
Epoch 2: loss 16.128024615173175
Epoch 3: loss 16.127556586148884
Epoch 4: loss 16.12707685763062
Epoch 5: loss 16.126584674820684
Epoch 6: loss 16.126117902063946
Epoch 7: loss 16.125632724274496
Epoch 8: loss 16.1251539815827
Epoch 9: loss 16.12467042643976
Epoch 10: loss 16.124191258769116
Epoch 11: loss 16.123723673392664
Epoch 12: loss 16.123233913921545
Epoch 13: loss 16.12275886613432
Epoch 14: loss 16.12228142690114
Epoch 15: loss 16.1217987339026
Epoch 16: loss 16.121319246525477
Epoch 17: loss 16.12083480434615
Epoch 18: loss 16.120359577647754
Epoch 19: loss 16.119885674114133
-----------Time: 0:03:50.678834, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 20, rmse: 4.016044616699219-------------


Epoch 0: loss 16.12894779109903
Epoch 1: loss 16.128465180036617
Epoch 2: loss 16.1279882881679
Epoch 3: loss 16.127512609265043
Epoch 4: loss 16.127034829582062
Epoch 5: loss 16.1265439484157
Epoch 6: loss 16.126072338834412
Epoch 7: loss 16.125586031830267
Epoch 8: loss 16.125110163125996
Epoch 9: loss 16.124632430634076
Epoch 10: loss 16.12415295077641
Epoch 11: loss 16.123676229002943
Epoch 12: loss 16.123197500831477
Epoch 13: loss 16.12271260229902
Epoch 14: loss 16.12223425398967
Epoch 15: loss 16.121754774909878
Epoch 16: loss 16.121281915543324
Epoch 17: loss 16.12080034657365
Epoch 18: loss 16.12031830854556
Epoch 19: loss 16.119838378557763
-----------Time: 0:02:39.146054, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 50, rmse: 4.016006946563721-------------


Epoch 0: loss 16.12893821312851
Epoch 1: loss 16.128448241297612
Epoch 2: loss 16.127973376829512
Epoch 3: loss 16.127494429815975
Epoch 4: loss 16.12701199221961
Epoch 5: loss 16.126539098626573
Epoch 6: loss 16.126056796639688
Epoch 7: loss 16.125578606757475
Epoch 8: loss 16.125091088861794
Epoch 9: loss 16.124622539439365
Epoch 10: loss 16.12413801610178
Epoch 11: loss 16.123661680413434
Epoch 12: loss 16.12318247592339
Epoch 13: loss 16.122708519753584
Epoch 14: loss 16.122218419054786
Epoch 15: loss 16.121741426062364
Epoch 16: loss 16.12126790368732
Epoch 17: loss 16.120784443704377
Epoch 18: loss 16.120301109218545
Epoch 19: loss 16.119823729881713
-----------Time: 0:03:20.467946, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 100, rmse: 4.0159993171691895-------------


Epoch 0: loss 16.128950696460834
Epoch 1: loss 16.128471676586372
Epoch 2: loss 16.12799071280317
Epoch 3: loss 16.127515698723847
Epoch 4: loss 16.127029817476423
Epoch 5: loss 16.126547735887353
Epoch 6: loss 16.126076467793037
Epoch 7: loss 16.125593626998313
Epoch 8: loss 16.125116948785827
Epoch 9: loss 16.124643829349854
Epoch 10: loss 16.124161637302585
Epoch 11: loss 16.123677612582647
Epoch 12: loss 16.12319767222319
Epoch 13: loss 16.12271538968317
Epoch 14: loss 16.12223757836662
Epoch 15: loss 16.12175870265826
Epoch 16: loss 16.12127710801872
Epoch 17: loss 16.120795693586803
Epoch 18: loss 16.120320589274023
Epoch 19: loss 16.119844002850748
-----------Time: 0:04:05.628525, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 150, rmse: 4.016001224517822-------------


Epoch 0: loss 16.128941105266446
Epoch 1: loss 16.12845336074995
Epoch 2: loss 16.127978689194762
Epoch 3: loss 16.127499988248907
Epoch 4: loss 16.127027595866434
Epoch 5: loss 16.12653635584057
Epoch 6: loss 16.12605931021196
Epoch 7: loss 16.12557843017993
Epoch 8: loss 16.12510720875809
Epoch 9: loss 16.124619413939037
Epoch 10: loss 16.12414649545401
Epoch 11: loss 16.123670177138195
Epoch 12: loss 16.12318927973363
Epoch 13: loss 16.122700546797752
Epoch 14: loss 16.12223108648411
Epoch 15: loss 16.121751880179026
Epoch 16: loss 16.12126959608326
Epoch 17: loss 16.120791377419685
Epoch 18: loss 16.120304515531622
Epoch 19: loss 16.119837048392117
-----------Time: 0:04:41.441728, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 200, rmse: 4.016001224517822-------------


Epoch 0: loss 16.128995063319163
Epoch 1: loss 16.128360087356857
Epoch 2: loss 16.127727931190147
Epoch 3: loss 16.127095644599788
Epoch 4: loss 16.126464496040033
Epoch 5: loss 16.125824667954976
Epoch 6: loss 16.125194100208294
Epoch 7: loss 16.124558442827798
Epoch 8: loss 16.123923398931108
Epoch 9: loss 16.123293167226272
Epoch 10: loss 16.12265923776466
Epoch 11: loss 16.1220254293922
Epoch 12: loss 16.12139238022539
Epoch 13: loss 16.120756636760103
Epoch 14: loss 16.12012951940606
Epoch 15: loss 16.119493664186113
Epoch 16: loss 16.118858811905877
Epoch 17: loss 16.118219825221896
Epoch 18: loss 16.11759251962219
Epoch 19: loss 16.11696008445777
-----------Time: 0:02:48.614490, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 20, rmse: 4.015665054321289-------------


Epoch 0: loss 16.128853541473255
Epoch 1: loss 16.1282213007775
Epoch 2: loss 16.12758457148574
Epoch 3: loss 16.126948170197796
Epoch 4: loss 16.12631611022825
Epoch 5: loss 16.12568290367147
Epoch 6: loss 16.125045824595414
Epoch 7: loss 16.124417529279864
Epoch 8: loss 16.123784929465312
Epoch 9: loss 16.123148834141393
Epoch 10: loss 16.122513356968525
Epoch 11: loss 16.12188513114311
Epoch 12: loss 16.12125658509263
Epoch 13: loss 16.12061922701887
Epoch 14: loss 16.119981429705224
Epoch 15: loss 16.119349898690437
Epoch 16: loss 16.118716690059326
Epoch 17: loss 16.11808489430787
Epoch 18: loss 16.11744466561737
Epoch 19: loss 16.11681676572143
-----------Time: 0:03:26.453596, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 50, rmse: 4.0156168937683105-------------


Epoch 0: loss 16.12885379635685
Epoch 1: loss 16.128230441323176
Epoch 2: loss 16.12759472845429
Epoch 3: loss 16.12696338931525
Epoch 4: loss 16.1263217998627
Epoch 5: loss 16.125689637213704
Epoch 6: loss 16.12506119280551
Epoch 7: loss 16.12442065607665
Epoch 8: loss 16.123796896288873
Epoch 9: loss 16.123149094210817
Epoch 10: loss 16.122524320333792
Epoch 11: loss 16.12189214731313
Epoch 12: loss 16.12126036322979
Epoch 13: loss 16.120623516738252
Epoch 14: loss 16.119992877686393
Epoch 15: loss 16.119356969052387
Epoch 16: loss 16.118718265255488
Epoch 17: loss 16.118087282642325
Epoch 18: loss 16.117462613778375
Epoch 19: loss 16.116827002811068
-----------Time: 0:04:01.586013, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 100, rmse: 4.015622615814209-------------


Epoch 0: loss 16.128859296708473
Epoch 1: loss 16.12822373500656
Epoch 2: loss 16.127593882645257
Epoch 3: loss 16.12695582370644
Epoch 4: loss 16.126323446623328
Epoch 5: loss 16.125693714832597
Epoch 6: loss 16.12505612676723
Epoch 7: loss 16.12442486437849
Epoch 8: loss 16.123796204758314
Epoch 9: loss 16.1231547615462
Epoch 10: loss 16.122523343323234
Epoch 11: loss 16.12189919070876
Epoch 12: loss 16.121263215436826
Epoch 13: loss 16.12062340550218
Epoch 14: loss 16.11998703921859
Epoch 15: loss 16.119359404059324
Epoch 16: loss 16.118719910460364
Epoch 17: loss 16.118094705900074
Epoch 18: loss 16.11745255715566
Epoch 19: loss 16.11682446175389
-----------Time: 0:04:49.797155, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 150, rmse: 4.015620708465576-------------


Epoch 0: loss 16.128862761362146
Epoch 1: loss 16.12823364279593
Epoch 2: loss 16.12759170459625
Epoch 3: loss 16.12696551550595
Epoch 4: loss 16.126327830465545
Epoch 5: loss 16.125695846727712
Epoch 6: loss 16.125063130750547
Epoch 7: loss 16.12442680673148
Epoch 8: loss 16.123790974847775
Epoch 9: loss 16.123163609351717
Epoch 10: loss 16.122530091904377
Epoch 11: loss 16.121897702893854
Epoch 12: loss 16.121261107915117
Epoch 13: loss 16.12062983774763
Epoch 14: loss 16.11999850560946
Epoch 15: loss 16.119363514089663
Epoch 16: loss 16.118729011593725
Epoch 17: loss 16.11809754125316
Epoch 18: loss 16.117454553182
Epoch 19: loss 16.116825021564345
-----------Time: 0:05:27.582973, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 200, rmse: 4.01561975479126-------------


Epoch 0: loss 16.12888722500119
Epoch 1: loss 16.128054639578774
Epoch 2: loss 16.127212493039785
Epoch 3: loss 16.126379458783695
Epoch 4: loss 16.125538684934167
Epoch 5: loss 16.12470334427975
Epoch 6: loss 16.123866360754082
Epoch 7: loss 16.123022153365866
Epoch 8: loss 16.122188710725585
Epoch 9: loss 16.121356333773573
Epoch 10: loss 16.120516736589092
Epoch 11: loss 16.119676787805272
Epoch 12: loss 16.11883730263474
Epoch 13: loss 16.11800185411504
Epoch 14: loss 16.117162911901012
Epoch 15: loss 16.116329213080682
Epoch 16: loss 16.115490716329536
Epoch 17: loss 16.114651276535024
Epoch 18: loss 16.1138171511801
Epoch 19: loss 16.11298147007588
-----------Time: 0:03:29.820103, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 20, rmse: 4.015106201171875-------------


Epoch 0: loss 16.128794098626408
Epoch 1: loss 16.127960650800297
Epoch 2: loss 16.12711182465172
Epoch 3: loss 16.126283737937673
Epoch 4: loss 16.125442514995182
Epoch 5: loss 16.12460396586714
Epoch 6: loss 16.123768375255676
Epoch 7: loss 16.122928466921334
Epoch 8: loss 16.12209704545868
Epoch 9: loss 16.121252043341865
Epoch 10: loss 16.120422939686364
Epoch 11: loss 16.11958335079921
Epoch 12: loss 16.118746393980572
Epoch 13: loss 16.11790785159411
Epoch 14: loss 16.117064044033462
Epoch 15: loss 16.11622976388148
Epoch 16: loss 16.115393312681363
Epoch 17: loss 16.11456240280093
Epoch 18: loss 16.11372300267803
Epoch 19: loss 16.112883341448537
-----------Time: 0:04:14.318530, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 50, rmse: 4.015112400054932-------------


Epoch 0: loss 16.12876601320301
Epoch 1: loss 16.127930247310452
Epoch 2: loss 16.1270829146812
Epoch 3: loss 16.12625343297862
Epoch 4: loss 16.12541304080625
Epoch 5: loss 16.12457775823314
Epoch 6: loss 16.12374471671689
Epoch 7: loss 16.12290194917464
Epoch 8: loss 16.122064790367887
Epoch 9: loss 16.121229123006117
Epoch 10: loss 16.120392855903
Epoch 11: loss 16.11955460547882
Epoch 12: loss 16.11871605764724
Epoch 13: loss 16.11787831206371
Epoch 14: loss 16.117040907448565
Epoch 15: loss 16.116209342856976
Epoch 16: loss 16.11536999796317
Epoch 17: loss 16.11453633907374
Epoch 18: loss 16.113694471013876
Epoch 19: loss 16.112856750322333
-----------Time: 0:04:58.137937, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 100, rmse: 4.015109539031982-------------


Epoch 0: loss 16.128755798153154
Epoch 1: loss 16.127921541596702
Epoch 2: loss 16.127091339063618
Epoch 3: loss 16.12624335613116
Epoch 4: loss 16.125413580651255
Epoch 5: loss 16.124566322179383
Epoch 6: loss 16.12372849621547
Epoch 7: loss 16.122893207160075
Epoch 8: loss 16.12205755665226
Epoch 9: loss 16.1212165841854
Epoch 10: loss 16.12038018614005
Epoch 11: loss 16.11954261091107
Epoch 12: loss 16.118707210101014
Epoch 13: loss 16.117872508340973
Epoch 14: loss 16.117036973217896
Epoch 15: loss 16.116198731609632
Epoch 16: loss 16.115366905392868
Epoch 17: loss 16.114523978904902
Epoch 18: loss 16.11368437834963
Epoch 19: loss 16.11284931098851
-----------Time: 0:04:39.270008, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 150, rmse: 4.015113353729248-------------


Epoch 0: loss 16.12875914897784
Epoch 1: loss 16.127914237613066
Epoch 2: loss 16.127082119434018
Epoch 3: loss 16.12624612017907
Epoch 4: loss 16.125411683933574
Epoch 5: loss 16.124568403771935
Epoch 6: loss 16.12372872024337
Epoch 7: loss 16.122891686933723
Epoch 8: loss 16.122053625533088
Epoch 9: loss 16.121212168191036
Epoch 10: loss 16.120377498323858
Epoch 11: loss 16.119544314975126
Epoch 12: loss 16.118697771888638
Epoch 13: loss 16.117866608939657
Epoch 14: loss 16.117030198707603
Epoch 15: loss 16.116197511642895
Epoch 16: loss 16.115355938656812
Epoch 17: loss 16.11452309160922
Epoch 18: loss 16.113685237641825
Epoch 19: loss 16.11283867173768
-----------Time: 0:04:17.731906, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 200, rmse: 4.015113830566406-------------


Epoch 0: loss 16.128862908899038
Epoch 1: loss 16.127749718421825
Epoch 2: loss 16.126641012132655
Epoch 3: loss 16.125536166435353
Epoch 4: loss 16.124430192560524
Epoch 5: loss 16.123327235024284
Epoch 6: loss 16.12221362475405
Epoch 7: loss 16.121104811896053
Epoch 8: loss 16.119994049165612
Epoch 9: loss 16.11889483320642
Epoch 10: loss 16.117786751032007
Epoch 11: loss 16.116681342931336
Epoch 12: loss 16.115577396716418
Epoch 13: loss 16.11446670555044
Epoch 14: loss 16.113360823983406
Epoch 15: loss 16.112253703262056
Epoch 16: loss 16.111149738378145
Epoch 17: loss 16.110044819042045
Epoch 18: loss 16.108936807394937
Epoch 19: loss 16.107826225909285
-----------Time: 0:02:24.920563, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 20, rmse: 4.014465808868408-------------


Epoch 0: loss 16.128617282014883
Epoch 1: loss 16.127507461290634
Epoch 2: loss 16.12640132561788
Epoch 3: loss 16.125292817427454
Epoch 4: loss 16.124192648571817
Epoch 5: loss 16.123081466048355
Epoch 6: loss 16.12196314056131
Epoch 7: loss 16.120870355291817
Epoch 8: loss 16.1197596991302
Epoch 9: loss 16.118654114192694
Epoch 10: loss 16.117553589848345
Epoch 11: loss 16.11643916177257
Epoch 12: loss 16.11533274399061
Epoch 13: loss 16.114225095092376
Epoch 14: loss 16.113123071524292
Epoch 15: loss 16.11200932901537
Epoch 16: loss 16.110904704753054
Epoch 17: loss 16.10980277712284
Epoch 18: loss 16.10869285708993
Epoch 19: loss 16.107588739742592
-----------Time: 0:02:57.862685, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 50, rmse: 4.01444673538208-------------


Epoch 0: loss 16.12862462463296
Epoch 1: loss 16.127528518098018
Epoch 2: loss 16.126413907999574
Epoch 3: loss 16.12531428987919
Epoch 4: loss 16.124209731217633
Epoch 5: loss 16.1230940562088
Epoch 6: loss 16.12199579236817
Epoch 7: loss 16.12087682306021
Epoch 8: loss 16.119778447464927
Epoch 9: loss 16.118668977821436
Epoch 10: loss 16.11755367878535
Epoch 11: loss 16.11645625504934
Epoch 12: loss 16.115348214361575
Epoch 13: loss 16.114237313428738
Epoch 14: loss 16.11313539305869
Epoch 15: loss 16.112024678556473
Epoch 16: loss 16.11091120000635
Epoch 17: loss 16.10980867626487
Epoch 18: loss 16.10870929539625
Epoch 19: loss 16.10760044701531
-----------Time: 0:03:41.591685, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 100, rmse: 4.014449119567871-------------


Epoch 0: loss 16.12863235774411
Epoch 1: loss 16.127521015756347
Epoch 2: loss 16.12641447973744
Epoch 3: loss 16.125304743023655
Epoch 4: loss 16.124200025157087
Epoch 5: loss 16.123086120331656
Epoch 6: loss 16.12198863099489
Epoch 7: loss 16.120878719777888
Epoch 8: loss 16.119770087905394
Epoch 9: loss 16.118664184557865
Epoch 10: loss 16.117556862626277
Epoch 11: loss 16.116451762823257
Epoch 12: loss 16.115346445215337
Epoch 13: loss 16.114228984206314
Epoch 14: loss 16.113126939117034
Epoch 15: loss 16.112020121507506
Epoch 16: loss 16.110916899493883
Epoch 17: loss 16.109810892689026
Epoch 18: loss 16.108701277064394
Epoch 19: loss 16.10759577613735
-----------Time: 0:04:26.276290, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 150, rmse: 4.014443874359131-------------


Epoch 0: loss 16.128613753575486
Epoch 1: loss 16.127505995256218
Epoch 2: loss 16.126407148787482
Epoch 3: loss 16.125302995398616
Epoch 4: loss 16.12418601770904
Epoch 5: loss 16.123090683862916
Epoch 6: loss 16.121981857003174
Epoch 7: loss 16.12087398874429
Epoch 8: loss 16.119767212362117
Epoch 9: loss 16.11865829552821
Epoch 10: loss 16.11755286746209
Epoch 11: loss 16.1164371359277
Epoch 12: loss 16.11533684079708
Epoch 13: loss 16.114229248165113
Epoch 14: loss 16.113127965911566
Epoch 15: loss 16.112020047868704
Epoch 16: loss 16.110909623773026
Epoch 17: loss 16.109798458622098
Epoch 18: loss 16.108693513616135
Epoch 19: loss 16.107587322714277
-----------Time: 0:04:59.228173, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 200, rmse: 4.014444828033447-------------


Epoch 0: loss 16.12850993609156
Epoch 1: loss 16.127040614117224
Epoch 2: loss 16.125584521975576
Epoch 3: loss 16.12412015869282
Epoch 4: loss 16.122650397478594
Epoch 5: loss 16.12118690074819
Epoch 6: loss 16.119725423380366
Epoch 7: loss 16.118262332700528
Epoch 8: loss 16.116796301135945
Epoch 9: loss 16.115330066805367
Epoch 10: loss 16.113864073615932
Epoch 11: loss 16.11240199028376
Epoch 12: loss 16.110942544205933
Epoch 13: loss 16.109475208404856
Epoch 14: loss 16.10801176301418
Epoch 15: loss 16.10655423932389
Epoch 16: loss 16.105080874735016
Epoch 17: loss 16.103625436526656
Epoch 18: loss 16.102160294332084
Epoch 19: loss 16.10069217884132
-----------Time: 0:03:12.483553, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 20, rmse: 4.013574123382568-------------


Epoch 0: loss 16.128428950784258
Epoch 1: loss 16.1269668394486
Epoch 2: loss 16.125501698809774
Epoch 3: loss 16.12403469153109
Epoch 4: loss 16.122573598951927
Epoch 5: loss 16.12110662797406
Epoch 6: loss 16.119650138597763
Epoch 7: loss 16.11818202077338
Epoch 8: loss 16.116717219806482
Epoch 9: loss 16.1152531119259
Epoch 10: loss 16.11379227837899
Epoch 11: loss 16.112332813891463
Epoch 12: loss 16.11086982485391
Epoch 13: loss 16.109405146013337
Epoch 14: loss 16.107942971929123
Epoch 15: loss 16.106476110372288
Epoch 16: loss 16.105015671467118
Epoch 17: loss 16.103550922617828
Epoch 18: loss 16.102091638597216
Epoch 19: loss 16.10062548283198
-----------Time: 0:03:39.348801, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 50, rmse: 4.01356840133667-------------


Epoch 0: loss 16.128458020478373
Epoch 1: loss 16.1269948019678
Epoch 2: loss 16.125535006364963
Epoch 3: loss 16.12406497704328
Epoch 4: loss 16.122608908756458
Epoch 5: loss 16.12114673726516
Epoch 6: loss 16.11969002800963
Epoch 7: loss 16.118224974752057
Epoch 8: loss 16.116757864534627
Epoch 9: loss 16.115290115163532
Epoch 10: loss 16.11382248843734
Epoch 11: loss 16.112364626371583
Epoch 12: loss 16.110903859721894
Epoch 13: loss 16.10943738788026
Epoch 14: loss 16.10796571516874
Epoch 15: loss 16.106511446365264
Epoch 16: loss 16.10504894194362
Epoch 17: loss 16.103588183072677
Epoch 18: loss 16.10212294934819
Epoch 19: loss 16.100661988489133
-----------Time: 0:04:29.794829, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 100, rmse: 4.013562202453613-------------


Epoch 0: loss 16.128443682952184
Epoch 1: loss 16.12697770687599
Epoch 2: loss 16.125520975321386
Epoch 3: loss 16.1240526212824
Epoch 4: loss 16.122591242963946
Epoch 5: loss 16.1211326595491
Epoch 6: loss 16.11966212175669
Epoch 7: loss 16.118193764606204
Epoch 8: loss 16.116735446705903
Epoch 9: loss 16.11527110909301
Epoch 10: loss 16.11380654563717
Epoch 11: loss 16.11234287181064
Epoch 12: loss 16.110880421580926
Epoch 13: loss 16.109417769103803
Epoch 14: loss 16.10795983677004
Epoch 15: loss 16.106499365712715
Epoch 16: loss 16.10502906361632
Epoch 17: loss 16.103568131020044
Epoch 18: loss 16.102106711214947
Epoch 19: loss 16.100641869539277
-----------Time: 0:05:11.636369, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 150, rmse: 4.013558864593506-------------


Epoch 0: loss 16.128451264118485
Epoch 1: loss 16.126988664536842
Epoch 2: loss 16.125529113964518
Epoch 3: loss 16.124062488622286
Epoch 4: loss 16.122599412203485
Epoch 5: loss 16.121135571911775
Epoch 6: loss 16.1196742096694
Epoch 7: loss 16.11820815839866
Epoch 8: loss 16.116746114997387
Epoch 9: loss 16.115279663899077
Epoch 10: loss 16.11381543155855
Epoch 11: loss 16.112351111058892
Epoch 12: loss 16.110890632222826
Epoch 13: loss 16.10943638675559
Epoch 14: loss 16.107962594854243
Epoch 15: loss 16.106505812478492
Epoch 16: loss 16.105043038912218
Epoch 17: loss 16.103582574337185
Epoch 18: loss 16.10211431908828
Epoch 19: loss 16.100655124782545
-----------Time: 0:05:56.147842, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 200, rmse: 4.013559818267822-------------


Epoch 0: loss 16.128315488173303
Epoch 1: loss 16.126382929660366
Epoch 2: loss 16.124445360338253
Epoch 3: loss 16.12251201980201
Epoch 4: loss 16.120573148577027
Epoch 5: loss 16.118643610292054
Epoch 6: loss 16.116706368195874
Epoch 7: loss 16.11476988167527
Epoch 8: loss 16.112838480121226
Epoch 9: loss 16.110906198013083
Epoch 10: loss 16.108963210275473
Epoch 11: loss 16.10702373308614
Epoch 12: loss 16.10510205366872
Epoch 13: loss 16.103165309671606
Epoch 14: loss 16.101234837677765
Epoch 15: loss 16.09930111857586
Epoch 16: loss 16.0973694598046
Epoch 17: loss 16.0954284291996
Epoch 18: loss 16.093497934128813
Epoch 19: loss 16.09157184701435
-----------Time: 0:03:52.698306, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.0124359130859375-------------


Epoch 0: loss 16.128255893122315
Epoch 1: loss 16.12632087004321
Epoch 2: loss 16.124387104787406
Epoch 3: loss 16.122451814638005
Epoch 4: loss 16.120526509028274
Epoch 5: loss 16.118577413678235
Epoch 6: loss 16.116649134513022
Epoch 7: loss 16.114720088622697
Epoch 8: loss 16.112786508760355
Epoch 9: loss 16.110860894075095
Epoch 10: loss 16.10891406311815
Epoch 11: loss 16.106982476689232
Epoch 12: loss 16.105050880147946
Epoch 13: loss 16.10311172836951
Epoch 14: loss 16.101184440216596
Epoch 15: loss 16.09924690563955
Epoch 16: loss 16.097314585934132
Epoch 17: loss 16.095382348164843
Epoch 18: loss 16.093452687234965
Epoch 19: loss 16.091522064333443
-----------Time: 0:02:53.329161, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012381553649902-------------


Epoch 0: loss 16.128219614085975
Epoch 1: loss 16.126286026704175
Epoch 2: loss 16.124353964215974
Epoch 3: loss 16.122412221855672
Epoch 4: loss 16.12047682098878
Epoch 5: loss 16.11854261364089
Epoch 6: loss 16.116606983041684
Epoch 7: loss 16.114681354613975
Epoch 8: loss 16.112740914415834
Epoch 9: loss 16.110808905600983
Epoch 10: loss 16.10887655166908
Epoch 11: loss 16.106935225471716
Epoch 12: loss 16.1050078514933
Epoch 13: loss 16.103081397999883
Epoch 14: loss 16.101141513982114
Epoch 15: loss 16.0992071476885
Epoch 16: loss 16.09727648511537
Epoch 17: loss 16.095344997995117
Epoch 18: loss 16.09341269695869
Epoch 19: loss 16.09147386644248
-----------Time: 0:03:20.690500, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012394428253174-------------


Epoch 0: loss 16.12822293016559
Epoch 1: loss 16.126284615639566
Epoch 2: loss 16.124352373980905
Epoch 3: loss 16.12241661606955
Epoch 4: loss 16.1204840334425
Epoch 5: loss 16.118547537068817
Epoch 6: loss 16.11661640621515
Epoch 7: loss 16.11467420309377
Epoch 8: loss 16.112751567409116
Epoch 9: loss 16.11080915452087
Epoch 10: loss 16.108879691430445
Epoch 11: loss 16.10694578875012
Epoch 12: loss 16.105009854779805
Epoch 13: loss 16.10308085815488
Epoch 14: loss 16.101147002665616
Epoch 15: loss 16.099207579927512
Epoch 16: loss 16.09727209868024
Epoch 17: loss 16.095345937667688
Epoch 18: loss 16.09341202046704
Epoch 19: loss 16.09147947932664
-----------Time: 0:04:04.118262, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012394905090332-------------


Epoch 0: loss 16.128214379248895
Epoch 1: loss 16.126284389278045
Epoch 2: loss 16.12433975789131
Epoch 3: loss 16.122401583123427
Epoch 4: loss 16.12047220119126
Epoch 5: loss 16.11854019626578
Epoch 6: loss 16.116603463677492
Epoch 7: loss 16.114672827811393
Epoch 8: loss 16.112735231004375
Epoch 9: loss 16.11079822227059
Epoch 10: loss 16.108873097387068
Epoch 11: loss 16.10693788787734
Epoch 12: loss 16.105003124867658
Epoch 13: loss 16.103071808879825
Epoch 14: loss 16.101142957458165
Epoch 15: loss 16.099199406798604
Epoch 16: loss 16.097275257629178
Epoch 17: loss 16.095332907230198
Epoch 18: loss 16.09340690620053
Epoch 19: loss 16.091472954773398
-----------Time: 0:04:38.268346, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012393474578857-------------


Epoch 0: loss 16.128081390169278
Epoch 1: loss 16.12552640384925
Epoch 2: loss 16.122962139559025
Epoch 3: loss 16.12040834416508
Epoch 4: loss 16.117858841602022
Epoch 5: loss 16.115297247755457
Epoch 6: loss 16.112735789777663
Epoch 7: loss 16.11017496447125
Epoch 8: loss 16.107622978413733
Epoch 9: loss 16.1050735209674
Epoch 10: loss 16.10250993005733
Epoch 11: loss 16.099949162054347
Epoch 12: loss 16.097397137103098
Epoch 13: loss 16.09484839218994
Epoch 14: loss 16.092284243284457
Epoch 15: loss 16.089734306926633
Epoch 16: loss 16.08717377332321
Epoch 17: loss 16.08461775969007
Epoch 18: loss 16.082068857905526
Epoch 19: loss 16.079510587139453
-----------Time: 0:02:42.397820, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 20, rmse: 4.010849475860596-------------


Epoch 0: loss 16.127915968124864
Epoch 1: loss 16.125348226475676
Epoch 2: loss 16.122788114998897
Epoch 3: loss 16.120241386336826
Epoch 4: loss 16.11768106483389
Epoch 5: loss 16.115122675571577
Epoch 6: loss 16.112564852602006
Epoch 7: loss 16.11000898909867
Epoch 8: loss 16.107458965618886
Epoch 9: loss 16.104897651288614
Epoch 10: loss 16.10234158579716
Epoch 11: loss 16.099783904400773
Epoch 12: loss 16.09723128593117
Epoch 13: loss 16.094678264003917
Epoch 14: loss 16.092126288318063
Epoch 15: loss 16.089568480387403
Epoch 16: loss 16.087012616884067
Epoch 17: loss 16.084457154245463
Epoch 18: loss 16.08190661503479
Epoch 19: loss 16.079357591901804
-----------Time: 0:03:18.266796, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 50, rmse: 4.0108513832092285-------------


Epoch 0: loss 16.127888185553996
Epoch 1: loss 16.125339414711682
Epoch 2: loss 16.122767175391292
Epoch 3: loss 16.120217485360442
Epoch 4: loss 16.117654853569807
Epoch 5: loss 16.11509954209818
Epoch 6: loss 16.11254502950381
Epoch 7: loss 16.109982104454435
Epoch 8: loss 16.107439627395888
Epoch 9: loss 16.104875299838525
Epoch 10: loss 16.102319953621826
Epoch 11: loss 16.09976458562464
Epoch 12: loss 16.097213401555884
Epoch 13: loss 16.09465300459911
Epoch 14: loss 16.092101363917937
Epoch 15: loss 16.089550253747273
Epoch 16: loss 16.08698015695295
Epoch 17: loss 16.08442814652203
Epoch 18: loss 16.081877135660026
Epoch 19: loss 16.079324121770814
-----------Time: 0:04:01.920760, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 100, rmse: 4.010845184326172-------------


Epoch 0: loss 16.127901370529248
Epoch 1: loss 16.125344391812963
Epoch 2: loss 16.122796542492814
Epoch 3: loss 16.120235716408526
Epoch 4: loss 16.117672497322534
Epoch 5: loss 16.11511453329836
Epoch 6: loss 16.11255840739198
Epoch 7: loss 16.110013773027745
Epoch 8: loss 16.107443708904157
Epoch 9: loss 16.10489558188276
Epoch 10: loss 16.10233623013022
Epoch 11: loss 16.09978435245657
Epoch 12: loss 16.09722423734971
Epoch 13: loss 16.094666639963787
Epoch 14: loss 16.092117035499147
Epoch 15: loss 16.089558437507137
Epoch 16: loss 16.087007030447648
Epoch 17: loss 16.084446257518128
Epoch 18: loss 16.08189751545718
Epoch 19: loss 16.07933965100096
-----------Time: 0:04:42.013183, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 150, rmse: 4.010848522186279-------------


Epoch 0: loss 16.1279082230863
Epoch 1: loss 16.125347039438967
Epoch 2: loss 16.12279386271466
Epoch 3: loss 16.12023951373326
Epoch 4: loss 16.117676020316807
Epoch 5: loss 16.115120854567028
Epoch 6: loss 16.112560322519357
Epoch 7: loss 16.110005241557914
Epoch 8: loss 16.10744697260688
Epoch 9: loss 16.104895169349906
Epoch 10: loss 16.102342261770225
Epoch 11: loss 16.099778121421362
Epoch 12: loss 16.097225174170074
Epoch 13: loss 16.094665320947676
Epoch 14: loss 16.0921148921952
Epoch 15: loss 16.089563485135717
Epoch 16: loss 16.08700795326627
Epoch 17: loss 16.08445137774834
Epoch 18: loss 16.08190018641942
Epoch 19: loss 16.079343038385755
-----------Time: 0:05:27.601315, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 200, rmse: 4.010848045349121-------------


Epoch 0: loss 16.127435300971193
Epoch 1: loss 16.12405354747181
Epoch 2: loss 16.120682938323196
Epoch 3: loss 16.117301138929207
Epoch 4: loss 16.113917857165436
Epoch 5: loss 16.110546597195036
Epoch 6: loss 16.107164682157016
Epoch 7: loss 16.103789931603785
Epoch 8: loss 16.100407920887186
Epoch 9: loss 16.097035705427427
Epoch 10: loss 16.09364831804127
Epoch 11: loss 16.09027939739922
Epoch 12: loss 16.086901330247787
Epoch 13: loss 16.083528497933433
Epoch 14: loss 16.080144421181775
Epoch 15: loss 16.07677470244034
Epoch 16: loss 16.07339973174858
Epoch 17: loss 16.070026191049713
Epoch 18: loss 16.066656173863706
Epoch 19: loss 16.06327516842044
-----------Time: 0:03:23.886704, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 20, rmse: 4.008835315704346-------------


Epoch 0: loss 16.12749612999066
Epoch 1: loss 16.124114365601034
Epoch 2: loss 16.120729031803947
Epoch 3: loss 16.117352391533906
Epoch 4: loss 16.113968833624636
Epoch 5: loss 16.110593339163948
Epoch 6: loss 16.10721097477368
Epoch 7: loss 16.10383586484236
Epoch 8: loss 16.100460344452518
Epoch 9: loss 16.09707756364002
Epoch 10: loss 16.093696656986836
Epoch 11: loss 16.09031748784632
Epoch 12: loss 16.086950199185278
Epoch 13: loss 16.083576661079327
Epoch 14: loss 16.080191821491933
Epoch 15: loss 16.07681819574544
Epoch 16: loss 16.073449042000284
Epoch 17: loss 16.07007505428279
Epoch 18: loss 16.066695145124193
Epoch 19: loss 16.063320618080276
-----------Time: 0:04:02.541772, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 50, rmse: 4.008815765380859-------------


Epoch 0: loss 16.12750054935582
Epoch 1: loss 16.12412697624551
Epoch 2: loss 16.12074450658287
Epoch 3: loss 16.11736246008404
Epoch 4: loss 16.113990819214347
Epoch 5: loss 16.11061025171451
Epoch 6: loss 16.10722888637458
Epoch 7: loss 16.103850722507396
Epoch 8: loss 16.100473422081073
Epoch 9: loss 16.09709172044
Epoch 10: loss 16.09371070566224
Epoch 11: loss 16.090332438856315
Epoch 12: loss 16.086965700930524
Epoch 13: loss 16.083583969730213
Epoch 14: loss 16.08020512755634
Epoch 15: loss 16.076824871984236
Epoch 16: loss 16.073454866207054
Epoch 17: loss 16.070083283418672
Epoch 18: loss 16.066704440726216
Epoch 19: loss 16.06333351809412
-----------Time: 0:04:58.677768, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 100, rmse: 4.008810520172119-------------


Epoch 0: loss 16.127490389794353
Epoch 1: loss 16.12411860242494
Epoch 2: loss 16.12073652429254
Epoch 3: loss 16.117358282378603
Epoch 4: loss 16.113978648846924
Epoch 5: loss 16.110598402609316
Epoch 6: loss 16.107221433816882
Epoch 7: loss 16.10384205750242
Epoch 8: loss 16.10046453201103
Epoch 9: loss 16.0970811416041
Epoch 10: loss 16.09370333089201
Epoch 11: loss 16.09032643133042
Epoch 12: loss 16.08694602562851
Epoch 13: loss 16.083578795048773
Epoch 14: loss 16.080201360828013
Epoch 15: loss 16.07681935451937
Epoch 16: loss 16.0734424298065
Epoch 17: loss 16.070067793082256
Epoch 18: loss 16.066696391279375
Epoch 19: loss 16.063320011856636
-----------Time: 0:05:14.625162, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 150, rmse: 4.008808612823486-------------


Epoch 0: loss 16.12750252178663
Epoch 1: loss 16.124123312974508
Epoch 2: loss 16.12074128255175
Epoch 3: loss 16.117362403039895
Epoch 4: loss 16.11398257581743
Epoch 5: loss 16.110600249024433
Epoch 6: loss 16.10721847400385
Epoch 7: loss 16.10383891514814
Epoch 8: loss 16.10046562544349
Epoch 9: loss 16.097086869872996
Epoch 10: loss 16.093708757864125
Epoch 11: loss 16.090328420874474
Epoch 12: loss 16.086954978706398
Epoch 13: loss 16.083581445786276
Epoch 14: loss 16.08019824492147
Epoch 15: loss 16.07682442444702
Epoch 16: loss 16.073445568012115
Epoch 17: loss 16.07007032791644
Epoch 18: loss 16.066697415740286
Epoch 19: loss 16.06331886086145
-----------Time: 0:04:30.789597, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 200, rmse: 4.008807182312012-------------


Epoch 0: loss 16.127243185121124
Epoch 1: loss 16.122770473839083
Epoch 2: loss 16.118301383563523
Epoch 3: loss 16.113839357167627
Epoch 4: loss 16.109364989012594
Epoch 5: loss 16.104898916112788
Epoch 6: loss 16.100439016685463
Epoch 7: loss 16.095973398842325
Epoch 8: loss 16.091509615486896
Epoch 9: loss 16.087036053469287
Epoch 10: loss 16.082575933384856
Epoch 11: loss 16.07812183742098
Epoch 12: loss 16.07365257497583
Epoch 13: loss 16.069189368284807
Epoch 14: loss 16.064730918815826
Epoch 15: loss 16.060275146272794
Epoch 16: loss 16.055811894205746
Epoch 17: loss 16.051354312326286
Epoch 18: loss 16.04689673563266
Epoch 19: loss 16.042445618669387
-----------Time: 0:02:52.461113, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 20, rmse: 4.006144046783447-------------


Epoch 0: loss 16.1269680850852
Epoch 1: loss 16.122504708039624
Epoch 2: loss 16.11803591361572
Epoch 3: loss 16.11356701884599
Epoch 4: loss 16.109097593047167
Epoch 5: loss 16.10463481159428
Epoch 6: loss 16.10016414871548
Epoch 7: loss 16.09570085827328
Epoch 8: loss 16.091239724358896
Epoch 9: loss 16.08677101316758
Epoch 10: loss 16.08231097839007
Epoch 11: loss 16.077846931594426
Epoch 12: loss 16.073378723169423
Epoch 13: loss 16.068921046907843
Epoch 14: loss 16.064464367362973
Epoch 15: loss 16.059995729291874
Epoch 16: loss 16.055544997895133
Epoch 17: loss 16.05108528049048
Epoch 18: loss 16.046624471209114
Epoch 19: loss 16.042175717947597
-----------Time: 0:03:23.711279, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 50, rmse: 4.006112575531006-------------


Epoch 0: loss 16.126950046950025
Epoch 1: loss 16.122489776735787
Epoch 2: loss 16.11802100798175
Epoch 3: loss 16.113545602141937
Epoch 4: loss 16.109082609366435
Epoch 5: loss 16.104614041304057
Epoch 6: loss 16.100152381805703
Epoch 7: loss 16.095689131035112
Epoch 8: loss 16.091217716210817
Epoch 9: loss 16.08676169137711
Epoch 10: loss 16.082301090306856
Epoch 11: loss 16.077835880588616
Epoch 12: loss 16.0733838843677
Epoch 13: loss 16.068914581473074
Epoch 14: loss 16.064446489469976
Epoch 15: loss 16.05999674338112
Epoch 16: loss 16.055535015429797
Epoch 17: loss 16.051086119039347
Epoch 18: loss 16.046616258408598
Epoch 19: loss 16.042153891044315
-----------Time: 0:04:24.166820, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 100, rmse: 4.006115913391113-------------


Epoch 0: loss 16.126948560431575
Epoch 1: loss 16.12248424552847
Epoch 2: loss 16.118013878760614
Epoch 3: loss 16.113552757292226
Epoch 4: loss 16.1090749034809
Epoch 5: loss 16.10460884328638
Epoch 6: loss 16.100142480239327
Epoch 7: loss 16.095680378648883
Epoch 8: loss 16.091212672990196
Epoch 9: loss 16.086752535014647
Epoch 10: loss 16.082283904463004
Epoch 11: loss 16.0778146713178
Epoch 12: loss 16.073355385374285
Epoch 13: loss 16.068901166765503
Epoch 14: loss 16.064433003975815
Epoch 15: loss 16.059977445089018
Epoch 16: loss 16.055507680914726
Epoch 17: loss 16.051048672413163
Epoch 18: loss 16.046582532356847
Epoch 19: loss 16.042117464989666
-----------Time: 0:05:14.991480, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 150, rmse: 4.006108283996582-------------


Epoch 0: loss 16.12695524989424
Epoch 1: loss 16.122489537409688
Epoch 2: loss 16.118020174878
Epoch 3: loss 16.11355428011149
Epoch 4: loss 16.109080107202992
Epoch 5: loss 16.10461317967824
Epoch 6: loss 16.100154696242065
Epoch 7: loss 16.09569036941155
Epoch 8: loss 16.09121748518205
Epoch 9: loss 16.08675129949042
Epoch 10: loss 16.082286164707433
Epoch 11: loss 16.07782651238496
Epoch 12: loss 16.073359760659876
Epoch 13: loss 16.068899402027224
Epoch 14: loss 16.064432092825314
Epoch 15: loss 16.059969842401518
Epoch 16: loss 16.055502462153722
Epoch 17: loss 16.05103344344263
Epoch 18: loss 16.046571064150935
Epoch 19: loss 16.042106004562502
-----------Time: 0:06:08.055932, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 200, rmse: 4.006107807159424-------------


Epoch 0: loss 16.12644271295702
Epoch 1: loss 16.12053422295185
Epoch 2: loss 16.11463327807146
Epoch 3: loss 16.10872555997723
Epoch 4: loss 16.102822132898844
Epoch 5: loss 16.096917711955946
Epoch 6: loss 16.09102584513197
Epoch 7: loss 16.085120282009804
Epoch 8: loss 16.079214451298757
Epoch 9: loss 16.073329566418316
Epoch 10: loss 16.067430250926016
Epoch 11: loss 16.061533787900043
Epoch 12: loss 16.05564062565549
Epoch 13: loss 16.049752546303164
Epoch 14: loss 16.043863281210587
Epoch 15: loss 16.037962551801474
Epoch 16: loss 16.03208091662201
Epoch 17: loss 16.026191968124415
Epoch 18: loss 16.0203104327722
Epoch 19: loss 16.014416825324055
-----------Time: 0:02:55.848521, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 20, rmse: 4.002533435821533-------------


Epoch 0: loss 16.1262566396372
Epoch 1: loss 16.120346484202415
Epoch 2: loss 16.11444113577365
Epoch 3: loss 16.108543496082632
Epoch 4: loss 16.102634447563467
Epoch 5: loss 16.09672638487077
Epoch 6: loss 16.090825059868965
Epoch 7: loss 16.084924898567635
Epoch 8: loss 16.079017452729534
Epoch 9: loss 16.073125461704905
Epoch 10: loss 16.067226292193745
Epoch 11: loss 16.06133549183591
Epoch 12: loss 16.05544215508814
Epoch 13: loss 16.049540987217018
Epoch 14: loss 16.04365697407476
Epoch 15: loss 16.037767584003657
Epoch 16: loss 16.031873424783097
Epoch 17: loss 16.0259880874389
Epoch 18: loss 16.020104234279533
Epoch 19: loss 16.014210899606095
-----------Time: 0:02:59.381060, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 50, rmse: 4.002545356750488-------------


Epoch 0: loss 16.12624570712763
Epoch 1: loss 16.120334428182563
Epoch 2: loss 16.114431166532185
Epoch 3: loss 16.108518891648277
Epoch 4: loss 16.102617369584898
Epoch 5: loss 16.096709059528063
Epoch 6: loss 16.090810150604913
Epoch 7: loss 16.084905297163715
Epoch 8: loss 16.07900331926579
Epoch 9: loss 16.073110559960302
Epoch 10: loss 16.067211401339392
Epoch 11: loss 16.06131607323253
Epoch 12: loss 16.05541850692102
Epoch 13: loss 16.04952296852871
Epoch 14: loss 16.043632525474628
Epoch 15: loss 16.037725411270415
Epoch 16: loss 16.031826543833912
Epoch 17: loss 16.025940599747493
Epoch 18: loss 16.020052867845845
Epoch 19: loss 16.01416919644985
-----------Time: 0:04:05.559777, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 100, rmse: 4.002539157867432-------------


Epoch 0: loss 16.12623803080133
Epoch 1: loss 16.120329726967494
Epoch 2: loss 16.114422872314123
Epoch 3: loss 16.108512832264072
Epoch 4: loss 16.10260892056978
Epoch 5: loss 16.096705265314828
Epoch 6: loss 16.090805798396264
Epoch 7: loss 16.084908004686067
Epoch 8: loss 16.079009381242912
Epoch 9: loss 16.073101559950647
Epoch 10: loss 16.067198288187896
Epoch 11: loss 16.06131059077202
Epoch 12: loss 16.055407241221808
Epoch 13: loss 16.04951414924526
Epoch 14: loss 16.04361922822613
Epoch 15: loss 16.037718745143987
Epoch 16: loss 16.03183205481649
Epoch 17: loss 16.025935329906055
Epoch 18: loss 16.020042645276533
Epoch 19: loss 16.014139296244903
-----------Time: 0:04:43.820971, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 150, rmse: 4.002542018890381-------------


Epoch 0: loss 16.12623898006769
Epoch 1: loss 16.120340833202402
Epoch 2: loss 16.11443066713666
Epoch 3: loss 16.108525509805766
Epoch 4: loss 16.102619063018004
Epoch 5: loss 16.096718071983716
Epoch 6: loss 16.090822806625408
Epoch 7: loss 16.084911806678047
Epoch 8: loss 16.079012065688314
Epoch 9: loss 16.07311389237529
Epoch 10: loss 16.067217041708453
Epoch 11: loss 16.061305362417233
Epoch 12: loss 16.055407538888506
Epoch 13: loss 16.049506763584787
Epoch 14: loss 16.04360354445822
Epoch 15: loss 16.03769961487281
Epoch 16: loss 16.031801160747037
Epoch 17: loss 16.025899892270793
Epoch 18: loss 16.019994402528134
Epoch 19: loss 16.014080516665835
-----------Time: 0:05:41.095627, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 200, rmse: 4.002532482147217-------------


Epoch 0: loss 16.125442190102873
Epoch 1: loss 16.117640373692556
Epoch 2: loss 16.109828686830852
Epoch 3: loss 16.10202533878536
Epoch 4: loss 16.094221780973
Epoch 5: loss 16.086419932669827
Epoch 6: loss 16.078624269766546
Epoch 7: loss 16.070824176867156
Epoch 8: loss 16.063031924944198
Epoch 9: loss 16.05524236317106
Epoch 10: loss 16.047451498457892
Epoch 11: loss 16.039672423731446
Epoch 12: loss 16.031868155200954
Epoch 13: loss 16.024090506837315
Epoch 14: loss 16.016310470917098
Epoch 15: loss 16.008526672676506
Epoch 16: loss 16.000759470391493
Epoch 17: loss 15.992977885722855
Epoch 18: loss 15.98521311714832
Epoch 19: loss 15.977438824535907
-----------Time: 0:03:51.947519, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 20, rmse: 3.997853994369507-------------


Epoch 0: loss 16.125309047004073
Epoch 1: loss 16.11749987630756
Epoch 2: loss 16.109692100340286
Epoch 3: loss 16.101898193100084
Epoch 4: loss 16.09409183856908
Epoch 5: loss 16.086281184465616
Epoch 6: loss 16.078482101247516
Epoch 7: loss 16.07068820334181
Epoch 8: loss 16.06289626075367
Epoch 9: loss 16.05510035644996
Epoch 10: loss 16.047306010488455
Epoch 11: loss 16.039513868245823
Epoch 12: loss 16.03173110068914
Epoch 13: loss 16.02394262327326
Epoch 14: loss 16.016165887356582
Epoch 15: loss 16.008381782374087
Epoch 16: loss 16.000612895212594
Epoch 17: loss 15.99283648470681
Epoch 18: loss 15.98505729900358
Epoch 19: loss 15.977298418162244
-----------Time: 0:03:44.833745, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 50, rmse: 3.9978418350219727-------------


Epoch 0: loss 16.125309251844396
Epoch 1: loss 16.117504710279903
Epoch 2: loss 16.109695155313315
Epoch 3: loss 16.10189070605661
Epoch 4: loss 16.09407824080168
Epoch 5: loss 16.08627692560193
Epoch 6: loss 16.078476170731214
Epoch 7: loss 16.070675829171226
Epoch 8: loss 16.062883885805213
Epoch 9: loss 16.05508042211569
Epoch 10: loss 16.04728701608605
Epoch 11: loss 16.039491025697544
Epoch 12: loss 16.03170555761938
Epoch 13: loss 16.023918820568372
Epoch 14: loss 16.01611334503572
Epoch 15: loss 16.00834403367325
Epoch 16: loss 16.00055546213454
Epoch 17: loss 15.992773913766717
Epoch 18: loss 15.984993718382196
Epoch 19: loss 15.977209220832298
-----------Time: 0:03:38.484712, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 100, rmse: 3.9978256225585938-------------


Epoch 0: loss 16.125299280528598
Epoch 1: loss 16.117488899977715
Epoch 2: loss 16.109685580972876
Epoch 3: loss 16.101877223414657
Epoch 4: loss 16.094080824382672
Epoch 5: loss 16.086278997600694
Epoch 6: loss 16.078482644981893
Epoch 7: loss 16.070687744136475
Epoch 8: loss 16.062879096431015
Epoch 9: loss 16.055101279787163
Epoch 10: loss 16.047303765542225
Epoch 11: loss 16.03950718085749
Epoch 12: loss 16.031715713032177
Epoch 13: loss 16.023918412962058
Epoch 14: loss 16.016127795353736
Epoch 15: loss 16.008332602027448
Epoch 16: loss 16.000544112424866
Epoch 17: loss 15.992756141664673
Epoch 18: loss 15.984957314885914
Epoch 19: loss 15.977168339630259
-----------Time: 0:04:27.115465, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 150, rmse: 3.997819662094116-------------


Epoch 0: loss 16.1252939173422
Epoch 1: loss 16.117498366712162
Epoch 2: loss 16.10968572565756
Epoch 3: loss 16.10188232808738
Epoch 4: loss 16.094080809862344
Epoch 5: loss 16.086264842356464
Epoch 6: loss 16.07846774401517
Epoch 7: loss 16.070671849823828
Epoch 8: loss 16.062860363653783
Epoch 9: loss 16.05506166003851
Epoch 10: loss 16.04726727233107
Epoch 11: loss 16.03946302506248
Epoch 12: loss 16.031656520660963
Epoch 13: loss 16.023855298028295
Epoch 14: loss 16.016051073058783
Epoch 15: loss 16.008241341255356
Epoch 16: loss 16.000426030016385
Epoch 17: loss 15.992591600433649
Epoch 18: loss 15.98476394302758
Epoch 19: loss 15.976922482235915
-----------Time: 0:05:10.461225, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 200, rmse: 3.9977855682373047-------------


Epoch 0: loss 16.124396927269856
Epoch 1: loss 16.114083155787075
Epoch 2: loss 16.10375753908847
Epoch 3: loss 16.0934535299325
Epoch 4: loss 16.0831391882676
Epoch 5: loss 16.07284063305007
Epoch 6: loss 16.062533052412306
Epoch 7: loss 16.052240969630414
Epoch 8: loss 16.041931963407716
Epoch 9: loss 16.03164513231685
Epoch 10: loss 16.021347877965017
Epoch 11: loss 16.01106724083067
Epoch 12: loss 16.00079727043206
Epoch 13: loss 15.990517273747837
Epoch 14: loss 15.980245232906206
Epoch 15: loss 15.969984804177297
Epoch 16: loss 15.959715170079823
Epoch 17: loss 15.949448148085411
Epoch 18: loss 15.939191952031743
Epoch 19: loss 15.92894310118907
-----------Time: 0:03:21.528889, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 20, rmse: 3.9916579723358154-------------


Epoch 0: loss 16.124030673808544
Epoch 1: loss 16.11371014674375
Epoch 2: loss 16.10339626013552
Epoch 3: loss 16.093080604381054
Epoch 4: loss 16.082770099712494
Epoch 5: loss 16.072463108963003
Epoch 6: loss 16.062155569293306
Epoch 7: loss 16.051853088142433
Epoch 8: loss 16.04156980182633
Epoch 9: loss 16.031276974099818
Epoch 10: loss 16.020986523817516
Epoch 11: loss 16.010700143634658
Epoch 12: loss 16.000414211507593
Epoch 13: loss 15.990138922002147
Epoch 14: loss 15.979861910282237
Epoch 15: loss 15.969588187157038
Epoch 16: loss 15.959329552207063
Epoch 17: loss 15.949059780425776
Epoch 18: loss 15.938801502520263
Epoch 19: loss 15.928540793237898
-----------Time: 0:04:04.030328, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 50, rmse: 3.9916152954101562-------------


Epoch 0: loss 16.124055315580883
Epoch 1: loss 16.11373740398994
Epoch 2: loss 16.103412177784406
Epoch 3: loss 16.093106232498442
Epoch 4: loss 16.08278821693159
Epoch 5: loss 16.072484005009628
Epoch 6: loss 16.062174152718608
Epoch 7: loss 16.051870697668676
Epoch 8: loss 16.041582681096852
Epoch 9: loss 16.031287337723732
Epoch 10: loss 16.020997396949323
Epoch 11: loss 16.010716379952857
Epoch 12: loss 16.00042847461711
Epoch 13: loss 15.990148388736594
Epoch 14: loss 15.979859494722168
Epoch 15: loss 15.96958488430129
Epoch 16: loss 15.959306657800472
Epoch 17: loss 15.949026099490757
Epoch 18: loss 15.938758179829003
Epoch 19: loss 15.92848865826406
-----------Time: 0:05:10.172617, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 100, rmse: 3.991597890853882-------------


Epoch 0: loss 16.124045155760125
Epoch 1: loss 16.1137274676785
Epoch 2: loss 16.103402534213124
Epoch 3: loss 16.093084636623928
Epoch 4: loss 16.08277676869113
Epoch 5: loss 16.07246486773758
Epoch 6: loss 16.062166424533995
Epoch 7: loss 16.051858603014384
Epoch 8: loss 16.041561921956163
Epoch 9: loss 16.031262082208297
Epoch 10: loss 16.02095732136613
Epoch 11: loss 16.01066420349237
Epoch 12: loss 16.000361229428265
Epoch 13: loss 15.990062608350678
Epoch 14: loss 15.979759864278174
Epoch 15: loss 15.969449887008626
Epoch 16: loss 15.959143155809976
Epoch 17: loss 15.948821042399487
Epoch 18: loss 15.938494799252501
Epoch 19: loss 15.928163325676392
-----------Time: 0:04:27.222928, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 150, rmse: 3.9915530681610107-------------


Epoch 0: loss 16.124053014886968
Epoch 1: loss 16.113728737947795
Epoch 2: loss 16.103415555834705
Epoch 3: loss 16.093095657033334
Epoch 4: loss 16.082791859718558
Epoch 5: loss 16.07248370552789
Epoch 6: loss 16.06217049385556
Epoch 7: loss 16.051853744409343
Epoch 8: loss 16.041549387802696
Epoch 9: loss 16.03122690516104
Epoch 10: loss 16.020929276651504
Epoch 11: loss 16.010607880182146
Epoch 12: loss 16.00028414516231
Epoch 13: loss 15.989945271923984
Epoch 14: loss 15.97960240144714
Epoch 15: loss 15.96925365516451
Epoch 16: loss 15.958855906409243
Epoch 17: loss 15.948444312003852
Epoch 18: loss 15.937986874852639
Epoch 19: loss 15.927489366526187
-----------Time: 0:04:49.215001, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 200, rmse: 3.9914603233337402-------------


Epoch 0: loss 16.122560734461025
Epoch 1: loss 16.10892064212262
Epoch 2: loss 16.09528690138999
Epoch 3: loss 16.081665809676984
Epoch 4: loss 16.06804093308524
Epoch 5: loss 16.054428099808064
Epoch 6: loss 16.04082558659382
Epoch 7: loss 16.027228632071797
Epoch 8: loss 16.013632568994122
Epoch 9: loss 16.000038699263655
Epoch 10: loss 15.98645322565283
Epoch 11: loss 15.97288333416763
Epoch 12: loss 15.95931382306313
Epoch 13: loss 15.945743601240498
Epoch 14: loss 15.932178538023226
Epoch 15: loss 15.918635260222592
Epoch 16: loss 15.90509514837177
Epoch 17: loss 15.891564280783243
Epoch 18: loss 15.878028687087657
Epoch 19: loss 15.864489172125978
-----------Time: 0:03:03.294796, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 20, rmse: 3.983368396759033-------------


Epoch 0: loss 16.12241273888302
Epoch 1: loss 16.10877167005264
Epoch 2: loss 16.095140143669838
Epoch 3: loss 16.081518938646425
Epoch 4: loss 16.067883696097145
Epoch 5: loss 16.054265521673354
Epoch 6: loss 16.040650802828036
Epoch 7: loss 16.027044170248956
Epoch 8: loss 16.013444226095377
Epoch 9: loss 15.999841204669698
Epoch 10: loss 15.986263757169473
Epoch 11: loss 15.972675547254935
Epoch 12: loss 15.959101807364567
Epoch 13: loss 15.945532535067585
Epoch 14: loss 15.931965228719125
Epoch 15: loss 15.918417913489785
Epoch 16: loss 15.904857142844104
Epoch 17: loss 15.891297249122442
Epoch 18: loss 15.877754627329425
Epoch 19: loss 15.864217033718123
-----------Time: 0:03:28.157054, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 50, rmse: 3.9833643436431885-------------


Epoch 0: loss 16.12238497394398
Epoch 1: loss 16.108747583423508
Epoch 2: loss 16.09510976636844
Epoch 3: loss 16.081481537655556
Epoch 4: loss 16.067853639539393
Epoch 5: loss 16.05423839433223
Epoch 6: loss 16.040613985502443
Epoch 7: loss 16.027001011948535
Epoch 8: loss 16.013389569511226
Epoch 9: loss 15.999787488017411
Epoch 10: loss 15.986196033587722
Epoch 11: loss 15.972598830923689
Epoch 12: loss 15.95900672852442
Epoch 13: loss 15.945412440815979
Epoch 14: loss 15.931816046622993
Epoch 15: loss 15.918223902477784
Epoch 16: loss 15.90462523974304
Epoch 17: loss 15.891023308379031
Epoch 18: loss 15.877416731806926
Epoch 19: loss 15.86379427207171
-----------Time: 0:04:42.884584, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 100, rmse: 3.9833121299743652-------------


Epoch 0: loss 16.12240923429845
Epoch 1: loss 16.108777358909222
Epoch 2: loss 16.095150702021705
Epoch 3: loss 16.081521620239624
Epoch 4: loss 16.067896831807005
Epoch 5: loss 16.054282812270994
Epoch 6: loss 16.040655045356356
Epoch 7: loss 16.027045996180043
Epoch 8: loss 16.01343435045816
Epoch 9: loss 15.999808554418584
Epoch 10: loss 15.986191190280882
Epoch 11: loss 15.972567813431459
Epoch 12: loss 15.958946517915296
Epoch 13: loss 15.945299964031491
Epoch 14: loss 15.931653153708345
Epoch 15: loss 15.917977632550587
Epoch 16: loss 15.90427343659974
Epoch 17: loss 15.890542329297629
Epoch 18: loss 15.876746394959659
Epoch 19: loss 15.862898430142343
-----------Time: 0:05:23.715232, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 150, rmse: 3.983182430267334-------------


Epoch 0: loss 16.122394518206598
Epoch 1: loss 16.10876635613164
Epoch 2: loss 16.095116973117747
Epoch 3: loss 16.08148840732585
Epoch 4: loss 16.067859736520873
Epoch 5: loss 16.054225228025967
Epoch 6: loss 16.04059699438654
Epoch 7: loss 16.026973928946266
Epoch 8: loss 16.013327156220395
Epoch 9: loss 15.999671909328127
Epoch 10: loss 15.986011025178296
Epoch 11: loss 15.97229716750576
Epoch 12: loss 15.958547220079714
Epoch 13: loss 15.944743711057729
Epoch 14: loss 15.930866978102369
Epoch 15: loss 15.916875132404636
Epoch 16: loss 15.902730798643526
Epoch 17: loss 15.888408055702197
Epoch 18: loss 15.87384836825422
Epoch 19: loss 15.858991261213093
-----------Time: 0:05:35.915950, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 200, rmse: 3.9826183319091797-------------


Epoch 0: loss 16.120433713768797
Epoch 1: loss 16.102412571785177
Epoch 2: loss 16.084388398942643
Epoch 3: loss 16.06637759465378
Epoch 4: loss 16.048383437141638
Epoch 5: loss 16.030387894234753
Epoch 6: loss 16.012414558871146
Epoch 7: loss 15.994438863695153
Epoch 8: loss 15.97648441020142
Epoch 9: loss 15.958537848764557
Epoch 10: loss 15.94060590203655
Epoch 11: loss 15.922695992237982
Epoch 12: loss 15.904789243721975
Epoch 13: loss 15.886886901865841
Epoch 14: loss 15.869002665819973
Epoch 15: loss 15.851128072308223
Epoch 16: loss 15.833259112683244
Epoch 17: loss 15.815404317377702
Epoch 18: loss 15.7975557520702
Epoch 19: loss 15.779725636912143
-----------Time: 0:02:39.539250, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 20, rmse: 3.9724769592285156-------------


Epoch 0: loss 16.12022956108645
Epoch 1: loss 16.10221668324307
Epoch 2: loss 16.08418516155946
Epoch 3: loss 16.066172909645882
Epoch 4: loss 16.04817905809777
Epoch 5: loss 16.03019195046355
Epoch 6: loss 16.012219089084894
Epoch 7: loss 15.994254926678407
Epoch 8: loss 15.976298209569453
Epoch 9: loss 15.958360848574616
Epoch 10: loss 15.94042555231838
Epoch 11: loss 15.922503701884699
Epoch 12: loss 15.904581848858102
Epoch 13: loss 15.88668090354625
Epoch 14: loss 15.868792759198868
Epoch 15: loss 15.850909452713204
Epoch 16: loss 15.833044772435946
Epoch 17: loss 15.815170869158298
Epoch 18: loss 15.79731019493547
Epoch 19: loss 15.779475294033297
-----------Time: 0:03:10.492409, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 50, rmse: 3.972480535507202-------------


Epoch 0: loss 16.120239798176087
Epoch 1: loss 16.10221331478656
Epoch 2: loss 16.08420940298565
Epoch 3: loss 16.066201641223817
Epoch 4: loss 16.04818833010426
Epoch 5: loss 16.030195304659024
Epoch 6: loss 16.012220832820557
Epoch 7: loss 15.994250679223546
Epoch 8: loss 15.97627330409761
Epoch 9: loss 15.958309154914993
Epoch 10: loss 15.940338591118751
Epoch 11: loss 15.922392974021708
Epoch 12: loss 15.90442945569543
Epoch 13: loss 15.886476977744007
Epoch 14: loss 15.8685226308191
Epoch 15: loss 15.850547670216065
Epoch 16: loss 15.832560550654433
Epoch 17: loss 15.814561612324717
Epoch 18: loss 15.796525421578188
Epoch 19: loss 15.778457798213658
-----------Time: 0:04:10.943667, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 100, rmse: 3.9723498821258545-------------


Epoch 0: loss 16.12023804743955
Epoch 1: loss 16.102213349272336
Epoch 2: loss 16.08419216294986
Epoch 3: loss 16.066167932803893
Epoch 4: loss 16.04815814130782
Epoch 5: loss 16.03015312181341
Epoch 6: loss 16.012149298171625
Epoch 7: loss 15.994156946365829
Epoch 8: loss 15.97613617802133
Epoch 9: loss 15.958125872609406
Epoch 10: loss 15.940078575627968
Epoch 11: loss 15.92198903097803
Epoch 12: loss 15.903863331492404
Epoch 13: loss 15.885658130625009
Epoch 14: loss 15.86735674011247
Epoch 15: loss 15.848929537710383
Epoch 16: loss 15.830311887999343
Epoch 17: loss 15.811483415849446
Epoch 18: loss 15.792363022124398
Epoch 19: loss 15.772905598651333
-----------Time: 0:05:00.016341, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 150, rmse: 3.9715492725372314-------------


Epoch 0: loss 16.120226501705467
Epoch 1: loss 16.102207040968246
Epoch 2: loss 16.084184298377895
Epoch 3: loss 16.06617357939595
Epoch 4: loss 16.0481436144988
Epoch 5: loss 16.03012490881857
Epoch 6: loss 16.012090474512995
Epoch 7: loss 15.994035901032822
Epoch 8: loss 15.975919414953799
Epoch 9: loss 15.957752127955956
Epoch 10: loss 15.939478973774497
Epoch 11: loss 15.921032608949082
Epoch 12: loss 15.90239290856615
Epoch 13: loss 15.88341007756435
Epoch 14: loss 15.864022501270819
Epoch 15: loss 15.844057329975437
Epoch 16: loss 15.823400508067477
Epoch 17: loss 15.80184260913378
Epoch 18: loss 15.779225428769484
Epoch 19: loss 15.755382173537171
-----------Time: 0:05:46.149503, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 200, rmse: 3.96903395652771-------------


Epoch 0: loss 16.11744979362633
Epoch 1: loss 16.09362906123582
Epoch 2: loss 16.06981240438987
Epoch 3: loss 16.046021357770197
Epoch 4: loss 16.022237785748
Epoch 5: loss 15.998481767082941
Epoch 6: loss 15.97474683763671
Epoch 7: loss 15.95103055332717
Epoch 8: loss 15.927326735496003
Epoch 9: loss 15.903643043874851
Epoch 10: loss 15.879980027643217
Epoch 11: loss 15.856341806165934
Epoch 12: loss 15.832715440016846
Epoch 13: loss 15.809109025055976
Epoch 14: loss 15.785523131465444
Epoch 15: loss 15.76195443279397
Epoch 16: loss 15.738414075985235
Epoch 17: loss 15.71489252014741
Epoch 18: loss 15.69138069552141
Epoch 19: loss 15.667895882333212
-----------Time: 0:03:59.824350, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 20, rmse: 3.958141803741455-------------


Epoch 0: loss 16.117372708841792
Epoch 1: loss 16.093564764451177
Epoch 2: loss 16.069759249622372
Epoch 3: loss 16.045965391245144
Epoch 4: loss 16.022186983014137
Epoch 5: loss 15.998409667956304
Epoch 6: loss 15.974658992771483
Epoch 7: loss 15.950949816421168
Epoch 8: loss 15.927242196857314
Epoch 9: loss 15.90355455026217
Epoch 10: loss 15.879891525992496
Epoch 11: loss 15.856242985489448
Epoch 12: loss 15.832596254063533
Epoch 13: loss 15.808966649363
Epoch 14: loss 15.785354178129428
Epoch 15: loss 15.76176119006283
Epoch 16: loss 15.73818191825987
Epoch 17: loss 15.714595531496812
Epoch 18: loss 15.691027921590551
Epoch 19: loss 15.667472644108933
-----------Time: 0:02:59.358408, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 50, rmse: 3.958055019378662-------------


Epoch 0: loss 16.11733965824453
Epoch 1: loss 16.09351697053645
Epoch 2: loss 16.069705152589385
Epoch 3: loss 16.045914932591167
Epoch 4: loss 16.022129711215428
Epoch 5: loss 15.998364633162947
Epoch 6: loss 15.974599214401369
Epoch 7: loss 15.950860313127196
Epoch 8: loss 15.92712001867953
Epoch 9: loss 15.903371393194401
Epoch 10: loss 15.879635809814884
Epoch 11: loss 15.855882191411693
Epoch 12: loss 15.832103409282276
Epoch 13: loss 15.80828924708032
Epoch 14: loss 15.784417679803795
Epoch 15: loss 15.76045163394706
Epoch 16: loss 15.736387441312573
Epoch 17: loss 15.712193296680377
Epoch 18: loss 15.687823936561971
Epoch 19: loss 15.663246751155718
-----------Time: 0:03:53.184938, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 100, rmse: 3.957467555999756-------------


Epoch 0: loss 16.117345984180446
Epoch 1: loss 16.093522504336683
Epoch 2: loss 16.06970456762765
Epoch 3: loss 16.045895130495587
Epoch 4: loss 16.022098755953166
Epoch 5: loss 15.998309000086824
Epoch 6: loss 15.974500713431363
Epoch 7: loss 15.950656232268607
Epoch 8: loss 15.926770485633739
Epoch 9: loss 15.902811627598027
Epoch 10: loss 15.87872632214918
Epoch 11: loss 15.854453949256719
Epoch 12: loss 15.829887580716008
Epoch 13: loss 15.804937877883205
Epoch 14: loss 15.779463665066626
Epoch 15: loss 15.753271658826353
Epoch 16: loss 15.726173155070517
Epoch 17: loss 15.697978423172005
Epoch 18: loss 15.668442075832049
Epoch 19: loss 15.637351910402879
-----------Time: 0:04:37.324530, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 150, rmse: 3.9537274837493896-------------


Epoch 0: loss 16.117343878214484
Epoch 1: loss 16.093519079354614
Epoch 2: loss 16.069694891644925
Epoch 3: loss 16.04588980049851
Epoch 4: loss 16.022067715902573
Epoch 5: loss 15.998220700683511
Epoch 6: loss 15.974309017633601
Epoch 7: loss 15.95031332801126
Epoch 8: loss 15.926123673271524
Epoch 9: loss 15.901624840844773
Epoch 10: loss 15.876643789663724
Epoch 11: loss 15.850932848848942
Epoch 12: loss 15.824213561576109
Epoch 13: loss 15.796149042522602
Epoch 14: loss 15.76637385706982
Epoch 15: loss 15.734538028524128
Epoch 16: loss 15.70019997015409
Epoch 17: loss 15.6630055840862
Epoch 18: loss 15.622648032221086
Epoch 19: loss 15.578790010998857
-----------Time: 0:05:19.793235, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 200, rmse: 3.945458173751831-------------


Epoch 0: loss 16.113652929396263
Epoch 1: loss 16.082160006145084
Epoch 2: loss 16.050694464859333
Epoch 3: loss 16.01926710166122
Epoch 4: loss 15.987885467639236
Epoch 5: loss 15.95653290045916
Epoch 6: loss 15.92520906278269
Epoch 7: loss 15.893927176404478
Epoch 8: loss 15.862678370182769
Epoch 9: loss 15.831448197948212
Epoch 10: loss 15.800276513589731
Epoch 11: loss 15.76912630194228
Epoch 12: loss 15.73800790355282
Epoch 13: loss 15.70691406655532
Epoch 14: loss 15.675860117413935
Epoch 15: loss 15.644844691736534
Epoch 16: loss 15.613867709920612
Epoch 17: loss 15.582902326734251
Epoch 18: loss 15.551983626398332
Epoch 19: loss 15.521094157295165
-----------Time: 0:03:37.011495, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 20, rmse: 3.939126968383789-------------


Epoch 0: loss 16.113544111474823
Epoch 1: loss 16.08204539176107
Epoch 2: loss 16.050598900625406
Epoch 3: loss 16.019172273556002
Epoch 4: loss 15.987771187482029
Epoch 5: loss 15.956412561436332
Epoch 6: loss 15.92508055011237
Epoch 7: loss 15.893792535119111
Epoch 8: loss 15.86255419883085
Epoch 9: loss 15.83132062728409
Epoch 10: loss 15.800112878841963
Epoch 11: loss 15.768917001285521
Epoch 12: loss 15.737746589789253
Epoch 13: loss 15.706614435464548
Epoch 14: loss 15.675479645441252
Epoch 15: loss 15.644370964261874
Epoch 16: loss 15.613277753712756
Epoch 17: loss 15.582186442992816
Epoch 18: loss 15.55109924645188
Epoch 19: loss 15.52001170271956
-----------Time: 0:03:49.366210, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 50, rmse: 3.939000368118286-------------


Epoch 0: loss 16.113546997908344
Epoch 1: loss 16.082072975455322
Epoch 2: loss 16.050608428552657
Epoch 3: loss 16.019184721883967
Epoch 4: loss 15.987771629314826
Epoch 5: loss 15.95638034653573
Epoch 6: loss 15.925021568804477
Epoch 7: loss 15.89365044983267
Epoch 8: loss 15.862263605865595
Epoch 9: loss 15.830880740208753
Epoch 10: loss 15.799437403614071
Epoch 11: loss 15.767921289067479
Epoch 12: loss 15.7362691991805
Epoch 13: loss 15.704405706560177
Epoch 14: loss 15.67225488727024
Epoch 15: loss 15.639696672728945
Epoch 16: loss 15.606607230737715
Epoch 17: loss 15.572819301652935
Epoch 18: loss 15.538141767896992
Epoch 19: loss 15.502361511263658
-----------Time: 0:04:42.468915, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 100, rmse: 3.9364206790924072-------------


Epoch 0: loss 16.113521058381803
Epoch 1: loss 16.082033624592004
Epoch 2: loss 16.050572400251248
Epoch 3: loss 16.019131397799086
Epoch 4: loss 15.98767161849013
Epoch 5: loss 15.95620444677912
Epoch 6: loss 15.924670907581158
Epoch 7: loss 15.893008882679196
Epoch 8: loss 15.86112097853743
Epoch 9: loss 15.828850185566457
Epoch 10: loss 15.795931027076891
Epoch 11: loss 15.76205706090756
Epoch 12: loss 15.726807886520891
Epoch 13: loss 15.689748680805499
Epoch 14: loss 15.650320034950177
Epoch 15: loss 15.607973434278147
Epoch 16: loss 15.562179528350995
Epoch 17: loss 15.51245448328737
Epoch 18: loss 15.458325191578702
Epoch 19: loss 15.39943549865092
-----------Time: 0:04:14.142012, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 150, rmse: 3.9217374324798584-------------


Epoch 0: loss 16.113537653300217
Epoch 1: loss 16.0820735744188
Epoch 2: loss 16.05060395058761
Epoch 3: loss 16.01912638776778
Epoch 4: loss 15.987606859388658
Epoch 5: loss 15.955932233436007
Epoch 6: loss 15.923943746731682
Epoch 7: loss 15.891328841464036
Epoch 8: loss 15.857604567158022
Epoch 9: loss 15.822046000390937
Epoch 10: loss 15.783727782002087
Epoch 11: loss 15.741637241846846
Epoch 12: loss 15.694813401106584
Epoch 13: loss 15.642344262432703
Epoch 14: loss 15.583554810839804
Epoch 15: loss 15.517890826459162
Epoch 16: loss 15.444914328267615
Epoch 17: loss 15.364411270313772
Epoch 18: loss 15.276124866323798
Epoch 19: loss 15.179970365558518
-----------Time: 0:04:49.462221, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 200, rmse: 3.8912718296051025-------------


Epoch 0: loss 16.10872276559221
Epoch 1: loss 16.067132024417564
Epoch 2: loss 16.0255724280473
Epoch 3: loss 15.984092324761479
Epoch 4: loss 15.942667750057284
Epoch 5: loss 15.90129063452243
Epoch 6: loss 15.859971934262017
Epoch 7: loss 15.818713945043418
Epoch 8: loss 15.777538120390087
Epoch 9: loss 15.736425890878467
Epoch 10: loss 15.69535254265317
Epoch 11: loss 15.654339465451928
Epoch 12: loss 15.613389378465214
Epoch 13: loss 15.572484934829124
Epoch 14: loss 15.531635106031242
Epoch 15: loss 15.490843323276637
Epoch 16: loss 15.450119990897996
Epoch 17: loss 15.409450568343843
Epoch 18: loss 15.368818103651304
Epoch 19: loss 15.328261354943214
-----------Time: 0:03:15.710711, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 20, rmse: 3.914057493209839-------------


Epoch 0: loss 16.108531085611233
Epoch 1: loss 16.06693200458321
Epoch 2: loss 16.025365664558347
Epoch 3: loss 15.98384995406059
Epoch 4: loss 15.942404192903238
Epoch 5: loss 15.901011462572024
Epoch 6: loss 15.859656833837446
Epoch 7: loss 15.818335316633647
Epoch 8: loss 15.777065186606858
Epoch 9: loss 15.735807588659206
Epoch 10: loss 15.694577676307384
Epoch 11: loss 15.653364042019701
Epoch 12: loss 15.612173685890102
Epoch 13: loss 15.570934547685683
Epoch 14: loss 15.529649107530105
Epoch 15: loss 15.488315927911025
Epoch 16: loss 15.446879259071123
Epoch 17: loss 15.405324952767037
Epoch 18: loss 15.36359787376242
Epoch 19: loss 15.321643054582555
-----------Time: 0:03:50.571053, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 50, rmse: 3.9131321907043457-------------


Epoch 0: loss 16.108503192582162
Epoch 1: loss 16.06691228390518
Epoch 2: loss 16.02534643886788
Epoch 3: loss 15.983823654896673
Epoch 4: loss 15.942347130093514
Epoch 5: loss 15.900881791905647
Epoch 6: loss 15.859365926610833
Epoch 7: loss 15.817791131088564
Epoch 8: loss 15.776072491245985
Epoch 9: loss 15.734083891303854
Epoch 10: loss 15.691654523213705
Epoch 11: loss 15.64853763372889
Epoch 12: loss 15.604422051728971
Epoch 13: loss 15.558917543173743
Epoch 14: loss 15.511517035176794
Epoch 15: loss 15.461698597445963
Epoch 16: loss 15.408895039052792
Epoch 17: loss 15.352602683052282
Epoch 18: loss 15.292290938555254
Epoch 19: loss 15.227470660092974
-----------Time: 0:04:47.202360, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 100, rmse: 3.899482011795044-------------


Epoch 0: loss 16.108521908505445
Epoch 1: loss 16.0669261663747
Epoch 2: loss 16.025345983551922
Epoch 3: loss 15.983776790282855
Epoch 4: loss 15.942132027788354
Epoch 5: loss 15.900311205295067
Epoch 6: loss 15.858069125046349
Epoch 7: loss 15.815039332889228
Epoch 8: loss 15.770576858987232
Epoch 9: loss 15.723788664871224
Epoch 10: loss 15.673544514756154
Epoch 11: loss 15.61861671760978
Epoch 12: loss 15.55774062894621
Epoch 13: loss 15.489841019348841
Epoch 14: loss 15.41402398587051
Epoch 15: loss 15.329539972391382
Epoch 16: loss 15.235824661451945
Epoch 17: loss 15.132565391264642
Epoch 18: loss 15.019567570522987
Epoch 19: loss 14.8966362119823
-----------Time: 0:05:45.863779, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 150, rmse: 3.8530073165893555-------------


Epoch 0: loss 16.108520530889447
Epoch 1: loss 16.066885136598604
Epoch 2: loss 16.02524200582582
Epoch 3: loss 15.983529833233051
Epoch 4: loss 15.94146830806141
Epoch 5: loss 15.898520515041549
Epoch 6: loss 15.853609040484862
Epoch 7: loss 15.805005145111831
Epoch 8: loss 15.750498093101497
Epoch 9: loss 15.687760809182214
Epoch 10: loss 15.61474952573294
Epoch 11: loss 15.529898733727112
Epoch 12: loss 15.43208969384837
Epoch 13: loss 15.320714189022244
Epoch 14: loss 15.195445140591778
Epoch 15: loss 15.056225516343648
Epoch 16: loss 14.903040398716472
Epoch 17: loss 14.736129437924209
Epoch 18: loss 14.555742389550346
Epoch 19: loss 14.362129870544898
-----------Time: 0:04:38.854741, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 200, rmse: 3.778557300567627-------------


Epoch 0: loss 16.102039840961684
Epoch 1: loss 16.047075208080017
Epoch 2: loss 15.992220078427355
Epoch 3: loss 15.937440043755885
Epoch 4: loss 15.882809918234049
Epoch 5: loss 15.828268787794752
Epoch 6: loss 15.77382736532762
Epoch 7: loss 15.71948301124469
Epoch 8: loss 15.665233810937165
Epoch 9: loss 15.611078879183696
Epoch 10: loss 15.557014634649413
Epoch 11: loss 15.503040339131275
Epoch 12: loss 15.449170050379891
Epoch 13: loss 15.395395490253277
Epoch 14: loss 15.341718646480443
Epoch 15: loss 15.288151948979655
Epoch 16: loss 15.234681997563671
Epoch 17: loss 15.181289862912267
Epoch 18: loss 15.127966652103193
Epoch 19: loss 15.074714983981092
-----------Time: 0:02:51.395405, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 20, rmse: 3.880875825881958-------------


Epoch 0: loss 16.101860185366988
Epoch 1: loss 16.04688836492359
Epoch 2: loss 15.992000539125211
Epoch 3: loss 15.937203581012936
Epoch 4: loss 15.882518730713272
Epoch 5: loss 15.827899397942863
Epoch 6: loss 15.773328872657846
Epoch 7: loss 15.71884128747912
Epoch 8: loss 15.664375249946682
Epoch 9: loss 15.609909441109387
Epoch 10: loss 15.555441497265482
Epoch 11: loss 15.50089424521201
Epoch 12: loss 15.44622408144496
Epoch 13: loss 15.391343272853767
Epoch 14: loss 15.336150701179525
Epoch 15: loss 15.280547051276766
Epoch 16: loss 15.224363638435518
Epoch 17: loss 15.16744911495663
Epoch 18: loss 15.109596122277049
Epoch 19: loss 15.050582914004966
-----------Time: 0:03:17.276519, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 50, rmse: 3.8773488998413086-------------


Epoch 0: loss 16.101839987073923
Epoch 1: loss 16.046892518255618
Epoch 2: loss 15.992017555651683
Epoch 3: loss 15.93719243017988
Epoch 4: loss 15.882386858661645
Epoch 5: loss 15.827544606464462
Epoch 6: loss 15.772560843311618
Epoch 7: loss 15.71716288006001
Epoch 8: loss 15.661026441253613
Epoch 9: loss 15.603553422188357
Epoch 10: loss 15.543875938786833
Epoch 11: loss 15.480959214744132
Epoch 12: loss 15.413502868716648
Epoch 13: loss 15.340206191875547
Epoch 14: loss 15.259735206730538
Epoch 15: loss 15.170966049067802
Epoch 16: loss 15.072951481224338
Epoch 17: loss 14.964952143720469
Epoch 18: loss 14.846362189147186
Epoch 19: loss 14.716818787728787
-----------Time: 0:04:20.878652, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 100, rmse: 3.8292291164398193-------------


Epoch 0: loss 16.10185342382127
Epoch 1: loss 16.046891409524957
Epoch 2: loss 15.991950373211395
Epoch 3: loss 15.936952767260635
Epoch 4: loss 15.881704871585798
Epoch 5: loss 15.825661401504924
Epoch 6: loss 15.767661968737338
Epoch 7: loss 15.705825839890547
Epoch 8: loss 15.637429288446159
Epoch 9: loss 15.559375061037231
Epoch 10: loss 15.468824571990137
Epoch 11: loss 15.363516134694583
Epoch 12: loss 15.24185336446425
Epoch 13: loss 15.10275676627727
Epoch 14: loss 14.945711955484844
Epoch 15: loss 14.770656278432874
Epoch 16: loss 14.577660661731613
Epoch 17: loss 14.36701487820715
Epoch 18: loss 14.139139461932201
Epoch 19: loss 13.894625459954167
-----------Time: 0:05:11.817231, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 150, rmse: 3.7128734588623047-------------


Epoch 0: loss 16.10184593833355
Epoch 1: loss 16.04685935357007
Epoch 2: loss 15.99180039872029
Epoch 3: loss 15.936335466425337
Epoch 4: loss 15.879249026426614
Epoch 5: loss 15.817762152665072
Epoch 6: loss 15.747117613242203
Epoch 7: loss 15.66163711044826
Epoch 8: loss 15.55648153563308
Epoch 9: loss 15.428501966921385
Epoch 10: loss 15.276038798642846
Epoch 11: loss 15.09876646458811
Epoch 12: loss 14.896894990393102
Epoch 13: loss 14.670726304212428
Epoch 14: loss 14.421028895894104
Epoch 15: loss 14.148539676687003
Epoch 16: loss 13.854289609495229
Epoch 17: loss 13.539227212882029
Epoch 18: loss 13.204425296295978
Epoch 19: loss 12.8508512663932
-----------Time: 0:06:05.696668, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.848035868435805e-06, embedding_dim: 200, rmse: 3.5625152587890625-------------


Epoch 0: loss 16.09321783391466
Epoch 1: loss 16.020638822147937
Epoch 2: loss 15.948233509789738
Epoch 3: loss 15.875982951443762
Epoch 4: loss 15.803925497107949
Epoch 5: loss 15.7320379948992
Epoch 6: loss 15.660363309599902
Epoch 7: loss 15.588868858634031
Epoch 8: loss 15.517533012159886
Epoch 9: loss 15.446365972262742
Epoch 10: loss 15.375377411295242
Epoch 11: loss 15.304585553045309
Epoch 12: loss 15.233952538353908
Epoch 13: loss 15.163464529090371
Epoch 14: loss 15.093169002574445
Epoch 15: loss 15.023048447292094
Epoch 16: loss 14.953083591917535
Epoch 17: loss 14.883243684213793
Epoch 18: loss 14.813580154659569
Epoch 19: loss 14.744029907767963
-----------Time: 0:03:07.512851, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 20, rmse: 3.8371729850769043-------------


Epoch 0: loss 16.093069686910393
Epoch 1: loss 16.02046943216495
Epoch 2: loss 15.948005989234431
Epoch 3: loss 15.87570600525201
Epoch 4: loss 15.803586356467433
Epoch 5: loss 15.731587963042019
Epoch 6: loss 15.659635973987921
Epoch 7: loss 15.58775109451319
Epoch 8: loss 15.515864506307148
Epoch 9: loss 15.443858950989865
Epoch 10: loss 15.371552016178896
Epoch 11: loss 15.298734344173347
Epoch 12: loss 15.225176665496411
Epoch 13: loss 15.150457611675169
Epoch 14: loss 15.074113134847769
Epoch 15: loss 14.995594112298747
Epoch 16: loss 14.914296586595196
Epoch 17: loss 14.82963202944782
Epoch 18: loss 14.740895341050695
Epoch 19: loss 14.647388728693038
-----------Time: 0:02:57.504287, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 50, rmse: 3.8228118419647217-------------


Epoch 0: loss 16.09306195846649
Epoch 1: loss 16.02046525860818
Epoch 2: loss 15.947957489009523
Epoch 3: loss 15.87550619155444
Epoch 4: loss 15.802904507593981
Epoch 5: loss 15.72979281271457
Epoch 6: loss 15.655302500711828
Epoch 7: loss 15.577919382320403
Epoch 8: loss 15.49532785980905
Epoch 9: loss 15.404586078940428
Epoch 10: loss 15.302626722075487
Epoch 11: loss 15.186829382858567
Epoch 12: loss 15.05532832817256
Epoch 13: loss 14.906631751057892
Epoch 14: loss 14.73994648177835
Epoch 15: loss 14.554710408667626
Epoch 16: loss 14.350653976898856
Epoch 17: loss 14.127768243506527
Epoch 18: loss 13.88614297496553
Epoch 19: loss 13.626142354294164
-----------Time: 0:04:01.729777, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 100, rmse: 3.6755049228668213-------------


Epoch 0: loss 16.09300632927974
Epoch 1: loss 16.02031313225651
Epoch 2: loss 15.947566701900447
Epoch 3: loss 15.874226286983022
Epoch 4: loss 15.798516334172758
Epoch 5: loss 15.71649815142965
Epoch 6: loss 15.621872842279448
Epoch 7: loss 15.507509479709395
Epoch 8: loss 15.367674499053292
Epoch 9: loss 15.19879653649592
Epoch 10: loss 14.999178418133036
Epoch 11: loss 14.768081351296289
Epoch 12: loss 14.505649816348152
Epoch 13: loss 14.212485686017484
Epoch 14: loss 13.889871655889928
Epoch 15: loss 13.539060906653432
Epoch 16: loss 13.161797262651238
Epoch 17: loss 12.759848239176813
Epoch 18: loss 12.335103935492693
Epoch 19: loss 11.889524305301103
-----------Time: 0:04:42.919202, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 150, rmse: 3.418647289276123-------------


Epoch 0: loss 16.093059343251927
Epoch 1: loss 16.02034184231325
Epoch 2: loss 15.947072675009018
Epoch 3: loss 15.870764155696435
Epoch 4: loss 15.784112167721407
Epoch 5: loss 15.6751439544155
Epoch 6: loss 15.532334514906251
Epoch 7: loss 15.348773525615565
Epoch 8: loss 15.121679896696422
Epoch 9: loss 14.850691052478833
Epoch 10: loss 14.537169067026028
Epoch 11: loss 14.182880439467894
Epoch 12: loss 13.790006149845839
Epoch 13: loss 13.360664597667386
Epoch 14: loss 12.89754529385673
Epoch 15: loss 12.403642108350944
Epoch 16: loss 11.882057837651695
Epoch 17: loss 11.3360483901795
Epoch 18: loss 10.769223186748063
Epoch 19: loss 10.18564181693421
-----------Time: 0:05:37.619750, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.764935806792471e-06, embedding_dim: 200, rmse: 3.1501402854919434-------------


Epoch 0: loss 16.081653007416055
Epoch 1: loss 15.985771705561064
Epoch 2: loss 15.89018688673815
Epoch 3: loss 15.794920138524498
Epoch 4: loss 15.699933437888813
Epoch 5: loss 15.605248915624074
Epoch 6: loss 15.510896747043562
Epoch 7: loss 15.416853572830938
Epoch 8: loss 15.323112498942505
Epoch 9: loss 15.229640306500782
Epoch 10: loss 15.136408147246634
Epoch 11: loss 15.043448535562405
Epoch 12: loss 14.950723215054403
Epoch 13: loss 14.858241346233756
Epoch 14: loss 14.765903058293205
Epoch 15: loss 14.673757479980369
Epoch 16: loss 14.581725779922841
Epoch 17: loss 14.48977888583359
Epoch 18: loss 14.397829241698174
Epoch 19: loss 14.305848223544645
-----------Time: 0:03:30.942296, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 20, rmse: 3.778428792953491-------------


Epoch 0: loss 16.08150703560859
Epoch 1: loss 15.985594014359142
Epoch 2: loss 15.890011113515815
Epoch 3: loss 15.794680099373217
Epoch 4: loss 15.699554856929396
Epoch 5: loss 15.604567277813944
Epoch 6: loss 15.509531821291883
Epoch 7: loss 15.414349276971532
Epoch 8: loss 15.318547942963809
Epoch 9: loss 15.221585724391387
Epoch 10: loss 15.122720560215944
Epoch 11: loss 15.020776467066604
Epoch 12: loss 14.914506564521478
Epoch 13: loss 14.802308382580888
Epoch 14: loss 14.682619528384102
Epoch 15: loss 14.55378116894961
Epoch 16: loss 14.41439226439882
Epoch 17: loss 14.263094222435425
Epoch 18: loss 14.098831957484666
Epoch 19: loss 13.920863051463755
-----------Time: 0:03:48.410474, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 50, rmse: 3.720823287963867-------------


Epoch 0: loss 16.081527627765517
Epoch 1: loss 15.98561752925041
Epoch 2: loss 15.889813646925345
Epoch 3: loss 15.793839634339923
Epoch 4: loss 15.696734920800413
Epoch 5: loss 15.596280351548561
Epoch 6: loss 15.488012710558843
Epoch 7: loss 15.36579256799313
Epoch 8: loss 15.222750786142935
Epoch 9: loss 15.053141626603841
Epoch 10: loss 14.852810748962684
Epoch 11: loss 14.619312976611056
Epoch 12: loss 14.351771033931648
Epoch 13: loss 14.050349654809102
Epoch 14: loss 13.71600394873855
Epoch 15: loss 13.350213193193346
Epoch 16: loss 12.954474480272701
Epoch 17: loss 12.530945176855257
Epoch 18: loss 12.081538477301793
Epoch 19: loss 11.608593529498467
-----------Time: 0:03:54.880667, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 100, rmse: 3.3752903938293457-------------


Epoch 0: loss 16.081477459777368
Epoch 1: loss 15.985463739543457
Epoch 2: loss 15.888908747423596
Epoch 3: loss 15.788849475916603
Epoch 4: loss 15.67605544199692
Epoch 5: loss 15.53448920434036
Epoch 6: loss 15.348342395852997
Epoch 7: loss 15.108213666860923
Epoch 8: loss 14.811226444700738
Epoch 9: loss 14.458450575896487
Epoch 10: loss 14.052481520104628
Epoch 11: loss 13.596244472080498
Epoch 12: loss 13.09352201686857
Epoch 13: loss 12.548602205310715
Epoch 14: loss 11.965498661593033
Epoch 15: loss 11.349355143525797
Epoch 16: loss 10.705419293300427
Epoch 17: loss 10.039256429853745
Epoch 18: loss 9.356829183914533
Epoch 19: loss 8.664957076716254
-----------Time: 0:04:26.767545, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 150, rmse: 2.890695095062256-------------


Epoch 0: loss 16.08141668339408
Epoch 1: loss 15.985204470812334
Epoch 2: loss 15.886316697969587
Epoch 3: loss 15.772170703394249
Epoch 4: loss 15.613971263964324
Epoch 5: loss 15.385107143862601
Epoch 6: loss 15.07696085354762
Epoch 7: loss 14.690477951510307
Epoch 8: loss 14.229766437578228
Epoch 9: loss 13.700134072503426
Epoch 10: loss 13.107783452916106
Epoch 11: loss 12.459437521726558
Epoch 12: loss 11.762432878078359
Epoch 13: loss 11.024713513383144
Epoch 14: loss 10.254409859010615
Epoch 15: loss 9.46148029885388
Epoch 16: loss 8.655511600287989
Epoch 17: loss 7.8471377720451665
Epoch 18: loss 7.047298251966732
Epoch 19: loss 6.267165156536092
-----------Time: 0:05:06.534029, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.977023564332114e-06, embedding_dim: 200, rmse: 2.4354641437530518-------------


Epoch 0: loss 16.06637458168598
Epoch 1: loss 15.939769394829456
Epoch 2: loss 15.813599564187744
Epoch 3: loss 15.688016837287558
Epoch 4: loss 15.563021747232327
Epoch 5: loss 15.438512925546801
Epoch 6: loss 15.314459338403383
Epoch 7: loss 15.1908621386123
Epoch 8: loss 15.067595249028749
Epoch 9: loss 14.944674479177039
Epoch 10: loss 14.822192402363083
Epoch 11: loss 14.699858917497695
Epoch 12: loss 14.577656056973
Epoch 13: loss 14.45538706439808
Epoch 14: loss 14.332877819534227
Epoch 15: loss 14.209899168807958
Epoch 16: loss 14.086162957891036
Epoch 17: loss 13.961345280040018
Epoch 18: loss 13.835049879946354
Epoch 19: loss 13.706846721020646
-----------Time: 0:03:23.751807, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 20, rmse: 3.695993423461914-------------


Epoch 0: loss 16.066177092277854
Epoch 1: loss 15.939570994530118
Epoch 2: loss 15.813344374631267
Epoch 3: loss 15.687445741946961
Epoch 4: loss 15.561707118077406
Epoch 5: loss 15.435456969544834
Epoch 6: loss 15.30778856884208
Epoch 7: loss 15.177167948722321
Epoch 8: loss 15.04113690706619
Epoch 9: loss 14.896648129021882
Epoch 10: loss 14.740037653354149
Epoch 11: loss 14.567563823152327
Epoch 12: loss 14.375903132948427
Epoch 13: loss 14.16243984907979
Epoch 14: loss 13.925146439724996
Epoch 15: loss 13.663121958541248
Epoch 16: loss 13.375969193434184
Epoch 17: loss 13.063633040023149
Epoch 18: loss 12.726954739919623
Epoch 19: loss 12.366661336254724
-----------Time: 0:04:02.235168, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 50, rmse: 3.493203639984131-------------


Epoch 0: loss 16.0661466032218
Epoch 1: loss 15.939434422041295
Epoch 2: loss 15.812553608009127
Epoch 3: loss 15.683385552369492
Epoch 4: loss 15.545813508886821
Epoch 5: loss 15.387625723315557
Epoch 6: loss 15.193135144335864
Epoch 7: loss 14.948823508780698
Epoch 8: loss 14.646505591530978
Epoch 9: loss 14.283191240631153
Epoch 10: loss 13.859568069524386
Epoch 11: loss 13.37812073014235
Epoch 12: loss 12.842844602658477
Epoch 13: loss 12.258590883376353
Epoch 14: loss 11.631135417301412
Epoch 15: loss 10.966819727660651
Epoch 16: loss 10.272349699610533
Epoch 17: loss 9.555207069701899
Epoch 18: loss 8.822873677683628
Epoch 19: loss 8.083418444617408
-----------Time: 0:05:03.933166, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 100, rmse: 2.7844440937042236-------------


Epoch 0: loss 16.06613725602076
Epoch 1: loss 15.939213229755529
Epoch 2: loss 15.809676753495815
Epoch 3: loss 15.663561442158933
Epoch 4: loss 15.4676772680796
Epoch 5: loss 15.188270792616263
Epoch 6: loss 14.809114544854468
Epoch 7: loss 14.329325812092419
Epoch 8: loss 13.755373269465386
Epoch 9: loss 13.096195798130253
Epoch 10: loss 12.36218508668021
Epoch 11: loss 11.564627103146941
Epoch 12: loss 10.71580859667066
Epoch 13: loss 9.828290910549693
Epoch 14: loss 8.91662682548303
Epoch 15: loss 7.9963104959542886
Epoch 16: loss 7.083836021988596
Epoch 17: loss 6.19652940204054
Epoch 18: loss 5.352042602157385
Epoch 19: loss 4.568397997189242
-----------Time: 0:04:44.548879, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 150, rmse: 2.06048321723938-------------


Epoch 0: loss 16.06619831503183
Epoch 1: loss 15.938227143476942
Epoch 2: loss 15.794725405126243
Epoch 3: loss 15.580792875896659
Epoch 4: loss 15.237722431336362
Epoch 5: loss 14.751936539416082
Epoch 6: loss 14.13200020180765
Epoch 7: loss 13.392142787713471
Epoch 8: loss 12.548531489763784
Epoch 9: loss 11.618250830786717
Epoch 10: loss 10.620221819418678
Epoch 11: loss 9.575950877442185
Epoch 12: loss 8.50772708391871
Epoch 13: loss 7.439766417514247
Epoch 14: loss 6.398641409466875
Epoch 15: loss 5.411794462043218
Epoch 16: loss 4.506447725609763
Epoch 17: loss 3.709674780624207
Epoch 18: loss 3.0433761884234016
Epoch 19: loss 2.51948750839083
-----------Time: 0:04:44.384719, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.579332246575683e-06, embedding_dim: 200, rmse: 1.5300335884094238-------------


Epoch 0: loss 16.046222120215543
Epoch 1: loss 15.879101485645984
Epoch 2: loss 15.712959804244505
Epoch 3: loss 15.547821422709662
Epoch 4: loss 15.383638709961817
Epoch 5: loss 15.220359748832035
Epoch 6: loss 15.057888039303188
Epoch 7: loss 14.896166778116397
Epoch 8: loss 14.735184625933648
Epoch 9: loss 14.574781249566465
Epoch 10: loss 14.414686202225056
Epoch 11: loss 14.25465776740111
Epoch 12: loss 14.09451454602356
Epoch 13: loss 13.93361987517929
Epoch 14: loss 13.771537287589194
Epoch 15: loss 13.607543732174847
Epoch 16: loss 13.440945093238918
Epoch 17: loss 13.270796560346595
Epoch 18: loss 13.096091970533958
Epoch 19: loss 12.915699480408362
-----------Time: 0:02:57.624079, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 20, rmse: 3.583986759185791-------------


Epoch 0: loss 16.045893113207338
Epoch 1: loss 15.878781382450272
Epoch 2: loss 15.712355489248553
Epoch 3: loss 15.546294862644514
Epoch 4: loss 15.379517635617196
Epoch 5: loss 15.209979074943837
Epoch 6: loss 15.033633538080727
Epoch 7: loss 14.844161474426004
Epoch 8: loss 14.63326594491184
Epoch 9: loss 14.392105894674744
Epoch 10: loss 14.113384496783224
Epoch 11: loss 13.79219354294512
Epoch 12: loss 13.426229127662994
Epoch 13: loss 13.015127853831238
Epoch 14: loss 12.560064409140857
Epoch 15: loss 12.063828995721764
Epoch 16: loss 11.529368477064738
Epoch 17: loss 10.960659245423093
Epoch 18: loss 10.362669178297365
Epoch 19: loss 9.740806204654264
-----------Time: 0:03:28.551654, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 50, rmse: 3.0751430988311768-------------


Epoch 0: loss 16.04580348882421
Epoch 1: loss 15.87818395527502
Epoch 2: loss 15.70805914911516
Epoch 3: loss 15.521596779442145
Epoch 4: loss 15.285584739656018
Epoch 5: loss 14.963553868926953
Epoch 6: loss 14.537160484994345
Epoch 7: loss 14.00349507090707
Epoch 8: loss 13.367576117098105
Epoch 9: loss 12.638399267274442
Epoch 10: loss 11.828295767080402
Epoch 11: loss 10.950966370371518
Epoch 12: loss 10.021957990979292
Epoch 13: loss 9.058387749995013
Epoch 14: loss 8.079942237300157
Epoch 15: loss 7.105918707310861
Epoch 16: loss 6.15829228693665
Epoch 17: loss 5.258793024993448
Epoch 18: loss 4.430225136359144
Epoch 19: loss 3.6937971666755076
-----------Time: 0:04:36.710030, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 100, rmse: 1.8447134494781494-------------


Epoch 0: loss 16.04579136435139
Epoch 1: loss 15.877129710064173
Epoch 2: loss 15.690104139441054
Epoch 3: loss 15.419011570306626
Epoch 4: loss 14.988173763799953
Epoch 5: loss 14.37606748308158
Epoch 6: loss 13.59634776035037
Epoch 7: loss 12.671401484107244
Epoch 8: loss 11.62730039443057
Epoch 9: loss 10.492543672465189
Epoch 10: loss 9.298887806135783
Epoch 11: loss 8.080755921665128
Epoch 12: loss 6.875538059393306
Epoch 13: loss 5.723276936016114
Epoch 14: loss 4.665415713941357
Epoch 15: loss 3.740968363190423
Epoch 16: loss 2.984211608273235
Epoch 17: loss 2.4114197133156576
Epoch 18: loss 2.011437831072784
Epoch 19: loss 1.7455115261902427
-----------Time: 0:05:17.416880, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 150, rmse: 1.2936378717422485-------------


Epoch 0: loss 16.04584830840558
Epoch 1: loss 15.873862548390441
Epoch 2: loss 15.635413310656151
Epoch 3: loss 15.184533203730705
Epoch 4: loss 14.475538866230817
Epoch 5: loss 13.54058824666756
Epoch 6: loss 12.41728659671827
Epoch 7: loss 11.148308396922822
Epoch 8: loss 9.780620427932863
Epoch 9: loss 8.365575764476118
Epoch 10: loss 6.9605205685240605
Epoch 11: loss 5.626908723915188
Epoch 12: loss 4.428152892108583
Epoch 13: loss 3.422555702351046
Epoch 14: loss 2.652439732017434
Epoch 15: loss 2.1214179241378517
Epoch 16: loss 1.7844665642339552
Epoch 17: loss 1.5728085783268202
Epoch 18: loss 1.431980810553046
Epoch 19: loss 1.3316896646291163
-----------Time: 0:05:54.488523, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.697490026177835e-06, embedding_dim: 200, rmse: 1.1463483572006226-------------


Epoch 0: loss 16.019147843884152
Epoch 1: loss 15.798746979969101
Epoch 2: loss 15.580032704640109
Epoch 3: loss 15.36283352065177
Epoch 4: loss 15.147020970303576
Epoch 5: loss 14.93236039863325
Epoch 6: loss 14.71846087592137
Epoch 7: loss 14.5048119885692
Epoch 8: loss 14.290605282122314
Epoch 9: loss 14.074564989265246
Epoch 10: loss 13.854930919172194
Epoch 11: loss 13.629436038641128
Epoch 12: loss 13.395363084773384
Epoch 13: loss 13.14987091787358
Epoch 14: loss 12.890086689742121
Epoch 15: loss 12.613467641470548
Epoch 16: loss 12.31784548922814
Epoch 17: loss 12.001807536944026
Epoch 18: loss 11.664614219261033
Epoch 19: loss 11.305710931262482
-----------Time: 0:02:35.708454, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 20, rmse: 3.338531732559204-------------


Epoch 0: loss 16.019156585380134
Epoch 1: loss 15.798763154835205
Epoch 2: loss 15.579549704899666
Epoch 3: loss 15.36004298325788
Epoch 4: loss 15.136151369010838
Epoch 5: loss 14.89827673975834
Epoch 6: loss 14.630028193382026
Epoch 7: loss 14.312073564710923
Epoch 8: loss 13.927549474196566
Epoch 9: loss 13.466362901327726
Epoch 10: loss 12.926053208200228
Epoch 11: loss 12.309528526111166
Epoch 12: loss 11.623274236609587
Epoch 13: loss 10.875563367155987
Epoch 14: loss 10.076826091173098
Epoch 15: loss 9.239701343788408
Epoch 16: loss 8.377980445376423
Epoch 17: loss 7.506962273158478
Epoch 18: loss 6.642658935285509
Epoch 19: loss 5.802399800354012
-----------Time: 0:03:08.801674, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 50, rmse: 2.331559658050537-------------


Epoch 0: loss 16.01922071051377
Epoch 1: loss 15.797445085659307
Epoch 2: loss 15.563472079867704
Epoch 3: loss 15.264305822093439
Epoch 4: loss 14.817014540972561
Epoch 5: loss 14.175326164031949
Epoch 6: loss 13.340882507467347
Epoch 7: loss 12.339649787216228
Epoch 8: loss 11.20475775084462
Epoch 9: loss 9.972875677806778
Epoch 10: loss 8.685821899582084
Epoch 11: loss 7.3879985831365955
Epoch 12: loss 6.128483792632219
Epoch 13: loss 4.957015586845767
Epoch 14: loss 3.924500088577623
Epoch 15: loss 3.075592034274045
Epoch 16: loss 2.4360109291496714
Epoch 17: loss 1.997553953241563
Epoch 18: loss 1.7154359967245751
Epoch 19: loss 1.5333173958227126
-----------Time: 0:04:10.860440, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 100, rmse: 1.2203290462493896-------------


Epoch 0: loss 16.019141008958925
Epoch 1: loss 15.792308679801605
Epoch 2: loss 15.47978983475113
Epoch 3: loss 14.895780954106856
Epoch 4: loss 13.985632187520245
Epoch 5: loss 12.79816101244314
Epoch 6: loss 11.396215041608121
Epoch 7: loss 9.850369136575903
Epoch 8: loss 8.237868913170304
Epoch 9: loss 6.6455133040357115
Epoch 10: loss 5.164445898920508
Epoch 11: loss 3.8868593815683736
Epoch 12: loss 2.8914315391325833
Epoch 13: loss 2.2097518020119598
Epoch 14: loss 1.7962862996059374
Epoch 15: loss 1.5539066783431563
Epoch 16: loss 1.4017749040135876
Epoch 17: loss 1.2977280231874362
Epoch 18: loss 1.222679035115203
Epoch 19: loss 1.1671686784089292
-----------Time: 0:04:54.681582, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 150, rmse: 1.0794607400894165-------------


Epoch 0: loss 16.018787296911242
Epoch 1: loss 15.765929516402842
Epoch 2: loss 15.241030004635137
Epoch 3: loss 14.249517606483199
Epoch 4: loss 12.868881969752682
Epoch 5: loss 11.2052815182174
Epoch 6: loss 9.371747184538725
Epoch 7: loss 7.492306888719303
Epoch 8: loss 5.704716618000132
Epoch 9: loss 4.15096712566187
Epoch 10: loss 2.956444987400256
Epoch 11: loss 2.174974645947295
Epoch 12: loss 1.735440684797454
Epoch 13: loss 1.4934759256695846
Epoch 14: loss 1.3459882351806334
Epoch 15: loss 1.247138158159453
Epoch 16: loss 1.177748701798002
Epoch 17: loss 1.128064011833861
Epoch 18: loss 1.092047520083147
Epoch 19: loss 1.0657145446120817
-----------Time: 0:05:42.990216, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.1497569953977357e-05, embedding_dim: 200, rmse: 1.0363801717758179-------------


Epoch 0: loss 15.984215040450643
Epoch 1: loss 15.693877410214514
Epoch 2: loss 15.40648655479145
Epoch 3: loss 15.121580308776242
Epoch 4: loss 14.839023636241786
Epoch 5: loss 14.55817074773103
Epoch 6: loss 14.277701147099174
Epoch 7: loss 13.995561438970686
Epoch 8: loss 13.708684964047753
Epoch 9: loss 13.412381936830952
Epoch 10: loss 13.100681857741222
Epoch 11: loss 12.767314377007374
Epoch 12: loss 12.406444356388343
Epoch 13: loss 12.012724163870633
Epoch 14: loss 11.582527141716248
Epoch 15: loss 11.11401164110359
Epoch 16: loss 10.607384564242588
Epoch 17: loss 10.064533805639735
Epoch 18: loss 9.488760869356522
Epoch 19: loss 8.884760062159112
-----------Time: 0:03:46.845894, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 20, rmse: 2.9341700077056885-------------


Epoch 0: loss 15.983970386169085
Epoch 1: loss 15.693231466478988
Epoch 2: loss 15.403120786097466
Epoch 3: loss 15.104421804624126
Epoch 4: loss 14.768724658555346
Epoch 5: loss 14.347648330458226
Epoch 6: loss 13.795887029190437
Epoch 7: loss 13.095031160320389
Epoch 8: loss 12.250706555727968
Epoch 9: loss 11.281961998516868
Epoch 10: loss 10.215154116020182
Epoch 11: loss 9.08118925968417
Epoch 12: loss 7.914259438024648
Epoch 13: loss 6.751226264669429
Epoch 14: loss 5.63158111898973
Epoch 15: loss 4.59615911349711
Epoch 16: loss 3.6858346514756293
Epoch 17: loss 2.934260617137409
Epoch 18: loss 2.361475220868742
Epoch 19: loss 1.9609428606583024
-----------Time: 0:03:18.604984, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 50, rmse: 1.3564426898956299-------------


Epoch 0: loss 15.98388441675963
Epoch 1: loss 15.68906729450817
Epoch 2: loss 15.335130716938892
Epoch 3: loss 14.742021624195376
Epoch 4: loss 13.797019525219916
Epoch 5: loss 12.52688004232866
Epoch 6: loss 11.01151533189061
Epoch 7: loss 9.341551700968532
Epoch 8: loss 7.6186860923601145
Epoch 9: loss 5.953409241489122
Epoch 10: loss 4.460535612101137
Epoch 11: loss 3.2496374544726563
Epoch 12: loss 2.3909133931357553
Epoch 13: loss 1.8684924220572094
Epoch 14: loss 1.576242010771029
Epoch 15: loss 1.4040408752708735
Epoch 16: loss 1.2915510065112183
Epoch 17: loss 1.2128452219802313
Epoch 18: loss 1.1560092445071979
Epoch 19: loss 1.1143433599286652
-----------Time: 0:03:45.437983, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 100, rmse: 1.0570628643035889-------------


Epoch 0: loss 15.98339932816647
Epoch 1: loss 15.645959768819058
Epoch 2: loss 14.950612162299102
Epoch 3: loss 13.672775850145632
Epoch 4: loss 11.930442430715058
Epoch 5: loss 9.897064035259037
Epoch 6: loss 7.754687495436468
Epoch 7: loss 5.706348994687046
Epoch 8: loss 3.9622998144264905
Epoch 9: loss 2.6972515650778246
Epoch 10: loss 1.9532771929895962
Epoch 11: loss 1.5778406242626786
Epoch 12: loss 1.3781458856036573
Epoch 13: loss 1.2561070997310502
Epoch 14: loss 1.1754341890074236
Epoch 15: loss 1.1204484247558464
Epoch 16: loss 1.0824302074181897
Epoch 17: loss 1.0557752663088338
Epoch 18: loss 1.0369245670553522
Epoch 19: loss 1.0233934142785335
-----------Time: 0:04:33.351393, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 150, rmse: 1.0177477598190308-------------


Epoch 0: loss 15.982845616509177
Epoch 1: loss 15.565905784640123
Epoch 2: loss 14.4517788915546
Epoch 3: loss 12.5836552787955
Epoch 4: loss 10.257220853729828
Epoch 5: loss 7.762643644769533
Epoch 6: loss 5.411824369184215
Epoch 7: loss 3.5196021823794896
Epoch 8: loss 2.3051586427592663
Epoch 9: loss 1.7018790285246341
Epoch 10: loss 1.4216836776955093
Epoch 11: loss 1.2685225996522556
Epoch 12: loss 1.1740772162623352
Epoch 13: loss 1.1131907001405128
Epoch 14: loss 1.073219588264426
Epoch 15: loss 1.046583761301554
Epoch 16: loss 1.028540606664665
Epoch 17: loss 1.0161419640811258
Epoch 18: loss 1.0074850769066306
Epoch 19: loss 1.0013769680776954
-----------Time: 0:05:11.987069, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 1.5199110829529332e-05, embedding_dim: 200, rmse: 1.0078343152999878-------------


Epoch 0: loss 15.93746256244902
Epoch 1: loss 15.555007605788628
Epoch 2: loss 15.177221969782885
Epoch 3: loss 14.803419533211489
Epoch 4: loss 14.431404993027174
Epoch 5: loss 14.057540040485492
Epoch 6: loss 13.674339867467916
Epoch 7: loss 13.27063020070394
Epoch 8: loss 12.833783037186706
Epoch 9: loss 12.351505625435943
Epoch 10: loss 11.814113033018792
Epoch 11: loss 11.216023856625082
Epoch 12: loss 10.555868118207307
Epoch 13: loss 9.836824663441229
Epoch 14: loss 9.066825496430887
Epoch 15: loss 8.256603281156988
Epoch 16: loss 7.4204360137107646
Epoch 17: loss 6.574842818992951
Epoch 18: loss 5.738982361124028
Epoch 19: loss 4.9338793194507895
-----------Time: 0:03:38.755170, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 20, rmse: 2.141944169998169-------------


Epoch 0: loss 15.937256690663705
Epoch 1: loss 15.553594716545549
Epoch 2: loss 15.162960990866349
Epoch 3: loss 14.711671258782225
Epoch 4: loss 14.090606964042875
Epoch 5: loss 13.21766961560812
Epoch 6: loss 12.090785106152799
Epoch 7: loss 10.757079987113666
Epoch 8: loss 9.281477581618466
Epoch 9: loss 7.740605398388645
Epoch 10: loss 6.219553287551739
Epoch 11: loss 4.81096465695222
Epoch 12: loss 3.6064506576008095
Epoch 13: loss 2.6806913083049513
Epoch 14: loss 2.059321448393267
Epoch 15: loss 1.6906841612821562
Epoch 16: loss 1.4776787876175304
Epoch 17: loss 1.3446137509377123
Epoch 18: loss 1.2538503265659597
Epoch 19: loss 1.1886427036571139
-----------Time: 0:03:52.934313, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 50, rmse: 1.0876151323318481-------------


Epoch 0: loss 15.937002627469717
Epoch 1: loss 15.525410566335141
Epoch 2: loss 14.831507433361823
Epoch 3: loss 13.524930967981236
Epoch 4: loss 11.672165668159804
Epoch 5: loss 9.486013238879893
Epoch 6: loss 7.203991071499839
Epoch 7: loss 5.085247691783521
Epoch 8: loss 3.3845384997994827
Epoch 9: loss 2.275208895299796
Epoch 10: loss 1.703413848678595
Epoch 11: loss 1.4300577381566013
Epoch 12: loss 1.2793008063136397
Epoch 13: loss 1.1850691402716893
Epoch 14: loss 1.123211208084733
Epoch 15: loss 1.0816752697994945
Epoch 16: loss 1.053436356889871
Epoch 17: loss 1.0340014988958608
Epoch 18: loss 1.0204030399480677
Epoch 19: loss 1.0107838004876117
-----------Time: 0:05:02.862350, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 100, rmse: 1.012104868888855-------------


Epoch 0: loss 15.935695695384421
Epoch 1: loss 15.372382632282003
Epoch 2: loss 13.902944111577709
Epoch 3: loss 11.544389472181477
Epoch 4: loss 8.75055341476847
Epoch 5: loss 5.979528457072197
Epoch 6: loss 3.708093571688832
Epoch 7: loss 2.2854694224946717
Epoch 8: loss 1.6371879810674999
Epoch 9: loss 1.3617810255454377
Epoch 10: loss 1.2178222799573404
Epoch 11: loss 1.1330697286414217
Epoch 12: loss 1.0811466020757312
Epoch 13: loss 1.0486576812885713
Epoch 14: loss 1.0279190080233278
Epoch 15: loss 1.014395735959374
Epoch 16: loss 1.005420199939257
Epoch 17: loss 0.9993494297577804
Epoch 18: loss 0.99517089190076
Epoch 19: loss 0.992254146371217
-----------Time: 0:04:02.921957, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 150, rmse: 1.0035890340805054-------------


Epoch 0: loss 15.930798613551909
Epoch 1: loss 15.044752506538213
Epoch 2: loss 12.71920940835817
Epoch 3: loss 9.54461010284175
Epoch 4: loss 6.275327777266178
Epoch 5: loss 3.63573459135961
Epoch 6: loss 2.123157392545133
Epoch 7: loss 1.5251315843157953
Epoch 8: loss 1.2846056259054408
Epoch 9: loss 1.1611820377689006
Epoch 10: loss 1.091728833850925
Epoch 11: loss 1.0514874516329473
Epoch 12: loss 1.027504138547224
Epoch 13: loss 1.012866777220519
Epoch 14: loss 1.0036644513772972
Epoch 15: loss 0.9977892962663701
Epoch 16: loss 0.9939134675825076
Epoch 17: loss 0.991337970166572
Epoch 18: loss 0.989587147917159
Epoch 19: loss 0.9883638227225776
-----------Time: 0:04:50.454815, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.0092330025650458e-05, embedding_dim: 200, rmse: 1.001562237739563-------------


Epoch 0: loss 15.87640042665408
Epoch 1: loss 15.373748433401948
Epoch 2: loss 14.880096204518106
Epoch 3: loss 14.394254364482471
Epoch 4: loss 13.912875200627354
Epoch 5: loss 13.42775042514687
Epoch 6: loss 12.920454346271493
Epoch 7: loss 12.363693333761143
Epoch 8: loss 11.730381075230028
Epoch 9: loss 11.00229155984892
Epoch 10: loss 10.172470479636429
Epoch 11: loss 9.244908721243448
Epoch 12: loss 8.23513134320577
Epoch 13: loss 7.1717051167148425
Epoch 14: loss 6.092255596883794
Epoch 15: loss 5.042319676680303
Epoch 16: loss 4.070639539374289
Epoch 17: loss 3.2245223347812715
Epoch 18: loss 2.5429886936363544
Epoch 19: loss 2.045028479219844
-----------Time: 0:03:04.921184, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 20, rmse: 1.3726205825805664-------------


Epoch 0: loss 15.876231812471408
Epoch 1: loss 15.368598094772164
Epoch 2: loss 14.810788495051854
Epoch 3: loss 14.008611777089353
Epoch 4: loss 12.790727564535302
Epoch 5: loss 11.175376125883318
Epoch 6: loss 9.27748427305486
Epoch 7: loss 7.263021394180953
Epoch 8: loss 5.332082431623636
Epoch 9: loss 3.693102084404104
Epoch 10: loss 2.5171933946498
Epoch 11: loss 1.838707215439826
Epoch 12: loss 1.5032599220592215
Epoch 13: loss 1.3269175718406285
Epoch 14: loss 1.2198395940711668
Epoch 15: loss 1.1493805614484918
Epoch 16: loss 1.1014932087415454
Epoch 17: loss 1.068414747552939
Epoch 18: loss 1.0453044015827095
Epoch 19: loss 1.0289191926802677
-----------Time: 0:03:32.998594, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 50, rmse: 1.0201199054718018-------------


Epoch 0: loss 15.875234044849257
Epoch 1: loss 15.241427779392161
Epoch 2: loss 13.75188126623663
Epoch 3: loss 11.230674639676433
Epoch 4: loss 8.223812621206353
Epoch 5: loss 5.332480208130878
Epoch 6: loss 3.13575316107487
Epoch 7: loss 1.9432926227568024
Epoch 8: loss 1.469370264910433
Epoch 9: loss 1.264717775220259
Epoch 10: loss 1.1545959398304397
Epoch 11: loss 1.0905815042381122
Epoch 12: loss 1.0522771318397035
Epoch 13: loss 1.0288083999295672
Epoch 14: loss 1.014089212056539
Epoch 15: loss 1.0046452232973286
Epoch 16: loss 0.9984585707961637
Epoch 17: loss 0.9943229376200168
Epoch 18: loss 0.9915012177434416
Epoch 19: loss 0.9895437070051569
-----------Time: 0:04:43.033176, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 100, rmse: 1.0023233890533447-------------


Epoch 0: loss 15.867276265350226
Epoch 1: loss 14.718639145602735
Epoch 2: loss 11.80165147418364
Epoch 3: loss 8.040773222924834
Epoch 4: loss 4.598819826529557
Epoch 5: loss 2.4223688932590473
Epoch 6: loss 1.5702980141914558
Epoch 7: loss 1.279267052128757
Epoch 8: loss 1.1459225602225158
Epoch 9: loss 1.0766072258183073
Epoch 10: loss 1.039105023489885
Epoch 11: loss 1.0181245273095618
Epoch 12: loss 1.005969836554364
Epoch 13: loss 0.9986943585221828
Epoch 14: loss 0.9942634999752045
Epoch 15: loss 0.9914437265678746
Epoch 16: loss 0.9896155659305849
Epoch 17: loss 0.9884073747896254
Epoch 18: loss 0.9875703033479158
Epoch 19: loss 0.9869756715948521
-----------Time: 0:05:31.306592, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 150, rmse: 1.0007197856903076-------------


Epoch 0: loss 15.83769749298635
Epoch 1: loss 13.903430118322243
Epoch 2: loss 9.827473550338082
Epoch 3: loss 5.549458362344428
Epoch 4: loss 2.672045499783226
Epoch 5: loss 1.583073326933961
Epoch 6: loss 1.2588670045996309
Epoch 7: loss 1.1236829380421227
Epoch 8: loss 1.0590992307714822
Epoch 9: loss 1.02674300345537
Epoch 10: loss 1.0097545876718204
Epoch 11: loss 1.0004612046003731
Epoch 12: loss 0.9951749808798671
Epoch 13: loss 0.9920565494705375
Epoch 14: loss 0.9901687559418992
Epoch 15: loss 0.9889943712168637
Epoch 16: loss 0.9881758968132615
Epoch 17: loss 0.987644640290264
Epoch 18: loss 0.9872852241850341
Epoch 19: loss 0.987018128981598
-----------Time: 0:05:11.174039, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 2.656087782946684e-05, embedding_dim: 200, rmse: 1.0003896951675415-------------


Epoch 0: loss 15.795862733572317
Epoch 1: loss 15.1358945736618
Epoch 2: loss 14.490008511836274
Epoch 3: loss 13.850423849944903
Epoch 4: loss 13.19157015441098
Epoch 5: loss 12.45883795721107
Epoch 6: loss 11.584717970947134
Epoch 7: loss 10.523777529751237
Epoch 8: loss 9.276488158675624
Epoch 9: loss 7.886091016828527
Epoch 10: loss 6.431086317620373
Epoch 11: loss 5.015162122774669
Epoch 12: loss 3.7499971632856335
Epoch 13: loss 2.739492599409258
Epoch 14: loss 2.044051204381137
Epoch 15: loss 1.636834588013245
Epoch 16: loss 1.415618092486105
Epoch 17: loss 1.2869689267021343
Epoch 18: loss 1.2037132194082396
Epoch 19: loss 1.1460715193765327
-----------Time: 0:02:42.383657, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 20, rmse: 1.0695338249206543-------------


Epoch 0: loss 15.795254289617741
Epoch 1: loss 15.109383007182318
Epoch 2: loss 14.136903011390475
Epoch 3: loss 12.401529647916345
Epoch 4: loss 9.963448692755312
Epoch 5: loss 7.241631866473747
Epoch 6: loss 4.719857795722073
Epoch 7: loss 2.854795211291041
Epoch 8: loss 1.8504211338335175
Epoch 9: loss 1.438952539815016
Epoch 10: loss 1.2543288539647408
Epoch 11: loss 1.1524894148386842
Epoch 12: loss 1.0917083757801378
Epoch 13: loss 1.0543190159993434
Epoch 14: loss 1.0308008034505425
Epoch 15: loss 1.015714932761807
Epoch 16: loss 1.0058204788692364
Epoch 17: loss 0.9992297989249942
Epoch 18: loss 0.9947163195244445
Epoch 19: loss 0.9916107289132506
-----------Time: 0:03:11.420766, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 50, rmse: 1.0035167932510376-------------


Epoch 0: loss 15.789380821862773
Epoch 1: loss 14.582483450053113
Epoch 2: loss 11.507397050634554
Epoch 3: loss 7.47575518163149
Epoch 4: loss 3.9813887433165633
Epoch 5: loss 2.056509065362278
Epoch 6: loss 1.421486947736123
Epoch 7: loss 1.202602337906061
Epoch 8: loss 1.1008799252236519
Epoch 9: loss 1.049533914699705
Epoch 10: loss 1.022581947569357
Epoch 11: loss 1.0077923521553451
Epoch 12: loss 0.9993817460737908
Epoch 13: loss 0.9944262924629945
Epoch 14: loss 0.9914246644741689
Epoch 15: loss 0.9895342610464464
Epoch 16: loss 0.9882995504247811
Epoch 17: loss 0.9874867319477324
Epoch 18: loss 0.9868760560115049
Epoch 19: loss 0.9864683690895653
-----------Time: 0:04:13.686206, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 100, rmse: 1.0004650354385376-------------


Epoch 0: loss 15.744766766899238
Epoch 1: loss 13.281250019706158
Epoch 2: loss 8.386509567191252
Epoch 3: loss 3.9768886879904883
Epoch 4: loss 1.8628358475017184
Epoch 5: loss 1.3113026218514134
Epoch 6: loss 1.133124166140419
Epoch 7: loss 1.0574507254047716
Epoch 8: loss 1.0230585376065084
Epoch 9: loss 1.00657456192781
Epoch 10: loss 0.9981995549877419
Epoch 11: loss 0.9937149732737777
Epoch 12: loss 0.9912156211804799
Epoch 13: loss 0.989764038621245
Epoch 14: loss 0.9888300210513
Epoch 15: loss 0.9882330964965105
Epoch 16: loss 0.9878377977631025
Epoch 17: loss 0.9875717256592952
Epoch 18: loss 0.9873182826420741
Epoch 19: loss 0.9871873280125639
-----------Time: 0:05:05.457590, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 150, rmse: 1.0002390146255493-------------


Epoch 0: loss 15.625045290714116
Epoch 1: loss 11.763365023158178
Epoch 2: loss 5.935135270474461
Epoch 3: loss 2.344732324120271
Epoch 4: loss 1.3776659800702946
Epoch 5: loss 1.141375164660894
Epoch 6: loss 1.0546138563546361
Epoch 7: loss 1.0193906723966801
Epoch 8: loss 1.003963107872165
Epoch 9: loss 0.9967343296254051
Epoch 10: loss 0.99312533101937
Epoch 11: loss 0.9911872926064326
Epoch 12: loss 0.9901454197673321
Epoch 13: loss 0.9894630673496927
Epoch 14: loss 0.989048715500549
Epoch 15: loss 0.9887562692813604
Epoch 16: loss 0.9885615258517636
Epoch 17: loss 0.988413787280301
Epoch 18: loss 0.9882949636707233
Epoch 19: loss 0.9881949568488405
-----------Time: 0:06:00.841540, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 3.511191734215127e-05, embedding_dim: 200, rmse: 1.000287413597107-------------


Epoch 0: loss 15.690100984900072
Epoch 1: loss 14.824496171732452
Epoch 2: loss 13.97492557041278
Epoch 3: loss 13.086720318299003
Epoch 4: loss 12.027538867409039
Epoch 5: loss 10.657160890173692
Epoch 6: loss 8.94985006061697
Epoch 7: loss 7.032434241417765
Epoch 8: loss 5.125347958108923
Epoch 9: loss 3.4817770951594134
Epoch 10: loss 2.3179879830284182
Epoch 11: loss 1.6803788171312355
Epoch 12: loss 1.3873855890204816
Epoch 13: loss 1.240992332765627
Epoch 14: loss 1.15506503013504
Epoch 15: loss 1.100357980151874
Epoch 16: loss 1.0642812683440992
Epoch 17: loss 1.039995443989493
Epoch 18: loss 1.0233763046663957
Epoch 19: loss 1.0118500141587965
-----------Time: 0:03:40.109879, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 20, rmse: 1.0129539966583252-------------


Epoch 0: loss 15.688818260792353
Epoch 1: loss 14.721005225842774
Epoch 2: loss 12.867498760057442
Epoch 3: loss 9.703251751173184
Epoch 4: loss 6.132194725698333
Epoch 5: loss 3.2853539283992546
Epoch 6: loss 1.8377387476006302
Epoch 7: loss 1.3612937170763
Epoch 8: loss 1.1816109272855466
Epoch 9: loss 1.0936252917687228
Epoch 10: loss 1.04724239077887
Epoch 11: loss 1.0218907529371728
Epoch 12: loss 1.0075443686027121
Epoch 13: loss 0.9991126729620093
Epoch 14: loss 0.9940004402071707
Epoch 15: loss 0.9908082501890868
Epoch 16: loss 0.988759497444937
Epoch 17: loss 0.9873887506466057
Epoch 18: loss 0.9864411243055747
Epoch 19: loss 0.9857467247812304
-----------Time: 0:02:50.707956, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 50, rmse: 1.0006061792373657-------------


Epoch 0: loss 15.657589738429921
Epoch 1: loss 13.16264612017409
Epoch 2: loss 7.9552815743540215
Epoch 3: loss 3.4767417003435566
Epoch 4: loss 1.6522457532872319
Epoch 5: loss 1.2321073350341116
Epoch 6: loss 1.0936367371731832
Epoch 7: loss 1.0370571589489357
Epoch 8: loss 1.0122721006748403
Epoch 9: loss 1.0006091996435889
Epoch 10: loss 0.9947992620615936
Epoch 11: loss 0.9917490757017307
Epoch 12: loss 0.9900392458256073
Epoch 13: loss 0.9890270651423977
Epoch 14: loss 0.9883896672187648
Epoch 15: loss 0.9879494598535431
Epoch 16: loss 0.9876311426094008
Epoch 17: loss 0.9873833276286859
Epoch 18: loss 0.9871710394957326
Epoch 19: loss 0.9870215447588159
-----------Time: 0:03:57.136062, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 100, rmse: 1.0001400709152222-------------


Epoch 0: loss 15.474505053971889
Epoch 1: loss 10.681785662839308
Epoch 2: loss 4.396404312900255
Epoch 3: loss 1.7073498040259436
Epoch 4: loss 1.204276682469688
Epoch 5: loss 1.0693510971586622
Epoch 6: loss 1.0223451160976198
Epoch 7: loss 1.0043230706125967
Epoch 8: loss 0.9966942220521142
Epoch 9: loss 0.9932363410239767
Epoch 10: loss 0.9915529344313945
Epoch 11: loss 0.9905787999860743
Epoch 12: loss 0.9900561474210999
Epoch 13: loss 0.9896787151232435
Epoch 14: loss 0.9894289082486452
Epoch 15: loss 0.9892248689739389
Epoch 16: loss 0.9890878926223228
Epoch 17: loss 0.9889399242861365
Epoch 18: loss 0.9887932058162441
Epoch 19: loss 0.9886866706489804
-----------Time: 0:04:39.370403, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 150, rmse: 1.000096082687378-------------


Epoch 0: loss 15.145362430580807
Epoch 1: loss 8.541550179549441
Epoch 2: loss 2.6991116735454224
Epoch 3: loss 1.324001983221359
Epoch 4: loss 1.0930322006126019
Epoch 5: loss 1.0270871208730503
Epoch 6: loss 1.0054101349867965
Epoch 7: loss 0.9973786361778865
Epoch 8: loss 0.9940363499475212
Epoch 9: loss 0.9924623423163303
Epoch 10: loss 0.9917099721701396
Epoch 11: loss 0.9912160557207043
Epoch 12: loss 0.9909809485215607
Epoch 13: loss 0.9908080613762221
Epoch 14: loss 0.9906112864945751
Epoch 15: loss 0.9904807188577014
Epoch 16: loss 0.9903647363218294
Epoch 17: loss 0.990250337733206
Epoch 18: loss 0.990148366842664
Epoch 19: loss 0.9899902133561789
-----------Time: 0:05:24.943624, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 4.641588833612791e-05, embedding_dim: 200, rmse: 1.0002838373184204-------------


Epoch 0: loss 15.550743958170871
Epoch 1: loss 14.41719458270423
Epoch 2: loss 13.267888047372342
Epoch 3: loss 11.841899512188794
Epoch 4: loss 9.8546493599245
Epoch 5: loss 7.378316173480865
Epoch 6: loss 4.859778304084479
Epoch 7: loss 2.8811115232286664
Epoch 8: loss 1.7929169404370555
Epoch 9: loss 1.372359569173847
Epoch 10: loss 1.2020798290210162
Epoch 11: loss 1.1142095232897962
Epoch 12: loss 1.0640132941747242
Epoch 13: loss 1.0342079998359142
Epoch 14: loss 1.016018657126201
Epoch 15: loss 1.0046415384078946
Epoch 16: loss 0.9973513904863754
Epoch 17: loss 0.9925312728354178
Epoch 18: loss 0.9892674359275699
Epoch 19: loss 0.9870121218122303
-----------Time: 0:03:34.603106, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 20, rmse: 1.002099871635437-------------


Epoch 0: loss 15.545203803997444
Epoch 1: loss 13.879568441462556
Epoch 2: loss 10.028998510678091
Epoch 3: loss 5.337369462275648
Epoch 4: loss 2.333748178128123
Epoch 5: loss 1.4036447836401154
Epoch 6: loss 1.16264445187606
Epoch 7: loss 1.0684998518037303
Epoch 8: loss 1.02712447728198
Epoch 9: loss 1.0078094318355149
Epoch 10: loss 0.9981747551772049
Epoch 11: loss 0.993136637621823
Epoch 12: loss 0.9903211517785152
Epoch 13: loss 0.9886630524624425
Epoch 14: loss 0.9876272096913946
Epoch 15: loss 0.9869059399459076
Epoch 16: loss 0.9864145851835341
Epoch 17: loss 0.985970785982133
Epoch 18: loss 0.9856942914276424
Epoch 19: loss 0.9854139336235437
-----------Time: 0:03:49.739971, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 50, rmse: 0.9999927878379822-------------


Epoch 0: loss 15.385991407387149
Epoch 1: loss 10.380591928732013
Epoch 2: loss 3.8773903719687866
Epoch 3: loss 1.5307166288085707
Epoch 4: loss 1.1483390509207136
Epoch 5: loss 1.0462273270167237
Epoch 6: loss 1.0123909787194374
Epoch 7: loss 0.9997749939893108
Epoch 8: loss 0.9946173923925707
Epoch 9: loss 0.992261148766372
Epoch 10: loss 0.9910255771347075
Epoch 11: loss 0.9904038588789379
Epoch 12: loss 0.9899503179968149
Epoch 13: loss 0.9896655054990853
Epoch 14: loss 0.9894247687076861
Epoch 15: loss 0.9892356390859719
Epoch 16: loss 0.9890286943360215
Epoch 17: loss 0.9888049619168027
Epoch 18: loss 0.9886869209463536
Epoch 19: loss 0.9884114000478343
-----------Time: 0:04:19.626555, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 100, rmse: 1.0001604557037354-------------


Epoch 0: loss 14.891259504051686
Epoch 1: loss 7.145649457314923
Epoch 2: loss 1.9544316687423162
Epoch 3: loss 1.1788932473002212
Epoch 4: loss 1.0451178126130303
Epoch 5: loss 1.009892575762854
Epoch 6: loss 0.9990251747371711
Epoch 7: loss 0.995096898584537
Epoch 8: loss 0.9935244878327607
Epoch 9: loss 0.9927525566511014
Epoch 10: loss 0.9922891642620798
Epoch 11: loss 0.9919809008694784
Epoch 12: loss 0.9918126956219904
Epoch 13: loss 0.9915402952324378
Epoch 14: loss 0.9914366891845664
Epoch 15: loss 0.991246336517028
Epoch 16: loss 0.9910662697669668
Epoch 17: loss 0.9908965593482438
Epoch 18: loss 0.9906317978335697
Epoch 19: loss 0.9904588219943158
-----------Time: 0:04:16.137531, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 150, rmse: 1.0004639625549316-------------


Epoch 0: loss 14.114045692825524
Epoch 1: loss 4.857668507027847
Epoch 2: loss 1.4195575232315218
Epoch 3: loss 1.0817923066148036
Epoch 4: loss 1.0175298765147751
Epoch 5: loss 1.0016756797530457
Epoch 6: loss 0.9969110089582617
Epoch 7: loss 0.9952051207350284
Epoch 8: loss 0.9944457163818509
Epoch 9: loss 0.993953070493416
Epoch 10: loss 0.9937571776405892
Epoch 11: loss 0.993552236922608
Epoch 12: loss 0.9933897881257657
Epoch 13: loss 0.9932426492308326
Epoch 14: loss 0.9930141584551678
Epoch 15: loss 0.9928448468592583
Epoch 16: loss 0.992640899746492
Epoch 17: loss 0.9923950400833463
Epoch 18: loss 0.9921728774475753
Epoch 19: loss 0.991852854470185
-----------Time: 0:05:00.424738, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 6.135907273413189e-05, embedding_dim: 200, rmse: 1.0004819631576538-------------


Epoch 0: loss 15.368592758811383
Epoch 1: loss 13.875403778652837
Epoch 2: loss 12.140898710233223
Epoch 3: loss 9.474128075952825
Epoch 4: loss 6.070287644376957
Epoch 5: loss 3.1753536582057924
Epoch 6: loss 1.7161570253937448
Epoch 7: loss 1.27965961274341
Epoch 8: loss 1.13006819537504
Epoch 9: loss 1.0610782232473828
Epoch 10: loss 1.026467020094168
Epoch 11: loss 1.008274418186921
Epoch 12: loss 0.9982581326111041
Epoch 13: loss 0.9924779316697061
Epoch 14: loss 0.9889805059254072
Epoch 15: loss 0.9867494219732778
Epoch 16: loss 0.9852515367498341
Epoch 17: loss 0.9841851565324463
Epoch 18: loss 0.9833811359472934
Epoch 19: loss 0.982698141880305
-----------Time: 0:03:08.243820, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 20, rmse: 1.0001391172409058-------------


Epoch 0: loss 15.343662639491386
Epoch 1: loss 12.07773589828599
Epoch 2: loss 5.8317376726538415
Epoch 3: loss 2.0570710394420075
Epoch 4: loss 1.2533220407462367
Epoch 5: loss 1.082144740533414
Epoch 6: loss 1.0255667216075122
Epoch 7: loss 1.0046944145990364
Epoch 8: loss 0.9961228917447558
Epoch 9: loss 0.9922646206964157
Epoch 10: loss 0.990408360925688
Epoch 11: loss 0.9893144339468896
Epoch 12: loss 0.9886832122183028
Epoch 13: loss 0.988154615540424
Epoch 14: loss 0.9878040719544389
Epoch 15: loss 0.9874315061700156
Epoch 16: loss 0.9870897681250269
Epoch 17: loss 0.9868160974985883
Epoch 18: loss 0.9864680415881335
Epoch 19: loss 0.9861137088775116
-----------Time: 0:03:45.621538, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 50, rmse: 0.9998927116394043-------------


Epoch 0: loss 14.751594463076652
Epoch 1: loss 6.5604056912054505
Epoch 2: loss 1.7078274879493163
Epoch 3: loss 1.1280302796563486
Epoch 4: loss 1.0290869696267342
Epoch 5: loss 1.0045556826329607
Epoch 6: loss 0.9971823925522114
Epoch 7: loss 0.9945519364126485
Epoch 8: loss 0.9934468589735265
Epoch 9: loss 0.9928365735303714
Epoch 10: loss 0.9925340184736278
Epoch 11: loss 0.9921811985023129
Epoch 12: loss 0.9919283671052382
Epoch 13: loss 0.9917183435598491
Epoch 14: loss 0.991469740041123
Epoch 15: loss 0.991244043326365
Epoch 16: loss 0.990971933343347
Epoch 17: loss 0.9906134465552335
Epoch 18: loss 0.99033507821416
Epoch 19: loss 0.9900089337522404
-----------Time: 0:04:56.447885, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 100, rmse: 1.0003712177276611-------------


Epoch 0: loss 13.666875149128941
Epoch 1: loss 3.7247281199307984
Epoch 2: loss 1.2387240862898232
Epoch 3: loss 1.041729375728127
Epoch 4: loss 1.0071072441653883
Epoch 5: loss 0.9991262724148053
Epoch 6: loss 0.9966818130716414
Epoch 7: loss 0.9959454530117757
Epoch 8: loss 0.9954152132033524
Epoch 9: loss 0.9950953263378195
Epoch 10: loss 0.9948223707726496
Epoch 11: loss 0.9945224071242617
Epoch 12: loss 0.9942832975980055
Epoch 13: loss 0.994070509794032
Epoch 14: loss 0.9937630022200895
Epoch 15: loss 0.9933266662235426
Epoch 16: loss 0.9929736646995264
Epoch 17: loss 0.9925311945779872
Epoch 18: loss 0.9919118193454752
Epoch 19: loss 0.9914219665618096
-----------Time: 0:05:35.355693, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 150, rmse: 1.0003961324691772-------------


Epoch 0: loss 12.438890590429176
Epoch 1: loss 2.399627313188005
Epoch 2: loss 1.116234440285593
Epoch 3: loss 1.0186935111742554
Epoch 4: loss 1.0028199567588663
Epoch 5: loss 0.9992409352863398
Epoch 6: loss 0.9981131900128234
Epoch 7: loss 0.9976834442837204
Epoch 8: loss 0.9973598095371128
Epoch 9: loss 0.9971061014429002
Epoch 10: loss 0.9968659140092644
Epoch 11: loss 0.9965434232245586
Epoch 12: loss 0.9963141088903177
Epoch 13: loss 0.9960802719280601
Epoch 14: loss 0.9956293475083386
Epoch 15: loss 0.9951879065959851
Epoch 16: loss 0.9946080749290284
Epoch 17: loss 0.9940873975984553
Epoch 18: loss 0.9934487369411856
Epoch 19: loss 0.9925473916621361
-----------Time: 0:04:31.577612, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 8.11130830789689e-05, embedding_dim: 200, rmse: 1.0009548664093018-------------


Epoch 0: loss 15.13122027383154
Epoch 1: loss 13.076785706514377
Epoch 2: loss 9.739329312274222
Epoch 3: loss 5.124469493963979
Epoch 4: loss 2.1213980536541257
Epoch 5: loss 1.2988589333898026
Epoch 6: loss 1.1092989620888862
Epoch 7: loss 1.0402041081827318
Epoch 8: loss 1.0115177463811011
Epoch 9: loss 0.9986478715827116
Epoch 10: loss 0.992393857422181
Epoch 11: loss 0.9891227683727997
Epoch 12: loss 0.9872089359441356
Epoch 13: loss 0.9860072009034492
Epoch 14: loss 0.9851505857663935
Epoch 15: loss 0.9845105342692562
Epoch 16: loss 0.9839151838572535
Epoch 17: loss 0.9833878838477931
Epoch 18: loss 0.9829092051523933
Epoch 19: loss 0.9824017334107539
-----------Time: 0:02:51.937321, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 20, rmse: 0.9998835325241089-------------


Epoch 0: loss 14.973383881204345
Epoch 1: loss 8.754271302163568
Epoch 2: loss 2.402657252929339
Epoch 3: loss 1.2133243667846014
Epoch 4: loss 1.0508385510824503
Epoch 5: loss 1.010297846719582
Epoch 6: loss 0.9982496608917709
Epoch 7: loss 0.9940071051343748
Epoch 8: loss 0.9922591622527881
Epoch 9: loss 0.9913177267729815
Epoch 10: loss 0.9907352288920572
Epoch 11: loss 0.990266744924538
Epoch 12: loss 0.9899063394924037
Epoch 13: loss 0.9894884992644085
Epoch 14: loss 0.9890926166388443
Epoch 15: loss 0.9886359593802136
Epoch 16: loss 0.9881601873592815
Epoch 17: loss 0.9876337507744728
Epoch 18: loss 0.9870291457771283
Epoch 19: loss 0.9863798010602083
-----------Time: 0:03:19.418911, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 50, rmse: 0.9997771978378296-------------


Epoch 0: loss 13.51899068844325
Epoch 1: loss 3.352106305488495
Epoch 2: loss 1.1801776892630673
Epoch 3: loss 1.0288230462071686
Epoch 4: loss 1.0039907272838262
Epoch 5: loss 0.9984349326279709
Epoch 6: loss 0.9968444745523765
Epoch 7: loss 0.9960424250205487
Epoch 8: loss 0.9955940489019629
Epoch 9: loss 0.9952682501729122
Epoch 10: loss 0.994855254629243
Epoch 11: loss 0.9943949212322681
Epoch 12: loss 0.994036452367683
Epoch 13: loss 0.9934735604768995
Epoch 14: loss 0.9929205653880069
Epoch 15: loss 0.9922310688561755
Epoch 16: loss 0.9915476442517762
Epoch 17: loss 0.9904956286079278
Epoch 18: loss 0.9893537421729527
Epoch 19: loss 0.9881207286630914
-----------Time: 0:04:20.719061, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 100, rmse: 0.9996294975280762-------------


Epoch 0: loss 11.735510273755018
Epoch 1: loss 1.8487968095590914
Epoch 2: loss 1.0651843997529824
Epoch 3: loss 1.0101789073201222
Epoch 4: loss 1.0020618158037602
Epoch 5: loss 1.0002922241197714
Epoch 6: loss 0.9994607015334114
Epoch 7: loss 0.9991078915124098
Epoch 8: loss 0.9986251236920515
Epoch 9: loss 0.9984011382042811
Epoch 10: loss 0.9979072913422151
Epoch 11: loss 0.9974234250007991
Epoch 12: loss 0.996836561314628
Epoch 13: loss 0.9962585854134914
Epoch 14: loss 0.9953867977180191
Epoch 15: loss 0.9944693855191781
Epoch 16: loss 0.9932975496624785
Epoch 17: loss 0.9919200763389946
Epoch 18: loss 0.9900002608550769
Epoch 19: loss 0.9878895149858442
-----------Time: 0:05:12.090030, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 150, rmse: 0.9996760487556458-------------


Epoch 0: loss 10.325576370341937
Epoch 1: loss 1.4049839024442639
Epoch 2: loss 1.0336218988720394
Epoch 3: loss 1.0071028451546176
Epoch 4: loss 1.0033550340400694
Epoch 5: loss 1.0023493159394214
Epoch 6: loss 1.0018542862865445
Epoch 7: loss 1.0015322521304617
Epoch 8: loss 1.0010966341122134
Epoch 9: loss 1.0007278706438842
Epoch 10: loss 1.0002747386321222
Epoch 11: loss 0.9996266793044383
Epoch 12: loss 0.9987680718683561
Epoch 13: loss 0.9977695308281327
Epoch 14: loss 0.9966234515515795
Epoch 15: loss 0.9951431939399651
Epoch 16: loss 0.9929673327674938
Epoch 17: loss 0.9905718982284519
Epoch 18: loss 0.9876705491393724
Epoch 19: loss 0.984030170259818
-----------Time: 0:05:58.972745, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00010722672220103253, embedding_dim: 200, rmse: 0.9987672567367554-------------


Epoch 0: loss 14.811290388848874
Epoch 1: loss 11.53577289327193
Epoch 2: loss 5.888245429209615
Epoch 3: loss 1.994250057474824
Epoch 4: loss 1.2009894670741335
Epoch 5: loss 1.0569517817297598
Epoch 6: loss 1.0130714070803448
Epoch 7: loss 0.9974559986798772
Epoch 8: loss 0.9910881216918859
Epoch 9: loss 0.9878676397570972
Epoch 10: loss 0.9859437683699506
Epoch 11: loss 0.9843708466679717
Epoch 12: loss 0.9829692533477744
Epoch 13: loss 0.9815060713648991
Epoch 14: loss 0.9799870209988205
Epoch 15: loss 0.978281767505093
Epoch 16: loss 0.9764251102404208
Epoch 17: loss 0.9744753667101256
Epoch 18: loss 0.9723313263637207
Epoch 19: loss 0.9700261999110025
-----------Time: 0:03:13.288427, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 20, rmse: 0.9964665770530701-------------


Epoch 0: loss 14.11436341728057
Epoch 1: loss 4.715975079055452
Epoch 2: loss 1.284629156402818
Epoch 3: loss 1.0446728599693802
Epoch 4: loss 1.006388121434954
Epoch 5: loss 0.9979136258509576
Epoch 6: loss 0.9953190235257992
Epoch 7: loss 0.9942674471056286
Epoch 8: loss 0.9935355774243932
Epoch 9: loss 0.9928945911163995
Epoch 10: loss 0.9922923967363784
Epoch 11: loss 0.9916484062278834
Epoch 12: loss 0.9908412983392878
Epoch 13: loss 0.9901200189027791
Epoch 14: loss 0.98907379503805
Epoch 15: loss 0.9879222025587353
Epoch 16: loss 0.9866188035962892
Epoch 17: loss 0.9850840722658895
Epoch 18: loss 0.9831817929611185
Epoch 19: loss 0.9810305046542563
-----------Time: 0:02:56.300729, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 50, rmse: 0.9980517625808716-------------


Epoch 0: loss 11.498362444612628
Epoch 1: loss 1.6639105634983626
Epoch 2: loss 1.0471590491596157
Epoch 3: loss 1.0076699635462634
Epoch 4: loss 1.0020755611080525
Epoch 5: loss 1.0006569260614862
Epoch 6: loss 0.999966422052342
Epoch 7: loss 0.9995713514144684
Epoch 8: loss 0.9988471315631794
Epoch 9: loss 0.9983802827367839
Epoch 10: loss 0.9977083759602158
Epoch 11: loss 0.9967132171999831
Epoch 12: loss 0.9957156342420324
Epoch 13: loss 0.9942898195421261
Epoch 14: loss 0.9926499558441532
Epoch 15: loss 0.9905684756934481
Epoch 16: loss 0.9881423097741935
Epoch 17: loss 0.9849961855736941
Epoch 18: loss 0.9814355915788077
Epoch 19: loss 0.977344121149923
-----------Time: 0:04:03.504519, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 100, rmse: 0.9971946477890015-------------


Epoch 0: loss 9.488441586656503
Epoch 1: loss 1.2348151618595549
Epoch 2: loss 1.020726817661553
Epoch 3: loss 1.0069681366479157
Epoch 4: loss 1.0050771704927095
Epoch 5: loss 1.0041962664953192
Epoch 6: loss 1.0035357030757812
Epoch 7: loss 1.0028392689230652
Epoch 8: loss 1.001761580596092
Epoch 9: loss 1.0005738859172746
Epoch 10: loss 0.9991207142897829
Epoch 11: loss 0.9969682152372394
Epoch 12: loss 0.9943051067236133
Epoch 13: loss 0.9909632554224096
Epoch 14: loss 0.9868627853890098
Epoch 15: loss 0.9824153108237683
Epoch 16: loss 0.9773466628229184
Epoch 17: loss 0.9720024139483121
Epoch 18: loss 0.9662232406117334
Epoch 19: loss 0.9601824467534796
-----------Time: 0:04:42.897202, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 150, rmse: 0.9944533705711365-------------


Epoch 0: loss 8.258746077633216
Epoch 1: loss 1.127962629227226
Epoch 2: loss 1.0156692248799997
Epoch 3: loss 1.009130528434714
Epoch 4: loss 1.007817133864091
Epoch 5: loss 1.0072343980507594
Epoch 6: loss 1.0062755191047403
Epoch 7: loss 1.0055036641547963
Epoch 8: loss 1.004356572004425
Epoch 9: loss 1.0025867070433496
Epoch 10: loss 1.0004135331684898
Epoch 11: loss 0.9973496374810371
Epoch 12: loss 0.9935445375031527
Epoch 13: loss 0.9887831293253098
Epoch 14: loss 0.9834033551911027
Epoch 15: loss 0.9776015440169206
Epoch 16: loss 0.9713686548250404
Epoch 17: loss 0.9645090741654853
Epoch 18: loss 0.9570810336370452
Epoch 19: loss 0.9483219778784078
-----------Time: 0:05:37.570842, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00014174741629268076, embedding_dim: 200, rmse: 0.9942204356193542-------------


Epoch 0: loss 14.386281117983264
Epoch 1: loss 8.789716380260897
Epoch 2: loss 2.384857432877259
Epoch 3: loss 1.1675044598464022
Epoch 4: loss 1.0331974159348847
Epoch 5: loss 1.0028628983077568
Epoch 6: loss 0.9941453677665416
Epoch 7: loss 0.9907088824074316
Epoch 8: loss 0.9887963504069133
Epoch 9: loss 0.9873735985073984
Epoch 10: loss 0.9859556649092943
Epoch 11: loss 0.9843830994739035
Epoch 12: loss 0.9826128019172903
Epoch 13: loss 0.9806359957715232
Epoch 14: loss 0.9783309732623046
Epoch 15: loss 0.9757983174059558
Epoch 16: loss 0.9731043605554227
Epoch 17: loss 0.9701732131629744
Epoch 18: loss 0.9671610098062746
Epoch 19: loss 0.9640884176099216
-----------Time: 0:03:41.328924, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 20, rmse: 0.9956834316253662-------------


Epoch 0: loss 12.677555156311566
Epoch 1: loss 2.3169417458749972
Epoch 2: loss 1.076097525060663
Epoch 3: loss 1.0088122196985756
Epoch 4: loss 0.9983569779689057
Epoch 5: loss 0.9942060100150666
Epoch 6: loss 0.9908389094700432
Epoch 7: loss 0.9872934329179138
Epoch 8: loss 0.9834632702564792
Epoch 9: loss 0.9797854641814022
Epoch 10: loss 0.9759965717241517
Epoch 11: loss 0.9723145437344317
Epoch 12: loss 0.9689577925846977
Epoch 13: loss 0.9654788466301089
Epoch 14: loss 0.9621562383594948
Epoch 15: loss 0.9586389798908016
Epoch 16: loss 0.955100963010679
Epoch 17: loss 0.9511740146900662
Epoch 18: loss 0.9473310921456647
Epoch 19: loss 0.9427073206539839
-----------Time: 0:03:47.735492, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 50, rmse: 0.992832601070404-------------


Epoch 0: loss 9.217663189715312
Epoch 1: loss 1.1814512508339439
Epoch 2: loss 1.0169002970092125
Epoch 3: loss 1.0073772219129722
Epoch 4: loss 1.0053410612577456
Epoch 5: loss 1.0048937768700201
Epoch 6: loss 1.0036184449536831
Epoch 7: loss 1.0026718914800523
Epoch 8: loss 1.0008350794830032
Epoch 9: loss 0.9986193465142875
Epoch 10: loss 0.995785642927791
Epoch 11: loss 0.9921250196012743
Epoch 12: loss 0.9880477782012718
Epoch 13: loss 0.9829665030585222
Epoch 14: loss 0.9775554690268975
Epoch 15: loss 0.9715457936387272
Epoch 16: loss 0.9651171564569676
Epoch 17: loss 0.9577353969094804
Epoch 18: loss 0.9497478205655955
Epoch 19: loss 0.9401738973756794
-----------Time: 0:03:49.376390, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 100, rmse: 0.9932494759559631-------------


Epoch 0: loss 7.470791990793289
Epoch 1: loss 1.0781959361230116
Epoch 2: loss 1.014989642700856
Epoch 3: loss 1.0116213424300422
Epoch 4: loss 1.0103642625806122
Epoch 5: loss 1.0096791848791495
Epoch 6: loss 1.0085683840976714
Epoch 7: loss 1.00736970453239
Epoch 8: loss 1.0057684580971198
Epoch 9: loss 1.0030060635434992
Epoch 10: loss 0.9997248602924689
Epoch 11: loss 0.9949615360797261
Epoch 12: loss 0.9885182323308014
Epoch 13: loss 0.9815089400207433
Epoch 14: loss 0.9734631866751449
Epoch 15: loss 0.9643736537738362
Epoch 16: loss 0.9540073095844647
Epoch 17: loss 0.9417035769450399
Epoch 18: loss 0.9274340296420408
Epoch 19: loss 0.9095670964615444
-----------Time: 0:04:24.287346, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 150, rmse: 0.9918205142021179-------------


Epoch 0: loss 6.5260133692238105
Epoch 1: loss 1.0496904390735948
Epoch 2: loss 1.0171575938119002
Epoch 3: loss 1.015039160547858
Epoch 4: loss 1.0140924524230241
Epoch 5: loss 1.013104228625938
Epoch 6: loss 1.0113842247814637
Epoch 7: loss 1.0088026104510122
Epoch 8: loss 1.0052382791411818
Epoch 9: loss 1.0003296631778822
Epoch 10: loss 0.9940365029619465
Epoch 11: loss 0.9869943264387689
Epoch 12: loss 0.9787707670250945
Epoch 13: loss 0.9697124327797809
Epoch 14: loss 0.958986026849223
Epoch 15: loss 0.9462297181839395
Epoch 16: loss 0.9298056123728594
Epoch 17: loss 0.9098487432849608
Epoch 18: loss 0.8847153126674868
Epoch 19: loss 0.8546950260927783
-----------Time: 0:05:04.112898, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00018738174228603868, embedding_dim: 200, rmse: 0.9899572134017944-------------


Epoch 0: loss 13.729747768586716
Epoch 1: loss 5.4410828143893015
Epoch 2: loss 1.299035506072804
Epoch 3: loss 1.0359819633374725
Epoch 4: loss 1.000337662322162
Epoch 5: loss 0.991492567274997
Epoch 6: loss 0.9869375882620653
Epoch 7: loss 0.9828863125446375
Epoch 8: loss 0.9784302342968703
Epoch 9: loss 0.9737532313035973
Epoch 10: loss 0.9690228091302677
Epoch 11: loss 0.9641041064664292
Epoch 12: loss 0.9592270762652013
Epoch 13: loss 0.9547430692385435
Epoch 14: loss 0.9500279282181726
Epoch 15: loss 0.945210337930558
Epoch 16: loss 0.9404957647748846
Epoch 17: loss 0.9355980408878803
Epoch 18: loss 0.930523566686958
Epoch 19: loss 0.925411351661179
-----------Time: 0:03:17.643082, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 20, rmse: 0.9934823513031006-------------


Epoch 0: loss 10.392900596888834
Epoch 1: loss 1.3014070768275943
Epoch 2: loss 1.020366553770193
Epoch 3: loss 1.0046974598162197
Epoch 4: loss 1.0012624015329712
Epoch 5: loss 0.9986122958584931
Epoch 6: loss 0.9956507781817253
Epoch 7: loss 0.9922065893355241
Epoch 8: loss 0.9881433069608729
Epoch 9: loss 0.9838675376576791
Epoch 10: loss 0.9790426219948742
Epoch 11: loss 0.9737217326634859
Epoch 12: loss 0.9681473675307014
Epoch 13: loss 0.9617268055304682
Epoch 14: loss 0.9547547084472308
Epoch 15: loss 0.9470657088012394
Epoch 16: loss 0.9385446626541082
Epoch 17: loss 0.9292003384519363
Epoch 18: loss 0.9189899118780247
Epoch 19: loss 0.9076457869397226
-----------Time: 0:03:59.915090, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 50, rmse: 0.9925128221511841-------------


Epoch 0: loss 7.3448936278046055
Epoch 1: loss 1.062510861334041
Epoch 2: loss 1.0149908832488597
Epoch 3: loss 1.0123755226095064
Epoch 4: loss 1.0108317050550086
Epoch 5: loss 1.009586923776469
Epoch 6: loss 1.0072108166025928
Epoch 7: loss 1.0042839307477773
Epoch 8: loss 1.000157612636726
Epoch 9: loss 0.9943778833304494
Epoch 10: loss 0.9875315496492412
Epoch 11: loss 0.9794198921868955
Epoch 12: loss 0.9704063422268924
Epoch 13: loss 0.9602147621133523
Epoch 14: loss 0.9479920016059803
Epoch 15: loss 0.9332507077509323
Epoch 16: loss 0.9153870585789299
Epoch 17: loss 0.89375130528338
Epoch 18: loss 0.8682764151000406
Epoch 19: loss 0.8389744154456649
-----------Time: 0:04:55.622131, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 100, rmse: 0.990795910358429-------------


Epoch 0: loss 6.024000517857859
Epoch 1: loss 1.0379648307148952
Epoch 2: loss 1.0199755614318557
Epoch 3: loss 1.0180256269495405
Epoch 4: loss 1.0160757106986624
Epoch 5: loss 1.0131702430712393
Epoch 6: loss 1.0087265728802186
Epoch 7: loss 1.0027034771610952
Epoch 8: loss 0.9953753939622637
Epoch 9: loss 0.9865140192627971
Epoch 10: loss 0.977496889912609
Epoch 11: loss 0.9670333025603272
Epoch 12: loss 0.9538033986914864
Epoch 13: loss 0.9365720107640956
Epoch 14: loss 0.9139418330057735
Epoch 15: loss 0.8860358504447813
Epoch 16: loss 0.8517159048534723
Epoch 17: loss 0.8109151466894954
Epoch 18: loss 0.7638856068196798
Epoch 19: loss 0.7117097840774571
-----------Time: 0:04:51.262907, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 150, rmse: 0.9944772124290466-------------


Epoch 0: loss 5.288966181924513
Epoch 1: loss 1.0340193956361121
Epoch 2: loss 1.0245511391974456
Epoch 3: loss 1.023006347079456
Epoch 4: loss 1.0208164290638966
Epoch 5: loss 1.0171868826073756
Epoch 6: loss 1.0116396199561384
Epoch 7: loss 1.0033433439130042
Epoch 8: loss 0.9938937151250533
Epoch 9: loss 0.9830113852750094
Epoch 10: loss 0.9702085427708961
Epoch 11: loss 0.9534227779178661
Epoch 12: loss 0.9313715083758555
Epoch 13: loss 0.9020020456142954
Epoch 14: loss 0.8644819881619157
Epoch 15: loss 0.8178303815574345
Epoch 16: loss 0.7619830536544161
Epoch 17: loss 0.6978711105514441
Epoch 18: loss 0.6285905593873496
Epoch 19: loss 0.5569902328270943
-----------Time: 0:04:40.416491, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0002477076355991714, embedding_dim: 200, rmse: 1.0042922496795654-------------


Epoch 0: loss 12.55438376406472
Epoch 1: loss 2.5322706390652336
Epoch 2: loss 1.0598293613518885
Epoch 3: loss 1.0047877759729928
Epoch 4: loss 0.9969677961734794
Epoch 5: loss 0.9938974942669653
Epoch 6: loss 0.9909019744091542
Epoch 7: loss 0.9872715185847601
Epoch 8: loss 0.9829307960679571
Epoch 9: loss 0.9779416586537281
Epoch 10: loss 0.9725718594491968
Epoch 11: loss 0.9666866344592699
Epoch 12: loss 0.9606520941945896
Epoch 13: loss 0.9542902340345761
Epoch 14: loss 0.9477993225260491
Epoch 15: loss 0.9411000088383933
Epoch 16: loss 0.9342050758667002
Epoch 17: loss 0.9268885239464488
Epoch 18: loss 0.9193657044161267
Epoch 19: loss 0.9115103442361654
-----------Time: 0:02:55.691578, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 20, rmse: 0.9955132603645325-------------


Epoch 0: loss 8.292217334921817
Epoch 1: loss 1.085899492461374
Epoch 2: loss 1.0126592755771449
Epoch 3: loss 1.0080448207873376
Epoch 4: loss 1.0046995718755303
Epoch 5: loss 1.0005577483405361
Epoch 6: loss 0.9952524423826123
Epoch 7: loss 0.9886652805223097
Epoch 8: loss 0.9821963959929086
Epoch 9: loss 0.9756684617188522
Epoch 10: loss 0.9687044005668831
Epoch 11: loss 0.9609587477269932
Epoch 12: loss 0.9515128909018197
Epoch 13: loss 0.9407124012109053
Epoch 14: loss 0.927030695658005
Epoch 15: loss 0.91133748848131
Epoch 16: loss 0.893567460833716
Epoch 17: loss 0.8739838687669589
Epoch 18: loss 0.8518955668332202
Epoch 19: loss 0.8277183321748758
-----------Time: 0:03:23.288570, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 50, rmse: 0.9952844381332397-------------


Epoch 0: loss 5.825112457327897
Epoch 1: loss 1.0338250571064744
Epoch 2: loss 1.020405860667387
Epoch 3: loss 1.0171689799682415
Epoch 4: loss 1.0117530139034503
Epoch 5: loss 1.0047262444516898
Epoch 6: loss 0.9965655148158973
Epoch 7: loss 0.9883777018051033
Epoch 8: loss 0.9803143898192277
Epoch 9: loss 0.97104764530813
Epoch 10: loss 0.9587881141638483
Epoch 11: loss 0.9423899620269031
Epoch 12: loss 0.9204880998567629
Epoch 13: loss 0.8924289413805303
Epoch 14: loss 0.8579525948543144
Epoch 15: loss 0.8171387688014221
Epoch 16: loss 0.7702456183281847
Epoch 17: loss 0.7181787069982132
Epoch 18: loss 0.6636215901005327
Epoch 19: loss 0.6084367936237017
-----------Time: 0:04:31.821949, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 100, rmse: 1.0141316652297974-------------


Epoch 0: loss 4.905827640223075
Epoch 1: loss 1.0339924751772318
Epoch 2: loss 1.0271012071457293
Epoch 3: loss 1.0212398082941625
Epoch 4: loss 1.0129343422130763
Epoch 5: loss 1.0041103464646985
Epoch 6: loss 0.9956948000850595
Epoch 7: loss 0.9852249698511863
Epoch 8: loss 0.9724404181456553
Epoch 9: loss 0.9536844765673259
Epoch 10: loss 0.9276455869455581
Epoch 11: loss 0.8915169072773483
Epoch 12: loss 0.8452919172067367
Epoch 13: loss 0.7887319782591825
Epoch 14: loss 0.7225521020327916
Epoch 15: loss 0.6499216805063815
Epoch 16: loss 0.5749181030868122
Epoch 17: loss 0.5024254093065152
Epoch 18: loss 0.43545621347498675
Epoch 19: loss 0.37584318436151487
-----------Time: 0:05:20.870303, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 150, rmse: 1.0512574911117554-------------


Epoch 0: loss 4.353312701327182
Epoch 1: loss 1.0387438365206114
Epoch 2: loss 1.0338953976367984
Epoch 3: loss 1.0285981235587123
Epoch 4: loss 1.020432928047341
Epoch 5: loss 1.0100394971770006
Epoch 6: loss 0.9979851673549125
Epoch 7: loss 0.9826449811004827
Epoch 8: loss 0.9612695608144224
Epoch 9: loss 0.9299655071155347
Epoch 10: loss 0.8861625789947779
Epoch 11: loss 0.8279062733013647
Epoch 12: loss 0.7543287823354504
Epoch 13: loss 0.6683469903232482
Epoch 14: loss 0.5757079096333756
Epoch 15: loss 0.48397097235435116
Epoch 16: loss 0.39992996561682825
Epoch 17: loss 0.32704072098423437
Epoch 18: loss 0.26617657673981215
Epoch 19: loss 0.21646715416621098
-----------Time: 0:06:10.297710, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00032745491628777317, embedding_dim: 200, rmse: 1.0820921659469604-------------


Epoch 0: loss 10.361826293231223
Epoch 1: loss 1.2828375661483338
Epoch 2: loss 1.0142444715414312
Epoch 3: loss 1.00239152921938
Epoch 4: loss 0.9989214038933146
Epoch 5: loss 0.9953417383243753
Epoch 6: loss 0.9906958632977388
Epoch 7: loss 0.9849069572097909
Epoch 8: loss 0.9781568956608484
Epoch 9: loss 0.9699943569155087
Epoch 10: loss 0.9611533480438608
Epoch 11: loss 0.951911086294559
Epoch 12: loss 0.9423329917356461
Epoch 13: loss 0.9318340038429984
Epoch 14: loss 0.9209812823814695
Epoch 15: loss 0.9093867996064394
Epoch 16: loss 0.8978046167149629
Epoch 17: loss 0.885758126994201
Epoch 18: loss 0.8737198941373125
Epoch 19: loss 0.8618119070230456
-----------Time: 0:02:36.956430, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 20, rmse: 1.0028703212738037-------------


Epoch 0: loss 6.545979665212491
Epoch 1: loss 1.0364899976283328
Epoch 2: loss 1.0176324302603073
Epoch 3: loss 1.015032855906261
Epoch 4: loss 1.0120709192143502
Epoch 5: loss 1.0073591327297746
Epoch 6: loss 1.0001516688965533
Epoch 7: loss 0.991034803489682
Epoch 8: loss 0.9797865218478353
Epoch 9: loss 0.9673288389675769
Epoch 10: loss 0.9525306092746365
Epoch 11: loss 0.9340305036504091
Epoch 12: loss 0.9111292028213208
Epoch 13: loss 0.8835790942971508
Epoch 14: loss 0.8518296155740283
Epoch 15: loss 0.8163928508985425
Epoch 16: loss 0.7790533119307191
Epoch 17: loss 0.7411479601852268
Epoch 18: loss 0.7044279677200862
Epoch 19: loss 0.6700943043418006
-----------Time: 0:03:05.866163, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 50, rmse: 1.0281753540039062-------------


Epoch 0: loss 4.789483535088814
Epoch 1: loss 1.034630874889969
Epoch 2: loss 1.029332030299178
Epoch 3: loss 1.0233216860036347
Epoch 4: loss 1.014337154884193
Epoch 5: loss 1.0024752782990973
Epoch 6: loss 0.9894869732039983
Epoch 7: loss 0.9736224526844315
Epoch 8: loss 0.9516602538246509
Epoch 9: loss 0.921066566321304
Epoch 10: loss 0.8798740870962978
Epoch 11: loss 0.8286418819667335
Epoch 12: loss 0.7684791404601995
Epoch 13: loss 0.7014879651485156
Epoch 14: loss 0.6322109984794085
Epoch 15: loss 0.5648388316071896
Epoch 16: loss 0.5035127081775095
Epoch 17: loss 0.44958572797051843
Epoch 18: loss 0.40377916646042616
Epoch 19: loss 0.3655263716967745
-----------Time: 0:04:07.492104, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 100, rmse: 1.0989019870758057-------------


Epoch 0: loss 4.064919809630022
Epoch 1: loss 1.042437214187593
Epoch 2: loss 1.0335648281053074
Epoch 3: loss 1.0220468304706956
Epoch 4: loss 1.0092078011083887
Epoch 5: loss 0.9943638314625178
Epoch 6: loss 0.9745489347071801
Epoch 7: loss 0.9470472300649013
Epoch 8: loss 0.9070109700851948
Epoch 9: loss 0.8484085118271463
Epoch 10: loss 0.7709096013008738
Epoch 11: loss 0.6786234927216322
Epoch 12: loss 0.5811648912132272
Epoch 13: loss 0.4878709564606349
Epoch 14: loss 0.405680051749008
Epoch 15: loss 0.3370172429815656
Epoch 16: loss 0.28132551352231644
Epoch 17: loss 0.23701107656343143
Epoch 18: loss 0.2018603219966191
Epoch 19: loss 0.17391406961267508
-----------Time: 0:04:51.518634, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 150, rmse: 1.1526942253112793-------------


Epoch 0: loss 3.676108303521884
Epoch 1: loss 1.0514052378671075
Epoch 2: loss 1.0423503468352988
Epoch 3: loss 1.029533748136648
Epoch 4: loss 1.0133007032965888
Epoch 5: loss 0.9932196525337257
Epoch 6: loss 0.960466341796681
Epoch 7: loss 0.9090845756812093
Epoch 8: loss 0.8356597077561306
Epoch 9: loss 0.7385061679325913
Epoch 10: loss 0.625467913404569
Epoch 11: loss 0.5091123147024416
Epoch 12: loss 0.40269857138605986
Epoch 13: loss 0.31352067220998625
Epoch 14: loss 0.24300427089053825
Epoch 15: loss 0.1887780407270444
Epoch 16: loss 0.14811889711785667
Epoch 17: loss 0.11763335760964395
Epoch 18: loss 0.09480352807018244
Epoch 19: loss 0.07761144229981191
-----------Time: 0:05:42.248455, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.00043287612810830614, embedding_dim: 200, rmse: 1.1688514947891235-------------


Epoch 0: loss 8.493989452225414
Epoch 1: loss 1.0791012938253384
Epoch 2: loss 1.0076078589258148
Epoch 3: loss 0.9986365554188826
Epoch 4: loss 0.9884085867507358
Epoch 5: loss 0.9778238441582411
Epoch 6: loss 0.9678573026126853
Epoch 7: loss 0.9577026624534363
Epoch 8: loss 0.9466919625920404
Epoch 9: loss 0.9355311125533097
Epoch 10: loss 0.923524431961266
Epoch 11: loss 0.9113997289705302
Epoch 12: loss 0.8988447319787115
Epoch 13: loss 0.8858942030207368
Epoch 14: loss 0.8722636465827948
Epoch 15: loss 0.858690760782583
Epoch 16: loss 0.8447746658305748
Epoch 17: loss 0.8309237910166196
Epoch 18: loss 0.8179437959077502
Epoch 19: loss 0.8057471248136389
-----------Time: 0:03:54.487664, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 20, rmse: 1.0237239599227905-------------


Epoch 0: loss 5.342127239674962
Epoch 1: loss 1.030735515118812
Epoch 2: loss 1.0208862220839094
Epoch 3: loss 1.011617168225044
Epoch 4: loss 0.9999855621928038
Epoch 5: loss 0.9885621051577007
Epoch 6: loss 0.9755762967809248
Epoch 7: loss 0.9575285457637791
Epoch 8: loss 0.933648895608011
Epoch 9: loss 0.9031211875294005
Epoch 10: loss 0.8669604598633163
Epoch 11: loss 0.8255331589839846
Epoch 12: loss 0.7815660997179943
Epoch 13: loss 0.7363425733814296
Epoch 14: loss 0.6931855011773926
Epoch 15: loss 0.6540631012562632
Epoch 16: loss 0.6200103297092268
Epoch 17: loss 0.5905001640498088
Epoch 18: loss 0.5663613177240381
Epoch 19: loss 0.5459002306793227
-----------Time: 0:03:24.300500, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 50, rmse: 1.1005769968032837-------------


Epoch 0: loss 4.009692148727331
Epoch 1: loss 1.0420364590362727
Epoch 2: loss 1.0266618021582052
Epoch 3: loss 1.006669183539722
Epoch 4: loss 0.9865040338642251
Epoch 5: loss 0.9633501555979285
Epoch 6: loss 0.9315535684134922
Epoch 7: loss 0.8875371193257021
Epoch 8: loss 0.8281224601420454
Epoch 9: loss 0.7542538833138475
Epoch 10: loss 0.6717389140312539
Epoch 11: loss 0.5887620948040336
Epoch 12: loss 0.5126863682623335
Epoch 13: loss 0.44773259471297716
Epoch 14: loss 0.3941818104693266
Epoch 15: loss 0.35154704860593394
Epoch 16: loss 0.3175892120651605
Epoch 17: loss 0.29014575914107565
Epoch 18: loss 0.26798885908380937
Epoch 19: loss 0.24964000102762557
-----------Time: 0:03:42.260409, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 100, rmse: 1.2097398042678833-------------


Epoch 0: loss 3.4683765298228604
Epoch 1: loss 1.0563200093307463
Epoch 2: loss 1.0433920357232511
Epoch 3: loss 1.022531292988205
Epoch 4: loss 0.9972439437796721
Epoch 5: loss 0.9604316170997371
Epoch 6: loss 0.9002180320209235
Epoch 7: loss 0.8132498679989508
Epoch 8: loss 0.7021299090607132
Epoch 9: loss 0.5811092737675232
Epoch 10: loss 0.46771919342439544
Epoch 11: loss 0.3728810888170677
Epoch 12: loss 0.298885856579347
Epoch 13: loss 0.24308236034069455
Epoch 14: loss 0.20133588241859907
Epoch 15: loss 0.16994877548330242
Epoch 16: loss 0.14584773035275347
Epoch 17: loss 0.12743694417871426
Epoch 18: loss 0.1126518498877263
Epoch 19: loss 0.10086259927120528
-----------Time: 0:04:29.624356, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 150, rmse: 1.2472180128097534-------------


Epoch 0: loss 3.1716918242470347
Epoch 1: loss 1.0671999147886035
Epoch 2: loss 1.0514658002700412
Epoch 3: loss 1.029106290347554
Epoch 4: loss 0.9940822560415118
Epoch 5: loss 0.93060646629515
Epoch 6: loss 0.8287676370066103
Epoch 7: loss 0.6927165800038079
Epoch 8: loss 0.543166050846451
Epoch 9: loss 0.4069297411815831
Epoch 10: loss 0.29884842475303947
Epoch 11: loss 0.21964483375520794
Epoch 12: loss 0.16393074056801102
Epoch 13: loss 0.1252833140741521
Epoch 14: loss 0.09834414625225377
Epoch 15: loss 0.07932166019155805
Epoch 16: loss 0.06555523762764037
Epoch 17: loss 0.05552922003072458
Epoch 18: loss 0.047840028341865654
Epoch 19: loss 0.042044125517598306
-----------Time: 0:05:11.908426, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.0005722367659350221, embedding_dim: 200, rmse: 1.2247151136398315-------------


Epoch 0: loss 6.752900146316613
Epoch 1: loss 1.0313260259674968
Epoch 2: loss 1.0131380004264403
Epoch 3: loss 1.0051459624528756
Epoch 4: loss 0.9941309124085487
Epoch 5: loss 0.9805539541590661
Epoch 6: loss 0.9657637935543009
Epoch 7: loss 0.9495040457685594
Epoch 8: loss 0.931458776544267
Epoch 9: loss 0.9120221869040599
Epoch 10: loss 0.8922640408848601
Epoch 11: loss 0.8719558253146955
Epoch 12: loss 0.8528537900633238
Epoch 13: loss 0.8345057302483273
Epoch 14: loss 0.8177268242725799
Epoch 15: loss 0.8029560856419844
Epoch 16: loss 0.7899159972047988
Epoch 17: loss 0.7790176345318021
Epoch 18: loss 0.7695576462848087
Epoch 19: loss 0.7616311679591686
-----------Time: 0:03:33.955510, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 20, rmse: 1.0515687465667725-------------


Epoch 0: loss 4.366391178315719
Epoch 1: loss 1.037201919297669
Epoch 2: loss 1.0229605852001837
Epoch 3: loss 1.0039282025624514
Epoch 4: loss 0.9841079567196966
Epoch 5: loss 0.9622932125395443
Epoch 6: loss 0.9338072399654875
Epoch 7: loss 0.8956238069632833
Epoch 8: loss 0.8460293589985325
Epoch 9: loss 0.7906096102168471
Epoch 10: loss 0.7344071109895152
Epoch 11: loss 0.6816831628724633
Epoch 12: loss 0.6363969290544962
Epoch 13: loss 0.5992098717649972
Epoch 14: loss 0.5697798300220501
Epoch 15: loss 0.5463475445290504
Epoch 16: loss 0.527714357728087
Epoch 17: loss 0.5121745467104946
Epoch 18: loss 0.49939262923693906
Epoch 19: loss 0.48843982837620215
-----------Time: 0:03:50.115245, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 50, rmse: 1.172694206237793-------------


Epoch 0: loss 3.409792943306109
Epoch 1: loss 1.0554876160757771
Epoch 2: loss 1.0364769569326187
Epoch 3: loss 1.0127526110057405
Epoch 4: loss 0.9749664468341317
Epoch 5: loss 0.9151413428874169
Epoch 6: loss 0.8329273886723386
Epoch 7: loss 0.7317032676196215
Epoch 8: loss 0.6251006047887087
Epoch 9: loss 0.5259924514615192
Epoch 10: loss 0.44524703255452946
Epoch 11: loss 0.3837263049591229
Epoch 12: loss 0.3375303818600667
Epoch 13: loss 0.3027604522832779
Epoch 14: loss 0.2756778956910007
Epoch 15: loss 0.2546352727329556
Epoch 16: loss 0.23734934273914776
Epoch 17: loss 0.22307637868456182
Epoch 18: loss 0.211165704047829
Epoch 19: loss 0.200848394809387
-----------Time: 0:04:59.922386, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 100, rmse: 1.3046797513961792-------------


Epoch 0: loss 2.998227447692696
Epoch 1: loss 1.0650751198388235
Epoch 2: loss 1.0340990352202784
Epoch 3: loss 0.9893696436971474
Epoch 4: loss 0.9203076235719062
Epoch 5: loss 0.8196520489671426
Epoch 6: loss 0.6874682213412218
Epoch 7: loss 0.5434936752391938
Epoch 8: loss 0.41614021415163344
Epoch 9: loss 0.3181136673870393
Epoch 10: loss 0.24895609621527792
Epoch 11: loss 0.20072176016141954
Epoch 12: loss 0.16679299722641885
Epoch 13: loss 0.14224104689816017
Epoch 14: loss 0.12389399464150497
Epoch 15: loss 0.1095267801366791
Epoch 16: loss 0.09839256571392088
Epoch 17: loss 0.08940568996381669
Epoch 18: loss 0.08196533061543712
Epoch 19: loss 0.07590084018356907
-----------Time: 0:04:16.228723, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 150, rmse: 1.3175162076950073-------------


Epoch 0: loss 2.8047326020308976
Epoch 1: loss 1.0736811569101334
Epoch 2: loss 1.0359142636591614
Epoch 3: loss 0.9734751948557061
Epoch 4: loss 0.868806542537989
Epoch 5: loss 0.7131798452676931
Epoch 6: loss 0.5306915091051881
Epoch 7: loss 0.3708937901349221
Epoch 8: loss 0.25583016069007347
Epoch 9: loss 0.18089643169382852
Epoch 10: loss 0.13372590531184336
Epoch 11: loss 0.10339652646541628
Epoch 12: loss 0.08368894662616887
Epoch 13: loss 0.07007644635125339
Epoch 14: loss 0.0605575118007269
Epoch 15: loss 0.05349711019316888
Epoch 16: loss 0.04846312068585081
Epoch 17: loss 0.04458212440498194
Epoch 18: loss 0.041581859954953386
Epoch 19: loss 0.03918910703493678
-----------Time: 0:04:50.413882, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.000756463327554629, embedding_dim: 200, rmse: 1.2542424201965332-------------


Epoch 0: loss 5.402208470760188
Epoch 1: loss 1.025993483231209
Epoch 2: loss 1.0094397756442743
Epoch 3: loss 0.9904071573591414
Epoch 4: loss 0.9726193655178947
Epoch 5: loss 0.9539894384330482
Epoch 6: loss 0.9359838622558888
Epoch 7: loss 0.9155181532582879
Epoch 8: loss 0.8927917164457175
Epoch 9: loss 0.8689467440713288
Epoch 10: loss 0.8452815289501265
Epoch 11: loss 0.8239062514850923
Epoch 12: loss 0.8057876742521404
Epoch 13: loss 0.791123666310842
Epoch 14: loss 0.779491288821421
Epoch 15: loss 0.7701062702367719
Epoch 16: loss 0.7625824864822085
Epoch 17: loss 0.7559527022771176
Epoch 18: loss 0.750387779991545
Epoch 19: loss 0.7456194901511746
-----------Time: 0:03:02.815753, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0748941898345947-------------


Epoch 0: loss 3.6994109654018192
Epoch 1: loss 1.0461421939683648
Epoch 2: loss 1.0224393403776448
Epoch 3: loss 0.9979893846671305
Epoch 4: loss 0.9640914367358927
Epoch 5: loss 0.91496872441674
Epoch 6: loss 0.8525951091947602
Epoch 7: loss 0.7825394960088922
Epoch 8: loss 0.7146206879385663
Epoch 9: loss 0.6562711249686506
Epoch 10: loss 0.6108408725135284
Epoch 11: loss 0.5764493554214734
Epoch 12: loss 0.55076058845017
Epoch 13: loss 0.5304664893925611
Epoch 14: loss 0.5143713016688921
Epoch 15: loss 0.501071381173489
Epoch 16: loss 0.49010059786043847
Epoch 17: loss 0.48040129646229446
Epoch 18: loss 0.4722243883384188
Epoch 19: loss 0.46479065095587485
-----------Time: 0:03:31.465746, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.2297813892364502-------------


Epoch 0: loss 2.965129061455698
Epoch 1: loss 1.067645656958814
Epoch 2: loss 1.0342117967068858
Epoch 3: loss 0.981248449070289
Epoch 4: loss 0.894146897421381
Epoch 5: loss 0.7705189865558545
Epoch 6: loss 0.635374771561747
Epoch 7: loss 0.5179531819846982
Epoch 8: loss 0.4293305304563843
Epoch 9: loss 0.36650317394422277
Epoch 10: loss 0.32243526940876666
Epoch 11: loss 0.29023863878374584
Epoch 12: loss 0.26561636269870437
Epoch 13: loss 0.24656791766662842
Epoch 14: loss 0.23100120199276028
Epoch 15: loss 0.21825677575366273
Epoch 16: loss 0.20743839894261873
Epoch 17: loss 0.19848316538999947
Epoch 18: loss 0.1900066415849686
Epoch 19: loss 0.18345197348725284
-----------Time: 0:04:43.939591, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.3756628036499023-------------


Epoch 0: loss 2.684276603919906
Epoch 1: loss 1.0841336880545696
Epoch 2: loss 1.0331155530067162
Epoch 3: loss 0.933759315335407
Epoch 4: loss 0.7742224602578709
Epoch 5: loss 0.5892623148416941
Epoch 6: loss 0.4290841093541099
Epoch 7: loss 0.3160491212999905
Epoch 8: loss 0.24206763198837242
Epoch 9: loss 0.19441599986713953
Epoch 10: loss 0.1631786203241789
Epoch 11: loss 0.1406316518147495
Epoch 12: loss 0.1241085290677882
Epoch 13: loss 0.1118652084523274
Epoch 14: loss 0.10193678730913418
Epoch 15: loss 0.09408435030916808
Epoch 16: loss 0.08781537938561207
Epoch 17: loss 0.08213263849538911
Epoch 18: loss 0.07790883698274319
Epoch 19: loss 0.07371823308263187
-----------Time: 0:05:42.878741, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.3567752838134766-------------


Epoch 0: loss 2.5551505018149205
Epoch 1: loss 1.0785886459113334
Epoch 2: loss 0.9885419114346475
Epoch 3: loss 0.8329112197537342
Epoch 4: loss 0.6219493987007333
Epoch 5: loss 0.4205675476867262
Epoch 6: loss 0.2764575239260797
Epoch 7: loss 0.189087892750348
Epoch 8: loss 0.1394741282174936
Epoch 9: loss 0.11025997347696247
Epoch 10: loss 0.09189647576625319
Epoch 11: loss 0.08006610333020107
Epoch 12: loss 0.07164991749708843
Epoch 13: loss 0.06601432388999852
Epoch 14: loss 0.061282662087327244
Epoch 15: loss 0.058165081412556253
Epoch 16: loss 0.055323287278470495
Epoch 17: loss 0.05307948927315537
Epoch 18: loss 0.05127540942384796
Epoch 19: loss 0.049813857050880976
-----------Time: 0:05:15.862822, Loss: regression, n_iter: 20, l2: 1.3848863713938746e-15, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2623720169067383-------------


Epoch 0: loss 16.129300474018557
Epoch 1: loss 16.129293458107828
Epoch 2: loss 16.12929360357039
Epoch 3: loss 16.1292967111796
Epoch 4: loss 16.129296157332853
Epoch 5: loss 16.129288421369495
Epoch 6: loss 16.12929020399889
Epoch 7: loss 16.12929390149638
Epoch 8: loss 16.12928952076566
Epoch 9: loss 16.129296879978398
Epoch 10: loss 16.129297098561175
Epoch 11: loss 16.129296786374148
Epoch 12: loss 16.12929675162908
Epoch 13: loss 16.12929299319808
Epoch 14: loss 16.129295726649588
Epoch 15: loss 16.129292990086583
Epoch 16: loss 16.12929514765156
Epoch 17: loss 16.129294237278934
Epoch 18: loss 16.12929386519556
Epoch 19: loss 16.129293937797193
-----------Time: 0:02:41.499004, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 20, rmse: 4.017196178436279-------------


Epoch 0: loss 16.12919361608193
Epoch 1: loss 16.12919172092
Epoch 2: loss 16.129192040885773
Epoch 3: loss 16.12919202558757
Epoch 4: loss 16.1291917577394
Epoch 5: loss 16.129189682110553
Epoch 6: loss 16.1291874286077
Epoch 7: loss 16.12919147951957
Epoch 8: loss 16.12919223794735
Epoch 9: loss 16.12919508419068
Epoch 10: loss 16.129188579084303
Epoch 11: loss 16.12918548132817
Epoch 12: loss 16.129186562055345
Epoch 13: loss 16.129184119269667
Epoch 14: loss 16.129190662232606
Epoch 15: loss 16.129185997577643
Epoch 16: loss 16.129189604323088
Epoch 17: loss 16.12919301322908
Epoch 18: loss 16.129188954538467
Epoch 19: loss 16.129183057989355
-----------Time: 0:03:12.954780, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 50, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129180703362803
Epoch 1: loss 16.12917194890224
Epoch 2: loss 16.129172485635745
Epoch 3: loss 16.129179058157924
Epoch 4: loss 16.129177530412118
Epoch 5: loss 16.129178689186052
Epoch 6: loss 16.129178274578862
Epoch 7: loss 16.12917793023969
Epoch 8: loss 16.129179912004993
Epoch 9: loss 16.129179314597266
Epoch 10: loss 16.129184375968297
Epoch 11: loss 16.129185615900486
Epoch 12: loss 16.129182959199277
Epoch 13: loss 16.129176721681777
Epoch 14: loss 16.129186015987344
Epoch 15: loss 16.129177942944974
Epoch 16: loss 16.12918176464311
Epoch 17: loss 16.12917762271991
Epoch 18: loss 16.129186806826567
Epoch 19: loss 16.129178750638147
-----------Time: 0:04:13.944227, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129169910352083
Epoch 1: loss 16.129163960907498
Epoch 2: loss 16.12917560698741
Epoch 3: loss 16.129178758157604
Epoch 4: loss 16.129174787626116
Epoch 5: loss 16.129171166101056
Epoch 6: loss 16.12917055935883
Epoch 7: loss 16.129175379588723
Epoch 8: loss 16.129172498859614
Epoch 9: loss 16.129173506466575
Epoch 10: loss 16.129168747170194
Epoch 11: loss 16.129170616921552
Epoch 12: loss 16.12917064285071
Epoch 13: loss 16.129175644584684
Epoch 14: loss 16.12917000317846
Epoch 15: loss 16.129177095580193
Epoch 16: loss 16.12917527016769
Epoch 17: loss 16.129173135679657
Epoch 18: loss 16.129170404561776
Epoch 19: loss 16.129173097045218
-----------Time: 0:05:06.775122, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917910768261
Epoch 1: loss 16.129173901626896
Epoch 2: loss 16.12918175945728
Epoch 3: loss 16.129167074221122
Epoch 4: loss 16.12917880145929
Epoch 5: loss 16.129179764727393
Epoch 6: loss 16.129172246568938
Epoch 7: loss 16.129169927465327
Epoch 8: loss 16.129174867747206
Epoch 9: loss 16.12917463801489
Epoch 10: loss 16.129183861533864
Epoch 11: loss 16.12916974207187
Epoch 12: loss 16.129172913985386
Epoch 13: loss 16.129178627993245
Epoch 14: loss 16.12917958296402
Epoch 15: loss 16.12917346731355
Epoch 16: loss 16.129178642772864
Epoch 17: loss 16.129177533782908
Epoch 18: loss 16.129178995150077
Epoch 19: loss 16.12917694596897
-----------Time: 0:05:58.544233, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-15, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129433013227338
Epoch 1: loss 16.129439890417085
Epoch 2: loss 16.129439476328482
Epoch 3: loss 16.12944632525545
Epoch 4: loss 16.129434998104145
Epoch 5: loss 16.129443549798715
Epoch 6: loss 16.12944577918745
Epoch 7: loss 16.129427210801058
Epoch 8: loss 16.129441852476234
Epoch 9: loss 16.129442890938886
Epoch 10: loss 16.129438626111494
Epoch 11: loss 16.129439373649028
Epoch 12: loss 16.129448689475794
Epoch 13: loss 16.129440375292283
Epoch 14: loss 16.129442508743146
Epoch 15: loss 16.12944288964243
Epoch 16: loss 16.129444823179508
Epoch 17: loss 16.129443730524923
Epoch 18: loss 16.129445143923157
Epoch 19: loss 16.129441104420117
-----------Time: 0:03:39.294248, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 20, rmse: 4.017214298248291-------------


Epoch 0: loss 16.12919005652755
Epoch 1: loss 16.129179903707666
Epoch 2: loss 16.12918328305442
Epoch 3: loss 16.12918051407997
Epoch 4: loss 16.1291780741465
Epoch 5: loss 16.129179593854264
Epoch 6: loss 16.129181967409103
Epoch 7: loss 16.129184899996517
Epoch 8: loss 16.129188157476246
Epoch 9: loss 16.12917735487174
Epoch 10: loss 16.129178455823656
Epoch 11: loss 16.129177863601758
Epoch 12: loss 16.12918493188938
Epoch 13: loss 16.129180596793976
Epoch 14: loss 16.129175311654336
Epoch 15: loss 16.129182214254655
Epoch 16: loss 16.129179086679994
Epoch 17: loss 16.129184810800226
Epoch 18: loss 16.12918387397986
Epoch 19: loss 16.129186828347766
-----------Time: 0:02:50.545833, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 50, rmse: 4.017192363739014-------------


Epoch 0: loss 16.129182081497383
Epoch 1: loss 16.129181442084423
Epoch 2: loss 16.129179587890558
Epoch 3: loss 16.12917672064461
Epoch 4: loss 16.12918701322264
Epoch 5: loss 16.129180212005316
Epoch 6: loss 16.12917984095911
Epoch 7: loss 16.129180320129894
Epoch 8: loss 16.129180367061664
Epoch 9: loss 16.129184282882633
Epoch 10: loss 16.129173993934685
Epoch 11: loss 16.12918607380936
Epoch 12: loss 16.12918502886442
Epoch 13: loss 16.129183662657248
Epoch 14: loss 16.12918767078601
Epoch 15: loss 16.129175114333467
Epoch 16: loss 16.129181553839082
Epoch 17: loss 16.129173476388754
Epoch 18: loss 16.12918354260526
Epoch 19: loss 16.12918202600899
-----------Time: 0:03:58.928035, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129178550983656
Epoch 1: loss 16.129178946143977
Epoch 2: loss 16.129177113471307
Epoch 3: loss 16.12917934156359
Epoch 4: loss 16.129180017536655
Epoch 5: loss 16.129175172414776
Epoch 6: loss 16.12917814545167
Epoch 7: loss 16.129181443640174
Epoch 8: loss 16.129176720903903
Epoch 9: loss 16.129176224101297
Epoch 10: loss 16.129172680104407
Epoch 11: loss 16.12918362091131
Epoch 12: loss 16.129178596618967
Epoch 13: loss 16.129178565763272
Epoch 14: loss 16.129184690229657
Epoch 15: loss 16.129180445627004
Epoch 16: loss 16.12917645538936
Epoch 17: loss 16.129177094802316
Epoch 18: loss 16.129176783393167
Epoch 19: loss 16.129181299214782
-----------Time: 0:04:40.515601, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 150, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12917569722087
Epoch 1: loss 16.12917777829484
Epoch 2: loss 16.12917346861001
Epoch 3: loss 16.129175724705775
Epoch 4: loss 16.129174625050315
Epoch 5: loss 16.12916759072989
Epoch 6: loss 16.129173414677364
Epoch 7: loss 16.12916954682533
Epoch 8: loss 16.129170190127663
Epoch 9: loss 16.129173640520303
Epoch 10: loss 16.129169226081686
Epoch 11: loss 16.129173899811853
Epoch 12: loss 16.129173693934362
Epoch 13: loss 16.129168519252925
Epoch 14: loss 16.129173293588213
Epoch 15: loss 16.12918066291332
Epoch 16: loss 16.129174394021543
Epoch 17: loss 16.12917570888899
Epoch 18: loss 16.12916988442293
Epoch 19: loss 16.129167864801055
-----------Time: 0:05:26.212922, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660287e-15, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129385712744426
Epoch 1: loss 16.129388033144497
Epoch 2: loss 16.129388120266455
Epoch 3: loss 16.129385014472284
Epoch 4: loss 16.129389102722136
Epoch 5: loss 16.129383721903913
Epoch 6: loss 16.129386899781135
Epoch 7: loss 16.12938774896096
Epoch 8: loss 16.12938782597055
Epoch 9: loss 16.12938549105015
Epoch 10: loss 16.129385593470314
Epoch 11: loss 16.12938962752823
Epoch 12: loss 16.129392321567423
Epoch 13: loss 16.129383966675135
Epoch 14: loss 16.129391503502585
Epoch 15: loss 16.129383706865006
Epoch 16: loss 16.12938439917344
Epoch 17: loss 16.12938513556144
Epoch 18: loss 16.129389514477115
Epoch 19: loss 16.12938551594214
-----------Time: 0:03:42.689997, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 20, rmse: 4.017235279083252-------------


Epoch 0: loss 16.1291788012
Epoch 1: loss 16.129183213823577
Epoch 2: loss 16.12918232523144
Epoch 3: loss 16.12917896351651
Epoch 4: loss 16.12918167415036
Epoch 5: loss 16.129169839565492
Epoch 6: loss 16.12917936723345
Epoch 7: loss 16.129176962822918
Epoch 8: loss 16.129177468960023
Epoch 9: loss 16.12917288261111
Epoch 10: loss 16.129185253151608
Epoch 11: loss 16.129180447701334
Epoch 12: loss 16.12918337354717
Epoch 13: loss 16.12918274761737
Epoch 14: loss 16.12917821001527
Epoch 15: loss 16.129179893854587
Epoch 16: loss 16.129179231623972
Epoch 17: loss 16.129179182358577
Epoch 18: loss 16.12918180327755
Epoch 19: loss 16.129175958068167
-----------Time: 0:03:47.923504, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129175583651172
Epoch 1: loss 16.129175284428722
Epoch 2: loss 16.12917924147705
Epoch 3: loss 16.129171775436195
Epoch 4: loss 16.12917720214902
Epoch 5: loss 16.129178768269973
Epoch 6: loss 16.129173954263077
Epoch 7: loss 16.129170823058335
Epoch 8: loss 16.12917073645496
Epoch 9: loss 16.129173627037144
Epoch 10: loss 16.129176853142592
Epoch 11: loss 16.129179760060147
Epoch 12: loss 16.12917355106472
Epoch 13: loss 16.129175816494982
Epoch 14: loss 16.129170724268256
Epoch 15: loss 16.129175487713297
Epoch 16: loss 16.129177754180727
Epoch 17: loss 16.1291782457975
Epoch 18: loss 16.129177235338336
Epoch 19: loss 16.129177859712385
-----------Time: 0:04:27.850008, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12917663689344
Epoch 1: loss 16.129175415630247
Epoch 2: loss 16.129177190221608
Epoch 3: loss 16.129181937331282
Epoch 4: loss 16.1291802892742
Epoch 5: loss 16.129176073193616
Epoch 6: loss 16.129173290995297
Epoch 7: loss 16.12917627129236
Epoch 8: loss 16.129177483480348
Epoch 9: loss 16.129172742075088
Epoch 10: loss 16.12917889298921
Epoch 11: loss 16.12917762790574
Epoch 12: loss 16.129173484945376
Epoch 13: loss 16.12917784363631
Epoch 14: loss 16.12917134112285
Epoch 15: loss 16.129172882092526
Epoch 16: loss 16.12917696463796
Epoch 17: loss 16.129180745886615
Epoch 18: loss 16.129178594285342
Epoch 19: loss 16.12917589143024
-----------Time: 0:04:21.469042, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129182515551435
Epoch 1: loss 16.12917488537903
Epoch 2: loss 16.129182317711983
Epoch 3: loss 16.129178532833247
Epoch 4: loss 16.12916941743885
Epoch 5: loss 16.129178250983333
Epoch 6: loss 16.12918132747756
Epoch 7: loss 16.12917890024937
Epoch 8: loss 16.12918819481423
Epoch 9: loss 16.129178756601853
Epoch 10: loss 16.129183740444713
Epoch 11: loss 16.12918157691603
Epoch 12: loss 16.129174984946985
Epoch 13: loss 16.129174267227977
Epoch 14: loss 16.129179184951493
Epoch 15: loss 16.129178805348666
Epoch 16: loss 16.129177642166777
Epoch 17: loss 16.129179853145814
Epoch 18: loss 16.129180891349176
Epoch 19: loss 16.12918254277705
-----------Time: 0:05:01.133271, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129303678343515
Epoch 1: loss 16.129306820438504
Epoch 2: loss 16.12930700246117
Epoch 3: loss 16.12931209339144
Epoch 4: loss 16.129307088545964
Epoch 5: loss 16.129307734959795
Epoch 6: loss 16.129304180591244
Epoch 7: loss 16.129308575323705
Epoch 8: loss 16.129313567463893
Epoch 9: loss 16.129304209372606
Epoch 10: loss 16.129305646625664
Epoch 11: loss 16.12930220219673
Epoch 12: loss 16.129303586035725
Epoch 13: loss 16.129303484393436
Epoch 14: loss 16.129304906348292
Epoch 15: loss 16.12930733513223
Epoch 16: loss 16.12930921577383
Epoch 17: loss 16.1293059707401
Epoch 18: loss 16.129302484565226
Epoch 19: loss 16.12930708958313
-----------Time: 0:03:09.184299, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 20, rmse: 4.0172319412231445-------------


Epoch 0: loss 16.129176853401884
Epoch 1: loss 16.129182144245938
Epoch 2: loss 16.129183823418007
Epoch 3: loss 16.12918264338217
Epoch 4: loss 16.12918085556694
Epoch 5: loss 16.129181696449432
Epoch 6: loss 16.1291759847752
Epoch 7: loss 16.12918398106727
Epoch 8: loss 16.12917752755991
Epoch 9: loss 16.129182722984673
Epoch 10: loss 16.12918107959484
Epoch 11: loss 16.12917541277804
Epoch 12: loss 16.129182789363313
Epoch 13: loss 16.129178398520224
Epoch 14: loss 16.129174231445745
Epoch 15: loss 16.129183055915025
Epoch 16: loss 16.129178957552803
Epoch 17: loss 16.129177773627593
Epoch 18: loss 16.129182259630678
Epoch 19: loss 16.12917941572097
-----------Time: 0:03:53.621762, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129191192483823
Epoch 1: loss 16.129191152552927
Epoch 2: loss 16.129199474774474
Epoch 3: loss 16.12919961116183
Epoch 4: loss 16.129195615479063
Epoch 5: loss 16.129191113918484
Epoch 6: loss 16.129186153930448
Epoch 7: loss 16.12919779456524
Epoch 8: loss 16.129195694822275
Epoch 9: loss 16.12920265628178
Epoch 10: loss 16.129192326365768
Epoch 11: loss 16.129187970008456
Epoch 12: loss 16.129191190928076
Epoch 13: loss 16.129194708217934
Epoch 14: loss 16.129199892493162
Epoch 15: loss 16.129191172259084
Epoch 16: loss 16.12919428712846
Epoch 17: loss 16.12918993621627
Epoch 18: loss 16.129200474343396
Epoch 19: loss 16.129196858004164
-----------Time: 0:05:00.389815, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.12917042037856
Epoch 1: loss 16.129170457716544
Epoch 2: loss 16.129176298517972
Epoch 3: loss 16.129175081922025
Epoch 4: loss 16.129167933254024
Epoch 5: loss 16.129175927990346
Epoch 6: loss 16.12918026334504
Epoch 7: loss 16.129175699813786
Epoch 8: loss 16.129173297996168
Epoch 9: loss 16.12917012556407
Epoch 10: loss 16.129176991863574
Epoch 11: loss 16.1291691060297
Epoch 12: loss 16.12916821173315
Epoch 13: loss 16.129178622807412
Epoch 14: loss 16.12918117164334
Epoch 15: loss 16.129178635771993
Epoch 16: loss 16.129171094536588
Epoch 17: loss 16.12917886342997
Epoch 18: loss 16.129168393496524
Epoch 19: loss 16.129180214857524
-----------Time: 0:05:14.206480, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129188306050303
Epoch 1: loss 16.12917469946699
Epoch 2: loss 16.129177855823013
Epoch 3: loss 16.12918507112894
Epoch 4: loss 16.12917882323978
Epoch 5: loss 16.12918452869102
Epoch 6: loss 16.129175329545454
Epoch 7: loss 16.129186729298397
Epoch 8: loss 16.12917675668614
Epoch 9: loss 16.129181236466227
Epoch 10: loss 16.129178908546702
Epoch 11: loss 16.12918995618172
Epoch 12: loss 16.129184676746494
Epoch 13: loss 16.129183037505324
Epoch 14: loss 16.12917991641295
Epoch 15: loss 16.129176169131487
Epoch 16: loss 16.129176273366692
Epoch 17: loss 16.12917518460148
Epoch 18: loss 16.12918087423593
Epoch 19: loss 16.129181641479626
-----------Time: 0:04:29.871107, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-15, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12918900847111
Epoch 1: loss 16.129192272692418
Epoch 2: loss 16.129191935872694
Epoch 3: loss 16.12918342670194
Epoch 4: loss 16.1291856063067
Epoch 5: loss 16.129190770875766
Epoch 6: loss 16.129187412272334
Epoch 7: loss 16.12919339335049
Epoch 8: loss 16.129187808469823
Epoch 9: loss 16.12918949412418
Epoch 10: loss 16.129188438288992
Epoch 11: loss 16.12918668651529
Epoch 12: loss 16.129193372866457
Epoch 13: loss 16.129186601986245
Epoch 14: loss 16.129191333538426
Epoch 15: loss 16.129189496457805
Epoch 16: loss 16.12919575420004
Epoch 17: loss 16.129186836645097
Epoch 18: loss 16.129189951514473
Epoch 19: loss 16.129189520053334
-----------Time: 0:02:52.860550, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 20, rmse: 4.0171709060668945-------------


Epoch 0: loss 16.129180477519864
Epoch 1: loss 16.129166336536667
Epoch 2: loss 16.12917392781534
Epoch 3: loss 16.129178716411662
Epoch 4: loss 16.129171520811894
Epoch 5: loss 16.129179673975354
Epoch 6: loss 16.129178362997283
Epoch 7: loss 16.12917258701874
Epoch 8: loss 16.129175165932487
Epoch 9: loss 16.129174174920188
Epoch 10: loss 16.129174931792218
Epoch 11: loss 16.129176629373987
Epoch 12: loss 16.1291727540025
Epoch 13: loss 16.129180404658936
Epoch 14: loss 16.129172365324468
Epoch 15: loss 16.12917693793093
Epoch 16: loss 16.129175758154382
Epoch 17: loss 16.12917339652696
Epoch 18: loss 16.12917503991679
Epoch 19: loss 16.129183332319815
-----------Time: 0:03:21.700835, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 50, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12917075382749
Epoch 1: loss 16.129176911742483
Epoch 2: loss 16.12917424570678
Epoch 3: loss 16.129176240695955
Epoch 4: loss 16.129175264981857
Epoch 5: loss 16.12917427785893
Epoch 6: loss 16.12917374008826
Epoch 7: loss 16.129172333172317
Epoch 8: loss 16.12917190378551
Epoch 9: loss 16.12917185866878
Epoch 10: loss 16.129178571208396
Epoch 11: loss 16.129172813380265
Epoch 12: loss 16.12917433127299
Epoch 13: loss 16.12917624640037
Epoch 14: loss 16.12917507155036
Epoch 15: loss 16.129170911736043
Epoch 16: loss 16.129178018917397
Epoch 17: loss 16.1291752053448
Epoch 18: loss 16.129179828513116
Epoch 19: loss 16.129174805517234
-----------Time: 0:04:25.322885, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129186069919985
Epoch 1: loss 16.129184631629766
Epoch 2: loss 16.129183844939206
Epoch 3: loss 16.12918846629248
Epoch 4: loss 16.1291737724997
Epoch 5: loss 16.129181728601587
Epoch 6: loss 16.1291808599749
Epoch 7: loss 16.12918296827448
Epoch 8: loss 16.129182035602778
Epoch 9: loss 16.129182345196888
Epoch 10: loss 16.129191259121754
Epoch 11: loss 16.129182775361567
Epoch 12: loss 16.129182724540424
Epoch 13: loss 16.129183804749015
Epoch 14: loss 16.12918371840493
Epoch 15: loss 16.12918672877981
Epoch 16: loss 16.129182261964303
Epoch 17: loss 16.129181343553636
Epoch 18: loss 16.12918053897196
Epoch 19: loss 16.129183751075665
-----------Time: 0:05:14.819633, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129174432915278
Epoch 1: loss 16.129174874488786
Epoch 2: loss 16.12916979263372
Epoch 3: loss 16.129166437141787
Epoch 4: loss 16.12916512123718
Epoch 5: loss 16.12916606065046
Epoch 6: loss 16.129170288658454
Epoch 7: loss 16.129173861695996
Epoch 8: loss 16.12917366852379
Epoch 9: loss 16.12916867119777
Epoch 10: loss 16.129169396695527
Epoch 11: loss 16.129163362981185
Epoch 12: loss 16.129172973881733
Epoch 13: loss 16.12917004544298
Epoch 14: loss 16.129169590904894
Epoch 15: loss 16.129175931101845
Epoch 16: loss 16.129169490559065
Epoch 17: loss 16.1291662618607
Epoch 18: loss 16.129169634725166
Epoch 19: loss 16.12917952125263
-----------Time: 0:06:01.388708, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334123e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129389262445727
Epoch 1: loss 16.129402311811504
Epoch 2: loss 16.12938753841622
Epoch 3: loss 16.129388200387545
Epoch 4: loss 16.129390177226313
Epoch 5: loss 16.129395430473092
Epoch 6: loss 16.12939513047277
Epoch 7: loss 16.129394312926514
Epoch 8: loss 16.12939433522559
Epoch 9: loss 16.12939998518844
Epoch 10: loss 16.12939096988058
Epoch 11: loss 16.129395578269275
Epoch 12: loss 16.129399733675633
Epoch 13: loss 16.12939565501957
Epoch 14: loss 16.129393303245227
Epoch 15: loss 16.129396366515582
Epoch 16: loss 16.12938940116671
Epoch 17: loss 16.129395000826996
Epoch 18: loss 16.129392512924586
Epoch 19: loss 16.129395164439963
-----------Time: 0:02:57.507385, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 20, rmse: 4.017219066619873-------------


Epoch 0: loss 16.12920363770029
Epoch 1: loss 16.12920699422939
Epoch 2: loss 16.12920046967615
Epoch 3: loss 16.129208012467302
Epoch 4: loss 16.12920908178565
Epoch 5: loss 16.129208262165065
Epoch 6: loss 16.12920837547547
Epoch 7: loss 16.129205322836068
Epoch 8: loss 16.1292058372705
Epoch 9: loss 16.129219031061666
Epoch 10: loss 16.129202484890065
Epoch 11: loss 16.129211632695906
Epoch 12: loss 16.129209886108033
Epoch 13: loss 16.12920735412606
Epoch 14: loss 16.129204867260814
Epoch 15: loss 16.129208168560815
Epoch 16: loss 16.12920979898607
Epoch 17: loss 16.12920982050727
Epoch 18: loss 16.129200607878545
Epoch 19: loss 16.129210861303548
-----------Time: 0:02:58.993663, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 50, rmse: 4.017180919647217-------------


Epoch 0: loss 16.12917103438095
Epoch 1: loss 16.12917141787315
Epoch 2: loss 16.129172730406967
Epoch 3: loss 16.129177016755563
Epoch 4: loss 16.12917324276707
Epoch 5: loss 16.129177110878395
Epoch 6: loss 16.129173960745366
Epoch 7: loss 16.129175301541967
Epoch 8: loss 16.12917790923707
Epoch 9: loss 16.12916749712564
Epoch 10: loss 16.129179336637048
Epoch 11: loss 16.12917452263015
Epoch 12: loss 16.12916831596835
Epoch 13: loss 16.129169266531168
Epoch 14: loss 16.129171459619087
Epoch 15: loss 16.129171495401323
Epoch 16: loss 16.129174735249222
Epoch 17: loss 16.12917194216066
Epoch 18: loss 16.129176444499112
Epoch 19: loss 16.129170101190663
-----------Time: 0:04:04.227493, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 100, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129170953481985
Epoch 1: loss 16.12916776030656
Epoch 2: loss 16.12917402453109
Epoch 3: loss 16.129172524270185
Epoch 4: loss 16.129165306371345
Epoch 5: loss 16.129173761350167
Epoch 6: loss 16.12916291518468
Epoch 7: loss 16.12916594007989
Epoch 8: loss 16.129167240427005
Epoch 9: loss 16.129172513898524
Epoch 10: loss 16.12916844846633
Epoch 11: loss 16.12916896056714
Epoch 12: loss 16.129169213894983
Epoch 13: loss 16.12916995157944
Epoch 14: loss 16.12916741207801
Epoch 15: loss 16.129172341988227
Epoch 16: loss 16.12916974544266
Epoch 17: loss 16.129170253913387
Epoch 18: loss 16.12917184855641
Epoch 19: loss 16.129173103527506
-----------Time: 0:04:42.773261, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 150, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129174459881597
Epoch 1: loss 16.129178955997055
Epoch 2: loss 16.12917898892708
Epoch 3: loss 16.12918286248353
Epoch 4: loss 16.12917872081962
Epoch 5: loss 16.12917753585724
Epoch 6: loss 16.12917501943276
Epoch 7: loss 16.129174547781435
Epoch 8: loss 16.129176828250603
Epoch 9: loss 16.12917724596929
Epoch 10: loss 16.129180345799757
Epoch 11: loss 16.129178559540275
Epoch 12: loss 16.12917715392079
Epoch 13: loss 16.129175312950792
Epoch 14: loss 16.129179359713994
Epoch 15: loss 16.129173397823415
Epoch 16: loss 16.129178011138652
Epoch 17: loss 16.12917932548751
Epoch 18: loss 16.129177792815167
Epoch 19: loss 16.129176984344117
-----------Time: 0:05:41.109767, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.037017258596549e-15, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12930221438343
Epoch 1: loss 16.129294109966782
Epoch 2: loss 16.129294333216805
Epoch 3: loss 16.12930096226454
Epoch 4: loss 16.12929349829802
Epoch 5: loss 16.12929044358428
Epoch 6: loss 16.12929680141306
Epoch 7: loss 16.129295384125452
Epoch 8: loss 16.12929199181412
Epoch 9: loss 16.129293562861616
Epoch 10: loss 16.12929716805131
Epoch 11: loss 16.12929781109435
Epoch 12: loss 16.12929775379092
Epoch 13: loss 16.12928972975465
Epoch 14: loss 16.129298267706766
Epoch 15: loss 16.129292463724738
Epoch 16: loss 16.12929566649395
Epoch 17: loss 16.12929949415579
Epoch 18: loss 16.129285744702834
Epoch 19: loss 16.129296635207176
-----------Time: 0:03:38.526523, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 20, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129193825589503
Epoch 1: loss 16.129201522140544
Epoch 2: loss 16.129203623439256
Epoch 3: loss 16.12920059050601
Epoch 4: loss 16.129203141675557
Epoch 5: loss 16.12920209206337
Epoch 6: loss 16.129202257232084
Epoch 7: loss 16.12919220812882
Epoch 8: loss 16.129196307268916
Epoch 9: loss 16.12919866241405
Epoch 10: loss 16.129201566220107
Epoch 11: loss 16.129202479444942
Epoch 12: loss 16.129199974688582
Epoch 13: loss 16.129202042538683
Epoch 14: loss 16.129201612633295
Epoch 15: loss 16.129193851000075
Epoch 16: loss 16.129197169413317
Epoch 17: loss 16.129194944432534
Epoch 18: loss 16.129202699583466
Epoch 19: loss 16.129197577278923
-----------Time: 0:03:46.463430, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 50, rmse: 4.017190933227539-------------


Epoch 0: loss 16.12916852184584
Epoch 1: loss 16.12916092615921
Epoch 2: loss 16.129173865326077
Epoch 3: loss 16.129166825301237
Epoch 4: loss 16.129164127631963
Epoch 5: loss 16.129165900667573
Epoch 6: loss 16.12916257525346
Epoch 7: loss 16.129166054946044
Epoch 8: loss 16.129166464626692
Epoch 9: loss 16.129164844573094
Epoch 10: loss 16.129168448984913
Epoch 11: loss 16.129164147856702
Epoch 12: loss 16.12916537093494
Epoch 13: loss 16.129169213894983
Epoch 14: loss 16.129173146310613
Epoch 15: loss 16.12917116687893
Epoch 16: loss 16.12916074050646
Epoch 17: loss 16.12916075061883
Epoch 18: loss 16.1291686488987
Epoch 19: loss 16.129167324178177
-----------Time: 0:03:33.514478, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.1291698208965
Epoch 1: loss 16.12917098278193
Epoch 2: loss 16.129173089785056
Epoch 3: loss 16.1291686488987
Epoch 4: loss 16.129173566362923
Epoch 5: loss 16.129171499809278
Epoch 6: loss 16.129181467754286
Epoch 7: loss 16.12917610612364
Epoch 8: loss 16.129176221767672
Epoch 9: loss 16.129175445189485
Epoch 10: loss 16.129171961866817
Epoch 11: loss 16.129170680707276
Epoch 12: loss 16.129174367314516
Epoch 13: loss 16.129172046655153
Epoch 14: loss 16.129171169471846
Epoch 15: loss 16.12917287924032
Epoch 16: loss 16.129166570676936
Epoch 17: loss 16.12917780344612
Epoch 18: loss 16.129176044671546
Epoch 19: loss 16.12917130404416
-----------Time: 0:04:21.659492, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129176919780523
Epoch 1: loss 16.12917538581172
Epoch 2: loss 16.129173193242384
Epoch 3: loss 16.129170544838505
Epoch 4: loss 16.129179779766304
Epoch 5: loss 16.12918066265403
Epoch 6: loss 16.129173453052516
Epoch 7: loss 16.12917060603131
Epoch 8: loss 16.129170841986618
Epoch 9: loss 16.12918018763191
Epoch 10: loss 16.129181969483433
Epoch 11: loss 16.129182505439065
Epoch 12: loss 16.12917281804751
Epoch 13: loss 16.12917171139118
Epoch 14: loss 16.129173745792674
Epoch 15: loss 16.12917319427955
Epoch 16: loss 16.129180307424605
Epoch 17: loss 16.129175594282124
Epoch 18: loss 16.12916785909664
Epoch 19: loss 16.12916904924485
-----------Time: 0:05:02.780998, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-15, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129413780535998
Epoch 1: loss 16.12940889522393
Epoch 2: loss 16.129411127983452
Epoch 3: loss 16.129408705941096
Epoch 4: loss 16.12941469350154
Epoch 5: loss 16.12940978251961
Epoch 6: loss 16.12941125088765
Epoch 7: loss 16.129412436109316
Epoch 8: loss 16.129408814584256
Epoch 9: loss 16.12941515244758
Epoch 10: loss 16.12941221389646
Epoch 11: loss 16.1294106112154
Epoch 12: loss 16.12940675943944
Epoch 13: loss 16.129408250365845
Epoch 14: loss 16.129409418474275
Epoch 15: loss 16.129412176039896
Epoch 16: loss 16.129409693582605
Epoch 17: loss 16.12941080412831
Epoch 18: loss 16.12940676747748
Epoch 19: loss 16.129407975776097
-----------Time: 0:03:20.318979, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 20, rmse: 4.017213821411133-------------


Epoch 0: loss 16.12922857765791
Epoch 1: loss 16.129228476015623
Epoch 2: loss 16.129229547926887
Epoch 3: loss 16.129228513353606
Epoch 4: loss 16.129227363654877
Epoch 5: loss 16.12923354153532
Epoch 6: loss 16.129229869189114
Epoch 7: loss 16.12922351706475
Epoch 8: loss 16.129231471610886
Epoch 9: loss 16.129224857083475
Epoch 10: loss 16.129234811286036
Epoch 11: loss 16.129233531163656
Epoch 12: loss 16.129225397187774
Epoch 13: loss 16.129232010937308
Epoch 14: loss 16.129226241700348
Epoch 15: loss 16.129228009031543
Epoch 16: loss 16.129226487768026
Epoch 17: loss 16.129232790886284
Epoch 18: loss 16.129231348706693
Epoch 19: loss 16.129235863231848
-----------Time: 0:04:02.038041, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129165983381576
Epoch 1: loss 16.129170310698235
Epoch 2: loss 16.129163870674038
Epoch 3: loss 16.129171197475333
Epoch 4: loss 16.12915928406583
Epoch 5: loss 16.12915992503454
Epoch 6: loss 16.129167578283894
Epoch 7: loss 16.129160711206517
Epoch 8: loss 16.129169023315693
Epoch 9: loss 16.12917032521856
Epoch 10: loss 16.12916243471744
Epoch 11: loss 16.12917377898199
Epoch 12: loss 16.12915820385724
Epoch 13: loss 16.12916242953161
Epoch 14: loss 16.129166357279992
Epoch 15: loss 16.129159651481956
Epoch 16: loss 16.12915980627901
Epoch 17: loss 16.129171599117942
Epoch 18: loss 16.1291682775932
Epoch 19: loss 16.129159106969702
-----------Time: 0:05:11.150061, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129177493852012
Epoch 1: loss 16.129179734649576
Epoch 2: loss 16.129179333266258
Epoch 3: loss 16.12917870266921
Epoch 4: loss 16.12918141122873
Epoch 5: loss 16.12918113508323
Epoch 6: loss 16.12917158589407
Epoch 7: loss 16.12917709350586
Epoch 8: loss 16.129177196703896
Epoch 9: loss 16.12917064181354
Epoch 10: loss 16.12917118399217
Epoch 11: loss 16.12917452703811
Epoch 12: loss 16.129185381500925
Epoch 13: loss 16.129179633785164
Epoch 14: loss 16.12917870240992
Epoch 15: loss 16.129173681229076
Epoch 16: loss 16.129175954697377
Epoch 17: loss 16.129170679151525
Epoch 18: loss 16.12918093413228
Epoch 19: loss 16.129175125223714
-----------Time: 0:04:54.742186, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129183504230113
Epoch 1: loss 16.12917478140312
Epoch 2: loss 16.129175117444966
Epoch 3: loss 16.129183143296274
Epoch 4: loss 16.12917923655051
Epoch 5: loss 16.129178653144525
Epoch 6: loss 16.12917087102727
Epoch 7: loss 16.12917986429535
Epoch 8: loss 16.12917559531929
Epoch 9: loss 16.129183485820413
Epoch 10: loss 16.129184928259296
Epoch 11: loss 16.129176211655302
Epoch 12: loss 16.12918017414875
Epoch 13: loss 16.129181799388178
Epoch 14: loss 16.129185725840102
Epoch 15: loss 16.129174685205953
Epoch 16: loss 16.129182103796456
Epoch 17: loss 16.129184825839136
Epoch 18: loss 16.12917536921706
Epoch 19: loss 16.129172367398798
-----------Time: 0:04:46.019579, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.054802310718631e-15, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129403062460536
Epoch 1: loss 16.129403378018353
Epoch 2: loss 16.12940148752367
Epoch 3: loss 16.12940715459976
Epoch 4: loss 16.129401896685735
Epoch 5: loss 16.129399682076617
Epoch 6: loss 16.12939881708001
Epoch 7: loss 16.12941161934094
Epoch 8: loss 16.129407953736315
Epoch 9: loss 16.129403779920253
Epoch 10: loss 16.129406369724244
Epoch 11: loss 16.12940103194842
Epoch 12: loss 16.129404801269665
Epoch 13: loss 16.12939556789761
Epoch 14: loss 16.129404557535608
Epoch 15: loss 16.12939698388876
Epoch 16: loss 16.12941102737833
Epoch 17: loss 16.12940253376507
Epoch 18: loss 16.129403245779663
Epoch 19: loss 16.12940565952469
-----------Time: 0:02:58.753111, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 20, rmse: 4.017241477966309-------------


Epoch 0: loss 16.1292019315619
Epoch 1: loss 16.12920097529467
Epoch 2: loss 16.12919407139789
Epoch 3: loss 16.12920035792149
Epoch 4: loss 16.129204520328724
Epoch 5: loss 16.129195774943366
Epoch 6: loss 16.12919521954087
Epoch 7: loss 16.12919513086316
Epoch 8: loss 16.12919671798673
Epoch 9: loss 16.129197721963607
Epoch 10: loss 16.129203010733328
Epoch 11: loss 16.129194742963
Epoch 12: loss 16.129196345903356
Epoch 13: loss 16.129203358702583
Epoch 14: loss 16.129196973648195
Epoch 15: loss 16.12920042870808
Epoch 16: loss 16.129204214883277
Epoch 17: loss 16.12919781530856
Epoch 18: loss 16.12920453147826
Epoch 19: loss 16.129195739420425
-----------Time: 0:03:25.228822, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129180960061436
Epoch 1: loss 16.12917889195204
Epoch 2: loss 16.129173986933814
Epoch 3: loss 16.129179741391155
Epoch 4: loss 16.12916854518208
Epoch 5: loss 16.12918133395985
Epoch 6: loss 16.129176072675033
Epoch 7: loss 16.12917912142506
Epoch 8: loss 16.129179304744188
Epoch 9: loss 16.12916748364248
Epoch 10: loss 16.129174966018702
Epoch 11: loss 16.129176290739224
Epoch 12: loss 16.129177153142916
Epoch 13: loss 16.12917856835619
Epoch 14: loss 16.129172356508555
Epoch 15: loss 16.129179009151823
Epoch 16: loss 16.129172475264085
Epoch 17: loss 16.129176948302593
Epoch 18: loss 16.129179955306682
Epoch 19: loss 16.129174103096428
-----------Time: 0:04:38.039835, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129183063175187
Epoch 1: loss 16.129176612260746
Epoch 2: loss 16.129180803967923
Epoch 3: loss 16.129187330336205
Epoch 4: loss 16.12918240250032
Epoch 5: loss 16.129181293251076
Epoch 6: loss 16.12918404796449
Epoch 7: loss 16.129184321776364
Epoch 8: loss 16.129184292995003
Epoch 9: loss 16.129181580027527
Epoch 10: loss 16.12918369273507
Epoch 11: loss 16.129184671819957
Epoch 12: loss 16.12918207760801
Epoch 13: loss 16.129176927040685
Epoch 14: loss 16.12917967890189
Epoch 15: loss 16.129184390488625
Epoch 16: loss 16.129179737761074
Epoch 17: loss 16.129178991779288
Epoch 18: loss 16.12918361779981
Epoch 19: loss 16.129180368617412
-----------Time: 0:05:29.534650, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917500257881
Epoch 1: loss 16.12917156229854
Epoch 2: loss 16.129173673709623
Epoch 3: loss 16.129178840353024
Epoch 4: loss 16.129179541995953
Epoch 5: loss 16.129173754867878
Epoch 6: loss 16.129176516582163
Epoch 7: loss 16.1291818110563
Epoch 8: loss 16.129171974831394
Epoch 9: loss 16.129176588924505
Epoch 10: loss 16.129163546559603
Epoch 11: loss 16.129174085723893
Epoch 12: loss 16.12916380377682
Epoch 13: loss 16.12917066307545
Epoch 14: loss 16.129173068004565
Epoch 15: loss 16.12917817345516
Epoch 16: loss 16.12917695634063
Epoch 17: loss 16.129174983131943
Epoch 18: loss 16.12917693300439
Epoch 19: loss 16.12917913309318
-----------Time: 0:05:45.213116, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-15, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12922123322479
Epoch 1: loss 16.12921539371982
Epoch 2: loss 16.12921832527007
Epoch 3: loss 16.129220025704047
Epoch 4: loss 16.12921906969611
Epoch 5: loss 16.129226017153865
Epoch 6: loss 16.129225284914533
Epoch 7: loss 16.129220773241585
Epoch 8: loss 16.12921796355836
Epoch 9: loss 16.129226785175433
Epoch 10: loss 16.129222596320464
Epoch 11: loss 16.12921733425777
Epoch 12: loss 16.129217788536565
Epoch 13: loss 16.129225725450873
Epoch 14: loss 16.129219954398874
Epoch 15: loss 16.129223413866715
Epoch 16: loss 16.12921678611544
Epoch 17: loss 16.129220693639077
Epoch 18: loss 16.129227352764634
Epoch 19: loss 16.129224358206535
-----------Time: 0:02:36.187123, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 20, rmse: 4.017177581787109-------------


Epoch 0: loss 16.129154740759315
Epoch 1: loss 16.129165672231718
Epoch 2: loss 16.129165682862673
Epoch 3: loss 16.129160688648152
Epoch 4: loss 16.12916025329764
Epoch 5: loss 16.129164196344224
Epoch 6: loss 16.12916817232083
Epoch 7: loss 16.12916568441842
Epoch 8: loss 16.129161409219364
Epoch 9: loss 16.1291621015278
Epoch 10: loss 16.129169219080815
Epoch 11: loss 16.129164358920026
Epoch 12: loss 16.12916179997173
Epoch 13: loss 16.129165195913142
Epoch 14: loss 16.1291721864133
Epoch 15: loss 16.12916357793388
Epoch 16: loss 16.129169311388605
Epoch 17: loss 16.129163560820636
Epoch 18: loss 16.12916753083354
Epoch 19: loss 16.129165926596727
-----------Time: 0:03:10.447039, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129175879502828
Epoch 1: loss 16.129162196687798
Epoch 2: loss 16.12916843472388
Epoch 3: loss 16.129164853129716
Epoch 4: loss 16.129163960129624
Epoch 5: loss 16.129165477763056
Epoch 6: loss 16.129163324606036
Epoch 7: loss 16.129164353993485
Epoch 8: loss 16.12916783550111
Epoch 9: loss 16.12916829574361
Epoch 10: loss 16.129168691681805
Epoch 11: loss 16.12916910265891
Epoch 12: loss 16.129166629276824
Epoch 13: loss 16.129165879405665
Epoch 14: loss 16.12916807975375
Epoch 15: loss 16.129172609577108
Epoch 16: loss 16.129168464283115
Epoch 17: loss 16.129169656246365
Epoch 18: loss 16.12916160939244
Epoch 19: loss 16.12916678692609
-----------Time: 0:04:09.349321, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129180121512565
Epoch 1: loss 16.12917536584627
Epoch 2: loss 16.129175374662182
Epoch 3: loss 16.129172485635745
Epoch 4: loss 16.129173584254037
Epoch 5: loss 16.129176259364947
Epoch 6: loss 16.12916863411908
Epoch 7: loss 16.129174564376093
Epoch 8: loss 16.129178385555647
Epoch 9: loss 16.129173847175668
Epoch 10: loss 16.1291766750093
Epoch 11: loss 16.129179274407075
Epoch 12: loss 16.12917919350811
Epoch 13: loss 16.12917632159492
Epoch 14: loss 16.129177202667602
Epoch 15: loss 16.129177975875
Epoch 16: loss 16.129175896356777
Epoch 17: loss 16.129173193501675
Epoch 18: loss 16.129170464976706
Epoch 19: loss 16.129181770088234
-----------Time: 0:04:56.585351, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129175100591016
Epoch 1: loss 16.129177354353157
Epoch 2: loss 16.12917571588986
Epoch 3: loss 16.1291713211574
Epoch 4: loss 16.129177759366556
Epoch 5: loss 16.12918298875851
Epoch 6: loss 16.129176266365818
Epoch 7: loss 16.129179029376562
Epoch 8: loss 16.12917684277093
Epoch 9: loss 16.129176093159064
Epoch 10: loss 16.129174909752436
Epoch 11: loss 16.129178119263226
Epoch 12: loss 16.12917296532511
Epoch 13: loss 16.129182164729972
Epoch 14: loss 16.129175411740874
Epoch 15: loss 16.129179148909966
Epoch 16: loss 16.129175917100103
Epoch 17: loss 16.129175855648004
Epoch 18: loss 16.129179486766855
Epoch 19: loss 16.12917548278676
-----------Time: 0:05:48.545622, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-14, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12931036443539
Epoch 1: loss 16.129313657697352
Epoch 2: loss 16.129310386993758
Epoch 3: loss 16.129320165915228
Epoch 4: loss 16.129315084060163
Epoch 5: loss 16.129307499004486
Epoch 6: loss 16.12930722363686
Epoch 7: loss 16.129316567985697
Epoch 8: loss 16.129315953724017
Epoch 9: loss 16.129315402729475
Epoch 10: loss 16.129312939978345
Epoch 11: loss 16.129308779645445
Epoch 12: loss 16.129323552781436
Epoch 13: loss 16.129319169976387
Epoch 14: loss 16.129315744216445
Epoch 15: loss 16.129313290281228
Epoch 16: loss 16.129312428136828
Epoch 17: loss 16.129311722604523
Epoch 18: loss 16.12931839676899
Epoch 19: loss 16.129313160894746
-----------Time: 0:03:39.685473, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 20, rmse: 4.017223834991455-------------


Epoch 0: loss 16.12915107230248
Epoch 1: loss 16.129156299620107
Epoch 2: loss 16.129152160808403
Epoch 3: loss 16.129151260029563
Epoch 4: loss 16.129154703939914
Epoch 5: loss 16.129150481377042
Epoch 6: loss 16.12915274136218
Epoch 7: loss 16.12915808795392
Epoch 8: loss 16.12915321171705
Epoch 9: loss 16.129146151986053
Epoch 10: loss 16.129153584319006
Epoch 11: loss 16.12915533764846
Epoch 12: loss 16.129156455195034
Epoch 13: loss 16.129149417503818
Epoch 14: loss 16.129145705486007
Epoch 15: loss 16.129147918280083
Epoch 16: loss 16.12914934619864
Epoch 17: loss 16.12915306806953
Epoch 18: loss 16.129156068332044
Epoch 19: loss 16.12915318682506
-----------Time: 0:03:13.886131, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 50, rmse: 4.017197132110596-------------


Epoch 0: loss 16.129190864220725
Epoch 1: loss 16.129188354278533
Epoch 2: loss 16.12919376543386
Epoch 3: loss 16.12918800916148
Epoch 4: loss 16.129189220571597
Epoch 5: loss 16.12919176629602
Epoch 6: loss 16.12919019421136
Epoch 7: loss 16.129182724799715
Epoch 8: loss 16.129186184786143
Epoch 9: loss 16.129192230427893
Epoch 10: loss 16.129185416505283
Epoch 11: loss 16.129182681757317
Epoch 12: loss 16.129188205963764
Epoch 13: loss 16.129187583404757
Epoch 14: loss 16.129194443221973
Epoch 15: loss 16.129188706137164
Epoch 16: loss 16.129190726796203
Epoch 17: loss 16.1291920491831
Epoch 18: loss 16.129192227057104
Epoch 19: loss 16.129190178394577
-----------Time: 0:03:47.443607, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 100, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129169552529746
Epoch 1: loss 16.129168732131287
Epoch 2: loss 16.12916635053841
Epoch 3: loss 16.129164512679914
Epoch 4: loss 16.12917203887641
Epoch 5: loss 16.12917392314809
Epoch 6: loss 16.129166142586588
Epoch 7: loss 16.12916974855416
Epoch 8: loss 16.129170049591647
Epoch 9: loss 16.12916716419529
Epoch 10: loss 16.12916828744628
Epoch 11: loss 16.129169906722
Epoch 12: loss 16.129175561092808
Epoch 13: loss 16.129174854264043
Epoch 14: loss 16.12916789928683
Epoch 15: loss 16.129166180183862
Epoch 16: loss 16.129166883382542
Epoch 17: loss 16.129167824870155
Epoch 18: loss 16.129161039210324
Epoch 19: loss 16.1291662618607
-----------Time: 0:04:33.972538, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129174590305247
Epoch 1: loss 16.12917987233339
Epoch 2: loss 16.129175074921154
Epoch 3: loss 16.129181862136733
Epoch 4: loss 16.12918642048216
Epoch 5: loss 16.129178921251988
Epoch 6: loss 16.129184906219514
Epoch 7: loss 16.129174755992548
Epoch 8: loss 16.129181197572493
Epoch 9: loss 16.12918155306121
Epoch 10: loss 16.129180423068636
Epoch 11: loss 16.129173062559442
Epoch 12: loss 16.129176061525495
Epoch 13: loss 16.129179032747352
Epoch 14: loss 16.12917011519241
Epoch 15: loss 16.129179601892304
Epoch 16: loss 16.129178917881198
Epoch 17: loss 16.12917322461666
Epoch 18: loss 16.12917522038371
Epoch 19: loss 16.129182656346746
-----------Time: 0:05:12.764480, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.629750834620647e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12918348556112
Epoch 1: loss 16.12919270363497
Epoch 2: loss 16.12918840769259
Epoch 3: loss 16.129192262320753
Epoch 4: loss 16.12918404666803
Epoch 5: loss 16.12919332100815
Epoch 6: loss 16.129183207081997
Epoch 7: loss 16.1291932917082
Epoch 8: loss 16.129188473552645
Epoch 9: loss 16.12919357926253
Epoch 10: loss 16.129190511843507
Epoch 11: loss 16.12919065600961
Epoch 12: loss 16.129191494039894
Epoch 13: loss 16.129191866123268
Epoch 14: loss 16.12919506085444
Epoch 15: loss 16.129194169669386
Epoch 16: loss 16.12919163068654
Epoch 17: loss 16.129195139419778
Epoch 18: loss 16.12918301728058
Epoch 19: loss 16.129191248231507
-----------Time: 0:03:39.955214, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 20, rmse: 4.017228126525879-------------


Epoch 0: loss 16.129169082174876
Epoch 1: loss 16.1291687930648
Epoch 2: loss 16.12917190871205
Epoch 3: loss 16.12916962201988
Epoch 4: loss 16.129164378885473
Epoch 5: loss 16.129167615103295
Epoch 6: loss 16.129174534298272
Epoch 7: loss 16.129169997733335
Epoch 8: loss 16.129169960913934
Epoch 9: loss 16.129173131530994
Epoch 10: loss 16.129174280711137
Epoch 11: loss 16.12917780163108
Epoch 12: loss 16.12917774017898
Epoch 13: loss 16.129168273444535
Epoch 14: loss 16.129176879849624
Epoch 15: loss 16.129173014071924
Epoch 16: loss 16.12917176506453
Epoch 17: loss 16.12917130611849
Epoch 18: loss 16.129173845101338
Epoch 19: loss 16.129167824610864
-----------Time: 0:03:54.520117, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 50, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129182724281133
Epoch 1: loss 16.129190599743342
Epoch 2: loss 16.12919401798383
Epoch 3: loss 16.129189245722877
Epoch 4: loss 16.129193960161818
Epoch 5: loss 16.129190172690162
Epoch 6: loss 16.129186921692725
Epoch 7: loss 16.129197253683067
Epoch 8: loss 16.129198071488613
Epoch 9: loss 16.12918863353553
Epoch 10: loss 16.129189225498134
Epoch 11: loss 16.12919437943625
Epoch 12: loss 16.12919614391524
Epoch 13: loss 16.129197839422677
Epoch 14: loss 16.129193549962586
Epoch 15: loss 16.129195202168333
Epoch 16: loss 16.12918841106338
Epoch 17: loss 16.129199731473108
Epoch 18: loss 16.129185544076726
Epoch 19: loss 16.12919639024221
-----------Time: 0:04:55.479172, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 100, rmse: 4.017192363739014-------------


Epoch 0: loss 16.12917733827708
Epoch 1: loss 16.12917880638583
Epoch 2: loss 16.129175605950245
Epoch 3: loss 16.129174659795382
Epoch 4: loss 16.12917727397278
Epoch 5: loss 16.129175624878528
Epoch 6: loss 16.129173445014477
Epoch 7: loss 16.129178143895924
Epoch 8: loss 16.129175835682556
Epoch 9: loss 16.129177717620617
Epoch 10: loss 16.129179053231386
Epoch 11: loss 16.12917111216841
Epoch 12: loss 16.129174509146992
Epoch 13: loss 16.12917276904141
Epoch 14: loss 16.129171495919906
Epoch 15: loss 16.129171069126016
Epoch 16: loss 16.129173240174154
Epoch 17: loss 16.129174560746012
Epoch 18: loss 16.12917536066044
Epoch 19: loss 16.12917930526277
-----------Time: 0:04:02.698558, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129171869040444
Epoch 1: loss 16.12917552842207
Epoch 2: loss 16.12917373360597
Epoch 3: loss 16.129179471727944
Epoch 4: loss 16.129179013559778
Epoch 5: loss 16.129172641469967
Epoch 6: loss 16.129170821502587
Epoch 7: loss 16.129178362997283
Epoch 8: loss 16.129175117704257
Epoch 9: loss 16.129169664543696
Epoch 10: loss 16.129171522886224
Epoch 11: loss 16.12917790405124
Epoch 12: loss 16.129182774842985
Epoch 13: loss 16.129175064030907
Epoch 14: loss 16.12917760586596
Epoch 15: loss 16.129179196101028
Epoch 16: loss 16.129177342166454
Epoch 17: loss 16.12917797431925
Epoch 18: loss 16.12917435123844
Epoch 19: loss 16.129174323234952
-----------Time: 0:04:56.439378, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318866e-14, embedding_dim: 200, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129247217090185
Epoch 1: loss 16.129252718478977
Epoch 2: loss 16.129244516568704
Epoch 3: loss 16.129252323577948
Epoch 4: loss 16.129252112514628
Epoch 5: loss 16.129259585815646
Epoch 6: loss 16.129252646136635
Epoch 7: loss 16.129249539564587
Epoch 8: loss 16.129250001362838
Epoch 9: loss 16.129249673099736
Epoch 10: loss 16.129252345358438
Epoch 11: loss 16.12924880680667
Epoch 12: loss 16.12925451848091
Epoch 13: loss 16.12925551986487
Epoch 14: loss 16.12925374579209
Epoch 15: loss 16.12925212651637
Epoch 16: loss 16.129256425570247
Epoch 17: loss 16.129252611132276
Epoch 18: loss 16.12925060006702
Epoch 19: loss 16.129247756416607
-----------Time: 0:03:15.948279, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 20, rmse: 4.0172038078308105-------------


Epoch 0: loss 16.129164222273378
Epoch 1: loss 16.12916008709176
Epoch 2: loss 16.129161036098825
Epoch 3: loss 16.129162730309805
Epoch 4: loss 16.129166920201943
Epoch 5: loss 16.12916236393085
Epoch 6: loss 16.129164244053868
Epoch 7: loss 16.129166662725435
Epoch 8: loss 16.129160532295348
Epoch 9: loss 16.12915877818802
Epoch 10: loss 16.12916702028848
Epoch 11: loss 16.129160068682058
Epoch 12: loss 16.129158630132544
Epoch 13: loss 16.12916945451754
Epoch 14: loss 16.129158314834022
Epoch 15: loss 16.12916162780214
Epoch 16: loss 16.1291647431901
Epoch 17: loss 16.129168512770637
Epoch 18: loss 16.129171132652445
Epoch 19: loss 16.12916316280811
-----------Time: 0:03:49.573465, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 50, rmse: 4.017195701599121-------------


Epoch 0: loss 16.12916791717795
Epoch 1: loss 16.129170663853323
Epoch 2: loss 16.129170579583572
Epoch 3: loss 16.12916673921644
Epoch 4: loss 16.129173005774593
Epoch 5: loss 16.12916567741755
Epoch 6: loss 16.129167973962797
Epoch 7: loss 16.129174696874074
Epoch 8: loss 16.129161100662422
Epoch 9: loss 16.12917044708559
Epoch 10: loss 16.129162585884412
Epoch 11: loss 16.129170094708375
Epoch 12: loss 16.12916715278646
Epoch 13: loss 16.12917232202278
Epoch 14: loss 16.129170869471523
Epoch 15: loss 16.129163588564833
Epoch 16: loss 16.12917037344679
Epoch 17: loss 16.129168794101965
Epoch 18: loss 16.12916918822512
Epoch 19: loss 16.129166464626692
-----------Time: 0:04:49.615264, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129176123755467
Epoch 1: loss 16.129173415973824
Epoch 2: loss 16.129183128516658
Epoch 3: loss 16.129177238968417
Epoch 4: loss 16.129172448557053
Epoch 5: loss 16.129175090219352
Epoch 6: loss 16.12918331987382
Epoch 7: loss 16.129172196007087
Epoch 8: loss 16.12917353265502
Epoch 9: loss 16.12917740673005
Epoch 10: loss 16.129182782881024
Epoch 11: loss 16.129175075180445
Epoch 12: loss 16.12917800569353
Epoch 13: loss 16.12918170267243
Epoch 14: loss 16.129178855132643
Epoch 15: loss 16.129177968355545
Epoch 16: loss 16.129180008980033
Epoch 17: loss 16.129176394196552
Epoch 18: loss 16.12917966178865
Epoch 19: loss 16.129181543208126
-----------Time: 0:05:46.895528, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 150, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129178592211012
Epoch 1: loss 16.129175961698248
Epoch 2: loss 16.12918035694929
Epoch 3: loss 16.12917545270894
Epoch 4: loss 16.129178618399457
Epoch 5: loss 16.129182517366477
Epoch 6: loss 16.12917413524858
Epoch 7: loss 16.12917316964685
Epoch 8: loss 16.129174452362143
Epoch 9: loss 16.12917615927841
Epoch 10: loss 16.129177968096254
Epoch 11: loss 16.129173878290654
Epoch 12: loss 16.129168144058053
Epoch 13: loss 16.12917548304605
Epoch 14: loss 16.129175131965294
Epoch 15: loss 16.129175121852924
Epoch 16: loss 16.129170332997308
Epoch 17: loss 16.129179781322055
Epoch 18: loss 16.129167262726078
Epoch 19: loss 16.12917930163269
-----------Time: 0:04:49.741646, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-14, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129185394724793
Epoch 1: loss 16.129182519959393
Epoch 2: loss 16.129178820387573
Epoch 3: loss 16.129184722381808
Epoch 4: loss 16.129178832055693
Epoch 5: loss 16.12918383560471
Epoch 6: loss 16.129184452200015
Epoch 7: loss 16.12918402462825
Epoch 8: loss 16.12918696784662
Epoch 9: loss 16.12918645211573
Epoch 10: loss 16.129182691351104
Epoch 11: loss 16.129178527128833
Epoch 12: loss 16.129171355643177
Epoch 13: loss 16.129186848053923
Epoch 14: loss 16.129183955138114
Epoch 15: loss 16.129185013047632
Epoch 16: loss 16.12918696447583
Epoch 17: loss 16.1291802892742
Epoch 18: loss 16.12918615937557
Epoch 19: loss 16.129181185126498
-----------Time: 0:02:49.041700, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 20, rmse: 4.017178535461426-------------


Epoch 0: loss 16.12922000625718
Epoch 1: loss 16.12921426165292
Epoch 2: loss 16.12922677739669
Epoch 3: loss 16.12922123478054
Epoch 4: loss 16.129215484471864
Epoch 5: loss 16.129213345057295
Epoch 6: loss 16.12921814246953
Epoch 7: loss 16.12922419926082
Epoch 8: loss 16.129216895277178
Epoch 9: loss 16.129219993033313
Epoch 10: loss 16.129221328644082
Epoch 11: loss 16.12922345301974
Epoch 12: loss 16.1292202971823
Epoch 13: loss 16.129220872809537
Epoch 14: loss 16.129212265367286
Epoch 15: loss 16.129220920259893
Epoch 16: loss 16.12922369571663
Epoch 17: loss 16.12922123970708
Epoch 18: loss 16.129214012473742
Epoch 19: loss 16.12922177721846
-----------Time: 0:03:16.402662, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 50, rmse: 4.017190456390381-------------


Epoch 0: loss 16.12917509825739
Epoch 1: loss 16.12917307552402
Epoch 2: loss 16.129176837066517
Epoch 3: loss 16.12917301873917
Epoch 4: loss 16.12916737525861
Epoch 5: loss 16.129176048042336
Epoch 6: loss 16.12917008926325
Epoch 7: loss 16.12916500585244
Epoch 8: loss 16.129171894191725
Epoch 9: loss 16.1291680291919
Epoch 10: loss 16.12916905572714
Epoch 11: loss 16.129166153476834
Epoch 12: loss 16.129167157972294
Epoch 13: loss 16.129169992806798
Epoch 14: loss 16.129169680879063
Epoch 15: loss 16.12917030966107
Epoch 16: loss 16.12916747871594
Epoch 17: loss 16.129171079238386
Epoch 18: loss 16.12916945788833
Epoch 19: loss 16.12917075719828
-----------Time: 0:04:17.417015, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129178415633465
Epoch 1: loss 16.129170634034796
Epoch 2: loss 16.129172445186263
Epoch 3: loss 16.129171154173644
Epoch 4: loss 16.12918173923254
Epoch 5: loss 16.129175435077112
Epoch 6: loss 16.129177611829665
Epoch 7: loss 16.129169252788717
Epoch 8: loss 16.12917498961423
Epoch 9: loss 16.129173426345485
Epoch 10: loss 16.129169215191443
Epoch 11: loss 16.129169793152304
Epoch 12: loss 16.129170927812122
Epoch 13: loss 16.129173987711688
Epoch 14: loss 16.12917255331084
Epoch 15: loss 16.129177314422257
Epoch 16: loss 16.12917266376904
Epoch 17: loss 16.129170360482213
Epoch 18: loss 16.12916936220975
Epoch 19: loss 16.129172632394763
-----------Time: 0:05:11.802618, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129168221845518
Epoch 1: loss 16.129172715886643
Epoch 2: loss 16.129163120543588
Epoch 3: loss 16.129170905513046
Epoch 4: loss 16.129167570764437
Epoch 5: loss 16.129164439818986
Epoch 6: loss 16.12916835615854
Epoch 7: loss 16.12916076358341
Epoch 8: loss 16.129163909049186
Epoch 9: loss 16.129163484848213
Epoch 10: loss 16.12917156722508
Epoch 11: loss 16.1291682591835
Epoch 12: loss 16.129173004737428
Epoch 13: loss 16.129168944231772
Epoch 14: loss 16.129168052787428
Epoch 15: loss 16.12917339030396
Epoch 16: loss 16.12917737120711
Epoch 17: loss 16.1291721065515
Epoch 18: loss 16.129169111474823
Epoch 19: loss 16.129168994793623
-----------Time: 0:06:15.131280, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-14, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129324729187193
Epoch 1: loss 16.129324302393304
Epoch 2: loss 16.129328613374597
Epoch 3: loss 16.129327978888174
Epoch 4: loss 16.129325743535734
Epoch 5: loss 16.129335167487074
Epoch 6: loss 16.129327137227808
Epoch 7: loss 16.129328188136455
Epoch 8: loss 16.12932769677897
Epoch 9: loss 16.12933032184661
Epoch 10: loss 16.129326137140303
Epoch 11: loss 16.129319860729073
Epoch 12: loss 16.129331490732913
Epoch 13: loss 16.129323165918446
Epoch 14: loss 16.129326577935938
Epoch 15: loss 16.129324508530086
Epoch 16: loss 16.129335519086414
Epoch 17: loss 16.129328968085435
Epoch 18: loss 16.129327749155863
Epoch 19: loss 16.129319472828918
-----------Time: 0:03:12.686923, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 20, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129196156101944
Epoch 1: loss 16.129188609162124
Epoch 2: loss 16.129194794043435
Epoch 3: loss 16.129186379254804
Epoch 4: loss 16.129195376152964
Epoch 5: loss 16.129195739679716
Epoch 6: loss 16.129195792056606
Epoch 7: loss 16.129189115299226
Epoch 8: loss 16.129190623857458
Epoch 9: loss 16.129191726624416
Epoch 10: loss 16.129194214267535
Epoch 11: loss 16.129195267769095
Epoch 12: loss 16.129192980299052
Epoch 13: loss 16.1291948933521
Epoch 14: loss 16.12919615247186
Epoch 15: loss 16.12919273993579
Epoch 16: loss 16.12918652756957
Epoch 17: loss 16.129197121444378
Epoch 18: loss 16.12919150026289
Epoch 19: loss 16.129190777876637
-----------Time: 0:02:55.620517, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12915616349204
Epoch 1: loss 16.12916830067015
Epoch 2: loss 16.129156711893668
Epoch 3: loss 16.129167046476926
Epoch 4: loss 16.129158503338978
Epoch 5: loss 16.12916475822901
Epoch 6: loss 16.129157349750876
Epoch 7: loss 16.129166607237043
Epoch 8: loss 16.129165799025287
Epoch 9: loss 16.129164940251677
Epoch 10: loss 16.129164501011793
Epoch 11: loss 16.129165080528406
Epoch 12: loss 16.12916945399896
Epoch 13: loss 16.129161920283007
Epoch 14: loss 16.12915844681342
Epoch 15: loss 16.12916506471162
Epoch 16: loss 16.12917110516754
Epoch 17: loss 16.12916268908245
Epoch 18: loss 16.12916452564449
Epoch 19: loss 16.12916357974892
-----------Time: 0:04:05.850293, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 100, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.12917926196108
Epoch 1: loss 16.129179495323477
Epoch 2: loss 16.129166967133713
Epoch 3: loss 16.129178923326318
Epoch 4: loss 16.129175999295523
Epoch 5: loss 16.129169079841255
Epoch 6: loss 16.129176520730827
Epoch 7: loss 16.129175489009757
Epoch 8: loss 16.12917583464539
Epoch 9: loss 16.12918098443484
Epoch 10: loss 16.12917663378194
Epoch 11: loss 16.12918021407965
Epoch 12: loss 16.129176230065003
Epoch 13: loss 16.12917785400797
Epoch 14: loss 16.129174437323233
Epoch 15: loss 16.129178043290803
Epoch 16: loss 16.12917777181255
Epoch 17: loss 16.129175813902066
Epoch 18: loss 16.12917591502577
Epoch 19: loss 16.129173835766842
-----------Time: 0:04:42.231053, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129174727470478
Epoch 1: loss 16.129177866194674
Epoch 2: loss 16.12917629696222
Epoch 3: loss 16.129178616065833
Epoch 4: loss 16.129181658852158
Epoch 5: loss 16.129169034724523
Epoch 6: loss 16.129184990489268
Epoch 7: loss 16.129180554788743
Epoch 8: loss 16.129173089785056
Epoch 9: loss 16.12917156904012
Epoch 10: loss 16.129177777516965
Epoch 11: loss 16.12917609575198
Epoch 12: loss 16.129180656949615
Epoch 13: loss 16.129176471206144
Epoch 14: loss 16.129171375090042
Epoch 15: loss 16.129165145869873
Epoch 16: loss 16.129174883823282
Epoch 17: loss 16.129181138194728
Epoch 18: loss 16.129177153661498
Epoch 19: loss 16.129175241127037
-----------Time: 0:05:37.310616, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.9770235643321137e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129389468063927
Epoch 1: loss 16.129388121303624
Epoch 2: loss 16.129390850347175
Epoch 3: loss 16.129389350345562
Epoch 4: loss 16.12938708102593
Epoch 5: loss 16.12938921577325
Epoch 6: loss 16.12938383313999
Epoch 7: loss 16.129386027783656
Epoch 8: loss 16.12939604603123
Epoch 9: loss 16.129388582064706
Epoch 10: loss 16.12939540013598
Epoch 11: loss 16.12938769580619
Epoch 12: loss 16.129384840487656
Epoch 13: loss 16.129388458123344
Epoch 14: loss 16.129384398654857
Epoch 15: loss 16.12938630781853
Epoch 16: loss 16.12939285752305
Epoch 17: loss 16.129388785349278
Epoch 18: loss 16.1293857350435
Epoch 19: loss 16.129388655444213
-----------Time: 0:03:41.052878, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 20, rmse: 4.017209053039551-------------


Epoch 0: loss 16.129187610111785
Epoch 1: loss 16.129198760944842
Epoch 2: loss 16.129196619196648
Epoch 3: loss 16.129196768807873
Epoch 4: loss 16.12919356915016
Epoch 5: loss 16.1291932089942
Epoch 6: loss 16.12919019758215
Epoch 7: loss 16.12919518349934
Epoch 8: loss 16.12918900354457
Epoch 9: loss 16.12919818142823
Epoch 10: loss 16.129200278578278
Epoch 11: loss 16.129198244954658
Epoch 12: loss 16.12919164079891
Epoch 13: loss 16.12919755394268
Epoch 14: loss 16.129188835523646
Epoch 15: loss 16.12919009360624
Epoch 16: loss 16.12919560147732
Epoch 17: loss 16.12919678514324
Epoch 18: loss 16.129192260246423
Epoch 19: loss 16.129194359730093
-----------Time: 0:03:50.275966, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 50, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129186696368368
Epoch 1: loss 16.129193000264504
Epoch 2: loss 16.12918312203437
Epoch 3: loss 16.129183913392175
Epoch 4: loss 16.12918807165074
Epoch 5: loss 16.12917874415586
Epoch 6: loss 16.12918243102239
Epoch 7: loss 16.129183045024778
Epoch 8: loss 16.12918605358462
Epoch 9: loss 16.129188234745126
Epoch 10: loss 16.12918635228848
Epoch 11: loss 16.12918468919249
Epoch 12: loss 16.129177460921984
Epoch 13: loss 16.129180268790165
Epoch 14: loss 16.12919144295946
Epoch 15: loss 16.12918642696445
Epoch 16: loss 16.129185333272698
Epoch 17: loss 16.129183188672297
Epoch 18: loss 16.12918555289264
Epoch 19: loss 16.12918893353585
-----------Time: 0:03:47.385005, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129170407932566
Epoch 1: loss 16.129162038779246
Epoch 2: loss 16.129177188406565
Epoch 3: loss 16.12916840309031
Epoch 4: loss 16.129170966446562
Epoch 5: loss 16.129165238177666
Epoch 6: loss 16.129171132133862
Epoch 7: loss 16.129162920629803
Epoch 8: loss 16.129167720375662
Epoch 9: loss 16.129165606371664
Epoch 10: loss 16.129168870852265
Epoch 11: loss 16.12916414552308
Epoch 12: loss 16.129173632222976
Epoch 13: loss 16.129162139384366
Epoch 14: loss 16.129172390216457
Epoch 15: loss 16.129173496354202
Epoch 16: loss 16.12917356817796
Epoch 17: loss 16.12916984604778
Epoch 18: loss 16.129167556244113
Epoch 19: loss 16.1291729933286
-----------Time: 0:04:21.885628, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12917386947474
Epoch 1: loss 16.129179608893175
Epoch 2: loss 16.129178406558264
Epoch 3: loss 16.12917561735907
Epoch 4: loss 16.129168844145234
Epoch 5: loss 16.129175281576515
Epoch 6: loss 16.129181010104702
Epoch 7: loss 16.129175886244408
Epoch 8: loss 16.129174874748077
Epoch 9: loss 16.129174261782854
Epoch 10: loss 16.12917534069499
Epoch 11: loss 16.129170897993593
Epoch 12: loss 16.129173756682917
Epoch 13: loss 16.129172052359568
Epoch 14: loss 16.129173014331215
Epoch 15: loss 16.129174017270923
Epoch 16: loss 16.129169634725166
Epoch 17: loss 16.12917158744982
Epoch 18: loss 16.12917223697515
Epoch 19: loss 16.129179505176555
-----------Time: 0:05:03.125294, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-14, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129513804584562
Epoch 1: loss 16.1295131161655
Epoch 2: loss 16.129514042614204
Epoch 3: loss 16.129509604320766
Epoch 4: loss 16.129505786771293
Epoch 5: loss 16.129511883234187
Epoch 6: loss 16.12951378565628
Epoch 7: loss 16.129513805103148
Epoch 8: loss 16.129511725584926
Epoch 9: loss 16.12950848832994
Epoch 10: loss 16.12950561226808
Epoch 11: loss 16.129510466205872
Epoch 12: loss 16.12950479524041
Epoch 13: loss 16.129517069324454
Epoch 14: loss 16.129510028003157
Epoch 15: loss 16.12951415748036
Epoch 16: loss 16.129510421348435
Epoch 17: loss 16.129511961021652
Epoch 18: loss 16.129505012008146
Epoch 19: loss 16.129506475968228
-----------Time: 0:03:20.422305, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 20, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12915297913253
Epoch 1: loss 16.12914812908411
Epoch 2: loss 16.129148475756914
Epoch 3: loss 16.12914401879448
Epoch 4: loss 16.12914511326411
Epoch 5: loss 16.12913959242845
Epoch 6: loss 16.129153868761836
Epoch 7: loss 16.12914738206516
Epoch 8: loss 16.12914144065861
Epoch 9: loss 16.12914741369873
Epoch 10: loss 16.129143779209087
Epoch 11: loss 16.129150005317758
Epoch 12: loss 16.129148173941548
Epoch 13: loss 16.129146026748234
Epoch 14: loss 16.129151699528737
Epoch 15: loss 16.12914874153075
Epoch 16: loss 16.129150249051815
Epoch 17: loss 16.129147954840192
Epoch 18: loss 16.12915505346492
Epoch 19: loss 16.129149746544794
-----------Time: 0:03:55.557943, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 50, rmse: 4.017197132110596-------------


Epoch 0: loss 16.129176482096387
Epoch 1: loss 16.12916889003984
Epoch 2: loss 16.12916963835525
Epoch 3: loss 16.12917324302636
Epoch 4: loss 16.129177796445248
Epoch 5: loss 16.129175412259457
Epoch 6: loss 16.129175425742616
Epoch 7: loss 16.129173250286524
Epoch 8: loss 16.12917341026941
Epoch 9: loss 16.129176227731378
Epoch 10: loss 16.129174380538384
Epoch 11: loss 16.129172220899076
Epoch 12: loss 16.12917399263823
Epoch 13: loss 16.12917309730451
Epoch 14: loss 16.12917998668096
Epoch 15: loss 16.12917951736326
Epoch 16: loss 16.129171906378428
Epoch 17: loss 16.12917391407289
Epoch 18: loss 16.1291792031019
Epoch 19: loss 16.129174234297952
-----------Time: 0:04:58.288980, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 100, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129177646574732
Epoch 1: loss 16.129175098516683
Epoch 2: loss 16.129178477863437
Epoch 3: loss 16.129170513204933
Epoch 4: loss 16.129181547356794
Epoch 5: loss 16.129174905085186
Epoch 6: loss 16.129170535244715
Epoch 7: loss 16.129178571208396
Epoch 8: loss 16.129172040691447
Epoch 9: loss 16.129172447260597
Epoch 10: loss 16.12917661640941
Epoch 11: loss 16.12917456230176
Epoch 12: loss 16.129176475354807
Epoch 13: loss 16.129175523495533
Epoch 14: loss 16.12917884476098
Epoch 15: loss 16.12918422687566
Epoch 16: loss 16.129175408110793
Epoch 17: loss 16.129176750981724
Epoch 18: loss 16.129174132914954
Epoch 19: loss 16.129171591857776
-----------Time: 0:04:54.290015, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129176189874812
Epoch 1: loss 16.12916798952029
Epoch 2: loss 16.129174492811625
Epoch 3: loss 16.129176276737482
Epoch 4: loss 16.12917016938434
Epoch 5: loss 16.129177653834898
Epoch 6: loss 16.12917382695093
Epoch 7: loss 16.129173237581238
Epoch 8: loss 16.12917076030978
Epoch 9: loss 16.129174253744818
Epoch 10: loss 16.129178084258868
Epoch 11: loss 16.129170305253112
Epoch 12: loss 16.129168816660332
Epoch 13: loss 16.12917064648079
Epoch 14: loss 16.12917522531025
Epoch 15: loss 16.12917427474743
Epoch 16: loss 16.129170114933114
Epoch 17: loss 16.129166198593563
Epoch 18: loss 16.1291743463119
Epoch 19: loss 16.129174954609873
-----------Time: 0:04:40.367551, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177834e-14, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129217184127967
Epoch 1: loss 16.129211426299833
Epoch 2: loss 16.129223379121647
Epoch 3: loss 16.12922319683969
Epoch 4: loss 16.129219446965312
Epoch 5: loss 16.129210621199576
Epoch 6: loss 16.129217448864637
Epoch 7: loss 16.12921517591492
Epoch 8: loss 16.129217212650037
Epoch 9: loss 16.129222611359374
Epoch 10: loss 16.12921610028929
Epoch 11: loss 16.12921650685844
Epoch 12: loss 16.129222746968853
Epoch 13: loss 16.129220747312427
Epoch 14: loss 16.12921577332265
Epoch 15: loss 16.129216773410153
Epoch 16: loss 16.12921552751426
Epoch 17: loss 16.129221065981742
Epoch 18: loss 16.129224983358462
Epoch 19: loss 16.12921201852173
-----------Time: 0:02:55.274771, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 20, rmse: 4.017227649688721-------------


Epoch 0: loss 16.12920294253965
Epoch 1: loss 16.129193424465477
Epoch 2: loss 16.129200734931402
Epoch 3: loss 16.12919598808102
Epoch 4: loss 16.129200305026014
Epoch 5: loss 16.12919970684041
Epoch 6: loss 16.129193369236376
Epoch 7: loss 16.129197182637185
Epoch 8: loss 16.129198663969802
Epoch 9: loss 16.129197783674993
Epoch 10: loss 16.129198227063544
Epoch 11: loss 16.12920255930674
Epoch 12: loss 16.129202805115128
Epoch 13: loss 16.129201182468616
Epoch 14: loss 16.129204990424302
Epoch 15: loss 16.129203862506063
Epoch 16: loss 16.12920626950951
Epoch 17: loss 16.129197082032064
Epoch 18: loss 16.12920941886466
Epoch 19: loss 16.12919378125065
-----------Time: 0:03:27.647078, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 50, rmse: 4.017181873321533-------------


Epoch 0: loss 16.129175892726696
Epoch 1: loss 16.12918050059681
Epoch 2: loss 16.129167233166843
Epoch 3: loss 16.129169759962984
Epoch 4: loss 16.129178259539955
Epoch 5: loss 16.12917580819765
Epoch 6: loss 16.12917158252328
Epoch 7: loss 16.12917934882375
Epoch 8: loss 16.129174182958224
Epoch 9: loss 16.12917864899586
Epoch 10: loss 16.129179622635625
Epoch 11: loss 16.129175368439185
Epoch 12: loss 16.12917290828097
Epoch 13: loss 16.12918024286101
Epoch 14: loss 16.129176009407892
Epoch 15: loss 16.129180568012615
Epoch 16: loss 16.12917380854123
Epoch 17: loss 16.129171479584535
Epoch 18: loss 16.129176527213115
Epoch 19: loss 16.12918152687276
-----------Time: 0:04:32.607304, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12916962305705
Epoch 1: loss 16.12916731328793
Epoch 2: loss 16.129169624094214
Epoch 3: loss 16.129176265328653
Epoch 4: loss 16.129166654687396
Epoch 5: loss 16.12916922037727
Epoch 6: loss 16.129171921417335
Epoch 7: loss 16.1291671455263
Epoch 8: loss 16.12917955962778
Epoch 9: loss 16.129175885207243
Epoch 10: loss 16.129166201964352
Epoch 11: loss 16.129171231701815
Epoch 12: loss 16.129169950801565
Epoch 13: loss 16.1291829094153
Epoch 14: loss 16.129177328424003
Epoch 15: loss 16.1291670565893
Epoch 16: loss 16.129166680875844
Epoch 17: loss 16.12917701182902
Epoch 18: loss 16.129171505772984
Epoch 19: loss 16.129175479156675
-----------Time: 0:05:26.063717, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917374008826
Epoch 1: loss 16.129174678205082
Epoch 2: loss 16.129171909749218
Epoch 3: loss 16.12916645425503
Epoch 4: loss 16.129173122974372
Epoch 5: loss 16.129163278970722
Epoch 6: loss 16.129172115626705
Epoch 7: loss 16.129169860308814
Epoch 8: loss 16.129174623235276
Epoch 9: loss 16.129168442761916
Epoch 10: loss 16.12916951441389
Epoch 11: loss 16.129171983128725
Epoch 12: loss 16.129165259439574
Epoch 13: loss 16.129168094274075
Epoch 14: loss 16.129170802833595
Epoch 15: loss 16.129164077847985
Epoch 16: loss 16.129168790212592
Epoch 17: loss 16.12916604042572
Epoch 18: loss 16.129164501530376
Epoch 19: loss 16.129177665503015
-----------Time: 0:06:02.325823, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129362604422308
Epoch 1: loss 16.129360021878483
Epoch 2: loss 16.12935610398318
Epoch 3: loss 16.12935956656252
Epoch 4: loss 16.129353771137115
Epoch 5: loss 16.129361117385276
Epoch 6: loss 16.12935811686347
Epoch 7: loss 16.129353483842078
Epoch 8: loss 16.129360015914777
Epoch 9: loss 16.129360365958366
Epoch 10: loss 16.129369764239843
Epoch 11: loss 16.12935943043446
Epoch 12: loss 16.129364706498894
Epoch 13: loss 16.129361994827875
Epoch 14: loss 16.12936094080773
Epoch 15: loss 16.12936075541427
Epoch 16: loss 16.129359336311627
Epoch 17: loss 16.12936052879346
Epoch 18: loss 16.129361676158563
Epoch 19: loss 16.129359353165576
-----------Time: 0:02:36.915271, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 20, rmse: 4.017192840576172-------------


Epoch 0: loss 16.129206589993867
Epoch 1: loss 16.1292047941406
Epoch 2: loss 16.129202574604943
Epoch 3: loss 16.12920090554524
Epoch 4: loss 16.12919484304954
Epoch 5: loss 16.129197889465946
Epoch 6: loss 16.129187597925082
Epoch 7: loss 16.12920087702317
Epoch 8: loss 16.12920313908264
Epoch 9: loss 16.12920190485487
Epoch 10: loss 16.129205163631056
Epoch 11: loss 16.12921517072909
Epoch 12: loss 16.129198006665725
Epoch 13: loss 16.129192158863425
Epoch 14: loss 16.12920030787822
Epoch 15: loss 16.129211625954326
Epoch 16: loss 16.129208209010297
Epoch 17: loss 16.12920340667152
Epoch 18: loss 16.1292043123769
Epoch 19: loss 16.12920016345283
-----------Time: 0:03:06.418577, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12916803956356
Epoch 1: loss 16.129173487797583
Epoch 2: loss 16.12917426074569
Epoch 3: loss 16.12917717933136
Epoch 4: loss 16.129178636809158
Epoch 5: loss 16.129177581751847
Epoch 6: loss 16.12917664467219
Epoch 7: loss 16.129180154961176
Epoch 8: loss 16.129173762646623
Epoch 9: loss 16.12917173654246
Epoch 10: loss 16.129176811915237
Epoch 11: loss 16.129176571551973
Epoch 12: loss 16.12917391744368
Epoch 13: loss 16.129177683912715
Epoch 14: loss 16.129173287105925
Epoch 15: loss 16.129180075877255
Epoch 16: loss 16.129171049160565
Epoch 17: loss 16.129176868181503
Epoch 18: loss 16.129172963769363
Epoch 19: loss 16.12917315616369
-----------Time: 0:04:10.145884, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129188026274722
Epoch 1: loss 16.12918912048506
Epoch 2: loss 16.129190903373747
Epoch 3: loss 16.129185748657758
Epoch 4: loss 16.129193809513428
Epoch 5: loss 16.129187871218377
Epoch 6: loss 16.129189426449084
Epoch 7: loss 16.12918534701515
Epoch 8: loss 16.12918873595569
Epoch 9: loss 16.12918767286034
Epoch 10: loss 16.129195133456076
Epoch 11: loss 16.12918967977693
Epoch 12: loss 16.129196754546836
Epoch 13: loss 16.129186139928706
Epoch 14: loss 16.12919404261653
Epoch 15: loss 16.129192654888158
Epoch 16: loss 16.129190683235223
Epoch 17: loss 16.129191067505296
Epoch 18: loss 16.129189501384342
Epoch 19: loss 16.12919171910496
-----------Time: 0:04:52.306510, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129178222461263
Epoch 1: loss 16.129170865063568
Epoch 2: loss 16.129170892289178
Epoch 3: loss 16.129166132474218
Epoch 4: loss 16.129169104992535
Epoch 5: loss 16.129176908371694
Epoch 6: loss 16.12917236584305
Epoch 7: loss 16.129170968780187
Epoch 8: loss 16.129175236459787
Epoch 9: loss 16.12916883507003
Epoch 10: loss 16.129170504648314
Epoch 11: loss 16.129167083296327
Epoch 12: loss 16.129169200671114
Epoch 13: loss 16.129176124792632
Epoch 14: loss 16.12916669358113
Epoch 15: loss 16.129170816576046
Epoch 16: loss 16.129168874741637
Epoch 17: loss 16.12917152703489
Epoch 18: loss 16.129168770247144
Epoch 19: loss 16.12916744034079
-----------Time: 0:05:43.859851, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.5199110829529332e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12943851331967
Epoch 1: loss 16.12943545782806
Epoch 2: loss 16.129434672174668
Epoch 3: loss 16.129435406229042
Epoch 4: loss 16.12943260043519
Epoch 5: loss 16.129435991190775
Epoch 6: loss 16.12943922507497
Epoch 7: loss 16.129430734573205
Epoch 8: loss 16.12943458894208
Epoch 9: loss 16.129435654111763
Epoch 10: loss 16.1294346190199
Epoch 11: loss 16.12943330674537
Epoch 12: loss 16.12943464080039
Epoch 13: loss 16.129431913571878
Epoch 14: loss 16.12943653233224
Epoch 15: loss 16.12944131107548
Epoch 16: loss 16.12943325462777
Epoch 17: loss 16.129430912187917
Epoch 18: loss 16.12943350432553
Epoch 19: loss 16.129433157134148
-----------Time: 0:03:52.013195, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 20, rmse: 4.0171637535095215-------------


Epoch 0: loss 16.1291892296468
Epoch 1: loss 16.12918537735226
Epoch 2: loss 16.129181227909605
Epoch 3: loss 16.129186602245536
Epoch 4: loss 16.129181568359407
Epoch 5: loss 16.129184921258425
Epoch 6: loss 16.129184691526113
Epoch 7: loss 16.129184497316743
Epoch 8: loss 16.12918392220809
Epoch 9: loss 16.129183245716437
Epoch 10: loss 16.12918103810819
Epoch 11: loss 16.12918480976306
Epoch 12: loss 16.129180718920296
Epoch 13: loss 16.129184326443614
Epoch 14: loss 16.12918238901716
Epoch 15: loss 16.12918354467959
Epoch 16: loss 16.129184444421266
Epoch 17: loss 16.129181240874182
Epoch 18: loss 16.12918920268048
Epoch 19: loss 16.129184183055386
-----------Time: 0:03:23.786286, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129176634041237
Epoch 1: loss 16.12917440335604
Epoch 2: loss 16.129178299730146
Epoch 3: loss 16.12917815530475
Epoch 4: loss 16.129173141384072
Epoch 5: loss 16.129170405858233
Epoch 6: loss 16.12917088062106
Epoch 7: loss 16.129175092034394
Epoch 8: loss 16.129172458928718
Epoch 9: loss 16.129174313900457
Epoch 10: loss 16.12917677042859
Epoch 11: loss 16.12917411502384
Epoch 12: loss 16.129174452621434
Epoch 13: loss 16.12917113498607
Epoch 14: loss 16.129172313206865
Epoch 15: loss 16.129173789094363
Epoch 16: loss 16.12917358632837
Epoch 17: loss 16.12917479307124
Epoch 18: loss 16.1291683175241
Epoch 19: loss 16.12917458382296
-----------Time: 0:03:47.729957, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 100, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12918001831453
Epoch 1: loss 16.129184856954122
Epoch 2: loss 16.129174186847596
Epoch 3: loss 16.129175220902294
Epoch 4: loss 16.12918116542034
Epoch 5: loss 16.1291798964475
Epoch 6: loss 16.129187858253797
Epoch 7: loss 16.12917933067334
Epoch 8: loss 16.1291802861627
Epoch 9: loss 16.129183737073923
Epoch 10: loss 16.12918175064137
Epoch 11: loss 16.129184368967426
Epoch 12: loss 16.129176743721562
Epoch 13: loss 16.12918489014344
Epoch 14: loss 16.129177963947587
Epoch 15: loss 16.12918741512454
Epoch 16: loss 16.129181901030464
Epoch 17: loss 16.129184969745946
Epoch 18: loss 16.129177303272723
Epoch 19: loss 16.129176841474475
-----------Time: 0:04:30.895354, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129174937755923
Epoch 1: loss 16.129185766548876
Epoch 2: loss 16.129174703356362
Epoch 3: loss 16.129177876047752
Epoch 4: loss 16.12917979402734
Epoch 5: loss 16.129178103965025
Epoch 6: loss 16.129175622804194
Epoch 7: loss 16.12918523551978
Epoch 8: loss 16.129179793249467
Epoch 9: loss 16.129179445020917
Epoch 10: loss 16.12917845297145
Epoch 11: loss 16.129178754268228
Epoch 12: loss 16.129176744499436
Epoch 13: loss 16.12917411865392
Epoch 14: loss 16.129181127045193
Epoch 15: loss 16.12917685962488
Epoch 16: loss 16.12917107353397
Epoch 17: loss 16.12917242755444
Epoch 18: loss 16.129181078816963
Epoch 19: loss 16.12917217993101
-----------Time: 0:05:12.339877, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-13, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129295062863225
Epoch 1: loss 16.129284338046183
Epoch 2: loss 16.129286949630664
Epoch 3: loss 16.129289167091986
Epoch 4: loss 16.129284169506676
Epoch 5: loss 16.129284385496536
Epoch 6: loss 16.129290290342976
Epoch 7: loss 16.129293988877627
Epoch 8: loss 16.12928317719792
Epoch 9: loss 16.129289920333935
Epoch 10: loss 16.12928470079506
Epoch 11: loss 16.129290924570107
Epoch 12: loss 16.129289002701146
Epoch 13: loss 16.129289151534493
Epoch 14: loss 16.12929851403374
Epoch 15: loss 16.129294965888185
Epoch 16: loss 16.129291472193856
Epoch 17: loss 16.12928833035816
Epoch 18: loss 16.129291084812284
Epoch 19: loss 16.129285710476353
-----------Time: 0:03:36.189343, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 20, rmse: 4.017230987548828-------------


Epoch 0: loss 16.129183520046897
Epoch 1: loss 16.129177202667602
Epoch 2: loss 16.129180134736433
Epoch 3: loss 16.129175831533892
Epoch 4: loss 16.129183591611362
Epoch 5: loss 16.129177326868252
Epoch 6: loss 16.12917861373221
Epoch 7: loss 16.12917562513782
Epoch 8: loss 16.12917961667192
Epoch 9: loss 16.12917981114058
Epoch 10: loss 16.129184297402958
Epoch 11: loss 16.12918006187551
Epoch 12: loss 16.129173989008144
Epoch 13: loss 16.12918014744172
Epoch 14: loss 16.129176336115247
Epoch 15: loss 16.129183750816374
Epoch 16: loss 16.129174029457626
Epoch 17: loss 16.129175128075918
Epoch 18: loss 16.12918865868681
Epoch 19: loss 16.129183307427827
-----------Time: 0:03:52.968543, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 50, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.12917484985609
Epoch 1: loss 16.129185770178957
Epoch 2: loss 16.129177631795116
Epoch 3: loss 16.12917462608748
Epoch 4: loss 16.129177663687976
Epoch 5: loss 16.129181266284753
Epoch 6: loss 16.129175994628277
Epoch 7: loss 16.129189056699335
Epoch 8: loss 16.129184281586173
Epoch 9: loss 16.12918799075178
Epoch 10: loss 16.129175314247252
Epoch 11: loss 16.12919020380515
Epoch 12: loss 16.129176503099004
Epoch 13: loss 16.129176416495625
Epoch 14: loss 16.129182953494862
Epoch 15: loss 16.129182089016837
Epoch 16: loss 16.12918241727994
Epoch 17: loss 16.129175514161037
Epoch 18: loss 16.129180322463515
Epoch 19: loss 16.1291821330964
-----------Time: 0:04:53.180517, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.12918093750307
Epoch 1: loss 16.129176498950336
Epoch 2: loss 16.129184465942465
Epoch 3: loss 16.129179838884777
Epoch 4: loss 16.129185276228554
Epoch 5: loss 16.129186438113987
Epoch 6: loss 16.129182725059007
Epoch 7: loss 16.129185492218415
Epoch 8: loss 16.129179259368165
Epoch 9: loss 16.12918452895031
Epoch 10: loss 16.129185688242828
Epoch 11: loss 16.1291824429498
Epoch 12: loss 16.129183730850926
Epoch 13: loss 16.129184527394564
Epoch 14: loss 16.129181599474393
Epoch 15: loss 16.12918770112312
Epoch 16: loss 16.129189238722002
Epoch 17: loss 16.12918070984509
Epoch 18: loss 16.12918716361174
Epoch 19: loss 16.129185179253515
-----------Time: 0:04:10.429570, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129166806632245
Epoch 1: loss 16.129183716330598
Epoch 2: loss 16.12917837959194
Epoch 3: loss 16.129179774580475
Epoch 4: loss 16.129176577256384
Epoch 5: loss 16.12917699471578
Epoch 6: loss 16.129174582007916
Epoch 7: loss 16.129177502927217
Epoch 8: loss 16.129176705864992
Epoch 9: loss 16.129177578121762
Epoch 10: loss 16.12918057086482
Epoch 11: loss 16.12918000327562
Epoch 12: loss 16.129176420644292
Epoch 13: loss 16.129175299467633
Epoch 14: loss 16.129171347345846
Epoch 15: loss 16.129177566712936
Epoch 16: loss 16.12917241588632
Epoch 17: loss 16.12917019764712
Epoch 18: loss 16.129173261695353
Epoch 19: loss 16.12917385599158
-----------Time: 0:04:46.913081, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.6560877829466837e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.1291835659415
Epoch 1: loss 16.12917612920059
Epoch 2: loss 16.129175090219352
Epoch 3: loss 16.129178684778093
Epoch 4: loss 16.129176147091705
Epoch 5: loss 16.12917559713433
Epoch 6: loss 16.129182277521792
Epoch 7: loss 16.12917719411098
Epoch 8: loss 16.129177810706285
Epoch 9: loss 16.12917702997943
Epoch 10: loss 16.12917271381231
Epoch 11: loss 16.129180055652512
Epoch 12: loss 16.129175505085833
Epoch 13: loss 16.129181008289663
Epoch 14: loss 16.129178373887527
Epoch 15: loss 16.129173185982218
Epoch 16: loss 16.129179433093505
Epoch 17: loss 16.1291788932485
Epoch 18: loss 16.12917544493019
Epoch 19: loss 16.129171639826716
-----------Time: 0:03:00.891461, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 20, rmse: 4.017213821411133-------------


Epoch 0: loss 16.12916702028848
Epoch 1: loss 16.129173193501675
Epoch 2: loss 16.12916590377907
Epoch 3: loss 16.129173700675942
Epoch 4: loss 16.12916580965624
Epoch 5: loss 16.129167074998996
Epoch 6: loss 16.12916785546656
Epoch 7: loss 16.12916117741272
Epoch 8: loss 16.12916763066079
Epoch 9: loss 16.12916728943311
Epoch 10: loss 16.12916406851349
Epoch 11: loss 16.12916564733973
Epoch 12: loss 16.12916891519112
Epoch 13: loss 16.129175262648232
Epoch 14: loss 16.12916838130982
Epoch 15: loss 16.1291626538188
Epoch 16: loss 16.1291704374918
Epoch 17: loss 16.129166256156285
Epoch 18: loss 16.129163002825223
Epoch 19: loss 16.129168155466882
-----------Time: 0:03:42.589884, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 50, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12917681243382
Epoch 1: loss 16.129181495757773
Epoch 2: loss 16.129179950380145
Epoch 3: loss 16.12918035020771
Epoch 4: loss 16.129179227475305
Epoch 5: loss 16.129183586425533
Epoch 6: loss 16.12916976722315
Epoch 7: loss 16.129171581486116
Epoch 8: loss 16.12918308573355
Epoch 9: loss 16.129174491255874
Epoch 10: loss 16.12918055219583
Epoch 11: loss 16.129173506466575
Epoch 12: loss 16.12918103240378
Epoch 13: loss 16.129181765680276
Epoch 14: loss 16.129178060144753
Epoch 15: loss 16.12918326620047
Epoch 16: loss 16.129171070681764
Epoch 17: loss 16.12917759705005
Epoch 18: loss 16.12917893551302
Epoch 19: loss 16.129179810362707
-----------Time: 0:04:44.205178, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.12917650543263
Epoch 1: loss 16.129176181836772
Epoch 2: loss 16.129172602576233
Epoch 3: loss 16.129168237921593
Epoch 4: loss 16.129169318130185
Epoch 5: loss 16.129171710354015
Epoch 6: loss 16.129169449072418
Epoch 7: loss 16.12916590689057
Epoch 8: loss 16.129174545966393
Epoch 9: loss 16.129164988479904
Epoch 10: loss 16.129167690297844
Epoch 11: loss 16.129173074746145
Epoch 12: loss 16.12916875650469
Epoch 13: loss 16.129167898508957
Epoch 14: loss 16.129167077591912
Epoch 15: loss 16.129169064543053
Epoch 16: loss 16.129174918049767
Epoch 17: loss 16.12917391640651
Epoch 18: loss 16.12917493723734
Epoch 19: loss 16.129173293588213
-----------Time: 0:05:28.521628, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129177626609284
Epoch 1: loss 16.12917593473193
Epoch 2: loss 16.129172319948445
Epoch 3: loss 16.129176143980207
Epoch 4: loss 16.12917800128557
Epoch 5: loss 16.129174802665027
Epoch 6: loss 16.129177844154892
Epoch 7: loss 16.129174857634833
Epoch 8: loss 16.129172669473455
Epoch 9: loss 16.12917898374125
Epoch 10: loss 16.12917809592699
Epoch 11: loss 16.129179317190182
Epoch 12: loss 16.12917515530153
Epoch 13: loss 16.129172289870628
Epoch 14: loss 16.129181308289986
Epoch 15: loss 16.12917693974597
Epoch 16: loss 16.129172758669746
Epoch 17: loss 16.129167794273755
Epoch 18: loss 16.12917600525923
Epoch 19: loss 16.129180849862525
-----------Time: 0:05:19.439892, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.5111917342151276e-13, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12927324451656
Epoch 1: loss 16.12927610216872
Epoch 2: loss 16.129280437264125
Epoch 3: loss 16.129280194307942
Epoch 4: loss 16.129277244347993
Epoch 5: loss 16.129270839846736
Epoch 6: loss 16.129274120662703
Epoch 7: loss 16.129283953257524
Epoch 8: loss 16.12927587632578
Epoch 9: loss 16.129272642182293
Epoch 10: loss 16.12927498773364
Epoch 11: loss 16.12927204529315
Epoch 12: loss 16.129275802168397
Epoch 13: loss 16.129271446848254
Epoch 14: loss 16.129272969408227
Epoch 15: loss 16.129271533192338
Epoch 16: loss 16.129277094477477
Epoch 17: loss 16.12927533674007
Epoch 18: loss 16.129278182205525
Epoch 19: loss 16.129281171837082
-----------Time: 0:02:41.271842, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 20, rmse: 4.017226696014404-------------


Epoch 0: loss 16.12915134844798
Epoch 1: loss 16.129152372131017
Epoch 2: loss 16.129143780246256
Epoch 3: loss 16.129154413014795
Epoch 4: loss 16.129154452686404
Epoch 5: loss 16.129150176968764
Epoch 6: loss 16.129149685092695
Epoch 7: loss 16.129145603584426
Epoch 8: loss 16.129142134263503
Epoch 9: loss 16.129148001512668
Epoch 10: loss 16.12915657187623
Epoch 11: loss 16.129156951219766
Epoch 12: loss 16.129145644811782
Epoch 13: loss 16.129155756922895
Epoch 14: loss 16.12914079787486
Epoch 15: loss 16.12914893262862
Epoch 16: loss 16.129151421049613
Epoch 17: loss 16.12914802018166
Epoch 18: loss 16.129147095548
Epoch 19: loss 16.129150084920266
-----------Time: 0:03:11.264767, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 50, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129181830243873
Epoch 1: loss 16.129184376486883
Epoch 2: loss 16.129182789881895
Epoch 3: loss 16.12917639212222
Epoch 4: loss 16.12917578097204
Epoch 5: loss 16.129179685902763
Epoch 6: loss 16.12918123542906
Epoch 7: loss 16.12918927839361
Epoch 8: loss 16.129185957387456
Epoch 9: loss 16.129181525835595
Epoch 10: loss 16.12918086567931
Epoch 11: loss 16.129181940961363
Epoch 12: loss 16.12918197933651
Epoch 13: loss 16.12918004528085
Epoch 14: loss 16.129183672510326
Epoch 15: loss 16.129183242604938
Epoch 16: loss 16.129182785992523
Epoch 17: loss 16.129183982363728
Epoch 18: loss 16.129187114605635
Epoch 19: loss 16.129179078641958
-----------Time: 0:04:14.040174, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 100, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129169950282982
Epoch 1: loss 16.129176630929738
Epoch 2: loss 16.129175366624146
Epoch 3: loss 16.129181938627738
Epoch 4: loss 16.129171239999145
Epoch 5: loss 16.129172689698194
Epoch 6: loss 16.129173624444228
Epoch 7: loss 16.129179113905607
Epoch 8: loss 16.12916942781051
Epoch 9: loss 16.129175735077435
Epoch 10: loss 16.129181085558546
Epoch 11: loss 16.129177325831087
Epoch 12: loss 16.12917720448264
Epoch 13: loss 16.12917551156812
Epoch 14: loss 16.129180674322146
Epoch 15: loss 16.129173160571646
Epoch 16: loss 16.129175107591887
Epoch 17: loss 16.129172345359017
Epoch 18: loss 16.129176434646034
Epoch 19: loss 16.12917876775139
-----------Time: 0:05:04.050463, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 150, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129172172930136
Epoch 1: loss 16.129180510190597
Epoch 2: loss 16.129179550811866
Epoch 3: loss 16.129174631791894
Epoch 4: loss 16.129173580105373
Epoch 5: loss 16.129167749157023
Epoch 6: loss 16.129179349860916
Epoch 7: loss 16.12917298995781
Epoch 8: loss 16.1291706094021
Epoch 9: loss 16.129175741559724
Epoch 10: loss 16.129173338964232
Epoch 11: loss 16.12917239903237
Epoch 12: loss 16.12916977915056
Epoch 13: loss 16.12917601251939
Epoch 14: loss 16.12917315227432
Epoch 15: loss 16.129171753655704
Epoch 16: loss 16.12917312582658
Epoch 17: loss 16.12916892245128
Epoch 18: loss 16.129171845704203
Epoch 19: loss 16.1291784114848
-----------Time: 0:05:54.762789, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612791e-13, embedding_dim: 200, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12930231498855
Epoch 1: loss 16.12930418655495
Epoch 2: loss 16.129305045847143
Epoch 3: loss 16.12930413962318
Epoch 4: loss 16.129307173852883
Epoch 5: loss 16.129302342732746
Epoch 6: loss 16.129306635045047
Epoch 7: loss 16.129307875236524
Epoch 8: loss 16.129303979381003
Epoch 9: loss 16.129303804099916
Epoch 10: loss 16.129309440579604
Epoch 11: loss 16.129305854058902
Epoch 12: loss 16.1293017919975
Epoch 13: loss 16.129301816370905
Epoch 14: loss 16.129312496330506
Epoch 15: loss 16.129305677481355
Epoch 16: loss 16.129305333920055
Epoch 17: loss 16.129306719833384
Epoch 18: loss 16.12930361715071
Epoch 19: loss 16.12931272684069
-----------Time: 0:03:53.295337, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 20, rmse: 4.017202854156494-------------


Epoch 0: loss 16.129179619005544
Epoch 1: loss 16.12917318753797
Epoch 2: loss 16.12917623525083
Epoch 3: loss 16.1291767701693
Epoch 4: loss 16.129177707508248
Epoch 5: loss 16.129175832571057
Epoch 6: loss 16.129170911736043
Epoch 7: loss 16.129178141043717
Epoch 8: loss 16.129172568868334
Epoch 9: loss 16.129176779503794
Epoch 10: loss 16.129174242076697
Epoch 11: loss 16.129180081063083
Epoch 12: loss 16.129176253919823
Epoch 13: loss 16.12917746325561
Epoch 14: loss 16.129185487032586
Epoch 15: loss 16.12917345564543
Epoch 16: loss 16.12917984277415
Epoch 17: loss 16.129176977861828
Epoch 18: loss 16.129178348995538
Epoch 19: loss 16.129176916409733
-----------Time: 0:02:51.102188, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12918612385263
Epoch 1: loss 16.129184773462242
Epoch 2: loss 16.12918833742458
Epoch 3: loss 16.12918729922122
Epoch 4: loss 16.12918775168497
Epoch 5: loss 16.12918663465698
Epoch 6: loss 16.129187653154183
Epoch 7: loss 16.1291702902142
Epoch 8: loss 16.129182348308387
Epoch 9: loss 16.1291818110563
Epoch 10: loss 16.12918885626697
Epoch 11: loss 16.129186865167167
Epoch 12: loss 16.129180732662746
Epoch 13: loss 16.129184440531894
Epoch 14: loss 16.129179076308333
Epoch 15: loss 16.129180310536103
Epoch 16: loss 16.1291780679235
Epoch 17: loss 16.129186633879105
Epoch 18: loss 16.129184074671517
Epoch 19: loss 16.129186560240306
-----------Time: 0:03:56.990088, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 100, rmse: 4.01719331741333-------------


Epoch 0: loss 16.129181939146324
Epoch 1: loss 16.12917707328112
Epoch 2: loss 16.129177474145852
Epoch 3: loss 16.12918088564476
Epoch 4: loss 16.129174382353426
Epoch 5: loss 16.129178324881426
Epoch 6: loss 16.12916985641944
Epoch 7: loss 16.12917495097979
Epoch 8: loss 16.129179307855686
Epoch 9: loss 16.129185758770127
Epoch 10: loss 16.129173541211642
Epoch 11: loss 16.12918381071272
Epoch 12: loss 16.129171447950966
Epoch 13: loss 16.12917266013896
Epoch 14: loss 16.129180713993755
Epoch 15: loss 16.129178570689813
Epoch 16: loss 16.12917692807785
Epoch 17: loss 16.1291761258298
Epoch 18: loss 16.129177083912072
Epoch 19: loss 16.12918169722731
-----------Time: 0:04:36.383270, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12917707976341
Epoch 1: loss 16.12916789565675
Epoch 2: loss 16.129169724958626
Epoch 3: loss 16.129176091344025
Epoch 4: loss 16.129170603956975
Epoch 5: loss 16.129168525475922
Epoch 6: loss 16.12917092288558
Epoch 7: loss 16.129173017961296
Epoch 8: loss 16.12916717249262
Epoch 9: loss 16.12917634026391
Epoch 10: loss 16.129171221589445
Epoch 11: loss 16.129176168612904
Epoch 12: loss 16.129170351147717
Epoch 13: loss 16.12917245140926
Epoch 14: loss 16.129176245363205
Epoch 15: loss 16.129172324615695
Epoch 16: loss 16.129170203610826
Epoch 17: loss 16.129175759710133
Epoch 18: loss 16.12917736316907
Epoch 19: loss 16.129177880715
-----------Time: 0:05:23.354499, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-13, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129291872280717
Epoch 1: loss 16.12928902681526
Epoch 2: loss 16.129293395618564
Epoch 3: loss 16.129291985331832
Epoch 4: loss 16.12929509242246
Epoch 5: loss 16.129287810219314
Epoch 6: loss 16.129287998205687
Epoch 7: loss 16.129288197600886
Epoch 8: loss 16.129290531743408
Epoch 9: loss 16.12930036304177
Epoch 10: loss 16.12929237997357
Epoch 11: loss 16.12928553597314
Epoch 12: loss 16.129295984903973
Epoch 13: loss 16.129292754909148
Epoch 14: loss 16.12929169388813
Epoch 15: loss 16.12929631420424
Epoch 16: loss 16.129293199334864
Epoch 17: loss 16.129290187922816
Epoch 18: loss 16.129295141947146
Epoch 19: loss 16.129296674100907
-----------Time: 0:03:42.265638, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 20, rmse: 4.0171990394592285-------------


Epoch 0: loss 16.129156918808324
Epoch 1: loss 16.129153346030073
Epoch 2: loss 16.129154160205537
Epoch 3: loss 16.1291579227852
Epoch 4: loss 16.12915365484631
Epoch 5: loss 16.129152112580176
Epoch 6: loss 16.12915485562547
Epoch 7: loss 16.129154163317036
Epoch 8: loss 16.129155450440283
Epoch 9: loss 16.12915433652379
Epoch 10: loss 16.129157955196643
Epoch 11: loss 16.129157511030222
Epoch 12: loss 16.129147201079657
Epoch 13: loss 16.12915004939732
Epoch 14: loss 16.129157314487227
Epoch 15: loss 16.12915007584506
Epoch 16: loss 16.129151444904437
Epoch 17: loss 16.1291515320264
Epoch 18: loss 16.129152958129914
Epoch 19: loss 16.12915658484081
-----------Time: 0:03:47.368144, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12917680232145
Epoch 1: loss 16.129172867312906
Epoch 2: loss 16.129175873539122
Epoch 3: loss 16.129174938015215
Epoch 4: loss 16.129173964894033
Epoch 5: loss 16.12917739169114
Epoch 6: loss 16.12917903585885
Epoch 7: loss 16.129175876909912
Epoch 8: loss 16.12917785530443
Epoch 9: loss 16.129176146573123
Epoch 10: loss 16.12917421874046
Epoch 11: loss 16.129177580455387
Epoch 12: loss 16.129179699645217
Epoch 13: loss 16.12917646550173
Epoch 14: loss 16.129176345968325
Epoch 15: loss 16.12917877267793
Epoch 16: loss 16.129180541305583
Epoch 17: loss 16.129172432480978
Epoch 18: loss 16.129169719513502
Epoch 19: loss 16.129174162214902
-----------Time: 0:04:32.286437, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12917140672361
Epoch 1: loss 16.12916679755704
Epoch 2: loss 16.129175036805293
Epoch 3: loss 16.12917357103017
Epoch 4: loss 16.129173017442714
Epoch 5: loss 16.12917318131497
Epoch 6: loss 16.129173538618726
Epoch 7: loss 16.129174227815664
Epoch 8: loss 16.129169904906963
Epoch 9: loss 16.12917219341417
Epoch 10: loss 16.129175451931065
Epoch 11: loss 16.129176520471535
Epoch 12: loss 16.129173670857416
Epoch 13: loss 16.12918329809333
Epoch 14: loss 16.1291783287708
Epoch 15: loss 16.12917179462377
Epoch 16: loss 16.129177365761986
Epoch 17: loss 16.12917997345709
Epoch 18: loss 16.129176138794378
Epoch 19: loss 16.129175384774552
-----------Time: 0:04:10.495852, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129175673625337
Epoch 1: loss 16.129174440694023
Epoch 2: loss 16.12917549497346
Epoch 3: loss 16.12917133464056
Epoch 4: loss 16.129170695746186
Epoch 5: loss 16.129176628855404
Epoch 6: loss 16.1291738523615
Epoch 7: loss 16.129174682613037
Epoch 8: loss 16.1291742358537
Epoch 9: loss 16.12917072867621
Epoch 10: loss 16.129169336539885
Epoch 11: loss 16.129172133517823
Epoch 12: loss 16.12917837907336
Epoch 13: loss 16.12917310741688
Epoch 14: loss 16.129174077426566
Epoch 15: loss 16.1291687715436
Epoch 16: loss 16.12917640742042
Epoch 17: loss 16.129174507072662
Epoch 18: loss 16.129169442071547
Epoch 19: loss 16.129174557893805
-----------Time: 0:05:00.450356, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.111308307896889e-13, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129220772723
Epoch 1: loss 16.12921944152019
Epoch 2: loss 16.129214891731383
Epoch 3: loss 16.129220765203545
Epoch 4: loss 16.129214174530958
Epoch 5: loss 16.129214374444743
Epoch 6: loss 16.129214989743588
Epoch 7: loss 16.129217035553907
Epoch 8: loss 16.129211571762394
Epoch 9: loss 16.129213340908628
Epoch 10: loss 16.129218473325547
Epoch 11: loss 16.129214958887893
Epoch 12: loss 16.12922044108911
Epoch 13: loss 16.129216556383124
Epoch 14: loss 16.129214575136402
Epoch 15: loss 16.129208083253896
Epoch 16: loss 16.129216495708903
Epoch 17: loss 16.129215667013113
Epoch 18: loss 16.129216974361103
Epoch 19: loss 16.129210059055495
-----------Time: 0:03:14.677786, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 20, rmse: 4.017194747924805-------------


Epoch 0: loss 16.12917240551466
Epoch 1: loss 16.129174057201823
Epoch 2: loss 16.129185322901034
Epoch 3: loss 16.12917976187519
Epoch 4: loss 16.129181068445302
Epoch 5: loss 16.12917425763419
Epoch 6: loss 16.1291705388748
Epoch 7: loss 16.12917509773881
Epoch 8: loss 16.129177069651035
Epoch 9: loss 16.129166555897317
Epoch 10: loss 16.129177279158608
Epoch 11: loss 16.12918018866908
Epoch 12: loss 16.129174883823282
Epoch 13: loss 16.129174141471577
Epoch 14: loss 16.129173590736325
Epoch 15: loss 16.129172798600646
Epoch 16: loss 16.12917577241542
Epoch 17: loss 16.129176774317962
Epoch 18: loss 16.129171864113903
Epoch 19: loss 16.12917675694543
-----------Time: 0:03:49.223060, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 50, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129170304215947
Epoch 1: loss 16.129166544229196
Epoch 2: loss 16.129172215453952
Epoch 3: loss 16.12916578476425
Epoch 4: loss 16.129173379932297
Epoch 5: loss 16.129163039903915
Epoch 6: loss 16.129168353824916
Epoch 7: loss 16.12916506004437
Epoch 8: loss 16.12916405140025
Epoch 9: loss 16.12916361164178
Epoch 10: loss 16.129168239995927
Epoch 11: loss 16.129165820546486
Epoch 12: loss 16.129171515885353
Epoch 13: loss 16.129176892295618
Epoch 14: loss 16.129166745958024
Epoch 15: loss 16.129168921673408
Epoch 16: loss 16.129169097473078
Epoch 17: loss 16.129167834463946
Epoch 18: loss 16.12916802452465
Epoch 19: loss 16.129164988998486
-----------Time: 0:04:56.625290, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129182469916124
Epoch 1: loss 16.129177409582258
Epoch 2: loss 16.129184142346613
Epoch 3: loss 16.129176792727662
Epoch 4: loss 16.12917590672844
Epoch 5: loss 16.129178821943324
Epoch 6: loss 16.12917831736197
Epoch 7: loss 16.129179288927403
Epoch 8: loss 16.12917368304412
Epoch 9: loss 16.129179967493386
Epoch 10: loss 16.129183509675233
Epoch 11: loss 16.12917845789799
Epoch 12: loss 16.129181719526382
Epoch 13: loss 16.129181598437228
Epoch 14: loss 16.129186411666247
Epoch 15: loss 16.129181951073733
Epoch 16: loss 16.129181836985452
Epoch 17: loss 16.129175001800935
Epoch 18: loss 16.12917756723152
Epoch 19: loss 16.12918378841365
-----------Time: 0:05:32.716302, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129186612357906
Epoch 1: loss 16.129175640954603
Epoch 2: loss 16.129173125567288
Epoch 3: loss 16.129178350291998
Epoch 4: loss 16.129176652191642
Epoch 5: loss 16.12918188339864
Epoch 6: loss 16.129183284350876
Epoch 7: loss 16.129176938708806
Epoch 8: loss 16.129177015977685
Epoch 9: loss 16.129171362384756
Epoch 10: loss 16.129180008980033
Epoch 11: loss 16.12917241095978
Epoch 12: loss 16.129180878384595
Epoch 13: loss 16.1291753373242
Epoch 14: loss 16.12917762790574
Epoch 15: loss 16.12917725763741
Epoch 16: loss 16.129175651326264
Epoch 17: loss 16.12917969342222
Epoch 18: loss 16.12917868088872
Epoch 19: loss 16.129182917453335
-----------Time: 0:04:27.034978, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103254e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129323089686732
Epoch 1: loss 16.12932459124409
Epoch 2: loss 16.129324275167694
Epoch 3: loss 16.129326134288096
Epoch 4: loss 16.12932674621615
Epoch 5: loss 16.129322752867008
Epoch 6: loss 16.12932595252472
Epoch 7: loss 16.129328226770895
Epoch 8: loss 16.129328497211983
Epoch 9: loss 16.12932297352412
Epoch 10: loss 16.129327336363716
Epoch 11: loss 16.129320639122305
Epoch 12: loss 16.129324418296626
Epoch 13: loss 16.12932327896956
Epoch 14: loss 16.129324831866647
Epoch 15: loss 16.129326310347057
Epoch 16: loss 16.129320486399582
Epoch 17: loss 16.129321151741696
Epoch 18: loss 16.12932598571404
Epoch 19: loss 16.12932151552774
-----------Time: 0:02:50.858408, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 20, rmse: 4.017202854156494-------------


Epoch 0: loss 16.129208669252794
Epoch 1: loss 16.129207051792115
Epoch 2: loss 16.129207621974228
Epoch 3: loss 16.129206997340887
Epoch 4: loss 16.129210035459966
Epoch 5: loss 16.129209070895403
Epoch 6: loss 16.12920664781588
Epoch 7: loss 16.1292100681307
Epoch 8: loss 16.129210446955653
Epoch 9: loss 16.12921511913007
Epoch 10: loss 16.12920989388678
Epoch 11: loss 16.129205399845656
Epoch 12: loss 16.129210753697556
Epoch 13: loss 16.12920922569246
Epoch 14: loss 16.12920971290128
Epoch 15: loss 16.129213516189715
Epoch 16: loss 16.12920963070586
Epoch 17: loss 16.129202858010604
Epoch 18: loss 16.129203360776916
Epoch 19: loss 16.129204616266595
-----------Time: 0:03:21.638309, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129165941635637
Epoch 1: loss 16.129158647245788
Epoch 2: loss 16.12916740870722
Epoch 3: loss 16.129163917346517
Epoch 4: loss 16.129162937743047
Epoch 5: loss 16.12916103169087
Epoch 6: loss 16.129160863669945
Epoch 7: loss 16.129164244831742
Epoch 8: loss 16.129166021756728
Epoch 9: loss 16.129161932988293
Epoch 10: loss 16.12916260066403
Epoch 11: loss 16.12916275572038
Epoch 12: loss 16.129162854510458
Epoch 13: loss 16.129162896774982
Epoch 14: loss 16.12916154042089
Epoch 15: loss 16.129164935584427
Epoch 16: loss 16.12915652312942
Epoch 17: loss 16.129161976289982
Epoch 18: loss 16.129161841717668
Epoch 19: loss 16.12916127283201
-----------Time: 0:04:21.154416, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129179382272362
Epoch 1: loss 16.129187176835607
Epoch 2: loss 16.12918180016605
Epoch 3: loss 16.129178747526648
Epoch 4: loss 16.12917329644042
Epoch 5: loss 16.129181033440943
Epoch 6: loss 16.129184965078696
Epoch 7: loss 16.129188902939447
Epoch 8: loss 16.129183286943793
Epoch 9: loss 16.129181471643662
Epoch 10: loss 16.129183375621505
Epoch 11: loss 16.1291838942046
Epoch 12: loss 16.12918456369538
Epoch 13: loss 16.12917989411388
Epoch 14: loss 16.129179625487833
Epoch 15: loss 16.12918063776204
Epoch 16: loss 16.129183280720795
Epoch 17: loss 16.129184772165786
Epoch 18: loss 16.12917946757928
Epoch 19: loss 16.12918429947729
-----------Time: 0:05:12.211743, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 150, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12917203939499
Epoch 1: loss 16.12917646550173
Epoch 2: loss 16.129179866628974
Epoch 3: loss 16.129178929290024
Epoch 4: loss 16.129175064290198
Epoch 5: loss 16.12917511588922
Epoch 6: loss 16.129179120128605
Epoch 7: loss 16.129177512002418
Epoch 8: loss 16.12917830361952
Epoch 9: loss 16.129175742337598
Epoch 10: loss 16.129179741909738
Epoch 11: loss 16.12918569861449
Epoch 12: loss 16.129177388838933
Epoch 13: loss 16.12917712565801
Epoch 14: loss 16.129183658249293
Epoch 15: loss 16.129179563776443
Epoch 16: loss 16.129175771637545
Epoch 17: loss 16.129174059535448
Epoch 18: loss 16.129176520730827
Epoch 19: loss 16.12918367147316
-----------Time: 0:06:08.468693, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268076e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.1294937504576
Epoch 1: loss 16.12949182910722
Epoch 2: loss 16.129491174914644
Epoch 3: loss 16.129491164542983
Epoch 4: loss 16.129496558844362
Epoch 5: loss 16.129500039574115
Epoch 6: loss 16.129496624445125
Epoch 7: loss 16.129495138445257
Epoch 8: loss 16.129489271196093
Epoch 9: loss 16.12950043603089
Epoch 10: loss 16.12950251477124
Epoch 11: loss 16.129498131447605
Epoch 12: loss 16.129503917019935
Epoch 13: loss 16.12950022004103
Epoch 14: loss 16.12949240317871
Epoch 15: loss 16.129485905332498
Epoch 16: loss 16.129498007765537
Epoch 17: loss 16.129492914242352
Epoch 18: loss 16.129494635160363
Epoch 19: loss 16.129492057024493
-----------Time: 0:03:03.354012, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 20, rmse: 4.01719331741333-------------


Epoch 0: loss 16.129201083678534
Epoch 1: loss 16.129192599140477
Epoch 2: loss 16.129191156960882
Epoch 3: loss 16.129195178832095
Epoch 4: loss 16.129195771053993
Epoch 5: loss 16.12919186793831
Epoch 6: loss 16.129188086689652
Epoch 7: loss 16.129196070017148
Epoch 8: loss 16.129190677530808
Epoch 9: loss 16.129195758089416
Epoch 10: loss 16.129191315128725
Epoch 11: loss 16.12919342498406
Epoch 12: loss 16.12919425860639
Epoch 13: loss 16.129194238122356
Epoch 14: loss 16.12919174788632
Epoch 15: loss 16.129197383588135
Epoch 16: loss 16.129198044263
Epoch 17: loss 16.1291854590291
Epoch 18: loss 16.12919427234884
Epoch 19: loss 16.129191847454276
-----------Time: 0:02:57.501768, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129180025833985
Epoch 1: loss 16.129180899646503
Epoch 2: loss 16.129183999217677
Epoch 3: loss 16.129176381491266
Epoch 4: loss 16.129182359976507
Epoch 5: loss 16.129186425927283
Epoch 6: loss 16.129185326271823
Epoch 7: loss 16.129179503102222
Epoch 8: loss 16.129182108722997
Epoch 9: loss 16.129181534392217
Epoch 10: loss 16.1291846152944
Epoch 11: loss 16.129176770947172
Epoch 12: loss 16.129182167582176
Epoch 13: loss 16.12918189325172
Epoch 14: loss 16.129179352453832
Epoch 15: loss 16.129178080369496
Epoch 16: loss 16.129178649773735
Epoch 17: loss 16.12917743810433
Epoch 18: loss 16.129186883317576
Epoch 19: loss 16.129174877859576
-----------Time: 0:04:07.373230, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 100, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12917521856867
Epoch 1: loss 16.129178617362292
Epoch 2: loss 16.129172539568387
Epoch 3: loss 16.129177357983238
Epoch 4: loss 16.129177826004483
Epoch 5: loss 16.129182234220107
Epoch 6: loss 16.129172997995845
Epoch 7: loss 16.129174067314192
Epoch 8: loss 16.129180575532068
Epoch 9: loss 16.129176260920698
Epoch 10: loss 16.129171383387373
Epoch 11: loss 16.129174467141763
Epoch 12: loss 16.129176676046466
Epoch 13: loss 16.129176076305114
Epoch 14: loss 16.129182699129853
Epoch 15: loss 16.12917504743625
Epoch 16: loss 16.129175252795154
Epoch 17: loss 16.129177953835217
Epoch 18: loss 16.1291739197773
Epoch 19: loss 16.129174458066558
-----------Time: 0:04:42.986105, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12917615279612
Epoch 1: loss 16.12916960464735
Epoch 2: loss 16.12916976955677
Epoch 3: loss 16.129166267305823
Epoch 4: loss 16.129176297740095
Epoch 5: loss 16.129171368348462
Epoch 6: loss 16.12917831554693
Epoch 7: loss 16.12917622461988
Epoch 8: loss 16.129171275522086
Epoch 9: loss 16.12917412409904
Epoch 10: loss 16.129168606374886
Epoch 11: loss 16.12917539436834
Epoch 12: loss 16.129174851152545
Epoch 13: loss 16.129179553923365
Epoch 14: loss 16.129175993072526
Epoch 15: loss 16.129176501802544
Epoch 16: loss 16.129168131612058
Epoch 17: loss 16.129177278121443
Epoch 18: loss 16.12916799444683
Epoch 19: loss 16.12916974207187
-----------Time: 0:05:39.143775, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.873817422860387e-12, embedding_dim: 200, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129275180646555
Epoch 1: loss 16.12927277753248
Epoch 2: loss 16.129269575281853
Epoch 3: loss 16.129272383668617
Epoch 4: loss 16.12926749706009
Epoch 5: loss 16.129264549433763
Epoch 6: loss 16.129266791527787
Epoch 7: loss 16.129274760594246
Epoch 8: loss 16.12927008038179
Epoch 9: loss 16.129266645805934
Epoch 10: loss 16.129270214954104
Epoch 11: loss 16.12927201728966
Epoch 12: loss 16.12927427753409
Epoch 13: loss 16.12926978893809
Epoch 14: loss 16.129278507357125
Epoch 15: loss 16.129266309245505
Epoch 16: loss 16.12927075168761
Epoch 17: loss 16.129273864482652
Epoch 18: loss 16.129270705274422
Epoch 19: loss 16.12926727251361
-----------Time: 0:03:48.008888, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 20, rmse: 4.017214298248291-------------


Epoch 0: loss 16.129157247330717
Epoch 1: loss 16.129158968767307
Epoch 2: loss 16.129153393739717
Epoch 3: loss 16.12915662529029
Epoch 4: loss 16.12915407074995
Epoch 5: loss 16.12915298898561
Epoch 6: loss 16.129154414570547
Epoch 7: loss 16.12916106176869
Epoch 8: loss 16.129159151827142
Epoch 9: loss 16.129156950441892
Epoch 10: loss 16.129159182423546
Epoch 11: loss 16.129165900926864
Epoch 12: loss 16.12915693825519
Epoch 13: loss 16.129162544657056
Epoch 14: loss 16.1291489056623
Epoch 15: loss 16.129151279995014
Epoch 16: loss 16.129151374377138
Epoch 17: loss 16.129154667379805
Epoch 18: loss 16.129157493916978
Epoch 19: loss 16.129156238168008
-----------Time: 0:03:47.335639, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 50, rmse: 4.017190456390381-------------


Epoch 0: loss 16.1291756686988
Epoch 1: loss 16.129180177000958
Epoch 2: loss 16.129180536379042
Epoch 3: loss 16.129175873798413
Epoch 4: loss 16.12918108503996
Epoch 5: loss 16.129175999295523
Epoch 6: loss 16.129178284172653
Epoch 7: loss 16.129175545794606
Epoch 8: loss 16.129186375884014
Epoch 9: loss 16.129178477863437
Epoch 10: loss 16.12916940732648
Epoch 11: loss 16.129175589096292
Epoch 12: loss 16.129169800153175
Epoch 13: loss 16.129184675190746
Epoch 14: loss 16.129176629114696
Epoch 15: loss 16.12918103940465
Epoch 16: loss 16.12917548589826
Epoch 17: loss 16.12918224173956
Epoch 18: loss 16.129184105527212
Epoch 19: loss 16.12917428926776
-----------Time: 0:03:47.444139, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 100, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129176144758084
Epoch 1: loss 16.129175476304468
Epoch 2: loss 16.129178462305944
Epoch 3: loss 16.12917746014411
Epoch 4: loss 16.129177645278276
Epoch 5: loss 16.129172895316394
Epoch 6: loss 16.129176516582163
Epoch 7: loss 16.129175073883985
Epoch 8: loss 16.12918302013279
Epoch 9: loss 16.129176391603636
Epoch 10: loss 16.12917217318943
Epoch 11: loss 16.12918079359626
Epoch 12: loss 16.12917674190652
Epoch 13: loss 16.129178822721197
Epoch 14: loss 16.12917481692606
Epoch 15: loss 16.12917303740816
Epoch 16: loss 16.12917625832778
Epoch 17: loss 16.129177360316863
Epoch 18: loss 16.129175164636028
Epoch 19: loss 16.12917886498572
-----------Time: 0:04:25.225883, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 150, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129168124351896
Epoch 1: loss 16.12916646644173
Epoch 2: loss 16.1291726251346
Epoch 3: loss 16.129165949673677
Epoch 4: loss 16.129174341903944
Epoch 5: loss 16.12917039833878
Epoch 6: loss 16.129167179493493
Epoch 7: loss 16.129173490390496
Epoch 8: loss 16.129168477247692
Epoch 9: loss 16.129162452349266
Epoch 10: loss 16.12917142461473
Epoch 11: loss 16.12917912453656
Epoch 12: loss 16.12917148840045
Epoch 13: loss 16.129168684680934
Epoch 14: loss 16.129178331104423
Epoch 15: loss 16.12917329488467
Epoch 16: loss 16.12916633498092
Epoch 17: loss 16.129165485023222
Epoch 18: loss 16.129167844835607
Epoch 19: loss 16.12917296013928
-----------Time: 0:05:10.766117, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.477076355991714e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12926924494442
Epoch 1: loss 16.129263075361308
Epoch 2: loss 16.129265754880173
Epoch 3: loss 16.129259168874835
Epoch 4: loss 16.129260408029147
Epoch 5: loss 16.1292630266145
Epoch 6: loss 16.129264410194203
Epoch 7: loss 16.129263995327722
Epoch 8: loss 16.129260734217915
Epoch 9: loss 16.129265131284
Epoch 10: loss 16.12926145504842
Epoch 11: loss 16.129266033359297
Epoch 12: loss 16.12926791555665
Epoch 13: loss 16.129260119178362
Epoch 14: loss 16.129257593419386
Epoch 15: loss 16.129264531801937
Epoch 16: loss 16.129260042687353
Epoch 17: loss 16.129264155829194
Epoch 18: loss 16.129260228340105
Epoch 19: loss 16.129261442602427
-----------Time: 0:03:17.326913, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 20, rmse: 4.0171966552734375-------------


Epoch 0: loss 16.129184579512163
Epoch 1: loss 16.129190606484922
Epoch 2: loss 16.12918380863839
Epoch 3: loss 16.129182016155912
Epoch 4: loss 16.12918567735258
Epoch 5: loss 16.129188710285828
Epoch 6: loss 16.129195990155353
Epoch 7: loss 16.12917790405124
Epoch 8: loss 16.129192497498188
Epoch 9: loss 16.129184867844366
Epoch 10: loss 16.12918540924512
Epoch 11: loss 16.129186611839323
Epoch 12: loss 16.129184308293205
Epoch 13: loss 16.129180743034407
Epoch 14: loss 16.12919048513648
Epoch 15: loss 16.129178391260062
Epoch 16: loss 16.129182831368542
Epoch 17: loss 16.129189227313177
Epoch 18: loss 16.12918622653208
Epoch 19: loss 16.129186616506573
-----------Time: 0:03:57.141787, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 50, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129172157113352
Epoch 1: loss 16.12917625003045
Epoch 2: loss 16.12916993965203
Epoch 3: loss 16.129171124095823
Epoch 4: loss 16.129173665671583
Epoch 5: loss 16.12917629307285
Epoch 6: loss 16.12917377327758
Epoch 7: loss 16.12917389903398
Epoch 8: loss 16.129171042937568
Epoch 9: loss 16.129169854085816
Epoch 10: loss 16.12917255823738
Epoch 11: loss 16.129169633688
Epoch 12: loss 16.129176255216283
Epoch 13: loss 16.12917173783892
Epoch 14: loss 16.129171390388244
Epoch 15: loss 16.12917228235117
Epoch 16: loss 16.129174613382194
Epoch 17: loss 16.129175180971394
Epoch 18: loss 16.129168877075262
Epoch 19: loss 16.129172476819832
-----------Time: 0:05:02.895303, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 100, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129166771887178
Epoch 1: loss 16.129168640082785
Epoch 2: loss 16.129175794973783
Epoch 3: loss 16.12916613973438
Epoch 4: loss 16.129169749591323
Epoch 5: loss 16.12916474033789
Epoch 6: loss 16.12917082876275
Epoch 7: loss 16.129162197724966
Epoch 8: loss 16.129172517010023
Epoch 9: loss 16.129170108450825
Epoch 10: loss 16.129170998858005
Epoch 11: loss 16.129169431440594
Epoch 12: loss 16.129166382431272
Epoch 13: loss 16.129170506981936
Epoch 14: loss 16.129167858059475
Epoch 15: loss 16.129170758754032
Epoch 16: loss 16.129166428066583
Epoch 17: loss 16.129163949239377
Epoch 18: loss 16.12917214000011
Epoch 19: loss 16.129172716145934
-----------Time: 0:04:42.480901, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129179049601305
Epoch 1: loss 16.129172873795195
Epoch 2: loss 16.129177024015725
Epoch 3: loss 16.129168281223283
Epoch 4: loss 16.129179052972095
Epoch 5: loss 16.12918305539644
Epoch 6: loss 16.12917783715402
Epoch 7: loss 16.12917520975276
Epoch 8: loss 16.12918324312352
Epoch 9: loss 16.129176294369305
Epoch 10: loss 16.12917085728482
Epoch 11: loss 16.129180397398773
Epoch 12: loss 16.12917377535191
Epoch 13: loss 16.129174830409223
Epoch 14: loss 16.12916893048932
Epoch 15: loss 16.12917797613429
Epoch 16: loss 16.129179384605983
Epoch 17: loss 16.129176851586845
Epoch 18: loss 16.129170942073156
Epoch 19: loss 16.129171766879573
-----------Time: 0:04:50.209773, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-12, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.1293317181316
Epoch 1: loss 16.129331380274714
Epoch 2: loss 16.129332617873274
Epoch 3: loss 16.12932620040744
Epoch 4: loss 16.129327635845456
Epoch 5: loss 16.129323674907756
Epoch 6: loss 16.12933664363386
Epoch 7: loss 16.129324935323975
Epoch 8: loss 16.12932565096865
Epoch 9: loss 16.129336551844652
Epoch 10: loss 16.129329699546894
Epoch 11: loss 16.129328565405658
Epoch 12: loss 16.129329098768373
Epoch 13: loss 16.129324312246382
Epoch 14: loss 16.129325328928548
Epoch 15: loss 16.129331289004085
Epoch 16: loss 16.129327002655494
Epoch 17: loss 16.12932816557809
Epoch 18: loss 16.129331641381302
Epoch 19: loss 16.129324231866004
-----------Time: 0:02:59.479847, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 20, rmse: 4.017203330993652-------------


Epoch 0: loss 16.129204910562503
Epoch 1: loss 16.129205543752466
Epoch 2: loss 16.129199903642697
Epoch 3: loss 16.12920206717138
Epoch 4: loss 16.129205148332854
Epoch 5: loss 16.129206405118993
Epoch 6: loss 16.129200609175
Epoch 7: loss 16.12920393355195
Epoch 8: loss 16.12920233320451
Epoch 9: loss 16.129205558013503
Epoch 10: loss 16.129199615829076
Epoch 11: loss 16.129202829488534
Epoch 12: loss 16.12920057339277
Epoch 13: loss 16.129201399754933
Epoch 14: loss 16.129204408055482
Epoch 15: loss 16.129204958790734
Epoch 16: loss 16.129200754118976
Epoch 17: loss 16.1292019836795
Epoch 18: loss 16.12919741807391
Epoch 19: loss 16.12919875057318
-----------Time: 0:03:37.462371, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 50, rmse: 4.017190933227539-------------


Epoch 0: loss 16.129182162396347
Epoch 1: loss 16.129187492912006
Epoch 2: loss 16.12919156845657
Epoch 3: loss 16.129180143811638
Epoch 4: loss 16.12918543413711
Epoch 5: loss 16.129180692991138
Epoch 6: loss 16.1291861492632
Epoch 7: loss 16.129185304232042
Epoch 8: loss 16.129185635606643
Epoch 9: loss 16.12918480172502
Epoch 10: loss 16.129191309424314
Epoch 11: loss 16.129191473555863
Epoch 12: loss 16.129180619870922
Epoch 13: loss 16.129184922814176
Epoch 14: loss 16.12918645055998
Epoch 15: loss 16.129190357046454
Epoch 16: loss 16.129181112784156
Epoch 17: loss 16.129187350560947
Epoch 18: loss 16.129182998093007
Epoch 19: loss 16.129184521171567
-----------Time: 0:04:41.474067, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 100, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12917893214223
Epoch 1: loss 16.12918173249096
Epoch 2: loss 16.129177783999253
Epoch 3: loss 16.1291784729369
Epoch 4: loss 16.12917781978149
Epoch 5: loss 16.129179554960533
Epoch 6: loss 16.129181387373908
Epoch 7: loss 16.129176872070875
Epoch 8: loss 16.129177330757628
Epoch 9: loss 16.129180681841603
Epoch 10: loss 16.129180498781768
Epoch 11: loss 16.12917695997071
Epoch 12: loss 16.12917832462213
Epoch 13: loss 16.129175004653142
Epoch 14: loss 16.129176340523202
Epoch 15: loss 16.129176731016273
Epoch 16: loss 16.12918423076503
Epoch 17: loss 16.129177666021597
Epoch 18: loss 16.129180629724
Epoch 19: loss 16.129179971382758
-----------Time: 0:05:29.503204, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 150, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129171302229118
Epoch 1: loss 16.12917032158848
Epoch 2: loss 16.129172192895588
Epoch 3: loss 16.129169566012905
Epoch 4: loss 16.129171775176903
Epoch 5: loss 16.12917706005725
Epoch 6: loss 16.129171808884802
Epoch 7: loss 16.12917424570678
Epoch 8: loss 16.12917206402769
Epoch 9: loss 16.129184230246448
Epoch 10: loss 16.129176531621074
Epoch 11: loss 16.129174703356362
Epoch 12: loss 16.129170543542045
Epoch 13: loss 16.129174097392013
Epoch 14: loss 16.129170543542045
Epoch 15: loss 16.129170170421506
Epoch 16: loss 16.129173699120194
Epoch 17: loss 16.129175281317224
Epoch 18: loss 16.129169258752423
Epoch 19: loss 16.129173662041502
-----------Time: 0:05:41.167758, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-12, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129335701627664
Epoch 1: loss 16.129339465503786
Epoch 2: loss 16.129326685023347
Epoch 3: loss 16.129334196699514
Epoch 4: loss 16.129333387969176
Epoch 5: loss 16.129334231703872
Epoch 6: loss 16.12932814353831
Epoch 7: loss 16.12932603134935
Epoch 8: loss 16.12934021433778
Epoch 9: loss 16.12933502643247
Epoch 10: loss 16.12932849513765
Epoch 11: loss 16.12933821727427
Epoch 12: loss 16.129326967132553
Epoch 13: loss 16.12933788278817
Epoch 14: loss 16.129337823151115
Epoch 15: loss 16.129338252019338
Epoch 16: loss 16.12933450447858
Epoch 17: loss 16.129329020203034
Epoch 18: loss 16.129343412958324
Epoch 19: loss 16.129329887792558
-----------Time: 0:02:39.078912, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 20, rmse: 4.017223358154297-------------


Epoch 0: loss 16.129193577706783
Epoch 1: loss 16.1291990720947
Epoch 2: loss 16.129200223349176
Epoch 3: loss 16.12920238350707
Epoch 4: loss 16.129193785658604
Epoch 5: loss 16.129199415137418
Epoch 6: loss 16.12920012715201
Epoch 7: loss 16.129198093528395
Epoch 8: loss 16.129201087308616
Epoch 9: loss 16.129201347896625
Epoch 10: loss 16.12920104945205
Epoch 11: loss 16.12919569845236
Epoch 12: loss 16.129189610546085
Epoch 13: loss 16.129198452906483
Epoch 14: loss 16.129191408214393
Epoch 15: loss 16.129198944523257
Epoch 16: loss 16.129193618415556
Epoch 17: loss 16.12919862663182
Epoch 18: loss 16.12919715489299
Epoch 19: loss 16.12919264166429
-----------Time: 0:03:10.696811, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.12918120483266
Epoch 1: loss 16.12917140646432
Epoch 2: loss 16.12917831087968
Epoch 3: loss 16.129177548043945
Epoch 4: loss 16.129181536207255
Epoch 5: loss 16.12917913620468
Epoch 6: loss 16.1291797369832
Epoch 7: loss 16.12917955651628
Epoch 8: loss 16.129187095936643
Epoch 9: loss 16.12917778633288
Epoch 10: loss 16.129182091869044
Epoch 11: loss 16.129176834732895
Epoch 12: loss 16.129173719085642
Epoch 13: loss 16.12917497405674
Epoch 14: loss 16.12917936930778
Epoch 15: loss 16.12917219341417
Epoch 16: loss 16.12917408987256
Epoch 17: loss 16.129185765252416
Epoch 18: loss 16.12917555668485
Epoch 19: loss 16.12917470594928
-----------Time: 0:04:11.761427, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129173152015028
Epoch 1: loss 16.129172388401415
Epoch 2: loss 16.12916916774109
Epoch 3: loss 16.129170714155883
Epoch 4: loss 16.129172381141252
Epoch 5: loss 16.129166430140916
Epoch 6: loss 16.12916890196725
Epoch 7: loss 16.12916888355755
Epoch 8: loss 16.129169456591875
Epoch 9: loss 16.12916613662288
Epoch 10: loss 16.1291708640264
Epoch 11: loss 16.12917091225463
Epoch 12: loss 16.12916934691155
Epoch 13: loss 16.129170684596648
Epoch 14: loss 16.12916893023003
Epoch 15: loss 16.129170920811248
Epoch 16: loss 16.129173537062975
Epoch 17: loss 16.129165948117926
Epoch 18: loss 16.129166120028223
Epoch 19: loss 16.129166840080856
-----------Time: 0:05:00.517975, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129172367658093
Epoch 1: loss 16.129172562645337
Epoch 2: loss 16.129171452099634
Epoch 3: loss 16.12917554994327
Epoch 4: loss 16.12917562047057
Epoch 5: loss 16.129174470512552
Epoch 6: loss 16.12917357803104
Epoch 7: loss 16.12917159367282
Epoch 8: loss 16.129173491686956
Epoch 9: loss 16.129173928074632
Epoch 10: loss 16.129169876903475
Epoch 11: loss 16.129174267227977
Epoch 12: loss 16.12917512314938
Epoch 13: loss 16.129170782608853
Epoch 14: loss 16.129165807581906
Epoch 15: loss 16.129175597912205
Epoch 16: loss 16.129167126338725
Epoch 17: loss 16.129177155995123
Epoch 18: loss 16.12917376523954
Epoch 19: loss 16.12916878476747
-----------Time: 0:05:48.660081, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-12, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.12919333941785
Epoch 1: loss 16.12918546265918
Epoch 2: loss 16.129188845895307
Epoch 3: loss 16.12918430725604
Epoch 4: loss 16.12919074339086
Epoch 5: loss 16.129186265166524
Epoch 6: loss 16.12919206214768
Epoch 7: loss 16.129184532839684
Epoch 8: loss 16.129181139491187
Epoch 9: loss 16.129174422802908
Epoch 10: loss 16.12918570561536
Epoch 11: loss 16.12918154994971
Epoch 12: loss 16.129179456689034
Epoch 13: loss 16.129186519790824
Epoch 14: loss 16.12918113223102
Epoch 15: loss 16.12917920491694
Epoch 16: loss 16.129178850724685
Epoch 17: loss 16.129183529640684
Epoch 18: loss 16.129183646581172
Epoch 19: loss 16.129179362825493
-----------Time: 0:04:09.914790, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 20, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129183240530605
Epoch 1: loss 16.129184161274896
Epoch 2: loss 16.129189338030667
Epoch 3: loss 16.129189421781838
Epoch 4: loss 16.129190243476756
Epoch 5: loss 16.129187661451514
Epoch 6: loss 16.129187586256965
Epoch 7: loss 16.12918319956254
Epoch 8: loss 16.129185612270405
Epoch 9: loss 16.129177738623234
Epoch 10: loss 16.129187110716263
Epoch 11: loss 16.12918175634578
Epoch 12: loss 16.129183533789348
Epoch 13: loss 16.12918806568704
Epoch 14: loss 16.129193028267988
Epoch 15: loss 16.12918720898776
Epoch 16: loss 16.12918514995357
Epoch 17: loss 16.12918542921057
Epoch 18: loss 16.129183521084062
Epoch 19: loss 16.12918657087126
-----------Time: 0:02:54.245923, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 50, rmse: 4.01719856262207-------------


Epoch 0: loss 16.12917747388656
Epoch 1: loss 16.12916863904562
Epoch 2: loss 16.129171039566778
Epoch 3: loss 16.129167179752784
Epoch 4: loss 16.129172643285006
Epoch 5: loss 16.129171237146938
Epoch 6: loss 16.12916606194692
Epoch 7: loss 16.129168003781324
Epoch 8: loss 16.129171469990748
Epoch 9: loss 16.12916123419757
Epoch 10: loss 16.129169469815743
Epoch 11: loss 16.129167958146013
Epoch 12: loss 16.12916569919804
Epoch 13: loss 16.129165628411446
Epoch 14: loss 16.129169324871768
Epoch 15: loss 16.129165015446226
Epoch 16: loss 16.129167607065256
Epoch 17: loss 16.129161079919097
Epoch 18: loss 16.129160447766303
Epoch 19: loss 16.129165860736673
-----------Time: 0:03:54.821364, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 100, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129173211911375
Epoch 1: loss 16.129174213295336
Epoch 2: loss 16.129174851671127
Epoch 3: loss 16.12917929022386
Epoch 4: loss 16.129170972150977
Epoch 5: loss 16.129177174664115
Epoch 6: loss 16.129177201371142
Epoch 7: loss 16.129173724012183
Epoch 8: loss 16.129171312341487
Epoch 9: loss 16.129177950464427
Epoch 10: loss 16.129173986415232
Epoch 11: loss 16.129172470078252
Epoch 12: loss 16.12916803930427
Epoch 13: loss 16.12917086195207
Epoch 14: loss 16.129172123405453
Epoch 15: loss 16.129168215881812
Epoch 16: loss 16.12917472772977
Epoch 17: loss 16.129171948124366
Epoch 18: loss 16.12916785028073
Epoch 19: loss 16.129174364203017
-----------Time: 0:04:38.587625, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 150, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129171822627256
Epoch 1: loss 16.129180760666234
Epoch 2: loss 16.129174015196593
Epoch 3: loss 16.129178741562942
Epoch 4: loss 16.12918396602836
Epoch 5: loss 16.129174742509388
Epoch 6: loss 16.12917815971271
Epoch 7: loss 16.12917479670132
Epoch 8: loss 16.129172136888613
Epoch 9: loss 16.129179985903086
Epoch 10: loss 16.12918060431343
Epoch 11: loss 16.12916855866524
Epoch 12: loss 16.129175863686044
Epoch 13: loss 16.129183368102048
Epoch 14: loss 16.12917673853573
Epoch 15: loss 16.129172909058845
Epoch 16: loss 16.129168745873738
Epoch 17: loss 16.12917033844243
Epoch 18: loss 16.12917128096721
Epoch 19: loss 16.129171000932338
-----------Time: 0:05:22.163774, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-12, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.12930900963705
Epoch 1: loss 16.129317282074624
Epoch 2: loss 16.129304235561055
Epoch 3: loss 16.12930556209662
Epoch 4: loss 16.129306360455296
Epoch 5: loss 16.129311609294117
Epoch 6: loss 16.129302755784185
Epoch 7: loss 16.12930491127483
Epoch 8: loss 16.12931168189575
Epoch 9: loss 16.129307566160996
Epoch 10: loss 16.129302529941246
Epoch 11: loss 16.12930847264425
Epoch 12: loss 16.129304389839525
Epoch 13: loss 16.129314520360335
Epoch 14: loss 16.129305678777815
Epoch 15: loss 16.12930414091964
Epoch 16: loss 16.129308090967093
Epoch 17: loss 16.129304880678426
Epoch 18: loss 16.129311781204414
Epoch 19: loss 16.12930643150118
-----------Time: 0:03:33.118792, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 20, rmse: 4.017204284667969-------------


Epoch 0: loss 16.129140748609466
Epoch 1: loss 16.129143262700325
Epoch 2: loss 16.129147695807934
Epoch 3: loss 16.129145206609067
Epoch 4: loss 16.12913919337876
Epoch 5: loss 16.129141117840632
Epoch 6: loss 16.129133590866264
Epoch 7: loss 16.129143726313615
Epoch 8: loss 16.129136875312312
Epoch 9: loss 16.12913985664654
Epoch 10: loss 16.129136093807585
Epoch 11: loss 16.129138630716096
Epoch 12: loss 16.12914206140258
Epoch 13: loss 16.12913994247204
Epoch 14: loss 16.129141021124884
Epoch 15: loss 16.129137309625655
Epoch 16: loss 16.129143074973243
Epoch 17: loss 16.129137029590783
Epoch 18: loss 16.129141053017744
Epoch 19: loss 16.129134356035625
-----------Time: 0:03:47.552756, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 50, rmse: 4.017179012298584-------------


Epoch 0: loss 16.12918500734322
Epoch 1: loss 16.12918606577132
Epoch 2: loss 16.129182752803203
Epoch 3: loss 16.129187505358
Epoch 4: loss 16.12918204753019
Epoch 5: loss 16.12918821659472
Epoch 6: loss 16.129185157473028
Epoch 7: loss 16.12919013690793
Epoch 8: loss 16.129184174239473
Epoch 9: loss 16.12918401788667
Epoch 10: loss 16.129173238618403
Epoch 11: loss 16.129180928168573
Epoch 12: loss 16.129181848653573
Epoch 13: loss 16.129176865588587
Epoch 14: loss 16.129176762909136
Epoch 15: loss 16.12918550466441
Epoch 16: loss 16.129178337327417
Epoch 17: loss 16.129182954272736
Epoch 18: loss 16.12917673879502
Epoch 19: loss 16.129181136120398
-----------Time: 0:04:35.218926, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 100, rmse: 4.017192840576172-------------


Epoch 0: loss 16.129173334037695
Epoch 1: loss 16.129170956334193
Epoch 2: loss 16.129177609755335
Epoch 3: loss 16.129178987371333
Epoch 4: loss 16.129174287193425
Epoch 5: loss 16.12917516152453
Epoch 6: loss 16.129169398510566
Epoch 7: loss 16.129177863083175
Epoch 8: loss 16.12917340378712
Epoch 9: loss 16.129170052703145
Epoch 10: loss 16.12917374838559
Epoch 11: loss 16.129178373109653
Epoch 12: loss 16.129174943201047
Epoch 13: loss 16.129175507419454
Epoch 14: loss 16.12917265339738
Epoch 15: loss 16.12916446056231
Epoch 16: loss 16.129171925566002
Epoch 17: loss 16.129172930839335
Epoch 18: loss 16.129173253657314
Epoch 19: loss 16.12917134112285
-----------Time: 0:04:06.764652, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 150, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129174730322685
Epoch 1: loss 16.12917064129496
Epoch 2: loss 16.129169118734985
Epoch 3: loss 16.129171751840662
Epoch 4: loss 16.12916886825935
Epoch 5: loss 16.12917081087163
Epoch 6: loss 16.12917442513653
Epoch 7: loss 16.129176672934967
Epoch 8: loss 16.129165413199463
Epoch 9: loss 16.12916738303736
Epoch 10: loss 16.129172326171442
Epoch 11: loss 16.129174883045405
Epoch 12: loss 16.12917631018609
Epoch 13: loss 16.1291672160536
Epoch 14: loss 16.129172940951705
Epoch 15: loss 16.129171844148456
Epoch 16: loss 16.129164881651786
Epoch 17: loss 16.129165893666702
Epoch 18: loss 16.12917450836912
Epoch 19: loss 16.129164856759797
-----------Time: 0:05:04.477998, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-11, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12927895515363
Epoch 1: loss 16.12928350364598
Epoch 2: loss 16.12928273355008
Epoch 3: loss 16.129277796898283
Epoch 4: loss 16.129282157144967
Epoch 5: loss 16.129280641585865
Epoch 6: loss 16.129279668983266
Epoch 7: loss 16.12927452489823
Epoch 8: loss 16.129279473736727
Epoch 9: loss 16.12927413829453
Epoch 10: loss 16.12927363578751
Epoch 11: loss 16.129277239680743
Epoch 12: loss 16.12927750052804
Epoch 13: loss 16.12927392904625
Epoch 14: loss 16.129280158007127
Epoch 15: loss 16.129274101993712
Epoch 16: loss 16.12927756379518
Epoch 17: loss 16.12927375013508
Epoch 18: loss 16.12927180000334
Epoch 19: loss 16.129280665699977
-----------Time: 0:03:13.735391, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 20, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129174087279644
Epoch 1: loss 16.129168895484963
Epoch 2: loss 16.129169019685612
Epoch 3: loss 16.129168619598754
Epoch 4: loss 16.129167630401493
Epoch 5: loss 16.129171237924812
Epoch 6: loss 16.129171448988135
Epoch 7: loss 16.129169765148816
Epoch 8: loss 16.129166126769803
Epoch 9: loss 16.1291730270365
Epoch 10: loss 16.129169011128994
Epoch 11: loss 16.129167045958344
Epoch 12: loss 16.12917032158848
Epoch 13: loss 16.12917539592409
Epoch 14: loss 16.12916295978283
Epoch 15: loss 16.129164932991515
Epoch 16: loss 16.129170900327217
Epoch 17: loss 16.129164887874783
Epoch 18: loss 16.129160868596486
Epoch 19: loss 16.129170630664007
-----------Time: 0:03:53.281110, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 50, rmse: 4.017200946807861-------------


Epoch 0: loss 16.129162497984577
Epoch 1: loss 16.129167079147663
Epoch 2: loss 16.129159809131217
Epoch 3: loss 16.129165760131553
Epoch 4: loss 16.12916401535872
Epoch 5: loss 16.12916009875988
Epoch 6: loss 16.12915892390987
Epoch 7: loss 16.129165252697995
Epoch 8: loss 16.12916797448138
Epoch 9: loss 16.12916225865848
Epoch 10: loss 16.12917106160656
Epoch 11: loss 16.129162104120716
Epoch 12: loss 16.129164188824767
Epoch 13: loss 16.129161918467968
Epoch 14: loss 16.129163421062493
Epoch 15: loss 16.12916909488016
Epoch 16: loss 16.129168792286926
Epoch 17: loss 16.1291602851905
Epoch 18: loss 16.129161918208677
Epoch 19: loss 16.129161964881153
-----------Time: 0:04:57.574242, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129174763512
Epoch 1: loss 16.12917237751117
Epoch 2: loss 16.12918046559245
Epoch 3: loss 16.1291744873665
Epoch 4: loss 16.12917428978634
Epoch 5: loss 16.129168342934673
Epoch 6: loss 16.129171445876636
Epoch 7: loss 16.129171799550306
Epoch 8: loss 16.12916614803171
Epoch 9: loss 16.129172050025943
Epoch 10: loss 16.129175523495533
Epoch 11: loss 16.129173947002915
Epoch 12: loss 16.129169005165288
Epoch 13: loss 16.129175547350354
Epoch 14: loss 16.12917400326918
Epoch 15: loss 16.129175170859025
Epoch 16: loss 16.129163040941084
Epoch 17: loss 16.129179419869637
Epoch 18: loss 16.129170679929402
Epoch 19: loss 16.129169042762562
-----------Time: 0:05:37.469706, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12916910265891
Epoch 1: loss 16.129171519515435
Epoch 2: loss 16.129176545104233
Epoch 3: loss 16.12916747249294
Epoch 4: loss 16.129174842855218
Epoch 5: loss 16.129179982532296
Epoch 6: loss 16.129175249942946
Epoch 7: loss 16.12917545296823
Epoch 8: loss 16.129173838619046
Epoch 9: loss 16.129171067829557
Epoch 10: loss 16.12917183170246
Epoch 11: loss 16.12917365037338
Epoch 12: loss 16.129166158921954
Epoch 13: loss 16.129171402056365
Epoch 14: loss 16.129168356417832
Epoch 15: loss 16.129170188831207
Epoch 16: loss 16.129168439391126
Epoch 17: loss 16.12917420240509
Epoch 18: loss 16.12916701225044
Epoch 19: loss 16.12917169427794
-----------Time: 0:04:35.804382, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-11, embedding_dim: 200, rmse: 4.017188549041748-------------


Epoch 0: loss 16.12933450914583
Epoch 1: loss 16.129337521595044
Epoch 2: loss 16.129328973530555
Epoch 3: loss 16.129342972162693
Epoch 4: loss 16.129340640353792
Epoch 5: loss 16.12933691770503
Epoch 6: loss 16.129339572591196
Epoch 7: loss 16.129332115884836
Epoch 8: loss 16.12933826031667
Epoch 9: loss 16.129333174831523
Epoch 10: loss 16.129330128415113
Epoch 11: loss 16.129332799118067
Epoch 12: loss 16.12933643594133
Epoch 13: loss 16.129334413985834
Epoch 14: loss 16.12933183196059
Epoch 15: loss 16.129336202060355
Epoch 16: loss 16.12933074889979
Epoch 17: loss 16.129332984252233
Epoch 18: loss 16.129332291165923
Epoch 19: loss 16.129329648207165
-----------Time: 0:02:52.995153, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 20, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12919778678649
Epoch 1: loss 16.129196226888535
Epoch 2: loss 16.129194048061652
Epoch 3: loss 16.129205519897642
Epoch 4: loss 16.12920410624012
Epoch 5: loss 16.129199209778513
Epoch 6: loss 16.129201562849317
Epoch 7: loss 16.12920057339277
Epoch 8: loss 16.12920344478738
Epoch 9: loss 16.129204942455363
Epoch 10: loss 16.12919667338858
Epoch 11: loss 16.129200685147424
Epoch 12: loss 16.129201986791
Epoch 13: loss 16.129197650139847
Epoch 14: loss 16.129200100444983
Epoch 15: loss 16.129198166130028
Epoch 16: loss 16.129196285747717
Epoch 17: loss 16.12919572930805
Epoch 18: loss 16.129190257219207
Epoch 19: loss 16.129198804765114
-----------Time: 0:03:24.726028, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 50, rmse: 4.017188549041748-------------


Epoch 0: loss 16.129170342331804
Epoch 1: loss 16.129180112696652
Epoch 2: loss 16.129175441818692
Epoch 3: loss 16.129174767401377
Epoch 4: loss 16.129172421331443
Epoch 5: loss 16.129180574235612
Epoch 6: loss 16.129171898081097
Epoch 7: loss 16.12917558987417
Epoch 8: loss 16.129165079231946
Epoch 9: loss 16.12917354924968
Epoch 10: loss 16.129172151927524
Epoch 11: loss 16.129173904738394
Epoch 12: loss 16.12916908113771
Epoch 13: loss 16.129169001794498
Epoch 14: loss 16.12917155529767
Epoch 15: loss 16.12917525201728
Epoch 16: loss 16.129172893760643
Epoch 17: loss 16.12916573446169
Epoch 18: loss 16.12916639228435
Epoch 19: loss 16.129172126516952
-----------Time: 0:04:24.401808, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 100, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129175362216188
Epoch 1: loss 16.129181304919193
Epoch 2: loss 16.12916687923388
Epoch 3: loss 16.129182322897815
Epoch 4: loss 16.12917745184678
Epoch 5: loss 16.12917584112768
Epoch 6: loss 16.129171983388016
Epoch 7: loss 16.129178862911388
Epoch 8: loss 16.12917873741428
Epoch 9: loss 16.12917779022225
Epoch 10: loss 16.129172704477813
Epoch 11: loss 16.12917283593863
Epoch 12: loss 16.129174550115057
Epoch 13: loss 16.129168122796145
Epoch 14: loss 16.12917615772266
Epoch 15: loss 16.12917349765066
Epoch 16: loss 16.129173465239216
Epoch 17: loss 16.129163348979443
Epoch 18: loss 16.12916746575136
Epoch 19: loss 16.12917713706684
-----------Time: 0:05:12.571307, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 150, rmse: 4.017186164855957-------------


Epoch 0: loss 16.12917413187779
Epoch 1: loss 16.129169228674602
Epoch 2: loss 16.129178699817004
Epoch 3: loss 16.129170492980194
Epoch 4: loss 16.129177005346733
Epoch 5: loss 16.1291778070762
Epoch 6: loss 16.12917234769264
Epoch 7: loss 16.1291697441462
Epoch 8: loss 16.129176717792404
Epoch 9: loss 16.129170337405263
Epoch 10: loss 16.129176873626626
Epoch 11: loss 16.129171118391408
Epoch 12: loss 16.12917774017898
Epoch 13: loss 16.129174743546553
Epoch 14: loss 16.129167351144496
Epoch 15: loss 16.12917179799456
Epoch 16: loss 16.129169839824783
Epoch 17: loss 16.129166816226032
Epoch 18: loss 16.129167534463623
Epoch 19: loss 16.129166473961188
-----------Time: 0:06:10.563823, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-11, embedding_dim: 200, rmse: 4.017189025878906-------------


Epoch 0: loss 16.129328999978295
Epoch 1: loss 16.129325067562664
Epoch 2: loss 16.129325753648104
Epoch 3: loss 16.129328795397264
Epoch 4: loss 16.12932586669922
Epoch 5: loss 16.12932692979457
Epoch 6: loss 16.12933176324833
Epoch 7: loss 16.12932386133838
Epoch 8: loss 16.12931960195611
Epoch 9: loss 16.129322318035083
Epoch 10: loss 16.129329038353443
Epoch 11: loss 16.12932255736118
Epoch 12: loss 16.12932321673959
Epoch 13: loss 16.129319535577473
Epoch 14: loss 16.12931662191834
Epoch 15: loss 16.12932261388674
Epoch 16: loss 16.1293236736113
Epoch 17: loss 16.129318581903156
Epoch 18: loss 16.12931881708059
Epoch 19: loss 16.1293211239975
-----------Time: 0:02:59.440978, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 20, rmse: 4.017214775085449-------------


Epoch 0: loss 16.129178430153793
Epoch 1: loss 16.129171025305745
Epoch 2: loss 16.12918087553239
Epoch 3: loss 16.129172443889807
Epoch 4: loss 16.12917134942018
Epoch 5: loss 16.1291792614425
Epoch 6: loss 16.129177625312828
Epoch 7: loss 16.129181759197987
Epoch 8: loss 16.129171695833687
Epoch 9: loss 16.129172981141895
Epoch 10: loss 16.12917774821702
Epoch 11: loss 16.12917347094363
Epoch 12: loss 16.129170531614633
Epoch 13: loss 16.129169887015845
Epoch 14: loss 16.129170315624773
Epoch 15: loss 16.12917102478716
Epoch 16: loss 16.129172699551273
Epoch 17: loss 16.129167943107102
Epoch 18: loss 16.12917236895455
Epoch 19: loss 16.12917504951058
-----------Time: 0:02:59.885145, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 50, rmse: 4.017185211181641-------------


Epoch 0: loss 16.129174603269824
Epoch 1: loss 16.129175560833513
Epoch 2: loss 16.129175792380867
Epoch 3: loss 16.129177713471954
Epoch 4: loss 16.129171840518374
Epoch 5: loss 16.129168484507858
Epoch 6: loss 16.129169257455963
Epoch 7: loss 16.129163973353492
Epoch 8: loss 16.129165146129168
Epoch 9: loss 16.129168827809867
Epoch 10: loss 16.129173523320524
Epoch 11: loss 16.129164953475545
Epoch 12: loss 16.129173782352783
Epoch 13: loss 16.129168112165193
Epoch 14: loss 16.12916977396473
Epoch 15: loss 16.129171294968955
Epoch 16: loss 16.129168522364424
Epoch 17: loss 16.1291680382671
Epoch 18: loss 16.12916432132275
Epoch 19: loss 16.129157645602536
-----------Time: 0:04:03.864515, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 100, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12918237034817
Epoch 1: loss 16.129175346917986
Epoch 2: loss 16.12916968865781
Epoch 3: loss 16.129172742075088
Epoch 4: loss 16.129169071284633
Epoch 5: loss 16.129164138522206
Epoch 6: loss 16.129177562823564
Epoch 7: loss 16.129171636455922
Epoch 8: loss 16.129172420294275
Epoch 9: loss 16.12917533784278
Epoch 10: loss 16.129168288483445
Epoch 11: loss 16.12916420775305
Epoch 12: loss 16.129157159690173
Epoch 13: loss 16.129168017005195
Epoch 14: loss 16.129169122624358
Epoch 15: loss 16.129176872070875
Epoch 16: loss 16.1291583462083
Epoch 17: loss 16.129165117347803
Epoch 18: loss 16.129168769209976
Epoch 19: loss 16.129157811549128
-----------Time: 0:04:43.291538, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 150, rmse: 4.017187118530273-------------


Epoch 0: loss 16.129169537490835
Epoch 1: loss 16.12916525736524
Epoch 2: loss 16.129169158925176
Epoch 3: loss 16.1291646910725
Epoch 4: loss 16.129169564457158
Epoch 5: loss 16.129166619423746
Epoch 6: loss 16.129172188746924
Epoch 7: loss 16.129175289355263
Epoch 8: loss 16.129165653044144
Epoch 9: loss 16.129173859621663
Epoch 10: loss 16.12916453212678
Epoch 11: loss 16.1291649731817
Epoch 12: loss 16.129166107063647
Epoch 13: loss 16.129168334896633
Epoch 14: loss 16.12916579798812
Epoch 15: loss 16.129166900236495
Epoch 16: loss 16.129162903775853
Epoch 17: loss 16.129166379060482
Epoch 18: loss 16.129166853304724
Epoch 19: loss 16.129161608614567
-----------Time: 0:05:41.571744, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-11, embedding_dim: 200, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129315879566633
Epoch 1: loss 16.1293239865762
Epoch 2: loss 16.129322344223528
Epoch 3: loss 16.12932072909647
Epoch 4: loss 16.129315091320326
Epoch 5: loss 16.12932125079107
Epoch 6: loss 16.129322502650666
Epoch 7: loss 16.129316684926184
Epoch 8: loss 16.129316834018827
Epoch 9: loss 16.129316464269078
Epoch 10: loss 16.129321499192372
Epoch 11: loss 16.1293046146453
Epoch 12: loss 16.12930603141432
Epoch 13: loss 16.12931194144659
Epoch 14: loss 16.12931667559169
Epoch 15: loss 16.129316088814914
Epoch 16: loss 16.129306199435245
Epoch 17: loss 16.1293112662514
Epoch 18: loss 16.12931336625365
Epoch 19: loss 16.12931505164872
-----------Time: 0:03:40.335157, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 20, rmse: 4.01719331741333-------------


Epoch 0: loss 16.129167983297293
Epoch 1: loss 16.129161107404002
Epoch 2: loss 16.12916217335156
Epoch 3: loss 16.129162946040374
Epoch 4: loss 16.12916529081385
Epoch 5: loss 16.129157448022376
Epoch 6: loss 16.129160342493932
Epoch 7: loss 16.12916222858066
Epoch 8: loss 16.12915574629194
Epoch 9: loss 16.129156190717655
Epoch 10: loss 16.129155885012917
Epoch 11: loss 16.129156727710452
Epoch 12: loss 16.129159730565878
Epoch 13: loss 16.129155417250963
Epoch 14: loss 16.129151603850158
Epoch 15: loss 16.129151717419855
Epoch 16: loss 16.129148331590812
Epoch 17: loss 16.12915786418531
Epoch 18: loss 16.12915417239224
Epoch 19: loss 16.129150801861396
-----------Time: 0:03:49.492424, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 50, rmse: 4.017191410064697-------------


Epoch 0: loss 16.129196984538442
Epoch 1: loss 16.12919076491206
Epoch 2: loss 16.129196113318837
Epoch 3: loss 16.129189084702823
Epoch 4: loss 16.129185195070303
Epoch 5: loss 16.129184767239245
Epoch 6: loss 16.129182760581948
Epoch 7: loss 16.129183966287652
Epoch 8: loss 16.12918534364436
Epoch 9: loss 16.129181894029593
Epoch 10: loss 16.12918513517395
Epoch 11: loss 16.12918345392755
Epoch 12: loss 16.12918445297789
Epoch 13: loss 16.129181708376844
Epoch 14: loss 16.129175116926383
Epoch 15: loss 16.129181675965402
Epoch 16: loss 16.12918091027746
Epoch 17: loss 16.12917909782953
Epoch 18: loss 16.129179310707894
Epoch 19: loss 16.129176746055183
-----------Time: 0:03:34.068531, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.12916377732908
Epoch 1: loss 16.12916878165597
Epoch 2: loss 16.12917023602227
Epoch 3: loss 16.129165588999133
Epoch 4: loss 16.129166917609027
Epoch 5: loss 16.129167873098385
Epoch 6: loss 16.129162331778694
Epoch 7: loss 16.129159988820263
Epoch 8: loss 16.12916501752056
Epoch 9: loss 16.129163599973662
Epoch 10: loss 16.129161928061755
Epoch 11: loss 16.129158278792495
Epoch 12: loss 16.12915521241064
Epoch 13: loss 16.129165090640775
Epoch 14: loss 16.129171582782572
Epoch 15: loss 16.12915782529158
Epoch 16: loss 16.12916080221785
Epoch 17: loss 16.1291600305662
Epoch 18: loss 16.129148998488674
Epoch 19: loss 16.129163843707715
-----------Time: 0:04:24.896787, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 150, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129175906987733
Epoch 1: loss 16.12917590154261
Epoch 2: loss 16.129171756767203
Epoch 3: loss 16.129178897397164
Epoch 4: loss 16.129172669473455
Epoch 5: loss 16.12917593706555
Epoch 6: loss 16.129173142939823
Epoch 7: loss 16.129181312957233
Epoch 8: loss 16.129170999117296
Epoch 9: loss 16.129169301016944
Epoch 10: loss 16.129174797219903
Epoch 11: loss 16.12916711752281
Epoch 12: loss 16.129167402224933
Epoch 13: loss 16.129173878290654
Epoch 14: loss 16.129167336883462
Epoch 15: loss 16.129165876034875
Epoch 16: loss 16.129168463764533
Epoch 17: loss 16.129173387970337
Epoch 18: loss 16.129167580098933
Epoch 19: loss 16.12916080247714
-----------Time: 0:05:08.631546, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334125e-11, embedding_dim: 200, rmse: 4.017187118530273-------------


Epoch 0: loss 16.12938731231399
Epoch 1: loss 16.129396681554812
Epoch 2: loss 16.12939019719176
Epoch 3: loss 16.129391063225537
Epoch 4: loss 16.129382772378264
Epoch 5: loss 16.129385309027484
Epoch 6: loss 16.129384865120354
Epoch 7: loss 16.12938845060389
Epoch 8: loss 16.129384907903457
Epoch 9: loss 16.12938258439189
Epoch 10: loss 16.129387053800315
Epoch 11: loss 16.129386258034554
Epoch 12: loss 16.129383698826967
Epoch 13: loss 16.129378225441663
Epoch 14: loss 16.129378710835443
Epoch 15: loss 16.129388767198872
Epoch 16: loss 16.129380496317047
Epoch 17: loss 16.129378496142042
Epoch 18: loss 16.129371521199378
Epoch 19: loss 16.129374582136112
-----------Time: 0:03:18.139029, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 20, rmse: 4.017209529876709-------------


Epoch 0: loss 16.12918522851891
Epoch 1: loss 16.12919241582135
Epoch 2: loss 16.129184351076308
Epoch 3: loss 16.12918590630702
Epoch 4: loss 16.129184170868683
Epoch 5: loss 16.1291899108057
Epoch 6: loss 16.129175763858797
Epoch 7: loss 16.129183790747273
Epoch 8: loss 16.129178834130027
Epoch 9: loss 16.129182828257044
Epoch 10: loss 16.12917675357464
Epoch 11: loss 16.129182393425115
Epoch 12: loss 16.129180892904923
Epoch 13: loss 16.1291800284269
Epoch 14: loss 16.12917728097365
Epoch 15: loss 16.129180089878997
Epoch 16: loss 16.129177045277633
Epoch 17: loss 16.129169182779997
Epoch 18: loss 16.129169728588707
Epoch 19: loss 16.129176502839712
-----------Time: 0:03:59.281722, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 50, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129162425123653
Epoch 1: loss 16.129163033680918
Epoch 2: loss 16.12916009901917
Epoch 3: loss 16.12915977438615
Epoch 4: loss 16.129165603260166
Epoch 5: loss 16.12915755588766
Epoch 6: loss 16.129155910682783
Epoch 7: loss 16.129154059859708
Epoch 8: loss 16.129160828406295
Epoch 9: loss 16.12915265916676
Epoch 10: loss 16.12916346228985
Epoch 11: loss 16.129153751302763
Epoch 12: loss 16.129158239639473
Epoch 13: loss 16.129153034361632
Epoch 14: loss 16.12915099373714
Epoch 15: loss 16.129154386048477
Epoch 16: loss 16.129157551738995
Epoch 17: loss 16.129152629866816
Epoch 18: loss 16.129152879823867
Epoch 19: loss 16.129147676101777
-----------Time: 0:05:10.233840, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 100, rmse: 4.017187595367432-------------


Epoch 0: loss 16.129171269039798
Epoch 1: loss 16.129167883470046
Epoch 2: loss 16.1291716893514
Epoch 3: loss 16.129171326861815
Epoch 4: loss 16.129165783986377
Epoch 5: loss 16.129167876469175
Epoch 6: loss 16.129162681303704
Epoch 7: loss 16.12916402987905
Epoch 8: loss 16.129169799634592
Epoch 9: loss 16.129166170330784
Epoch 10: loss 16.129167971888464
Epoch 11: loss 16.129162911554598
Epoch 12: loss 16.129161238605526
Epoch 13: loss 16.12916234603973
Epoch 14: loss 16.129160138690775
Epoch 15: loss 16.1291603098232
Epoch 16: loss 16.12915466660193
Epoch 17: loss 16.12916564863619
Epoch 18: loss 16.12916154093947
Epoch 19: loss 16.12915861250072
-----------Time: 0:04:45.080395, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 150, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129171763508783
Epoch 1: loss 16.129168905856623
Epoch 2: loss 16.12917456204247
Epoch 3: loss 16.12917648754151
Epoch 4: loss 16.129180648392992
Epoch 5: loss 16.12917085728482
Epoch 6: loss 16.129174786848242
Epoch 7: loss 16.12916636402157
Epoch 8: loss 16.129170175348047
Epoch 9: loss 16.129173731272346
Epoch 10: loss 16.12917434760836
Epoch 11: loss 16.12916928053291
Epoch 12: loss 16.12917295391628
Epoch 13: loss 16.129170557284496
Epoch 14: loss 16.12916720257044
Epoch 15: loss 16.129164472230432
Epoch 16: loss 16.129165263069655
Epoch 17: loss 16.129152827187685
Epoch 18: loss 16.129161011725422
Epoch 19: loss 16.12916188553794
-----------Time: 0:04:46.161292, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.03701725859655e-11, embedding_dim: 200, rmse: 4.0171895027160645-------------


Epoch 0: loss 16.129210282564813
Epoch 1: loss 16.12921055715456
Epoch 2: loss 16.129211728374486
Epoch 3: loss 16.129212095790614
Epoch 4: loss 16.12921273624074
Epoch 5: loss 16.129210496998923
Epoch 6: loss 16.12921196692271
Epoch 7: loss 16.129207234851947
Epoch 8: loss 16.12920750192224
Epoch 9: loss 16.12921106614387
Epoch 10: loss 16.129200114965307
Epoch 11: loss 16.12920817426523
Epoch 12: loss 16.129200486011516
Epoch 13: loss 16.12920756518938
Epoch 14: loss 16.129199770366842
Epoch 15: loss 16.129197237866283
Epoch 16: loss 16.129202646169407
Epoch 17: loss 16.12920187140626
Epoch 18: loss 16.129200871059467
Epoch 19: loss 16.129189086517865
-----------Time: 0:02:59.058361, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 20, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12921145559978
Epoch 1: loss 16.129205354469637
Epoch 2: loss 16.12920633096161
Epoch 3: loss 16.12920383294683
Epoch 4: loss 16.129205860088156
Epoch 5: loss 16.129209708493324
Epoch 6: loss 16.12920363458879
Epoch 7: loss 16.129205414625275
Epoch 8: loss 16.129201606410298
Epoch 9: loss 16.129202969765263
Epoch 10: loss 16.129197871315537
Epoch 11: loss 16.12919903709034
Epoch 12: loss 16.129196263707936
Epoch 13: loss 16.12919870338212
Epoch 14: loss 16.129198515395746
Epoch 15: loss 16.129192647109413
Epoch 16: loss 16.129191858863106
Epoch 17: loss 16.12919286128423
Epoch 18: loss 16.129193479175992
Epoch 19: loss 16.12918804105434
-----------Time: 0:03:44.033632, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129181502240062
Epoch 1: loss 16.129178894026374
Epoch 2: loss 16.12918726240182
Epoch 3: loss 16.129178808460164
Epoch 4: loss 16.129181225057398
Epoch 5: loss 16.129180969136637
Epoch 6: loss 16.12917864510649
Epoch 7: loss 16.1291756471776
Epoch 8: loss 16.129175664550132
Epoch 9: loss 16.12917026921159
Epoch 10: loss 16.1291750826999
Epoch 11: loss 16.12917502591505
Epoch 12: loss 16.1291744414719
Epoch 13: loss 16.12916669046963
Epoch 14: loss 16.129165521064746
Epoch 15: loss 16.129167664109396
Epoch 16: loss 16.129164556759477
Epoch 17: loss 16.129165050191293
Epoch 18: loss 16.129162166091398
Epoch 19: loss 16.129165921410898
-----------Time: 0:04:43.258571, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 100, rmse: 4.01718807220459-------------


Epoch 0: loss 16.129179231623972
Epoch 1: loss 16.12917307344969
Epoch 2: loss 16.129171178287756
Epoch 3: loss 16.129180670951357
Epoch 4: loss 16.129168097385573
Epoch 5: loss 16.129169671285275
Epoch 6: loss 16.12917365841142
Epoch 7: loss 16.129171013637624
Epoch 8: loss 16.12916915996234
Epoch 9: loss 16.129163191848765
Epoch 10: loss 16.129165480355972
Epoch 11: loss 16.12916819202699
Epoch 12: loss 16.129159336702017
Epoch 13: loss 16.12916417223011
Epoch 14: loss 16.129162199021422
Epoch 15: loss 16.129160696167606
Epoch 16: loss 16.12915492070765
Epoch 17: loss 16.129155812670575
Epoch 18: loss 16.129153908952027
Epoch 19: loss 16.129150881723195
-----------Time: 0:05:30.291619, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 150, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129167929883234
Epoch 1: loss 16.12917542107537
Epoch 2: loss 16.129171767398155
Epoch 3: loss 16.12916997050772
Epoch 4: loss 16.129173677339704
Epoch 5: loss 16.1291737909094
Epoch 6: loss 16.129167014584066
Epoch 7: loss 16.129163068944568
Epoch 8: loss 16.129169071803215
Epoch 9: loss 16.129163421840367
Epoch 10: loss 16.129163761512295
Epoch 11: loss 16.129164067735616
Epoch 12: loss 16.129166938352352
Epoch 13: loss 16.129158231342142
Epoch 14: loss 16.129163220370835
Epoch 15: loss 16.12915231249396
Epoch 16: loss 16.12915867939794
Epoch 17: loss 16.129158477669115
Epoch 18: loss 16.1291537072232
Epoch 19: loss 16.129152746807303
-----------Time: 0:05:29.346735, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-11, embedding_dim: 200, rmse: 4.017186641693115-------------


Epoch 0: loss 16.129448174004196
Epoch 1: loss 16.129444316523823
Epoch 2: loss 16.12944680727844
Epoch 3: loss 16.129441371749703
Epoch 4: loss 16.12944423692132
Epoch 5: loss 16.12943874305198
Epoch 6: loss 16.12943645402619
Epoch 7: loss 16.12943488375657
Epoch 8: loss 16.129432297064078
Epoch 9: loss 16.12943357096346
Epoch 10: loss 16.129437357397947
Epoch 11: loss 16.129427014517358
Epoch 12: loss 16.12942275306075
Epoch 13: loss 16.12943264866342
Epoch 14: loss 16.12942428158443
Epoch 15: loss 16.12941902963411
Epoch 16: loss 16.129421573284205
Epoch 17: loss 16.129421405522574
Epoch 18: loss 16.12941777232939
Epoch 19: loss 16.129420585124112
-----------Time: 0:02:38.115190, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 20, rmse: 4.017215728759766-------------


Epoch 0: loss 16.12913319337232
Epoch 1: loss 16.12913343477275
Epoch 2: loss 16.1291334731479
Epoch 3: loss 16.129129412642246
Epoch 4: loss 16.12912328221216
Epoch 5: loss 16.12913234004383
Epoch 6: loss 16.129132011002856
Epoch 7: loss 16.129124503734644
Epoch 8: loss 16.129123630959292
Epoch 9: loss 16.129121815399866
Epoch 10: loss 16.129129400455543
Epoch 11: loss 16.129121540291532
Epoch 12: loss 16.129119719286987
Epoch 13: loss 16.129115007700253
Epoch 14: loss 16.129112283583243
Epoch 15: loss 16.12911580579964
Epoch 16: loss 16.12911390519259
Epoch 17: loss 16.129104510800484
Epoch 18: loss 16.129109068368038
Epoch 19: loss 16.12910826534211
-----------Time: 0:03:11.652502, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 50, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129179366455578
Epoch 1: loss 16.129170920033374
Epoch 2: loss 16.129166857194097
Epoch 3: loss 16.12917084276449
Epoch 4: loss 16.129169611388928
Epoch 5: loss 16.12916785365152
Epoch 6: loss 16.129163876637744
Epoch 7: loss 16.129168292113526
Epoch 8: loss 16.129163227630997
Epoch 9: loss 16.129155949576514
Epoch 10: loss 16.129156134192097
Epoch 11: loss 16.129160696167606
Epoch 12: loss 16.129151386823132
Epoch 13: loss 16.129156606880592
Epoch 14: loss 16.129153377922933
Epoch 15: loss 16.129154065045537
Epoch 16: loss 16.129153706445326
Epoch 17: loss 16.12914705224631
Epoch 18: loss 16.129151381118717
Epoch 19: loss 16.12914164809185
-----------Time: 0:04:10.534619, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 100, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129180996880834
Epoch 1: loss 16.129173830062427
Epoch 2: loss 16.129173865066786
Epoch 3: loss 16.12917682876919
Epoch 4: loss 16.129171931270413
Epoch 5: loss 16.129174625309606
Epoch 6: loss 16.129167270245535
Epoch 7: loss 16.129164239386622
Epoch 8: loss 16.129165345265076
Epoch 9: loss 16.12916671225012
Epoch 10: loss 16.12916233800169
Epoch 11: loss 16.1291625280624
Epoch 12: loss 16.129158102474243
Epoch 13: loss 16.129170128416277
Epoch 14: loss 16.129163140509036
Epoch 15: loss 16.129156783458136
Epoch 16: loss 16.129160131949195
Epoch 17: loss 16.129158970323058
Epoch 18: loss 16.129150607392734
Epoch 19: loss 16.129152250004697
-----------Time: 0:04:59.316338, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 150, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129175350548067
Epoch 1: loss 16.129175984515904
Epoch 2: loss 16.129175083477772
Epoch 3: loss 16.129163922273055
Epoch 4: loss 16.129170765754903
Epoch 5: loss 16.129159433677057
Epoch 6: loss 16.129160483807826
Epoch 7: loss 16.129163636533768
Epoch 8: loss 16.129167004471697
Epoch 9: loss 16.129163191589473
Epoch 10: loss 16.129160818812508
Epoch 11: loss 16.129162874475906
Epoch 12: loss 16.1291606380863
Epoch 13: loss 16.129163272229142
Epoch 14: loss 16.12915492122623
Epoch 15: loss 16.129152963575038
Epoch 16: loss 16.129145244465633
Epoch 17: loss 16.129152894862777
Epoch 18: loss 16.129150080253016
Epoch 19: loss 16.129148596586774
-----------Time: 0:05:52.146762, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071863e-11, embedding_dim: 200, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129252667917125
Epoch 1: loss 16.12925194034504
Epoch 2: loss 16.129248720981167
Epoch 3: loss 16.12924393108839
Epoch 4: loss 16.129246988135748
Epoch 5: loss 16.12924303756971
Epoch 6: loss 16.12923778743443
Epoch 7: loss 16.129244226421463
Epoch 8: loss 16.129233510938917
Epoch 9: loss 16.129241687438615
Epoch 10: loss 16.1292309281358
Epoch 11: loss 16.129234898667285
Epoch 12: loss 16.12922775025858
Epoch 13: loss 16.12923486910805
Epoch 14: loss 16.129232175068857
Epoch 15: loss 16.129223221731678
Epoch 16: loss 16.129217530282183
Epoch 17: loss 16.129225723376543
Epoch 18: loss 16.129220504356248
Epoch 19: loss 16.129214889138467
-----------Time: 0:03:57.526728, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 20, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129203126377355
Epoch 1: loss 16.129188895419993
Epoch 2: loss 16.1291968906749
Epoch 3: loss 16.12918905384713
Epoch 4: loss 16.129192679520855
Epoch 5: loss 16.129190300002314
Epoch 6: loss 16.129182011229375
Epoch 7: loss 16.129180047355185
Epoch 8: loss 16.129185512183867
Epoch 9: loss 16.12917588365149
Epoch 10: loss 16.129171835851125
Epoch 11: loss 16.129170655296704
Epoch 12: loss 16.129171078979095
Epoch 13: loss 16.12917956792511
Epoch 14: loss 16.129173643631802
Epoch 15: loss 16.129166530486746
Epoch 16: loss 16.129168765579895
Epoch 17: loss 16.129166993840744
Epoch 18: loss 16.12915502183135
Epoch 19: loss 16.129164809309444
-----------Time: 0:03:00.757794, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 50, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.12916777015964
Epoch 1: loss 16.12916541838529
Epoch 2: loss 16.129162581217166
Epoch 3: loss 16.12916648925939
Epoch 4: loss 16.129172744668004
Epoch 5: loss 16.129164641547813
Epoch 6: loss 16.129165930486103
Epoch 7: loss 16.12915845718508
Epoch 8: loss 16.129143064860873
Epoch 9: loss 16.129154176800196
Epoch 10: loss 16.129151135569618
Epoch 11: loss 16.12914403201835
Epoch 12: loss 16.129154643524984
Epoch 13: loss 16.12914883565358
Epoch 14: loss 16.12914263625194
Epoch 15: loss 16.129142581022844
Epoch 16: loss 16.129142653105895
Epoch 17: loss 16.12914126096957
Epoch 18: loss 16.129142065810534
Epoch 19: loss 16.12913053018882
-----------Time: 0:03:52.591143, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 100, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129171729282298
Epoch 1: loss 16.12916854984933
Epoch 2: loss 16.12917156852154
Epoch 3: loss 16.129169608018138
Epoch 4: loss 16.1291597974631
Epoch 5: loss 16.12916318977443
Epoch 6: loss 16.129166206631602
Epoch 7: loss 16.129162244656737
Epoch 8: loss 16.1291575577027
Epoch 9: loss 16.129161657361376
Epoch 10: loss 16.129150518974317
Epoch 11: loss 16.129157576890275
Epoch 12: loss 16.12915018293247
Epoch 13: loss 16.129148769015654
Epoch 14: loss 16.129146315339728
Epoch 15: loss 16.129146331156512
Epoch 16: loss 16.12914278690033
Epoch 17: loss 16.12914030340588
Epoch 18: loss 16.12914309260507
Epoch 19: loss 16.12913734722293
-----------Time: 0:04:37.544633, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 150, rmse: 4.017185688018799-------------


Epoch 0: loss 16.12917849108731
Epoch 1: loss 16.12916840179385
Epoch 2: loss 16.129171678201864
Epoch 3: loss 16.129167639476698
Epoch 4: loss 16.12917116376743
Epoch 5: loss 16.129161729185135
Epoch 6: loss 16.12916624163596
Epoch 7: loss 16.129158347764047
Epoch 8: loss 16.129156159343378
Epoch 9: loss 16.129157674383897
Epoch 10: loss 16.129156326327134
Epoch 11: loss 16.129155785185674
Epoch 12: loss 16.129149696501525
Epoch 13: loss 16.129147920354416
Epoch 14: loss 16.1291454295998
Epoch 15: loss 16.129145229426722
Epoch 16: loss 16.129143629597866
Epoch 17: loss 16.12915146072122
Epoch 18: loss 16.129143713349038
Epoch 19: loss 16.129139044286116
-----------Time: 0:05:19.518322, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.32603346883218e-11, embedding_dim: 200, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129319065741186
Epoch 1: loss 16.12931055942264
Epoch 2: loss 16.129316954330104
Epoch 3: loss 16.12930777592786
Epoch 4: loss 16.129306231068814
Epoch 5: loss 16.129302585170347
Epoch 6: loss 16.12930085180634
Epoch 7: loss 16.129303469095234
Epoch 8: loss 16.129292144018258
Epoch 9: loss 16.12930271974266
Epoch 10: loss 16.129295617228554
Epoch 11: loss 16.1292902558572
Epoch 12: loss 16.129289969080748
Epoch 13: loss 16.12928972327236
Epoch 14: loss 16.12928072818924
Epoch 15: loss 16.129286075558852
Epoch 16: loss 16.129277362584936
Epoch 17: loss 16.12927917425499
Epoch 18: loss 16.12928191704099
Epoch 19: loss 16.12927193224203
-----------Time: 0:03:41.380953, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 20, rmse: 4.0171990394592285-------------


Epoch 0: loss 16.129182685128107
Epoch 1: loss 16.129180020907445
Epoch 2: loss 16.129169347430132
Epoch 3: loss 16.12917569255362
Epoch 4: loss 16.129177954872386
Epoch 5: loss 16.129170794017682
Epoch 6: loss 16.129170987189887
Epoch 7: loss 16.129175953141626
Epoch 8: loss 16.129159972484892
Epoch 9: loss 16.129163304122002
Epoch 10: loss 16.129160344049684
Epoch 11: loss 16.129158810340172
Epoch 12: loss 16.12915808328667
Epoch 13: loss 16.129153060031495
Epoch 14: loss 16.129145317067266
Epoch 15: loss 16.129143054748504
Epoch 16: loss 16.129143690272087
Epoch 17: loss 16.129141958982416
Epoch 18: loss 16.129145661665735
Epoch 19: loss 16.129140213691002
-----------Time: 0:03:48.035620, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129169611129637
Epoch 1: loss 16.129158999623
Epoch 2: loss 16.129158913019626
Epoch 3: loss 16.129150520530064
Epoch 4: loss 16.1291542779239
Epoch 5: loss 16.129156706707835
Epoch 6: loss 16.129154690716046
Epoch 7: loss 16.129145585174726
Epoch 8: loss 16.129145260023126
Epoch 9: loss 16.1291350869785
Epoch 10: loss 16.129137765200905
Epoch 11: loss 16.12913801982521
Epoch 12: loss 16.129138362349345
Epoch 13: loss 16.129143979122873
Epoch 14: loss 16.129139270388347
Epoch 15: loss 16.12913203667272
Epoch 16: loss 16.12912561635468
Epoch 17: loss 16.129118069674153
Epoch 18: loss 16.12912943182982
Epoch 19: loss 16.129122092841826
-----------Time: 0:04:44.780557, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 100, rmse: 4.01718282699585-------------


Epoch 0: loss 16.12917644398053
Epoch 1: loss 16.129165374046437
Epoch 2: loss 16.129153762711592
Epoch 3: loss 16.129162498762454
Epoch 4: loss 16.129158893054175
Epoch 5: loss 16.129159403339944
Epoch 6: loss 16.129160811552346
Epoch 7: loss 16.129149244297064
Epoch 8: loss 16.129147744554746
Epoch 9: loss 16.12914548534748
Epoch 10: loss 16.12914793487474
Epoch 11: loss 16.129146775063642
Epoch 12: loss 16.12914342942479
Epoch 13: loss 16.129133960616013
Epoch 14: loss 16.129135178508417
Epoch 15: loss 16.129130399246588
Epoch 16: loss 16.129132824659738
Epoch 17: loss 16.129130628460317
Epoch 18: loss 16.12912906959953
Epoch 19: loss 16.12912791964151
-----------Time: 0:04:13.091677, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 150, rmse: 4.017182350158691-------------


Epoch 0: loss 16.12917495097979
Epoch 1: loss 16.129174396355168
Epoch 2: loss 16.129165904297654
Epoch 3: loss 16.129168492545894
Epoch 4: loss 16.1291698514929
Epoch 5: loss 16.12915436349011
Epoch 6: loss 16.12916475330247
Epoch 7: loss 16.129154237993
Epoch 8: loss 16.129163104726803
Epoch 9: loss 16.129153222088714
Epoch 10: loss 16.12914989174806
Epoch 11: loss 16.129146654233782
Epoch 12: loss 16.129154255884117
Epoch 13: loss 16.129150513529193
Epoch 14: loss 16.12914794654286
Epoch 15: loss 16.12913498948488
Epoch 16: loss 16.129138401243075
Epoch 17: loss 16.12913267997505
Epoch 18: loss 16.12912963770731
Epoch 19: loss 16.129134136934265
-----------Time: 0:04:54.187761, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420634e-10, embedding_dim: 200, rmse: 4.017181873321533-------------


Epoch 0: loss 16.129426048656338
Epoch 1: loss 16.12941552219733
Epoch 2: loss 16.129411528329605
Epoch 3: loss 16.12940978251961
Epoch 4: loss 16.129408772838318
Epoch 5: loss 16.12940543394104
Epoch 6: loss 16.129407641289998
Epoch 7: loss 16.129404639212446
Epoch 8: loss 16.12940077447191
Epoch 9: loss 16.129401089770436
Epoch 10: loss 16.129392091057234
Epoch 11: loss 16.12938077427759
Epoch 12: loss 16.129385815942463
Epoch 13: loss 16.129383339967465
Epoch 14: loss 16.129372397864106
Epoch 15: loss 16.129375724574675
Epoch 16: loss 16.12938338663994
Epoch 17: loss 16.129372109531904
Epoch 18: loss 16.129368901058278
Epoch 19: loss 16.129355598623953
-----------Time: 0:03:13.354251, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 20, rmse: 4.017200946807861-------------


Epoch 0: loss 16.12915921820578
Epoch 1: loss 16.129158449147045
Epoch 2: loss 16.12916045269284
Epoch 3: loss 16.12915259849254
Epoch 4: loss 16.12914562951358
Epoch 5: loss 16.12915015985552
Epoch 6: loss 16.12914597774213
Epoch 7: loss 16.129135783694892
Epoch 8: loss 16.1291337492934
Epoch 9: loss 16.129131546611692
Epoch 10: loss 16.12913572120563
Epoch 11: loss 16.12912597443631
Epoch 12: loss 16.12912489682063
Epoch 13: loss 16.129117770192416
Epoch 14: loss 16.129113424725347
Epoch 15: loss 16.12911376828665
Epoch 16: loss 16.12910611088863
Epoch 17: loss 16.129105907604057
Epoch 18: loss 16.1290999112277
Epoch 19: loss 16.129104573289748
-----------Time: 0:03:36.037091, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 50, rmse: 4.017177104949951-------------


Epoch 0: loss 16.12919425082764
Epoch 1: loss 16.129188192999187
Epoch 2: loss 16.12918614589241
Epoch 3: loss 16.129180779594517
Epoch 4: loss 16.129178187197613
Epoch 5: loss 16.129174546225684
Epoch 6: loss 16.129179279074325
Epoch 7: loss 16.129170675521443
Epoch 8: loss 16.12916841968497
Epoch 9: loss 16.129166091765445
Epoch 10: loss 16.12916834137892
Epoch 11: loss 16.129157622007003
Epoch 12: loss 16.129155349835163
Epoch 13: loss 16.129156471789692
Epoch 14: loss 16.129154498321714
Epoch 15: loss 16.12914910868758
Epoch 16: loss 16.129147275496333
Epoch 17: loss 16.129139112998377
Epoch 18: loss 16.129133952318682
Epoch 19: loss 16.12913336294899
-----------Time: 0:04:51.435516, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 100, rmse: 4.017180442810059-------------


Epoch 0: loss 16.1291807585919
Epoch 1: loss 16.129173043112576
Epoch 2: loss 16.129166425992253
Epoch 3: loss 16.12917220015575
Epoch 4: loss 16.12915946971858
Epoch 5: loss 16.12916126946122
Epoch 6: loss 16.129156343181084
Epoch 7: loss 16.129157384495944
Epoch 8: loss 16.129152658129595
Epoch 9: loss 16.12914834351822
Epoch 10: loss 16.129142056476038
Epoch 11: loss 16.129139949732206
Epoch 12: loss 16.12913896623936
Epoch 13: loss 16.129136152666764
Epoch 14: loss 16.129126999415803
Epoch 15: loss 16.129130959316335
Epoch 16: loss 16.12912448947361
Epoch 17: loss 16.129121299409686
Epoch 18: loss 16.129112831725575
Epoch 19: loss 16.129116293267753
-----------Time: 0:05:48.633709, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 150, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129162514319948
Epoch 1: loss 16.12916530585276
Epoch 2: loss 16.12916086289207
Epoch 3: loss 16.12916245986872
Epoch 4: loss 16.129162183204638
Epoch 5: loss 16.12915962529351
Epoch 6: loss 16.129154317336216
Epoch 7: loss 16.129149932456833
Epoch 8: loss 16.12914659693035
Epoch 9: loss 16.129138253706184
Epoch 10: loss 16.12914145517894
Epoch 11: loss 16.12913945993047
Epoch 12: loss 16.129125650062583
Epoch 13: loss 16.129123507536516
Epoch 14: loss 16.129128340730983
Epoch 15: loss 16.12912102067127
Epoch 16: loss 16.129116752732376
Epoch 17: loss 16.129123290768778
Epoch 18: loss 16.12911171547546
Epoch 19: loss 16.129106391182795
-----------Time: 0:04:34.721422, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206402e-10, embedding_dim: 200, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12935046387341
Epoch 1: loss 16.12934683405102
Epoch 2: loss 16.12934015107064
Epoch 3: loss 16.12933628529294
Epoch 4: loss 16.129332552272512
Epoch 5: loss 16.129326536449287
Epoch 6: loss 16.12932279824303
Epoch 7: loss 16.12931123824791
Epoch 8: loss 16.129313056918836
Epoch 9: loss 16.129307522340724
Epoch 10: loss 16.129307792003935
Epoch 11: loss 16.129294205645362
Epoch 12: loss 16.12929739285708
Epoch 13: loss 16.129286201315253
Epoch 14: loss 16.129289814024403
Epoch 15: loss 16.129278108566723
Epoch 16: loss 16.129280906322535
Epoch 17: loss 16.12927138306253
Epoch 18: loss 16.129274710291686
Epoch 19: loss 16.129269081850037
-----------Time: 0:02:49.282021, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 20, rmse: 4.017192840576172-------------


Epoch 0: loss 16.129171112686997
Epoch 1: loss 16.129164149153162
Epoch 2: loss 16.129161222010868
Epoch 3: loss 16.129155609386004
Epoch 4: loss 16.12915499953228
Epoch 5: loss 16.12915567446818
Epoch 6: loss 16.12914647480403
Epoch 7: loss 16.129139837977547
Epoch 8: loss 16.129136552494334
Epoch 9: loss 16.1291275763395
Epoch 10: loss 16.12912286527135
Epoch 11: loss 16.129118841844384
Epoch 12: loss 16.12912106371367
Epoch 13: loss 16.129119017384763
Epoch 14: loss 16.12911035134262
Epoch 15: loss 16.12910684727663
Epoch 16: loss 16.129104273548716
Epoch 17: loss 16.12909935375087
Epoch 18: loss 16.129099106905315
Epoch 19: loss 16.129092517010605
-----------Time: 0:03:21.762432, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 50, rmse: 4.017177581787109-------------


Epoch 0: loss 16.129180128254145
Epoch 1: loss 16.129174605344158
Epoch 2: loss 16.12916567430605
Epoch 3: loss 16.129163370500642
Epoch 4: loss 16.12915808458313
Epoch 5: loss 16.129148326923563
Epoch 6: loss 16.129151361412557
Epoch 7: loss 16.1291478378997
Epoch 8: loss 16.129144595718177
Epoch 9: loss 16.129140070043483
Epoch 10: loss 16.129135764507318
Epoch 11: loss 16.129129738571724
Epoch 12: loss 16.12912611471304
Epoch 13: loss 16.129120666738306
Epoch 14: loss 16.129122860085516
Epoch 15: loss 16.129118999234354
Epoch 16: loss 16.129105046237534
Epoch 17: loss 16.12910245928575
Epoch 18: loss 16.129098504052465
Epoch 19: loss 16.129094716840104
-----------Time: 0:04:18.824456, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 100, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129171471027917
Epoch 1: loss 16.129164138262915
Epoch 2: loss 16.129159229614608
Epoch 3: loss 16.129157110424778
Epoch 4: loss 16.12915596694905
Epoch 5: loss 16.12915923013319
Epoch 6: loss 16.129139116109876
Epoch 7: loss 16.129142211273095
Epoch 8: loss 16.12913761662685
Epoch 9: loss 16.129126276770254
Epoch 10: loss 16.12912979354153
Epoch 11: loss 16.129119686875544
Epoch 12: loss 16.129123649368992
Epoch 13: loss 16.12912184469981
Epoch 14: loss 16.129109389630266
Epoch 15: loss 16.12910283422133
Epoch 16: loss 16.129099821253533
Epoch 17: loss 16.129100273717285
Epoch 18: loss 16.129099700423673
Epoch 19: loss 16.12909694285805
-----------Time: 0:05:12.032263, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 150, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129164888652657
Epoch 1: loss 16.129158934540825
Epoch 2: loss 16.12916208104377
Epoch 3: loss 16.129156830649197
Epoch 4: loss 16.129152617939404
Epoch 5: loss 16.12915063824843
Epoch 6: loss 16.12914605967826
Epoch 7: loss 16.129140840657968
Epoch 8: loss 16.129139543422347
Epoch 9: loss 16.129127850669956
Epoch 10: loss 16.129131227423795
Epoch 11: loss 16.12913431014102
Epoch 12: loss 16.129120293877058
Epoch 13: loss 16.129119640203065
Epoch 14: loss 16.129115421529566
Epoch 15: loss 16.129105338199818
Epoch 16: loss 16.129106349436857
Epoch 17: loss 16.129098418745546
Epoch 18: loss 16.129101865767396
Epoch 19: loss 16.129094059017444
-----------Time: 0:06:06.532547, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318867e-10, embedding_dim: 200, rmse: 4.017179489135742-------------


Epoch 0: loss 16.129326738696697
Epoch 1: loss 16.129331275002343
Epoch 2: loss 16.129319044479278
Epoch 3: loss 16.129314312149223
Epoch 4: loss 16.129312850522762
Epoch 5: loss 16.12930167038976
Epoch 6: loss 16.12929966995546
Epoch 7: loss 16.12929297815917
Epoch 8: loss 16.129287527591526
Epoch 9: loss 16.12928375852957
Epoch 10: loss 16.129276555151055
Epoch 11: loss 16.12926862731195
Epoch 12: loss 16.12926128469387
Epoch 13: loss 16.129259792730302
Epoch 14: loss 16.12925283801238
Epoch 15: loss 16.129248194100743
Epoch 16: loss 16.129251516403357
Epoch 17: loss 16.129234563403315
Epoch 18: loss 16.129232307048255
Epoch 19: loss 16.1292291486179
-----------Time: 0:03:10.080397, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 20, rmse: 4.017273902893066-------------


Epoch 0: loss 16.12918290293301
Epoch 1: loss 16.129169860308814
Epoch 2: loss 16.129162210689543
Epoch 3: loss 16.129163158400154
Epoch 4: loss 16.12915653350108
Epoch 5: loss 16.12915017852451
Epoch 6: loss 16.12914334593291
Epoch 7: loss 16.129138029159705
Epoch 8: loss 16.129130840560805
Epoch 9: loss 16.129127165621686
Epoch 10: loss 16.129125494487653
Epoch 11: loss 16.129110282889652
Epoch 12: loss 16.12911514927344
Epoch 13: loss 16.129103772597446
Epoch 14: loss 16.129105268191097
Epoch 15: loss 16.129097472849978
Epoch 16: loss 16.129098835686356
Epoch 17: loss 16.12908082244317
Epoch 18: loss 16.12907944742009
Epoch 19: loss 16.129074960639127
-----------Time: 0:02:54.777780, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 50, rmse: 4.0171732902526855-------------


Epoch 0: loss 16.129184643557178
Epoch 1: loss 16.129171169212555
Epoch 2: loss 16.12917725815599
Epoch 3: loss 16.129169155813678
Epoch 4: loss 16.12915306469874
Epoch 5: loss 16.12915399814832
Epoch 6: loss 16.129149418800274
Epoch 7: loss 16.129142193641268
Epoch 8: loss 16.129141309197795
Epoch 9: loss 16.12913134047491
Epoch 10: loss 16.129127089389968
Epoch 11: loss 16.129120030955427
Epoch 12: loss 16.12911573216084
Epoch 13: loss 16.12910988591429
Epoch 14: loss 16.12910934658787
Epoch 15: loss 16.129103141222522
Epoch 16: loss 16.129086441550324
Epoch 17: loss 16.129092397736493
Epoch 18: loss 16.129080235666397
Epoch 19: loss 16.129079091672082
-----------Time: 0:03:58.534134, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 100, rmse: 4.017172813415527-------------


Epoch 0: loss 16.129175097479518
Epoch 1: loss 16.12916765607136
Epoch 2: loss 16.129162624000273
Epoch 3: loss 16.129160998760845
Epoch 4: loss 16.12915095484341
Epoch 5: loss 16.12914881854034
Epoch 6: loss 16.129139323802406
Epoch 7: loss 16.12913603961565
Epoch 8: loss 16.129133112473355
Epoch 9: loss 16.12912789812031
Epoch 10: loss 16.1291233975969
Epoch 11: loss 16.12911784305334
Epoch 12: loss 16.129109039586677
Epoch 13: loss 16.12911070112692
Epoch 14: loss 16.129096637931188
Epoch 15: loss 16.129083787701322
Epoch 16: loss 16.12909438494692
Epoch 17: loss 16.129077567037776
Epoch 18: loss 16.12907730333827
Epoch 19: loss 16.129073378182806
-----------Time: 0:04:43.189260, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 150, rmse: 4.017174243927002-------------


Epoch 0: loss 16.129170783905312
Epoch 1: loss 16.12916138458667
Epoch 2: loss 16.129149521220437
Epoch 3: loss 16.129143394679723
Epoch 4: loss 16.12914033737307
Epoch 5: loss 16.129142499605297
Epoch 6: loss 16.129136126219027
Epoch 7: loss 16.129126441161098
Epoch 8: loss 16.129122289384817
Epoch 9: loss 16.12911572697501
Epoch 10: loss 16.129120156452537
Epoch 11: loss 16.12911744089215
Epoch 12: loss 16.12909903845235
Epoch 13: loss 16.129097398433302
Epoch 14: loss 16.12909093922153
Epoch 15: loss 16.12908683126552
Epoch 16: loss 16.1290814198509
Epoch 17: loss 16.129070014134253
Epoch 18: loss 16.129072149918738
Epoch 19: loss 16.12907335536515
-----------Time: 0:05:34.472162, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.848035868435805e-10, embedding_dim: 200, rmse: 4.017175674438477-------------


Epoch 0: loss 16.129304882493468
Epoch 1: loss 16.129300651892557
Epoch 2: loss 16.12928900140469
Epoch 3: loss 16.129286496129744
Epoch 4: loss 16.129278346855656
Epoch 5: loss 16.129265587637125
Epoch 6: loss 16.12926002349978
Epoch 7: loss 16.129257246228
Epoch 8: loss 16.129248387532236
Epoch 9: loss 16.129244496862547
Epoch 10: loss 16.129228983967767
Epoch 11: loss 16.129221285860975
Epoch 12: loss 16.129213365022743
Epoch 13: loss 16.12920904522554
Epoch 14: loss 16.129206634851304
Epoch 15: loss 16.129187636559525
Epoch 16: loss 16.129191630945833
Epoch 17: loss 16.12918434589048
Epoch 18: loss 16.129173156422983
Epoch 19: loss 16.129165208359137
-----------Time: 0:03:49.143095, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 20, rmse: 4.017218112945557-------------


Epoch 0: loss 16.129171028935826
Epoch 1: loss 16.1291702227984
Epoch 2: loss 16.129152575934175
Epoch 3: loss 16.129150060287568
Epoch 4: loss 16.129147552160415
Epoch 5: loss 16.12913706070577
Epoch 6: loss 16.12913061186566
Epoch 7: loss 16.129128276945263
Epoch 8: loss 16.129112163531254
Epoch 9: loss 16.129107943561298
Epoch 10: loss 16.129099119869895
Epoch 11: loss 16.129095218828542
Epoch 12: loss 16.12908439159134
Epoch 13: loss 16.1290791953887
Epoch 14: loss 16.129076763233975
Epoch 15: loss 16.12905679674755
Epoch 16: loss 16.129054317401764
Epoch 17: loss 16.129046120677323
Epoch 18: loss 16.129040352736823
Epoch 19: loss 16.129039728622065
-----------Time: 0:03:48.335246, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 50, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129174205516588
Epoch 1: loss 16.129163445954482
Epoch 2: loss 16.12915712598227
Epoch 3: loss 16.129137795019435
Epoch 4: loss 16.129143130980218
Epoch 5: loss 16.129143273590568
Epoch 6: loss 16.12912831480183
Epoch 7: loss 16.129126184721756
Epoch 8: loss 16.12911390726692
Epoch 9: loss 16.12910923975975
Epoch 10: loss 16.12909963689724
Epoch 11: loss 16.129090522799302
Epoch 12: loss 16.129089399548313
Epoch 13: loss 16.129076691410216
Epoch 14: loss 16.12906904775465
Epoch 15: loss 16.129063940489015
Epoch 16: loss 16.12905026545273
Epoch 17: loss 16.12904488904247
Epoch 18: loss 16.129042417216134
Epoch 19: loss 16.129037621618938
-----------Time: 0:03:52.134642, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 100, rmse: 4.017169952392578-------------


Epoch 0: loss 16.129164906543775
Epoch 1: loss 16.12915940904436
Epoch 2: loss 16.12915915493864
Epoch 3: loss 16.129143264515363
Epoch 4: loss 16.129141414470165
Epoch 5: loss 16.12912796709186
Epoch 6: loss 16.129134198904946
Epoch 7: loss 16.129118433200905
Epoch 8: loss 16.12910463889051
Epoch 9: loss 16.12910885289676
Epoch 10: loss 16.12909314734836
Epoch 11: loss 16.129092718220846
Epoch 12: loss 16.12907760593151
Epoch 13: loss 16.129076626328036
Epoch 14: loss 16.129066797363297
Epoch 15: loss 16.12906348439518
Epoch 16: loss 16.12905469000372
Epoch 17: loss 16.129051652143932
Epoch 18: loss 16.129040692927333
Epoch 19: loss 16.129036809777098
-----------Time: 0:04:17.099949, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 150, rmse: 4.017170429229736-------------


Epoch 0: loss 16.129171901451887
Epoch 1: loss 16.129159564619286
Epoch 2: loss 16.129158579570692
Epoch 3: loss 16.129146022858862
Epoch 4: loss 16.129138590266614
Epoch 5: loss 16.129135105906784
Epoch 6: loss 16.129126708231393
Epoch 7: loss 16.129119524559034
Epoch 8: loss 16.129113971052643
Epoch 9: loss 16.129101741826034
Epoch 10: loss 16.129089742850322
Epoch 11: loss 16.129084288134013
Epoch 12: loss 16.129080647939958
Epoch 13: loss 16.129076562542316
Epoch 14: loss 16.12906433513075
Epoch 15: loss 16.12905886174545
Epoch 16: loss 16.12904840529516
Epoch 17: loss 16.12904700278717
Epoch 18: loss 16.129033705020092
Epoch 19: loss 16.129032116600065
-----------Time: 0:05:04.760868, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.7649358067924715e-10, embedding_dim: 200, rmse: 4.017171859741211-------------


Epoch 0: loss 16.129119163625198
Epoch 1: loss 16.129105525149022
Epoch 2: loss 16.129097948649967
Epoch 3: loss 16.12909023109631
Epoch 4: loss 16.12907969763643
Epoch 5: loss 16.129072122174545
Epoch 6: loss 16.129059495453994
Epoch 7: loss 16.129052676864134
Epoch 8: loss 16.12904638282108
Epoch 9: loss 16.1290273108905
Epoch 10: loss 16.129022096796746
Epoch 11: loss 16.129010081485667
Epoch 12: loss 16.12899731293264
Epoch 13: loss 16.128989542483506
Epoch 14: loss 16.128985696930545
Epoch 15: loss 16.128974009363983
Epoch 16: loss 16.12896259223851
Epoch 17: loss 16.12895362567746
Epoch 18: loss 16.12894674304259
Epoch 19: loss 16.12893348053916
-----------Time: 0:03:12.489397, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 20, rmse: 4.01721715927124-------------


Epoch 0: loss 16.12917872289395
Epoch 1: loss 16.129162899108604
Epoch 2: loss 16.12915831094465
Epoch 3: loss 16.12913673088692
Epoch 4: loss 16.129141701505908
Epoch 5: loss 16.129123668815858
Epoch 6: loss 16.12911529681033
Epoch 7: loss 16.129106231977786
Epoch 8: loss 16.129089108363903
Epoch 9: loss 16.12908834734321
Epoch 10: loss 16.129069562448375
Epoch 11: loss 16.1290641709992
Epoch 12: loss 16.12906132138508
Epoch 13: loss 16.129053525266084
Epoch 14: loss 16.129033968460305
Epoch 15: loss 16.12902440267649
Epoch 16: loss 16.12901984770185
Epoch 17: loss 16.12900515650199
Epoch 18: loss 16.1290007350625
Epoch 19: loss 16.12899167774941
-----------Time: 0:03:47.245008, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 50, rmse: 4.017175197601318-------------


Epoch 0: loss 16.12918906603383
Epoch 1: loss 16.129174147435283
Epoch 2: loss 16.129170978633265
Epoch 3: loss 16.129166878974587
Epoch 4: loss 16.129156001434826
Epoch 5: loss 16.129143998310447
Epoch 6: loss 16.129131092332898
Epoch 7: loss 16.129124468470994
Epoch 8: loss 16.129116429136523
Epoch 9: loss 16.129105652201883
Epoch 10: loss 16.129101692560642
Epoch 11: loss 16.1290872829514
Epoch 12: loss 16.12907331517496
Epoch 13: loss 16.12907387420754
Epoch 14: loss 16.12905678637589
Epoch 15: loss 16.1290539388361
Epoch 16: loss 16.1290387456478
Epoch 17: loss 16.12903052843933
Epoch 18: loss 16.129022549779084
Epoch 19: loss 16.129013906295302
-----------Time: 0:05:05.245882, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 100, rmse: 4.017160415649414-------------


Epoch 0: loss 16.129173729716598
Epoch 1: loss 16.129170851061822
Epoch 2: loss 16.12915318137994
Epoch 3: loss 16.12914140202417
Epoch 4: loss 16.129137542469465
Epoch 5: loss 16.129129540732272
Epoch 6: loss 16.12911939491326
Epoch 7: loss 16.129112608734843
Epoch 8: loss 16.12910127769416
Epoch 9: loss 16.129090631442462
Epoch 10: loss 16.129080651829334
Epoch 11: loss 16.129069610935893
Epoch 12: loss 16.129064675061972
Epoch 13: loss 16.129055200289486
Epoch 14: loss 16.129048195009712
Epoch 15: loss 16.12902611529717
Epoch 16: loss 16.129021855914896
Epoch 17: loss 16.12901245426263
Epoch 18: loss 16.128997921489905
Epoch 19: loss 16.128997601524134
-----------Time: 0:05:02.796183, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 150, rmse: 4.017165184020996-------------


Epoch 0: loss 16.129174731878432
Epoch 1: loss 16.12916045061851
Epoch 2: loss 16.129145768493853
Epoch 3: loss 16.12914632908218
Epoch 4: loss 16.129129555252597
Epoch 5: loss 16.12911625930056
Epoch 6: loss 16.129113777880438
Epoch 7: loss 16.129098153231
Epoch 8: loss 16.129094562302342
Epoch 9: loss 16.12907904577748
Epoch 10: loss 16.129074320966875
Epoch 11: loss 16.12906625207317
Epoch 12: loss 16.129050530448694
Epoch 13: loss 16.129048409962408
Epoch 14: loss 16.12903550917069
Epoch 15: loss 16.1290337662129
Epoch 16: loss 16.12901598503565
Epoch 17: loss 16.129001957881446
Epoch 18: loss 16.128995364875234
Epoch 19: loss 16.128987620095963
-----------Time: 0:04:29.809332, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.977023564332114e-10, embedding_dim: 200, rmse: 4.017165184020996-------------


Epoch 0: loss 16.12933719047974
Epoch 1: loss 16.129330605770857
Epoch 2: loss 16.12931592935061
Epoch 3: loss 16.129302045584634
Epoch 4: loss 16.129289017480765
Epoch 5: loss 16.129283936662866
Epoch 6: loss 16.129265254188194
Epoch 7: loss 16.129245493579262
Epoch 8: loss 16.129240281300547
Epoch 9: loss 16.12922741395744
Epoch 10: loss 16.12921920063834
Epoch 11: loss 16.1291991579202
Epoch 12: loss 16.12918314614848
Epoch 13: loss 16.12917677509584
Epoch 14: loss 16.12916105995365
Epoch 15: loss 16.129152001084808
Epoch 16: loss 16.129139066325898
Epoch 17: loss 16.12912599803184
Epoch 18: loss 16.12911176111077
Epoch 19: loss 16.12909690266786
-----------Time: 0:02:53.518111, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 20, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129261202757743
Epoch 1: loss 16.129253640001142
Epoch 2: loss 16.129242202650925
Epoch 3: loss 16.129224709546584
Epoch 4: loss 16.12920583234396
Epoch 5: loss 16.12920361410476
Epoch 6: loss 16.1291781200411
Epoch 7: loss 16.129172533863976
Epoch 8: loss 16.1291637337681
Epoch 9: loss 16.129152954499833
Epoch 10: loss 16.129139438668563
Epoch 11: loss 16.129121499842054
Epoch 12: loss 16.129113357568837
Epoch 13: loss 16.129095926953763
Epoch 14: loss 16.129081882945606
Epoch 15: loss 16.129072313791
Epoch 16: loss 16.12906926867105
Epoch 17: loss 16.129044729578165
Epoch 18: loss 16.12902926128153
Epoch 19: loss 16.129021999821706
-----------Time: 0:03:21.550225, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 50, rmse: 4.017172813415527-------------


Epoch 0: loss 16.12916393160755
Epoch 1: loss 16.129153580170343
Epoch 2: loss 16.129133386025938
Epoch 3: loss 16.12912180632466
Epoch 4: loss 16.129111170963206
Epoch 5: loss 16.12909569203562
Epoch 6: loss 16.129083836188844
Epoch 7: loss 16.12907053297664
Epoch 8: loss 16.129056093548872
Epoch 9: loss 16.12903947166414
Epoch 10: loss 16.12903103898439
Epoch 11: loss 16.129017937760302
Epoch 12: loss 16.129013568957
Epoch 13: loss 16.128994427795575
Epoch 14: loss 16.128986468841486
Epoch 15: loss 16.128970536931565
Epoch 16: loss 16.128954885056512
Epoch 17: loss 16.128939096794106
Epoch 18: loss 16.12893055417474
Epoch 19: loss 16.12891575588747
-----------Time: 0:04:26.278210, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 100, rmse: 4.017154693603516-------------


Epoch 0: loss 16.129164827459853
Epoch 1: loss 16.129158792189763
Epoch 2: loss 16.129135392164653
Epoch 3: loss 16.12913224929179
Epoch 4: loss 16.129116688428073
Epoch 5: loss 16.12910388564856
Epoch 6: loss 16.129099049861175
Epoch 7: loss 16.12907594905851
Epoch 8: loss 16.129064359244865
Epoch 9: loss 16.12905621515661
Epoch 10: loss 16.129045243753307
Epoch 11: loss 16.12903099412695
Epoch 12: loss 16.129017070689365
Epoch 13: loss 16.129003177329597
Epoch 14: loss 16.12898967601865
Epoch 15: loss 16.12897612647948
Epoch 16: loss 16.128964407538643
Epoch 17: loss 16.12895379810634
Epoch 18: loss 16.128941034739142
Epoch 19: loss 16.128920220110064
-----------Time: 0:05:13.709852, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 150, rmse: 4.017155647277832-------------


Epoch 0: loss 16.12916840620181
Epoch 1: loss 16.129157665827275
Epoch 2: loss 16.1291448804203
Epoch 3: loss 16.12912952906415
Epoch 4: loss 16.129121118164893
Epoch 5: loss 16.129107358858857
Epoch 6: loss 16.129092013984998
Epoch 7: loss 16.129077893226544
Epoch 8: loss 16.129072817075894
Epoch 9: loss 16.12905527081679
Epoch 10: loss 16.129044630528792
Epoch 11: loss 16.129024870438442
Epoch 12: loss 16.129018249688038
Epoch 13: loss 16.12900675840518
Epoch 14: loss 16.128989483365032
Epoch 15: loss 16.12897093312905
Epoch 16: loss 16.128964347642295
Epoch 17: loss 16.128953189289785
Epoch 18: loss 16.128939308635303
Epoch 19: loss 16.128931555558705
-----------Time: 0:06:05.123675, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.579332246575682e-10, embedding_dim: 200, rmse: 4.01715612411499-------------


Epoch 0: loss 16.129000181993625
Epoch 1: loss 16.128980057080067
Epoch 2: loss 16.128966939520616
Epoch 3: loss 16.12894096913838
Epoch 4: loss 16.12892908399166
Epoch 5: loss 16.12890934075526
Epoch 6: loss 16.128901877825903
Epoch 7: loss 16.12888486285518
Epoch 8: loss 16.128869832501973
Epoch 9: loss 16.128847507240334
Epoch 10: loss 16.128830822607043
Epoch 11: loss 16.128808874355315
Epoch 12: loss 16.128796370538957
Epoch 13: loss 16.128772913210415
Epoch 14: loss 16.128768513810705
Epoch 15: loss 16.12875150324794
Epoch 16: loss 16.128734523281576
Epoch 17: loss 16.128714223346222
Epoch 18: loss 16.128694967577935
Epoch 19: loss 16.128677957533753
-----------Time: 0:02:56.640388, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 20, rmse: 4.017172813415527-------------


Epoch 0: loss 16.12916285969629
Epoch 1: loss 16.12914131464292
Epoch 2: loss 16.129135923453035
Epoch 3: loss 16.129124138133562
Epoch 4: loss 16.129097604570084
Epoch 5: loss 16.1290853377462
Epoch 6: loss 16.129072340498027
Epoch 7: loss 16.129050522929237
Epoch 8: loss 16.129031557567487
Epoch 9: loss 16.12901090421775
Epoch 10: loss 16.129004478973172
Epoch 11: loss 16.128979944806826
Epoch 12: loss 16.1289640468641
Epoch 13: loss 16.128950887299414
Epoch 14: loss 16.128934517964648
Epoch 15: loss 16.12891712053889
Epoch 16: loss 16.128895991389165
Epoch 17: loss 16.12887539300924
Epoch 18: loss 16.128862729209995
Epoch 19: loss 16.12885280223305
-----------Time: 0:03:00.301651, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 50, rmse: 4.017153263092041-------------


Epoch 0: loss 16.12915360324729
Epoch 1: loss 16.129139079549766
Epoch 2: loss 16.129119933980387
Epoch 3: loss 16.129108315903963
Epoch 4: loss 16.129089783299804
Epoch 5: loss 16.129071745942507
Epoch 6: loss 16.129058849299454
Epoch 7: loss 16.1290302071771
Epoch 8: loss 16.129022871559894
Epoch 9: loss 16.129007141378796
Epoch 10: loss 16.128990658474333
Epoch 11: loss 16.12896899777693
Epoch 12: loss 16.128952995339706
Epoch 13: loss 16.12893692263447
Epoch 14: loss 16.12892142918656
Epoch 15: loss 16.128905278434566
Epoch 16: loss 16.128888859575117
Epoch 17: loss 16.128867367935804
Epoch 18: loss 16.128853021852994
Epoch 19: loss 16.128829470142325
-----------Time: 0:04:05.053322, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 100, rmse: 4.017148017883301-------------


Epoch 0: loss 16.129169629280046
Epoch 1: loss 16.129150571869793
Epoch 2: loss 16.12913189769245
Epoch 3: loss 16.129118153684615
Epoch 4: loss 16.129101530503423
Epoch 5: loss 16.129081293835206
Epoch 6: loss 16.129063523288913
Epoch 7: loss 16.129055751284028
Epoch 8: loss 16.129026005357552
Epoch 9: loss 16.1290149618712
Epoch 10: loss 16.129006980618037
Epoch 11: loss 16.12898594040531
Epoch 12: loss 16.1289695080627
Epoch 13: loss 16.128948975542823
Epoch 14: loss 16.1289328442377
Epoch 15: loss 16.12891757611414
Epoch 16: loss 16.128900841178293
Epoch 17: loss 16.128882686361923
Epoch 18: loss 16.128862230592347
Epoch 19: loss 16.128842397900364
-----------Time: 0:04:45.036698, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 150, rmse: 4.017147064208984-------------


Epoch 0: loss 16.129164656068138
Epoch 1: loss 16.129153721224945
Epoch 2: loss 16.129135371421327
Epoch 3: loss 16.12912233657588
Epoch 4: loss 16.129099031710766
Epoch 5: loss 16.12908991164912
Epoch 6: loss 16.129067267977458
Epoch 7: loss 16.129047212813326
Epoch 8: loss 16.129029758084137
Epoch 9: loss 16.129019424538043
Epoch 10: loss 16.129004125040208
Epoch 11: loss 16.128978256559552
Epoch 12: loss 16.128964592154226
Epoch 13: loss 16.128953773732935
Epoch 14: loss 16.128930914589994
Epoch 15: loss 16.128921731520503
Epoch 16: loss 16.12889735681846
Epoch 17: loss 16.128889643413466
Epoch 18: loss 16.1288628088125
Epoch 19: loss 16.12883654413436
-----------Time: 0:05:41.132919, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.697490026177835e-10, embedding_dim: 200, rmse: 4.017147541046143-------------


Epoch 0: loss 16.12912748247596
Epoch 1: loss 16.129095203789632
Epoch 2: loss 16.129067668582902
Epoch 3: loss 16.129050301494257
Epoch 4: loss 16.129031671655767
Epoch 5: loss 16.129004723744394
Epoch 6: loss 16.12898775518686
Epoch 7: loss 16.128958627411436
Epoch 8: loss 16.12893701805376
Epoch 9: loss 16.12892096427681
Epoch 10: loss 16.128896473671446
Epoch 11: loss 16.128875049966517
Epoch 12: loss 16.128850392118103
Epoch 13: loss 16.128835587867126
Epoch 14: loss 16.128806548769415
Epoch 15: loss 16.128780352025657
Epoch 16: loss 16.128762869552272
Epoch 17: loss 16.12873904921556
Epoch 18: loss 16.128721495177707
Epoch 19: loss 16.12869471191647
-----------Time: 0:03:48.195479, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 20, rmse: 4.0171589851379395-------------


Epoch 0: loss 16.129169858234484
Epoch 1: loss 16.12914318958011
Epoch 2: loss 16.12912502880003
Epoch 3: loss 16.12910826534211
Epoch 4: loss 16.129077031082144
Epoch 5: loss 16.129057190352125
Epoch 6: loss 16.12903206163026
Epoch 7: loss 16.129015588578874
Epoch 8: loss 16.128990086477174
Epoch 9: loss 16.128976271164163
Epoch 10: loss 16.12894938807568
Epoch 11: loss 16.128924309915664
Epoch 12: loss 16.12890446348123
Epoch 13: loss 16.128884311082768
Epoch 14: loss 16.128864423680266
Epoch 15: loss 16.12883431111554
Epoch 16: loss 16.1288198462772
Epoch 17: loss 16.128790916859813
Epoch 18: loss 16.128766839565177
Epoch 19: loss 16.12875023220077
-----------Time: 0:03:43.640056, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 50, rmse: 4.017138481140137-------------


Epoch 0: loss 16.1291627795752
Epoch 1: loss 16.129138199773543
Epoch 2: loss 16.129126939519455
Epoch 3: loss 16.129094896269855
Epoch 4: loss 16.129070246978063
Epoch 5: loss 16.12905393002019
Epoch 6: loss 16.129031406141223
Epoch 7: loss 16.129007031957762
Epoch 8: loss 16.128986346715166
Epoch 9: loss 16.128968127075908
Epoch 10: loss 16.128941940185232
Epoch 11: loss 16.128919452347787
Epoch 12: loss 16.12889946122867
Epoch 13: loss 16.128878275553387
Epoch 14: loss 16.128852608282973
Epoch 15: loss 16.128831470058042
Epoch 16: loss 16.12880520563919
Epoch 17: loss 16.128788921870637
Epoch 18: loss 16.128760283637657
Epoch 19: loss 16.12874423063858
-----------Time: 0:03:34.278852, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 100, rmse: 4.0171356201171875-------------


Epoch 0: loss 16.12916238571134
Epoch 1: loss 16.129133812041953
Epoch 2: loss 16.129121466393443
Epoch 3: loss 16.129095359623854
Epoch 4: loss 16.12907837965749
Epoch 5: loss 16.129060718013644
Epoch 6: loss 16.129034811676423
Epoch 7: loss 16.129010533430836
Epoch 8: loss 16.1289857228597
Epoch 9: loss 16.12896851938402
Epoch 10: loss 16.1289340592779
Epoch 11: loss 16.128924098074467
Epoch 12: loss 16.128895211440184
Epoch 13: loss 16.128878915225638
Epoch 14: loss 16.128851746138572
Epoch 15: loss 16.128827211194352
Epoch 16: loss 16.128806947819108
Epoch 17: loss 16.128786381850624
Epoch 18: loss 16.128766625390355
Epoch 19: loss 16.12874563703594
-----------Time: 0:04:25.313741, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 150, rmse: 4.017135143280029-------------


Epoch 0: loss 16.129167700669505
Epoch 1: loss 16.129143032449427
Epoch 2: loss 16.12912293787298
Epoch 3: loss 16.129095014506802
Epoch 4: loss 16.1290781369606
Epoch 5: loss 16.129050694320952
Epoch 6: loss 16.129039975467617
Epoch 7: loss 16.12900555321806
Epoch 8: loss 16.12899551500504
Epoch 9: loss 16.12896224789933
Epoch 10: loss 16.12894809732235
Epoch 11: loss 16.128922945782826
Epoch 12: loss 16.128901037202702
Epoch 13: loss 16.1288801865321
Epoch 14: loss 16.128858653665432
Epoch 15: loss 16.12883801379886
Epoch 16: loss 16.128810388358666
Epoch 17: loss 16.128787042006905
Epoch 18: loss 16.12876569038503
Epoch 19: loss 16.128747323468172
-----------Time: 0:05:10.799581, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.1497569953977357e-09, embedding_dim: 200, rmse: 4.01713228225708-------------


Epoch 0: loss 16.129244270760317
Epoch 1: loss 16.129220065894238
Epoch 2: loss 16.129184765683497
Epoch 3: loss 16.12915581941216
Epoch 4: loss 16.129128858276914
Epoch 5: loss 16.12909977406247
Epoch 6: loss 16.12907536124457
Epoch 7: loss 16.129039094654228
Epoch 8: loss 16.129010830578952
Epoch 9: loss 16.128983314819088
Epoch 10: loss 16.12894885730588
Epoch 11: loss 16.128919414750513
Epoch 12: loss 16.12889859934356
Epoch 13: loss 16.12886812091846
Epoch 14: loss 16.128837075941327
Epoch 15: loss 16.128803691117255
Epoch 16: loss 16.1287832286061
Epoch 17: loss 16.128746839630143
Epoch 18: loss 16.128725017134816
Epoch 19: loss 16.12869608330947
-----------Time: 0:03:33.606033, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 20, rmse: 4.017134189605713-------------


Epoch 0: loss 16.129172440519017
Epoch 1: loss 16.12915020497225
Epoch 2: loss 16.129116710727146
Epoch 3: loss 16.129088061085337
Epoch 4: loss 16.129062284653184
Epoch 5: loss 16.12903002593231
Epoch 6: loss 16.129004636363142
Epoch 7: loss 16.12897440037564
Epoch 8: loss 16.128944360671838
Epoch 9: loss 16.12891577066709
Epoch 10: loss 16.12887885766291
Epoch 11: loss 16.12885083343232
Epoch 12: loss 16.12882827688262
Epoch 13: loss 16.12880130122705
Epoch 14: loss 16.128768451580733
Epoch 15: loss 16.128738834262865
Epoch 16: loss 16.128715402344895
Epoch 17: loss 16.12867684672876
Epoch 18: loss 16.128655915418484
Epoch 19: loss 16.12861987544895
-----------Time: 0:03:55.275355, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 50, rmse: 4.01711368560791-------------


Epoch 0: loss 16.12916578294921
Epoch 1: loss 16.129132290519145
Epoch 2: loss 16.129104819876012
Epoch 3: loss 16.1290786708419
Epoch 4: loss 16.12904082153594
Epoch 5: loss 16.129020367581408
Epoch 6: loss 16.12897964714013
Epoch 7: loss 16.128955334149474
Epoch 8: loss 16.128919868251433
Epoch 9: loss 16.128890630277102
Epoch 10: loss 16.1288701231678
Epoch 11: loss 16.128838077065996
Epoch 12: loss 16.128805406330848
Epoch 13: loss 16.128780349692033
Epoch 14: loss 16.12875170731039
Epoch 15: loss 16.12872362966574
Epoch 16: loss 16.12869523464823
Epoch 17: loss 16.12866292121684
Epoch 18: loss 16.12863452230996
Epoch 19: loss 16.128602253217423
-----------Time: 0:04:53.422280, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 100, rmse: 4.017118453979492-------------


Epoch 0: loss 16.129163700578783
Epoch 1: loss 16.12913001756943
Epoch 2: loss 16.129102516848473
Epoch 3: loss 16.129076771531306
Epoch 4: loss 16.129052239439293
Epoch 5: loss 16.129021052370387
Epoch 6: loss 16.128994044044084
Epoch 7: loss 16.128952280991488
Epoch 8: loss 16.12892823221892
Epoch 9: loss 16.128899641436295
Epoch 10: loss 16.12886910000335
Epoch 11: loss 16.128834812844687
Epoch 12: loss 16.12880506665892
Epoch 13: loss 16.128784016852407
Epoch 14: loss 16.128752416213484
Epoch 15: loss 16.128724618862996
Epoch 16: loss 16.128689057545664
Epoch 17: loss 16.128667884057084
Epoch 18: loss 16.128635447980788
Epoch 19: loss 16.128611595491925
-----------Time: 0:04:42.222461, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 150, rmse: 4.017116546630859-------------


Epoch 0: loss 16.129158067469884
Epoch 1: loss 16.129134334514426
Epoch 2: loss 16.12909918261845
Epoch 3: loss 16.129072048795035
Epoch 4: loss 16.129049435460484
Epoch 5: loss 16.12901776170134
Epoch 6: loss 16.128987123293356
Epoch 7: loss 16.128959338388864
Epoch 8: loss 16.128926880013495
Epoch 9: loss 16.12889566234819
Epoch 10: loss 16.128873471140277
Epoch 11: loss 16.12883603410788
Epoch 12: loss 16.128815626047952
Epoch 13: loss 16.128777970692074
Epoch 14: loss 16.128748432198837
Epoch 15: loss 16.128718716609473
Epoch 16: loss 16.128692484861357
Epoch 17: loss 16.128666462361522
Epoch 18: loss 16.128637417559393
Epoch 19: loss 16.128602062638134
-----------Time: 0:05:03.930305, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.519911082952933e-09, embedding_dim: 200, rmse: 4.017115116119385-------------


Epoch 0: loss 16.12935895333801
Epoch 1: loss 16.129322755200633
Epoch 2: loss 16.129290323013713
Epoch 3: loss 16.129243097206768
Epoch 4: loss 16.12920989699828
Epoch 5: loss 16.129165909742778
Epoch 6: loss 16.12912373312016
Epoch 7: loss 16.129095010098847
Epoch 8: loss 16.12905778905631
Epoch 9: loss 16.12900838934902
Epoch 10: loss 16.128975532701833
Epoch 11: loss 16.128939083051655
Epoch 12: loss 16.128901370910928
Epoch 13: loss 16.128859432836535
Epoch 14: loss 16.128822107040214
Epoch 15: loss 16.128780152630455
Epoch 16: loss 16.128748893737793
Epoch 17: loss 16.12870116257162
Epoch 18: loss 16.128668541361158
Epoch 19: loss 16.128626511238267
-----------Time: 0:03:37.385254, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 20, rmse: 4.017136096954346-------------


Epoch 0: loss 16.129210852487635
Epoch 1: loss 16.12916956523503
Epoch 2: loss 16.12911837745322
Epoch 3: loss 16.129093718826933
Epoch 4: loss 16.129053794151417
Epoch 5: loss 16.129015305951082
Epoch 6: loss 16.128976639876747
Epoch 7: loss 16.128930618479046
Epoch 8: loss 16.128896521899673
Epoch 9: loss 16.128857012090638
Epoch 10: loss 16.128818850597654
Epoch 11: loss 16.128779868965502
Epoch 12: loss 16.128740811101636
Epoch 13: loss 16.128700399476593
Epoch 14: loss 16.128663407647785
Epoch 15: loss 16.128623516939463
Epoch 16: loss 16.128588743609146
Epoch 17: loss 16.128547660678286
Epoch 18: loss 16.128503712575807
Epoch 19: loss 16.128475224991217
-----------Time: 0:03:42.272341, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 50, rmse: 4.01708984375-------------


Epoch 0: loss 16.129162997380103
Epoch 1: loss 16.129126473831832
Epoch 2: loss 16.129081372659837
Epoch 3: loss 16.12904415939605
Epoch 4: loss 16.129001676550114
Epoch 5: loss 16.128960454120396
Epoch 6: loss 16.128928523921914
Epoch 7: loss 16.12888436190391
Epoch 8: loss 16.12884306142744
Epoch 9: loss 16.128810324054363
Epoch 10: loss 16.128775121855828
Epoch 11: loss 16.12872808144234
Epoch 12: loss 16.12869523620398
Epoch 13: loss 16.12865635154687
Epoch 14: loss 16.128613486764483
Epoch 15: loss 16.128582155529475
Epoch 16: loss 16.128544172947663
Epoch 17: loss 16.128498170737537
Epoch 18: loss 16.12846059628062
Epoch 19: loss 16.128415981280277
-----------Time: 0:03:47.033884, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 100, rmse: 4.017091751098633-------------


Epoch 0: loss 16.129141191479434
Epoch 1: loss 16.129113411242187
Epoch 2: loss 16.129076311288806
Epoch 3: loss 16.129022538888837
Epoch 4: loss 16.128994494692797
Epoch 5: loss 16.128956908308467
Epoch 6: loss 16.12891961647934
Epoch 7: loss 16.128872303291143
Epoch 8: loss 16.128836928663727
Epoch 9: loss 16.128805107626984
Epoch 10: loss 16.128761482083195
Epoch 11: loss 16.12871589655459
Epoch 12: loss 16.128677668164386
Epoch 13: loss 16.12864144824652
Epoch 14: loss 16.128605678458783
Epoch 15: loss 16.128568233129055
Epoch 16: loss 16.128526511303818
Epoch 17: loss 16.12848481670419
Epoch 18: loss 16.12844829445238
Epoch 19: loss 16.128407089135905
-----------Time: 0:04:49.702494, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 150, rmse: 4.017091751098633-------------


Epoch 0: loss 16.129154152167498
Epoch 1: loss 16.129114166039887
Epoch 2: loss 16.129082601183196
Epoch 3: loss 16.12904593087591
Epoch 4: loss 16.12900329012142
Epoch 5: loss 16.128961818512526
Epoch 6: loss 16.128915962802125
Epoch 7: loss 16.128881757838887
Epoch 8: loss 16.12884685071413
Epoch 9: loss 16.12880640952985
Epoch 10: loss 16.128767713377695
Epoch 11: loss 16.128732343158234
Epoch 12: loss 16.128696658936708
Epoch 13: loss 16.128653536937104
Epoch 14: loss 16.128613594629766
Epoch 15: loss 16.128573288795675
Epoch 16: loss 16.128538564990045
Epoch 17: loss 16.128500950083648
Epoch 18: loss 16.12845756957037
Epoch 19: loss 16.128423552852794
-----------Time: 0:05:59.793576, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.009233002565046e-09, embedding_dim: 200, rmse: 4.017091274261475-------------


Epoch 0: loss 16.129302065550082
Epoch 1: loss 16.12924785416952
Epoch 2: loss 16.129197476155216
Epoch 3: loss 16.129151738422472
Epoch 4: loss 16.12909388399565
Epoch 5: loss 16.129041142020295
Epoch 6: loss 16.128992189331637
Epoch 7: loss 16.12894144960562
Epoch 8: loss 16.12888622724731
Epoch 9: loss 16.12883612589709
Epoch 10: loss 16.128791057914416
Epoch 11: loss 16.128736993292872
Epoch 12: loss 16.128688958755586
Epoch 13: loss 16.128634453079115
Epoch 14: loss 16.12857612388947
Epoch 15: loss 16.128533159279836
Epoch 16: loss 16.1284727005287
Epoch 17: loss 16.12842522139391
Epoch 18: loss 16.128375614253383
Epoch 19: loss 16.12832580045749
-----------Time: 0:02:58.689766, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 20, rmse: 4.017143726348877-------------


Epoch 0: loss 16.129149796847354
Epoch 1: loss 16.129104415381196
Epoch 2: loss 16.129051385851515
Epoch 3: loss 16.128993004803558
Epoch 4: loss 16.128943941138115
Epoch 5: loss 16.12889923875652
Epoch 6: loss 16.12884543394511
Epoch 7: loss 16.128790907525318
Epoch 8: loss 16.128738712136546
Epoch 9: loss 16.12869252427367
Epoch 10: loss 16.12863770822522
Epoch 11: loss 16.12859286401115
Epoch 12: loss 16.12853356636757
Epoch 13: loss 16.128481735283426
Epoch 14: loss 16.12843695355862
Epoch 15: loss 16.128385325499757
Epoch 16: loss 16.128335890528817
Epoch 17: loss 16.128286926172038
Epoch 18: loss 16.128234248241696
Epoch 19: loss 16.12817678975307
-----------Time: 0:03:15.368965, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 50, rmse: 4.017050266265869-------------


Epoch 0: loss 16.1291558365254
Epoch 1: loss 16.1290999542701
Epoch 2: loss 16.129057247655556
Epoch 3: loss 16.128994899706196
Epoch 4: loss 16.128952893438125
Epoch 5: loss 16.128902069183066
Epoch 6: loss 16.12884315892106
Epoch 7: loss 16.128797801309723
Epoch 8: loss 16.128748699528423
Epoch 9: loss 16.128691230927426
Epoch 10: loss 16.12864430797301
Epoch 11: loss 16.12859100437216
Epoch 12: loss 16.128546202681907
Epoch 13: loss 16.12847645195883
Epoch 14: loss 16.12843663022206
Epoch 15: loss 16.128383408557337
Epoch 16: loss 16.12833273235775
Epoch 17: loss 16.128285689610642
Epoch 18: loss 16.128237794053625
Epoch 19: loss 16.128184057695183
-----------Time: 0:04:43.510360, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 100, rmse: 4.017061233520508-------------


Epoch 0: loss 16.12915021508462
Epoch 1: loss 16.12910005383805
Epoch 2: loss 16.129041461208192
Epoch 3: loss 16.12899211776717
Epoch 4: loss 16.12894411797495
Epoch 5: loss 16.12888787608227
Epoch 6: loss 16.128838375251277
Epoch 7: loss 16.128783334397053
Epoch 8: loss 16.128741902719053
Epoch 9: loss 16.128689141037544
Epoch 10: loss 16.12863154227219
Epoch 11: loss 16.12857864238828
Epoch 12: loss 16.128523771888606
Epoch 13: loss 16.12848194375383
Epoch 14: loss 16.12842825121566
Epoch 15: loss 16.12837446714757
Epoch 16: loss 16.12831996276756
Epoch 17: loss 16.12827071085645
Epoch 18: loss 16.128220903802138
Epoch 19: loss 16.128162371327917
-----------Time: 0:04:57.586481, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 150, rmse: 4.017059326171875-------------


Epoch 0: loss 16.12914659018877
Epoch 1: loss 16.129100511228344
Epoch 2: loss 16.12904910019651
Epoch 3: loss 16.128994099273182
Epoch 4: loss 16.12894370907218
Epoch 5: loss 16.12889384108435
Epoch 6: loss 16.12884332305261
Epoch 7: loss 16.12879139862351
Epoch 8: loss 16.12874187160407
Epoch 9: loss 16.128690314331802
Epoch 10: loss 16.128630783585116
Epoch 11: loss 16.12858314913469
Epoch 12: loss 16.12853509203904
Epoch 13: loss 16.128473272266564
Epoch 14: loss 16.128430485271643
Epoch 15: loss 16.12838066992
Epoch 16: loss 16.128335002455263
Epoch 17: loss 16.128276159609058
Epoch 18: loss 16.128222520484947
Epoch 19: loss 16.128173906431048
-----------Time: 0:04:53.248552, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.656087782946695e-09, embedding_dim: 200, rmse: 4.01706075668335-------------


Epoch 0: loss 16.129227940578573
Epoch 1: loss 16.129169535157214
Epoch 2: loss 16.12909469635607
Epoch 3: loss 16.129028282455934
Epoch 4: loss 16.12896104193434
Epoch 5: loss 16.12889097331982
Epoch 6: loss 16.128825935998517
Epoch 7: loss 16.128751869108314
Epoch 8: loss 16.12868760577228
Epoch 9: loss 16.128623840535315
Epoch 10: loss 16.12855280580049
Epoch 11: loss 16.128489051194475
Epoch 12: loss 16.128423025453785
Epoch 13: loss 16.128344793822727
Epoch 14: loss 16.128284806982208
Epoch 15: loss 16.128215071816623
Epoch 16: loss 16.128143923512095
Epoch 17: loss 16.128073102952087
Epoch 18: loss 16.12800786156833
Epoch 19: loss 16.12794425916646
-----------Time: 0:03:27.225218, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 20, rmse: 4.017040252685547-------------


Epoch 0: loss 16.12915351327312
Epoch 1: loss 16.12908479012245
Epoch 2: loss 16.12902754840156
Epoch 3: loss 16.128954263016087
Epoch 4: loss 16.128888328546022
Epoch 5: loss 16.128813038318295
Epoch 6: loss 16.128756307142464
Epoch 7: loss 16.128679079747577
Epoch 8: loss 16.12861402349799
Epoch 9: loss 16.128549738640757
Epoch 10: loss 16.12848448014376
Epoch 11: loss 16.128409752319403
Epoch 12: loss 16.128341830120327
Epoch 13: loss 16.12827579141506
Epoch 14: loss 16.128204526429336
Epoch 15: loss 16.128141773985156
Epoch 16: loss 16.12807430554629
Epoch 17: loss 16.12800667297587
Epoch 18: loss 16.12793380219759
Epoch 19: loss 16.127873245952827
-----------Time: 0:03:54.407887, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 50, rmse: 4.0170183181762695-------------


Epoch 0: loss 16.129159255803053
Epoch 1: loss 16.12908643895741
Epoch 2: loss 16.129016605520327
Epoch 3: loss 16.128944944337114
Epoch 4: loss 16.128880909177646
Epoch 5: loss 16.12880835110497
Epoch 6: loss 16.128745379300142
Epoch 7: loss 16.128673482680203
Epoch 8: loss 16.128616477692496
Epoch 9: loss 16.128540669659547
Epoch 10: loss 16.12847920122712
Epoch 11: loss 16.128405788529495
Epoch 12: loss 16.128341299869106
Epoch 13: loss 16.128271365567613
Epoch 14: loss 16.128199455205223
Epoch 15: loss 16.12813269696591
Epoch 16: loss 16.128066686782713
Epoch 17: loss 16.1279924606875
Epoch 18: loss 16.127930224752088
Epoch 19: loss 16.127859525799817
-----------Time: 0:03:59.301443, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 100, rmse: 4.017022132873535-------------


Epoch 0: loss 16.129145189755114
Epoch 1: loss 16.129077718464043
Epoch 2: loss 16.12900882469953
Epoch 3: loss 16.128941949001145
Epoch 4: loss 16.12887050017771
Epoch 5: loss 16.128804127764223
Epoch 6: loss 16.128736380068357
Epoch 7: loss 16.128663784398405
Epoch 8: loss 16.128600000233156
Epoch 9: loss 16.12853319661782
Epoch 10: loss 16.128464355748786
Epoch 11: loss 16.12839928990541
Epoch 12: loss 16.128328918179072
Epoch 13: loss 16.128255989319477
Epoch 14: loss 16.12819495960835
Epoch 15: loss 16.12812061994344
Epoch 16: loss 16.128051185037467
Epoch 17: loss 16.127988983847125
Epoch 18: loss 16.12792508144493
Epoch 19: loss 16.127854143425846
-----------Time: 0:04:36.469888, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 150, rmse: 4.017021179199219-------------


Epoch 0: loss 16.12915242943445
Epoch 1: loss 16.129074175245027
Epoch 2: loss 16.129010092635205
Epoch 3: loss 16.128939750986685
Epoch 4: loss 16.128872725677077
Epoch 5: loss 16.128803713157037
Epoch 6: loss 16.128733961915373
Epoch 7: loss 16.128667680253926
Epoch 8: loss 16.128602293666905
Epoch 9: loss 16.12853479022368
Epoch 10: loss 16.128467514957016
Epoch 11: loss 16.128397449713287
Epoch 12: loss 16.128328830279237
Epoch 13: loss 16.128253421814563
Epoch 14: loss 16.128189094952102
Epoch 15: loss 16.128123928244314
Epoch 16: loss 16.12805925444976
Epoch 17: loss 16.127986385486512
Epoch 18: loss 16.127920216357598
Epoch 19: loss 16.127856081370883
-----------Time: 0:05:48.238611, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.511191734215142e-09, embedding_dim: 200, rmse: 4.0170207023620605-------------


Epoch 0: loss 16.12934224847998
Epoch 1: loss 16.129252192376423
Epoch 2: loss 16.129163741546847
Epoch 3: loss 16.129076083890116
Epoch 4: loss 16.128981349648438
Epoch 5: loss 16.128893341429535
Epoch 6: loss 16.12879958030904
Epoch 7: loss 16.128707631117887
Epoch 8: loss 16.128626145118602
Epoch 9: loss 16.1285336820116
Epoch 10: loss 16.128444051405474
Epoch 11: loss 16.12835345623479
Epoch 12: loss 16.128271308523473
Epoch 13: loss 16.128175297270914
Epoch 14: loss 16.128083668304825
Epoch 15: loss 16.127997588955754
Epoch 16: loss 16.12790461685944
Epoch 17: loss 16.127815538285024
Epoch 18: loss 16.12772813240039
Epoch 19: loss 16.12763644535299
-----------Time: 0:03:17.816047, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 20, rmse: 4.016992092132568-------------


Epoch 0: loss 16.12914402060952
Epoch 1: loss 16.129054923884695
Epoch 2: loss 16.12897332016705
Epoch 3: loss 16.12887746448942
Epoch 4: loss 16.128783442003044
Epoch 5: loss 16.128689284944354
Epoch 6: loss 16.128606139479157
Epoch 7: loss 16.12851775217601
Epoch 8: loss 16.12842281050109
Epoch 9: loss 16.12833897194958
Epoch 10: loss 16.128239205636817
Epoch 11: loss 16.128150508480267
Epoch 12: loss 16.128060307432737
Epoch 13: loss 16.127978207171772
Epoch 14: loss 16.127891622463473
Epoch 15: loss 16.127793205503927
Epoch 16: loss 16.127703671354258
Epoch 17: loss 16.127615085174494
Epoch 18: loss 16.127525214464395
Epoch 19: loss 16.127433884980043
-----------Time: 0:03:03.798957, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 50, rmse: 4.016963958740234-------------


Epoch 0: loss 16.129123918254326
Epoch 1: loss 16.129032306920063
Epoch 2: loss 16.12893768443304
Epoch 3: loss 16.128848217699176
Epoch 4: loss 16.128765842848463
Epoch 5: loss 16.128673846206954
Epoch 6: loss 16.128586723727985
Epoch 7: loss 16.128500615338258
Epoch 8: loss 16.128401944791577
Epoch 9: loss 16.128316586532303
Epoch 10: loss 16.128213990311572
Epoch 11: loss 16.128133573371343
Epoch 12: loss 16.128041883471738
Epoch 13: loss 16.127956533250504
Epoch 14: loss 16.127860036085583
Epoch 15: loss 16.127779425973152
Epoch 16: loss 16.127690409110123
Epoch 17: loss 16.127597810134347
Epoch 18: loss 16.127503284622367
Epoch 19: loss 16.127417213570624
-----------Time: 0:04:30.820594, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 100, rmse: 4.016964912414551-------------


Epoch 0: loss 16.12913062405236
Epoch 1: loss 16.129038858439625
Epoch 2: loss 16.128952706748212
Epoch 3: loss 16.128870074939574
Epoch 4: loss 16.128777145107783
Epoch 5: loss 16.1286819814793
Epoch 6: loss 16.128598118295095
Epoch 7: loss 16.128502310586402
Epoch 8: loss 16.128416736337268
Epoch 9: loss 16.128324995097937
Epoch 10: loss 16.12823139214529
Epoch 11: loss 16.128142925498924
Epoch 12: loss 16.12805030681699
Epoch 13: loss 16.127962624268275
Epoch 14: loss 16.12787485200468
Epoch 15: loss 16.127783248708454
Epoch 16: loss 16.127690002800264
Epoch 17: loss 16.127614889409376
Epoch 18: loss 16.127514566138363
Epoch 19: loss 16.127425256794467
-----------Time: 0:05:26.479394, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 150, rmse: 4.016965389251709-------------


Epoch 0: loss 16.129124083682335
Epoch 1: loss 16.12903967624517
Epoch 2: loss 16.128945006048504
Epoch 3: loss 16.128851790218135
Epoch 4: loss 16.128771333087716
Epoch 5: loss 16.12868210593924
Epoch 6: loss 16.12858285587595
Epoch 7: loss 16.128497174020822
Epoch 8: loss 16.128410467704786
Epoch 9: loss 16.128322276685342
Epoch 10: loss 16.128226473125313
Epoch 11: loss 16.1281383088129
Epoch 12: loss 16.12804206445724
Epoch 13: loss 16.127959780877152
Epoch 14: loss 16.127873975080664
Epoch 15: loss 16.127781371696933
Epoch 16: loss 16.127687128812738
Epoch 17: loss 16.12760034004199
Epoch 18: loss 16.127505784970772
Epoch 19: loss 16.12742577900765
-----------Time: 0:04:45.844391, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.641588833612792e-09, embedding_dim: 200, rmse: 4.016966342926025-------------


Epoch 0: loss 16.129274836048086
Epoch 1: loss 16.12915480350787
Epoch 2: loss 16.129034456965584
Epoch 3: loss 16.128919717343752
Epoch 4: loss 16.12879806215702
Epoch 5: loss 16.12867503713304
Epoch 6: loss 16.128557433117468
Epoch 7: loss 16.128442874481134
Epoch 8: loss 16.128319831566035
Epoch 9: loss 16.12821265521939
Epoch 10: loss 16.12808466372508
Epoch 11: loss 16.12797011520112
Epoch 12: loss 16.127851195540227
Epoch 13: loss 16.127733427133816
Epoch 14: loss 16.127607367620836
Epoch 15: loss 16.127493568968035
Epoch 16: loss 16.127368792170348
Epoch 17: loss 16.127262294908267
Epoch 18: loss 16.127141037993354
Epoch 19: loss 16.127012207172303
-----------Time: 0:03:14.855495, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 20, rmse: 4.016941547393799-------------


Epoch 0: loss 16.129107749870514
Epoch 1: loss 16.12898543297175
Epoch 2: loss 16.128869099225476
Epoch 3: loss 16.12875185173578
Epoch 4: loss 16.128635037262974
Epoch 5: loss 16.128520470070022
Epoch 6: loss 16.128400077373843
Epoch 7: loss 16.128277217259285
Epoch 8: loss 16.128158088609407
Epoch 9: loss 16.128045486846393
Epoch 10: loss 16.127925934254833
Epoch 11: loss 16.127805027642804
Epoch 12: loss 16.127688301329123
Epoch 13: loss 16.1275723158114
Epoch 14: loss 16.12745508569424
Epoch 15: loss 16.12732756118401
Epoch 16: loss 16.12721404256608
Epoch 17: loss 16.12709389075175
Epoch 18: loss 16.126976667376173
Epoch 19: loss 16.126853197926476
-----------Time: 0:03:52.740445, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 50, rmse: 4.016891956329346-------------


Epoch 0: loss 16.129121138648923
Epoch 1: loss 16.12900652089412
Epoch 2: loss 16.12888990166785
Epoch 3: loss 16.128766978286155
Epoch 4: loss 16.12864332888812
Epoch 5: loss 16.128526301536954
Epoch 6: loss 16.12841427228968
Epoch 7: loss 16.128295401894185
Epoch 8: loss 16.128168695708084
Epoch 9: loss 16.128054017279055
Epoch 10: loss 16.127933229422556
Epoch 11: loss 16.12781408599306
Epoch 12: loss 16.12769246814431
Epoch 13: loss 16.127585621097932
Epoch 14: loss 16.12745405475104
Epoch 15: loss 16.127347050573984
Epoch 16: loss 16.12721969745547
Epoch 17: loss 16.127101840371346
Epoch 18: loss 16.126987444310814
Epoch 19: loss 16.126861085056806
-----------Time: 0:04:21.732833, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 100, rmse: 4.016895771026611-------------


Epoch 0: loss 16.12911870701278
Epoch 1: loss 16.12899565372602
Epoch 2: loss 16.128882452999527
Epoch 3: loss 16.128765019597793
Epoch 4: loss 16.12864929207516
Epoch 5: loss 16.128524158492304
Epoch 6: loss 16.128407733993985
Epoch 7: loss 16.128293305003428
Epoch 8: loss 16.128172496144312
Epoch 9: loss 16.12805427890423
Epoch 10: loss 16.127934509804223
Epoch 11: loss 16.127811419698062
Epoch 12: loss 16.127691384046347
Epoch 13: loss 16.127578821436355
Epoch 14: loss 16.127461889245186
Epoch 15: loss 16.127346909000796
Epoch 16: loss 16.12721996582222
Epoch 17: loss 16.12709942740419
Epoch 18: loss 16.126986027023207
Epoch 19: loss 16.12686381358177
-----------Time: 0:04:29.373655, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 150, rmse: 4.016894340515137-------------


Epoch 0: loss 16.129116958350576
Epoch 1: loss 16.128996416821046
Epoch 2: loss 16.128876190849333
Epoch 3: loss 16.128753450527473
Epoch 4: loss 16.128638909263675
Epoch 5: loss 16.128524242762058
Epoch 6: loss 16.1284049048639
Epoch 7: loss 16.128282549849278
Epoch 8: loss 16.128169169174452
Epoch 9: loss 16.128037383985493
Epoch 10: loss 16.127927659062212
Epoch 11: loss 16.127817185564233
Epoch 12: loss 16.127691239880246
Epoch 13: loss 16.127572340703388
Epoch 14: loss 16.12745239217363
Epoch 15: loss 16.127334783231518
Epoch 16: loss 16.127214190880846
Epoch 17: loss 16.127094290320027
Epoch 18: loss 16.126982405498147
Epoch 19: loss 16.126860703120354
-----------Time: 0:05:37.101909, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 6.135907273413188e-09, embedding_dim: 200, rmse: 4.016895771026611-------------


Epoch 0: loss 16.129235613274794
Epoch 1: loss 16.129077628230583
Epoch 2: loss 16.128924389258877
Epoch 3: loss 16.128770213466808
Epoch 4: loss 16.128602807582755
Epoch 5: loss 16.12845548201411
Epoch 6: loss 16.128295088669994
Epoch 7: loss 16.128140479861038
Epoch 8: loss 16.12796987743077
Epoch 9: loss 16.12782184788557
Epoch 10: loss 16.127667582378628
Epoch 11: loss 16.127514232948723
Epoch 12: loss 16.127350427846405
Epoch 13: loss 16.127191603216158
Epoch 14: loss 16.127041970730605
Epoch 15: loss 16.126886574193925
Epoch 16: loss 16.126726322682284
Epoch 17: loss 16.126571399871263
Epoch 18: loss 16.126414982244462
Epoch 19: loss 16.126252203677385
-----------Time: 0:03:38.496180, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 20, rmse: 4.016791820526123-------------


Epoch 0: loss 16.12910746646485
Epoch 1: loss 16.12894707286144
Epoch 2: loss 16.128789813833563
Epoch 3: loss 16.128635702086505
Epoch 4: loss 16.12847814150256
Epoch 5: loss 16.128321373313586
Epoch 6: loss 16.12815774608527
Epoch 7: loss 16.12800133156997
Epoch 8: loss 16.12784969761295
Epoch 9: loss 16.1276949996077
Epoch 10: loss 16.127530416112155
Epoch 11: loss 16.127383924425136
Epoch 12: loss 16.12722707404074
Epoch 13: loss 16.127064817686843
Epoch 14: loss 16.126912799719044
Epoch 15: loss 16.126758235508234
Epoch 16: loss 16.126599710359724
Epoch 17: loss 16.126435954782092
Epoch 18: loss 16.126275253658907
Epoch 19: loss 16.12612639412146
-----------Time: 0:03:01.240022, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 50, rmse: 4.016818523406982-------------


Epoch 0: loss 16.129112212537358
Epoch 1: loss 16.12895176889068
Epoch 2: loss 16.128781820393698
Epoch 3: loss 16.12863693968507
Epoch 4: loss 16.128480718860555
Epoch 5: loss 16.1283227810074
Epoch 6: loss 16.128161436322593
Epoch 7: loss 16.128008074187402
Epoch 8: loss 16.127851906517655
Epoch 9: loss 16.12768972535831
Epoch 10: loss 16.12753727229929
Epoch 11: loss 16.12737707290525
Epoch 12: loss 16.127213659592464
Epoch 13: loss 16.12706805831262
Epoch 14: loss 16.126909296171636
Epoch 15: loss 16.126751036796964
Epoch 16: loss 16.126588280788255
Epoch 17: loss 16.126438685381395
Epoch 18: loss 16.126277633955326
Epoch 19: loss 16.126124464733046
-----------Time: 0:04:18.373676, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 100, rmse: 4.016798973083496-------------


Epoch 0: loss 16.129092556941504
Epoch 1: loss 16.128939572334808
Epoch 2: loss 16.128782537594123
Epoch 3: loss 16.128627384532205
Epoch 4: loss 16.128466583581773
Epoch 5: loss 16.128315141759792
Epoch 6: loss 16.128153388950086
Epoch 7: loss 16.128001714802878
Epoch 8: loss 16.127835324304527
Epoch 9: loss 16.127685988189217
Epoch 10: loss 16.127527328986975
Epoch 11: loss 16.127371321040822
Epoch 12: loss 16.127212656912043
Epoch 13: loss 16.127058066512788
Epoch 14: loss 16.126903679138813
Epoch 15: loss 16.12674441500939
Epoch 16: loss 16.12658811406379
Epoch 17: loss 16.126421355371438
Epoch 18: loss 16.12627199617918
Epoch 19: loss 16.126112703009102
-----------Time: 0:05:26.126002, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 150, rmse: 4.016801834106445-------------


Epoch 0: loss 16.12909392081505
Epoch 1: loss 16.128935115631666
Epoch 2: loss 16.12878237449974
Epoch 3: loss 16.128628379174586
Epoch 4: loss 16.128467521698596
Epoch 5: loss 16.12830852152797
Epoch 6: loss 16.12815557166634
Epoch 7: loss 16.12798963907687
Epoch 8: loss 16.127846517661396
Epoch 9: loss 16.127687567015457
Epoch 10: loss 16.1275270582866
Epoch 11: loss 16.127371135906657
Epoch 12: loss 16.12721444109719
Epoch 13: loss 16.127056734013518
Epoch 14: loss 16.126899938598932
Epoch 15: loss 16.12674343514663
Epoch 16: loss 16.126588160476974
Epoch 17: loss 16.126434965066252
Epoch 18: loss 16.12626895054065
Epoch 19: loss 16.12611150897152
-----------Time: 0:04:57.429050, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 8.11130830789689e-09, embedding_dim: 200, rmse: 4.0167999267578125-------------


Epoch 0: loss 16.129075688729795
Epoch 1: loss 16.128871513229793
Epoch 2: loss 16.128664740147055
Epoch 3: loss 16.128462805185322
Epoch 4: loss 16.12824710132377
Epoch 5: loss 16.128037464625166
Epoch 6: loss 16.127837280659264
Epoch 7: loss 16.12762421690426
Epoch 8: loss 16.127422158260458
Epoch 9: loss 16.127206116801307
Epoch 10: loss 16.127007133095987
Epoch 11: loss 16.126798035983096
Epoch 12: loss 16.126583206971226
Epoch 13: loss 16.126378431989163
Epoch 14: loss 16.12617183911405
Epoch 15: loss 16.125971283324066
Epoch 16: loss 16.125758394590857
Epoch 17: loss 16.125546310180034
Epoch 18: loss 16.125345781874955
Epoch 19: loss 16.1251307897687
-----------Time: 0:03:07.641546, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 20, rmse: 4.0166707038879395-------------


Epoch 0: loss 16.129056758372403
Epoch 1: loss 16.12883956410303
Epoch 2: loss 16.12863956708633
Epoch 3: loss 16.128428277922687
Epoch 4: loss 16.128223876579746
Epoch 5: loss 16.128014728127127
Epoch 6: loss 16.127807603963632
Epoch 7: loss 16.127599518347072
Epoch 8: loss 16.127395538612188
Epoch 9: loss 16.12718466768581
Epoch 10: loss 16.1269754515581
Epoch 11: loss 16.126768657732036
Epoch 12: loss 16.12656311887707
Epoch 13: loss 16.126354850200677
Epoch 14: loss 16.126153915326448
Epoch 15: loss 16.12593985329898
Epoch 16: loss 16.12573492118624
Epoch 17: loss 16.125525016898756
Epoch 18: loss 16.12531481909324
Epoch 19: loss 16.125103753698202
-----------Time: 0:03:50.785384, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 50, rmse: 4.016667366027832-------------


Epoch 0: loss 16.129057980672766
Epoch 1: loss 16.12885008667266
Epoch 2: loss 16.12864272707244
Epoch 3: loss 16.12843707620352
Epoch 4: loss 16.128225709252412
Epoch 5: loss 16.128020770398088
Epoch 6: loss 16.127811631279968
Epoch 7: loss 16.127611305481587
Epoch 8: loss 16.127398069297705
Epoch 9: loss 16.127183528099454
Epoch 10: loss 16.126983642318834
Epoch 11: loss 16.12677266482363
Epoch 12: loss 16.126559689746337
Epoch 13: loss 16.12636357121263
Epoch 14: loss 16.126149086280645
Epoch 15: loss 16.12593895744668
Epoch 16: loss 16.1257405462571
Epoch 17: loss 16.125531452255704
Epoch 18: loss 16.125327697067302
Epoch 19: loss 16.125110142123383
-----------Time: 0:04:45.544637, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 100, rmse: 4.0166754722595215-------------


Epoch 0: loss 16.129076135229845
Epoch 1: loss 16.128870475804305
Epoch 2: loss 16.128655495884757
Epoch 3: loss 16.128452320731547
Epoch 4: loss 16.128245873837578
Epoch 5: loss 16.128033789686043
Epoch 6: loss 16.127827866820294
Epoch 7: loss 16.127619650780083
Epoch 8: loss 16.127416600605404
Epoch 9: loss 16.127201931317128
Epoch 10: loss 16.126996598858234
Epoch 11: loss 16.126783812545188
Epoch 12: loss 16.126587852179323
Epoch 13: loss 16.12638029836973
Epoch 14: loss 16.126167134268897
Epoch 15: loss 16.125956223930206
Epoch 16: loss 16.12575536606557
Epoch 17: loss 16.12555332427572
Epoch 18: loss 16.12533410364289
Epoch 19: loss 16.12512449805927
-----------Time: 0:04:22.211951, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 150, rmse: 4.016674995422363-------------


Epoch 0: loss 16.129067647320994
Epoch 1: loss 16.128865643906295
Epoch 2: loss 16.128655341865578
Epoch 3: loss 16.128447514244108
Epoch 4: loss 16.128235941934093
Epoch 5: loss 16.128029187261053
Epoch 6: loss 16.12782858635434
Epoch 7: loss 16.127617102981326
Epoch 8: loss 16.127409966631127
Epoch 9: loss 16.127207711185044
Epoch 10: loss 16.12699165442769
Epoch 11: loss 16.126783935967964
Epoch 12: loss 16.12657885761479
Epoch 13: loss 16.126372408387194
Epoch 14: loss 16.126166936948028
Epoch 15: loss 16.125954772156824
Epoch 16: loss 16.12575180184394
Epoch 17: loss 16.12554591294538
Epoch 18: loss 16.12533065325025
Epoch 19: loss 16.125127626411807
-----------Time: 0:05:31.427999, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.0722672220103231e-08, embedding_dim: 200, rmse: 4.016674041748047-------------


Epoch 0: loss 16.129197627840774
Epoch 1: loss 16.128924609915988
Epoch 2: loss 16.12864449570009
Epoch 3: loss 16.12836641899719
Epoch 4: loss 16.128091160274835
Epoch 5: loss 16.127824806402142
Epoch 6: loss 16.127550903256008
Epoch 7: loss 16.127270462332763
Epoch 8: loss 16.12700553689721
Epoch 9: loss 16.12672363538467
Epoch 10: loss 16.126449170872334
Epoch 11: loss 16.12617857265628
Epoch 12: loss 16.125908182132758
Epoch 13: loss 16.12563195340072
Epoch 14: loss 16.12535897177675
Epoch 15: loss 16.12507863716304
Epoch 16: loss 16.124802260116237
Epoch 17: loss 16.124534620676044
Epoch 18: loss 16.124252938005572
Epoch 19: loss 16.123986157857573
-----------Time: 0:03:53.051075, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 20, rmse: 4.016559600830078-------------


Epoch 0: loss 16.129082141199987
Epoch 1: loss 16.128819704704014
Epoch 2: loss 16.12853993845738
Epoch 3: loss 16.12826289062334
Epoch 4: loss 16.12798915238663
Epoch 5: loss 16.127710955891033
Epoch 6: loss 16.12743982975738
Epoch 7: loss 16.127165660059536
Epoch 8: loss 16.12688991127616
Epoch 9: loss 16.126615903376237
Epoch 10: loss 16.126341600921116
Epoch 11: loss 16.126066184549504
Epoch 12: loss 16.125794531276135
Epoch 13: loss 16.12552100902929
Epoch 14: loss 16.125247487560312
Epoch 15: loss 16.12496412105052
Epoch 16: loss 16.12469570295781
Epoch 17: loss 16.124426762651918
Epoch 18: loss 16.124156255447197
Epoch 19: loss 16.123876905104204
-----------Time: 0:02:58.821423, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 50, rmse: 4.016506195068359-------------


Epoch 0: loss 16.12904663225955
Epoch 1: loss 16.12876568001337
Epoch 2: loss 16.12849194514745
Epoch 3: loss 16.128220865167698
Epoch 4: loss 16.127942599181964
Epoch 5: loss 16.127668685923457
Epoch 6: loss 16.127390719938045
Epoch 7: loss 16.127117462168574
Epoch 8: loss 16.126846444937378
Epoch 9: loss 16.126572698921922
Epoch 10: loss 16.12630312101811
Epoch 11: loss 16.126027098422856
Epoch 12: loss 16.125748934338702
Epoch 13: loss 16.125480128605123
Epoch 14: loss 16.125208435400857
Epoch 15: loss 16.1249261724359
Epoch 16: loss 16.124652966265447
Epoch 17: loss 16.124374204773563
Epoch 18: loss 16.124104962393016
Epoch 19: loss 16.123824294071085
-----------Time: 0:04:09.828852, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 100, rmse: 4.016509532928467-------------


Epoch 0: loss 16.12904061384341
Epoch 1: loss 16.12876384163629
Epoch 2: loss 16.128492837888253
Epoch 3: loss 16.128214777261423
Epoch 4: loss 16.12793486866373
Epoch 5: loss 16.12767152775883
Epoch 6: loss 16.12738923238243
Epoch 7: loss 16.127116209790394
Epoch 8: loss 16.126846345888005
Epoch 9: loss 16.12656907247032
Epoch 10: loss 16.126298239854705
Epoch 11: loss 16.12602641052238
Epoch 12: loss 16.12575011411525
Epoch 13: loss 16.12548261961903
Epoch 14: loss 16.1252037951193
Epoch 15: loss 16.124918594640388
Epoch 16: loss 16.12464898691805
Epoch 17: loss 16.12438136977693
Epoch 18: loss 16.12410155893215
Epoch 19: loss 16.123831948876187
-----------Time: 0:05:22.460689, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 150, rmse: 4.016508102416992-------------


Epoch 0: loss 16.129042494225722
Epoch 1: loss 16.128768269817357
Epoch 2: loss 16.128493247568898
Epoch 3: loss 16.128218749867244
Epoch 4: loss 16.12794364671982
Epoch 5: loss 16.12767248687827
Epoch 6: loss 16.127400894538418
Epoch 7: loss 16.1271194802347
Epoch 8: loss 16.126847179510335
Epoch 9: loss 16.126575574465196
Epoch 10: loss 16.12629359335015
Epoch 11: loss 16.126026442415238
Epoch 12: loss 16.12575230279521
Epoch 13: loss 16.125477237245065
Epoch 14: loss 16.125202531591583
Epoch 15: loss 16.12492975221502
Epoch 16: loss 16.124646997373993
Epoch 17: loss 16.124379104865252
Epoch 18: loss 16.12410782393455
Epoch 19: loss 16.123831647320117
-----------Time: 0:05:10.775487, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.4174741629268047e-08, embedding_dim: 200, rmse: 4.016506671905518-------------


Epoch 0: loss 16.129132388790644
Epoch 1: loss 16.12877977121259
Epoch 2: loss 16.128406175392488
Epoch 3: loss 16.128047497911645
Epoch 4: loss 16.127687238233772
Epoch 5: loss 16.127326440266646
Epoch 6: loss 16.126967013692518
Epoch 7: loss 16.12659909194938
Epoch 8: loss 16.126236493202445
Epoch 9: loss 16.125875876739403
Epoch 10: loss 16.125519155094715
Epoch 11: loss 16.1251449460501
Epoch 12: loss 16.124792600987462
Epoch 13: loss 16.124429231885337
Epoch 14: loss 16.124065195626056
Epoch 15: loss 16.12369586800414
Epoch 16: loss 16.123329852572468
Epoch 17: loss 16.122972764030237
Epoch 18: loss 16.122614135036915
Epoch 19: loss 16.122253003102273
-----------Time: 0:03:07.387294, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 20, rmse: 4.016252517700195-------------


Epoch 0: loss 16.128997406018307
Epoch 1: loss 16.128646235546384
Epoch 2: loss 16.12828231259751
Epoch 3: loss 16.127911619287005
Epoch 4: loss 16.127551222962488
Epoch 5: loss 16.127193623356614
Epoch 6: loss 16.126826005762464
Epoch 7: loss 16.12646303778436
Epoch 8: loss 16.12610291708676
Epoch 9: loss 16.12574576216589
Epoch 10: loss 16.12537456868199
Epoch 11: loss 16.12501179413539
Epoch 12: loss 16.124652769981743
Epoch 13: loss 16.124296865105435
Epoch 14: loss 16.123932992977704
Epoch 15: loss 16.123567117044885
Epoch 16: loss 16.123205761082346
Epoch 17: loss 16.12283546552508
Epoch 18: loss 16.122483660826028
Epoch 19: loss 16.122118218428678
-----------Time: 0:03:46.093775, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 50, rmse: 4.01629638671875-------------


Epoch 0: loss 16.128982750600677
Epoch 1: loss 16.12861451485547
Epoch 2: loss 16.128248956295508
Epoch 3: loss 16.12789052721597
Epoch 4: loss 16.12753061628523
Epoch 5: loss 16.127169671299797
Epoch 6: loss 16.126801574534863
Epoch 7: loss 16.12644009126017
Epoch 8: loss 16.126077536333508
Epoch 9: loss 16.12571008131516
Epoch 10: loss 16.125352052841063
Epoch 11: loss 16.124988294283032
Epoch 12: loss 16.12462581662525
Epoch 13: loss 16.12425836523698
Epoch 14: loss 16.123903571165666
Epoch 15: loss 16.123537814247666
Epoch 16: loss 16.12317572734225
Epoch 17: loss 16.122811157979545
Epoch 18: loss 16.1224433806618
Epoch 19: loss 16.122088214766404
-----------Time: 0:04:49.917669, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 100, rmse: 4.016289710998535-------------


Epoch 0: loss 16.128987103327905
Epoch 1: loss 16.128630466471552
Epoch 2: loss 16.128265675155284
Epoch 3: loss 16.127904900005813
Epoch 4: loss 16.12754459910058
Epoch 5: loss 16.127180313921414
Epoch 6: loss 16.126813764608443
Epoch 7: loss 16.126460795949633
Epoch 8: loss 16.1260914657348
Epoch 9: loss 16.12572926266677
Epoch 10: loss 16.125367137645494
Epoch 11: loss 16.12501006517934
Epoch 12: loss 16.124640483710998
Epoch 13: loss 16.12428531755631
Epoch 14: loss 16.1239135567425
Epoch 15: loss 16.12355803717343
Epoch 16: loss 16.12319638484065
Epoch 17: loss 16.1228330717455
Epoch 18: loss 16.122475107575706
Epoch 19: loss 16.12211328333263
-----------Time: 0:04:17.109867, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 150, rmse: 4.016289234161377-------------


Epoch 0: loss 16.128996621402077
Epoch 1: loss 16.128635210210437
Epoch 2: loss 16.128272105067108
Epoch 3: loss 16.127913117732867
Epoch 4: loss 16.127549743963492
Epoch 5: loss 16.127181863707
Epoch 6: loss 16.12682651293673
Epoch 7: loss 16.12647187236601
Epoch 8: loss 16.126098095819703
Epoch 9: loss 16.125738136660736
Epoch 10: loss 16.125378645263723
Epoch 11: loss 16.125010656882655
Epoch 12: loss 16.12464813021877
Epoch 13: loss 16.124285027149774
Epoch 14: loss 16.12392682443176
Epoch 15: loss 16.123566328798578
Epoch 16: loss 16.12320394889371
Epoch 17: loss 16.122833109342775
Epoch 18: loss 16.122475291413416
Epoch 19: loss 16.12210666880522
-----------Time: 0:05:24.620500, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.8738174228603867e-08, embedding_dim: 200, rmse: 4.016290664672852-------------


Epoch 0: loss 16.129025198701544
Epoch 1: loss 16.128544790061547
Epoch 2: loss 16.128068086697784
Epoch 3: loss 16.127584063015014
Epoch 4: loss 16.127097167937634
Epoch 5: loss 16.126626859222046
Epoch 6: loss 16.126155664507237
Epoch 7: loss 16.125668097605455
Epoch 8: loss 16.12518717686465
Epoch 9: loss 16.124709683180246
Epoch 10: loss 16.124230276702086
Epoch 11: loss 16.12374835639236
Epoch 12: loss 16.123272096676434
Epoch 13: loss 16.12279903791468
Epoch 14: loss 16.12231458536369
Epoch 15: loss 16.121836571540438
Epoch 16: loss 16.121358232824875
Epoch 17: loss 16.120881311915504
Epoch 18: loss 16.120399806212966
Epoch 19: loss 16.1199217314562
-----------Time: 0:03:51.956428, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 20, rmse: 4.016017436981201-------------


Epoch 0: loss 16.12891467282667
Epoch 1: loss 16.128444109486782
Epoch 2: loss 16.12796263489923
Epoch 3: loss 16.127474766959956
Epoch 4: loss 16.12699246497307
Epoch 5: loss 16.126523900771023
Epoch 6: loss 16.126038388754765
Epoch 7: loss 16.125563234917298
Epoch 8: loss 16.125078243299175
Epoch 9: loss 16.12460217571829
Epoch 10: loss 16.124125217211642
Epoch 11: loss 16.123641226718192
Epoch 12: loss 16.123171150587122
Epoch 13: loss 16.12268715516713
Epoch 14: loss 16.122206651367136
Epoch 15: loss 16.121725134255772
Epoch 16: loss 16.121250206520532
Epoch 17: loss 16.120764242559105
Epoch 18: loss 16.120289115687957
Epoch 19: loss 16.11981259071678
-----------Time: 0:03:09.006418, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 50, rmse: 4.015993118286133-------------


Epoch 0: loss 16.128941957817055
Epoch 1: loss 16.128465123251768
Epoch 2: loss 16.127982295421624
Epoch 3: loss 16.127495908296385
Epoch 4: loss 16.12702419396131
Epoch 5: loss 16.126540331039305
Epoch 6: loss 16.12606504237023
Epoch 7: loss 16.125584693626582
Epoch 8: loss 16.125101447299876
Epoch 9: loss 16.12462884177975
Epoch 10: loss 16.124151731846837
Epoch 11: loss 16.1236642310644
Epoch 12: loss 16.1231861236369
Epoch 13: loss 16.12270376227225
Epoch 14: loss 16.12222725078423
Epoch 15: loss 16.121752832816178
Epoch 16: loss 16.12127645045535
Epoch 17: loss 16.12079438649811
Epoch 18: loss 16.1203150305818
Epoch 19: loss 16.119831711912752
-----------Time: 0:04:06.446997, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 100, rmse: 4.0160017013549805-------------


Epoch 0: loss 16.128937396619424
Epoch 1: loss 16.128459837593546
Epoch 2: loss 16.127976052199713
Epoch 3: loss 16.1274950427812
Epoch 4: loss 16.127023746683392
Epoch 5: loss 16.126536341838825
Epoch 6: loss 16.126059107964554
Epoch 7: loss 16.125575127842765
Epoch 8: loss 16.125105719906017
Epoch 9: loss 16.12461779881198
Epoch 10: loss 16.124144579548755
Epoch 11: loss 16.123665930201916
Epoch 12: loss 16.123184373418944
Epoch 13: loss 16.12270648120343
Epoch 14: loss 16.12222747533071
Epoch 15: loss 16.121742921396724
Epoch 16: loss 16.121273466268917
Epoch 17: loss 16.120792678803966
Epoch 18: loss 16.120311934640707
Epoch 19: loss 16.119836296446625
-----------Time: 0:05:14.755186, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 150, rmse: 4.0160017013549805-------------


Epoch 0: loss 16.128935908804515
Epoch 1: loss 16.128460211751253
Epoch 2: loss 16.127974283312767
Epoch 3: loss 16.127498993865817
Epoch 4: loss 16.127021739507512
Epoch 5: loss 16.126543915744968
Epoch 6: loss 16.12605734607848
Epoch 7: loss 16.125580367865673
Epoch 8: loss 16.12510465421775
Epoch 9: loss 16.124621839352184
Epoch 10: loss 16.124150302372527
Epoch 11: loss 16.12365588758094
Epoch 12: loss 16.12318881767609
Epoch 13: loss 16.122703181977762
Epoch 14: loss 16.12223255122207
Epoch 15: loss 16.121747878791847
Epoch 16: loss 16.121269528148872
Epoch 17: loss 16.120787627026722
Epoch 18: loss 16.12031203576441
Epoch 19: loss 16.11982902824522
-----------Time: 0:05:23.684662, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.4770763559917138e-08, embedding_dim: 200, rmse: 4.016000270843506-------------


Epoch 0: loss 16.128949171307944
Epoch 1: loss 16.128317121710058
Epoch 2: loss 16.127689445582728
Epoch 3: loss 16.127056004107807
Epoch 4: loss 16.126418734193173
Epoch 5: loss 16.125784842847416
Epoch 6: loss 16.125150910274304
Epoch 7: loss 16.12451512065512
Epoch 8: loss 16.12388274201626
Epoch 9: loss 16.123239480911096
Epoch 10: loss 16.122615663560595
Epoch 11: loss 16.121976540748552
Epoch 12: loss 16.121350385625444
Epoch 13: loss 16.12071546955949
Epoch 14: loss 16.120082979684554
Epoch 15: loss 16.119454214792007
Epoch 16: loss 16.118814313586736
Epoch 17: loss 16.118181794411854
Epoch 18: loss 16.11754890782085
Epoch 19: loss 16.116919719764496
-----------Time: 0:03:01.934945, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 20, rmse: 4.015622138977051-------------


Epoch 0: loss 16.12886645004372
Epoch 1: loss 16.128236269160027
Epoch 2: loss 16.12759623649394
Epoch 3: loss 16.12695716450304
Epoch 4: loss 16.126322428385418
Epoch 5: loss 16.12569135813171
Epoch 6: loss 16.125062146479827
Epoch 7: loss 16.12443075911256
Epoch 8: loss 16.123798374769283
Epoch 9: loss 16.123153757050737
Epoch 10: loss 16.122526899766118
Epoch 11: loss 16.121890221036214
Epoch 12: loss 16.121262688037817
Epoch 13: loss 16.12063111346205
Epoch 14: loss 16.11999815919595
Epoch 15: loss 16.11936855264304
Epoch 16: loss 16.11873543969051
Epoch 17: loss 16.118096311174053
Epoch 18: loss 16.117463371946865
Epoch 19: loss 16.11682565708793
-----------Time: 0:03:41.510899, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 50, rmse: 4.015614032745361-------------


Epoch 0: loss 16.128864803283093
Epoch 1: loss 16.128228813750127
Epoch 2: loss 16.127597920073963
Epoch 3: loss 16.126964585686455
Epoch 4: loss 16.12633086495454
Epoch 5: loss 16.125680981802514
Epoch 6: loss 16.125061713981523
Epoch 7: loss 16.124428404745295
Epoch 8: loss 16.123790679255407
Epoch 9: loss 16.12317115369862
Epoch 10: loss 16.122525476514806
Epoch 11: loss 16.121892816026033
Epoch 12: loss 16.121267485968634
Epoch 13: loss 16.12063481536749
Epoch 14: loss 16.119995463601008
Epoch 15: loss 16.119366432675335
Epoch 16: loss 16.118727050831033
Epoch 17: loss 16.118099641255416
Epoch 18: loss 16.11746151749371
Epoch 19: loss 16.116831188035956
-----------Time: 0:05:03.467728, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 100, rmse: 4.015617847442627-------------


Epoch 0: loss 16.1288663678483
Epoch 1: loss 16.12823063605113
Epoch 2: loss 16.127597123271034
Epoch 3: loss 16.12696784057327
Epoch 4: loss 16.126328025193498
Epoch 5: loss 16.12569282753692
Epoch 6: loss 16.125065567313232
Epoch 7: loss 16.124422578723486
Epoch 8: loss 16.12379554123124
Epoch 9: loss 16.123159742018267
Epoch 10: loss 16.122524488873296
Epoch 11: loss 16.12189768396557
Epoch 12: loss 16.121260547845374
Epoch 13: loss 16.12062406902925
Epoch 14: loss 16.119995046400906
Epoch 15: loss 16.119361540362394
Epoch 16: loss 16.11873228540882
Epoch 17: loss 16.118096691036172
Epoch 18: loss 16.117461544200737
Epoch 19: loss 16.11682555829785
-----------Time: 0:04:15.515412, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 150, rmse: 4.015618324279785-------------


Epoch 0: loss 16.128864493429692
Epoch 1: loss 16.12823269197382
Epoch 2: loss 16.127595135282732
Epoch 3: loss 16.12696343235765
Epoch 4: loss 16.12632697947068
Epoch 5: loss 16.125689500567056
Epoch 6: loss 16.125056276119164
Epoch 7: loss 16.124427465332015
Epoch 8: loss 16.1237940298208
Epoch 9: loss 16.123155704848852
Epoch 10: loss 16.12252783088207
Epoch 11: loss 16.121888075398648
Epoch 12: loss 16.121258805146873
Epoch 13: loss 16.120622092449775
Epoch 14: loss 16.119989253568416
Epoch 15: loss 16.1193632428707
Epoch 16: loss 16.118724520145516
Epoch 17: loss 16.118087309090065
Epoch 18: loss 16.11745922405996
Epoch 19: loss 16.11682272372264
-----------Time: 0:05:17.058054, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.274549162877732e-08, embedding_dim: 200, rmse: 4.015619277954102-------------


Epoch 0: loss 16.128751400568486
Epoch 1: loss 16.127921663723022
Epoch 2: loss 16.127089256174607
Epoch 3: loss 16.126244939106066
Epoch 4: loss 16.125411272178596
Epoch 5: loss 16.124575751316552
Epoch 6: loss 16.12373675283626
Epoch 7: loss 16.122898161702988
Epoch 8: loss 16.122066730646544
Epoch 9: loss 16.121221908737358
Epoch 10: loss 16.120384776896923
Epoch 11: loss 16.11955209994459
Epoch 12: loss 16.118714737853256
Epoch 13: loss 16.11787156192682
Epoch 14: loss 16.117038253858855
Epoch 15: loss 16.11620074500851
Epoch 16: loss 16.11536220780788
Epoch 17: loss 16.11452380932823
Epoch 18: loss 16.113692512066226
Epoch 19: loss 16.11285316717242
-----------Time: 0:03:52.642229, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 20, rmse: 4.015158653259277-------------


Epoch 0: loss 16.128767922107393
Epoch 1: loss 16.1279262726303
Epoch 2: loss 16.12708716213606
Epoch 3: loss 16.126252774378084
Epoch 4: loss 16.12540976232391
Epoch 5: loss 16.12457432417587
Epoch 6: loss 16.1237437407435
Epoch 7: loss 16.122896838278923
Epoch 8: loss 16.122061925714856
Epoch 9: loss 16.121223055065293
Epoch 10: loss 16.12038461587687
Epoch 11: loss 16.119543611257864
Epoch 12: loss 16.118717010025097
Epoch 13: loss 16.11788211431498
Epoch 14: loss 16.11704202603231
Epoch 15: loss 16.11619659141719
Epoch 16: loss 16.115360213077995
Epoch 17: loss 16.11452215141807
Epoch 18: loss 16.113696452779184
Epoch 19: loss 16.11285324003335
-----------Time: 0:03:18.881585, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 50, rmse: 4.015121936798096-------------


Epoch 0: loss 16.12875493419371
Epoch 1: loss 16.127914474864834
Epoch 2: loss 16.12707837293043
Epoch 3: loss 16.12624808249751
Epoch 4: loss 16.125400207689626
Epoch 5: loss 16.12457236704326
Epoch 6: loss 16.123731498293022
Epoch 7: loss 16.122891166017002
Epoch 8: loss 16.12205732925357
Epoch 9: loss 16.12121765142942
Epoch 10: loss 16.12037836254259
Epoch 11: loss 16.119542060953695
Epoch 12: loss 16.11870814899571
Epoch 13: loss 16.11786892467248
Epoch 14: loss 16.11702534295477
Epoch 15: loss 16.11619827396005
Epoch 16: loss 16.11535822197819
Epoch 17: loss 16.114516160746124
Epoch 18: loss 16.113669505127103
Epoch 19: loss 16.11284575013767
-----------Time: 0:04:03.044724, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 100, rmse: 4.015115737915039-------------


Epoch 0: loss 16.128761473267282
Epoch 1: loss 16.127926795880647
Epoch 2: loss 16.127093058425878
Epoch 3: loss 16.12625614490893
Epoch 4: loss 16.125413828533976
Epoch 5: loss 16.124580392635277
Epoch 6: loss 16.123740271163285
Epoch 7: loss 16.122909665950424
Epoch 8: loss 16.122061115169473
Epoch 9: loss 16.121227257662717
Epoch 10: loss 16.120390147862064
Epoch 11: loss 16.11955329061138
Epoch 12: loss 16.118719947279764
Epoch 13: loss 16.11787181136529
Epoch 14: loss 16.117045125603482
Epoch 15: loss 16.11620746506758
Epoch 16: loss 16.115367763129314
Epoch 17: loss 16.114532947540283
Epoch 18: loss 16.113690749142986
Epoch 19: loss 16.11285109621082
-----------Time: 0:05:10.492841, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 150, rmse: 4.01511287689209-------------


Epoch 0: loss 16.128752398063074
Epoch 1: loss 16.127926719908224
Epoch 2: loss 16.127084430499593
Epoch 3: loss 16.126247473940246
Epoch 4: loss 16.125408680818857
Epoch 5: loss 16.124572084934055
Epoch 6: loss 16.123731048681478
Epoch 7: loss 16.122897230068453
Epoch 8: loss 16.12205044195145
Epoch 9: loss 16.12121962904606
Epoch 10: loss 16.120381377066135
Epoch 11: loss 16.119542415664533
Epoch 12: loss 16.118700236973392
Epoch 13: loss 16.117862720085007
Epoch 14: loss 16.117035292490076
Epoch 15: loss 16.116190870408456
Epoch 16: loss 16.115361885249193
Epoch 17: loss 16.114520461874335
Epoch 18: loss 16.113685442482147
Epoch 19: loss 16.11284749983704
-----------Time: 0:05:34.957294, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.328761281083061e-08, embedding_dim: 200, rmse: 4.015114784240723-------------


Epoch 0: loss 16.128700084178067
Epoch 1: loss 16.127585476153957
Epoch 2: loss 16.126483477996445
Epoch 3: loss 16.12537372442871
Epoch 4: loss 16.12426562228885
Epoch 5: loss 16.123163903906917
Epoch 6: loss 16.122054929250993
Epoch 7: loss 16.120955254086468
Epoch 8: loss 16.119841005699733
Epoch 9: loss 16.11873813632262
Epoch 10: loss 16.117626717065974
Epoch 11: loss 16.116523602658344
Epoch 12: loss 16.115416898877807
Epoch 13: loss 16.1143092797981
Epoch 14: loss 16.113202252681
Epoch 15: loss 16.112093117264315
Epoch 16: loss 16.110993634753413
Epoch 17: loss 16.109878713505108
Epoch 18: loss 16.10877567429203
Epoch 19: loss 16.107666827985422
-----------Time: 0:02:57.306796, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 20, rmse: 4.0144829750061035-------------


Epoch 0: loss 16.128656182229488
Epoch 1: loss 16.127555637660397
Epoch 2: loss 16.126452864480445
Epoch 3: loss 16.12533992784966
Epoch 4: loss 16.124234141701912
Epoch 5: loss 16.123120354594842
Epoch 6: loss 16.12202539464713
Epoch 7: loss 16.120909250839176
Epoch 8: loss 16.119801322943236
Epoch 9: loss 16.118696643192525
Epoch 10: loss 16.117591755749284
Epoch 11: loss 16.11648172059093
Epoch 12: loss 16.115374231934872
Epoch 13: loss 16.11427311096067
Epoch 14: loss 16.113157935865942
Epoch 15: loss 16.112057340475708
Epoch 16: loss 16.110941226745574
Epoch 17: loss 16.109840506376813
Epoch 18: loss 16.108735100091184
Epoch 19: loss 16.107628026560896
-----------Time: 0:03:32.986501, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 50, rmse: 4.014441967010498-------------


Epoch 0: loss 16.12862139360097
Epoch 1: loss 16.127516107367327
Epoch 2: loss 16.12640950082112
Epoch 3: loss 16.125307227036693
Epoch 4: loss 16.12419794278666
Epoch 5: loss 16.123082075124206
Epoch 6: loss 16.12198223790246
Epoch 7: loss 16.120870294358305
Epoch 8: loss 16.119770138467246
Epoch 9: loss 16.11866592492274
Epoch 10: loss 16.117556329782143
Epoch 11: loss 16.116450928941635
Epoch 12: loss 16.115335358945877
Epoch 13: loss 16.114226817566134
Epoch 14: loss 16.113132665570888
Epoch 15: loss 16.112020042682875
Epoch 16: loss 16.11091570364126
Epoch 17: loss 16.109804331316386
Epoch 18: loss 16.108697741105544
Epoch 19: loss 16.10759901702242
-----------Time: 0:05:04.524963, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 100, rmse: 4.0144453048706055-------------


Epoch 0: loss 16.128625991618005
Epoch 1: loss 16.12752913936057
Epoch 2: loss 16.12641658077686
Epoch 3: loss 16.12530816593139
Epoch 4: loss 16.124198179779135
Epoch 5: loss 16.123093099163693
Epoch 6: loss 16.121989256406103
Epoch 7: loss 16.12087925884502
Epoch 8: loss 16.119774542534202
Epoch 9: loss 16.118662727598654
Epoch 10: loss 16.117557468849917
Epoch 11: loss 16.116447748730792
Epoch 12: loss 16.115345604592136
Epoch 13: loss 16.114231170552653
Epoch 14: loss 16.11313089305386
Epoch 15: loss 16.11202613603427
Epoch 16: loss 16.110919079098643
Epoch 17: loss 16.109810854054587
Epoch 18: loss 16.10870524266934
Epoch 19: loss 16.10759656282791
-----------Time: 0:04:27.497387, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 150, rmse: 4.014444351196289-------------


Epoch 0: loss 16.128623519273088
Epoch 1: loss 16.127518883083358
Epoch 2: loss 16.126409303500253
Epoch 3: loss 16.1253066654112
Epoch 4: loss 16.12419844814589
Epoch 5: loss 16.123085379794993
Epoch 6: loss 16.121984981466337
Epoch 7: loss 16.12087500957512
Epoch 8: loss 16.119769694819407
Epoch 9: loss 16.118661255859823
Epoch 10: loss 16.117560875422285
Epoch 11: loss 16.116445376990995
Epoch 12: loss 16.115347550056626
Epoch 13: loss 16.114237281276584
Epoch 14: loss 16.113125923212746
Epoch 15: loss 16.112026622724507
Epoch 16: loss 16.110918512028025
Epoch 17: loss 16.109805575656527
Epoch 18: loss 16.108698543353597
Epoch 19: loss 16.107593364466656
-----------Time: 0:05:09.301571, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.72236765935022e-08, embedding_dim: 200, rmse: 4.014445781707764-------------


Epoch 0: loss 16.128568713077712
Epoch 1: loss 16.127104435879748
Epoch 2: loss 16.12563674536783
Epoch 3: loss 16.1241752407744
Epoch 4: loss 16.12271422779774
Epoch 5: loss 16.121252517326816
Epoch 6: loss 16.119797368487824
Epoch 7: loss 16.11832678661585
Epoch 8: loss 16.1168600544455
Epoch 9: loss 16.115390746991487
Epoch 10: loss 16.113935245256698
Epoch 11: loss 16.112469160537348
Epoch 12: loss 16.11101348974447
Epoch 13: loss 16.109542590499636
Epoch 14: loss 16.10808591339626
Epoch 15: loss 16.10662393974441
Epoch 16: loss 16.10516604163713
Epoch 17: loss 16.10369846884358
Epoch 18: loss 16.102233463814233
Epoch 19: loss 16.10077290537566
-----------Time: 0:03:44.769819, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 20, rmse: 4.013615131378174-------------


Epoch 0: loss 16.128488684556224
Epoch 1: loss 16.12703297642536
Epoch 2: loss 16.125564375281527
Epoch 3: loss 16.124098881746907
Epoch 4: loss 16.122642610434802
Epoch 5: loss 16.121178803850995
Epoch 6: loss 16.119714000291186
Epoch 7: loss 16.11825176034692
Epoch 8: loss 16.11678168694567
Epoch 9: loss 16.11532178684834
Epoch 10: loss 16.113857465052227
Epoch 11: loss 16.112395419835913
Epoch 12: loss 16.11092929285204
Epoch 13: loss 16.10947540805935
Epoch 14: loss 16.108012665867353
Epoch 15: loss 16.106548120302634
Epoch 16: loss 16.10508353065835
Epoch 17: loss 16.103620543695133
Epoch 18: loss 16.102160323113445
Epoch 19: loss 16.100689860774875
-----------Time: 0:03:29.800675, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 50, rmse: 4.013571739196777-------------


Epoch 0: loss 16.128459785735238
Epoch 1: loss 16.126995991597425
Epoch 2: loss 16.125530955193806
Epoch 3: loss 16.124070356824333
Epoch 4: loss 16.122611849641203
Epoch 5: loss 16.121139282114548
Epoch 6: loss 16.11967270240763
Epoch 7: loss 16.118213189951163
Epoch 8: loss 16.116754674729997
Epoch 9: loss 16.11529258750845
Epoch 10: loss 16.113819258701806
Epoch 11: loss 16.112358794126774
Epoch 12: loss 16.110900605353667
Epoch 13: loss 16.10942651889712
Epoch 14: loss 16.107970077230465
Epoch 15: loss 16.106509596579357
Epoch 16: loss 16.105051414547827
Epoch 17: loss 16.103591316870336
Epoch 18: loss 16.102126235868568
Epoch 19: loss 16.100659767397726
-----------Time: 0:03:53.282310, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 100, rmse: 4.0135626792907715-------------


Epoch 0: loss 16.128454280197783
Epoch 1: loss 16.12699022624984
Epoch 2: loss 16.125528922866646
Epoch 3: loss 16.124064092081223
Epoch 4: loss 16.122601698377068
Epoch 5: loss 16.121133187207402
Epoch 6: loss 16.119675592471232
Epoch 7: loss 16.118206380177217
Epoch 8: loss 16.116747664782974
Epoch 9: loss 16.11528704618876
Epoch 10: loss 16.11382077270516
Epoch 11: loss 16.112353876662553
Epoch 12: loss 16.11089204043523
Epoch 13: loss 16.109428946643888
Epoch 14: loss 16.107968105577527
Epoch 15: loss 16.10650659709472
Epoch 16: loss 16.105046683514228
Epoch 17: loss 16.10358286992955
Epoch 18: loss 16.102112704998387
Epoch 19: loss 16.100663530755263
-----------Time: 0:05:01.692344, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 150, rmse: 4.013558864593506-------------


Epoch 0: loss 16.128454791261426
Epoch 1: loss 16.126986667473332
Epoch 2: loss 16.125514069609565
Epoch 3: loss 16.124062795104898
Epoch 4: loss 16.12259171254094
Epoch 5: loss 16.121136516251596
Epoch 6: loss 16.11966932591308
Epoch 7: loss 16.118203936872955
Epoch 8: loss 16.11674106840597
Epoch 9: loss 16.11528146234526
Epoch 10: loss 16.113807114263537
Epoch 11: loss 16.112359557999678
Epoch 12: loss 16.11089447155279
Epoch 13: loss 16.10942446660451
Epoch 14: loss 16.107965075496487
Epoch 15: loss 16.106499450760342
Epoch 16: loss 16.105033740198696
Epoch 17: loss 16.103578249354154
Epoch 18: loss 16.102110898514166
Epoch 19: loss 16.100651647682874
-----------Time: 0:05:55.263737, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.56463327554629e-08, embedding_dim: 200, rmse: 4.013561725616455-------------


Epoch 0: loss 16.128354883375472
Epoch 1: loss 16.12641797498752
Epoch 2: loss 16.124478195982825
Epoch 3: loss 16.122551263577915
Epoch 4: loss 16.12062522209877
Epoch 5: loss 16.118687926847823
Epoch 6: loss 16.116750452167125
Epoch 7: loss 16.114807676011417
Epoch 8: loss 16.112886332635842
Epoch 9: loss 16.110950411889394
Epoch 10: loss 16.109013411712233
Epoch 11: loss 16.10708219970031
Epoch 12: loss 16.105152270662973
Epoch 13: loss 16.10321772312457
Epoch 14: loss 16.101280528738034
Epoch 15: loss 16.09935005596632
Epoch 16: loss 16.09741585458213
Epoch 17: loss 16.095487983541815
Epoch 18: loss 16.093551334445408
Epoch 19: loss 16.091616772905258
-----------Time: 0:02:52.905721, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012434959411621-------------


Epoch 0: loss 16.128236330871417
Epoch 1: loss 16.126298728100693
Epoch 2: loss 16.124366537003883
Epoch 3: loss 16.12243531799109
Epoch 4: loss 16.12050078626947
Epoch 5: loss 16.118555522470647
Epoch 6: loss 16.116626643823373
Epoch 7: loss 16.114694182026184
Epoch 8: loss 16.112759308558303
Epoch 9: loss 16.1108289765819
Epoch 10: loss 16.108893362318064
Epoch 11: loss 16.106957203801265
Epoch 12: loss 16.105025734053545
Epoch 13: loss 16.10308792229383
Epoch 14: loss 16.10115884476994
Epoch 15: loss 16.09922709550593
Epoch 16: loss 16.097295368281703
Epoch 17: loss 16.09535339307759
Epoch 18: loss 16.093424409935825
Epoch 19: loss 16.091502198970726
-----------Time: 0:03:25.097647, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012394428253174-------------


Epoch 0: loss 16.12821154234006
Epoch 1: loss 16.126289155316
Epoch 2: loss 16.12435077596709
Epoch 3: loss 16.122410140522412
Epoch 4: loss 16.120479555477456
Epoch 5: loss 16.118544234990942
Epoch 6: loss 16.116609354003607
Epoch 7: loss 16.11467745927704
Epoch 8: loss 16.112747411484172
Epoch 9: loss 16.110806816748266
Epoch 10: loss 16.108878613555476
Epoch 11: loss 16.106936475516274
Epoch 12: loss 16.10500643498357
Epoch 13: loss 16.103063682423397
Epoch 14: loss 16.10114344051832
Epoch 15: loss 16.09920017326442
Epoch 16: loss 16.097267297637924
Epoch 17: loss 16.095341813376315
Epoch 18: loss 16.09341036333475
Epoch 19: loss 16.091476876817364
-----------Time: 0:04:58.067641, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.01239013671875-------------


Epoch 0: loss 16.128216948828143
Epoch 1: loss 16.126283209501498
Epoch 2: loss 16.12434667267833
Epoch 3: loss 16.12241591935316
Epoch 4: loss 16.120478433782214
Epoch 5: loss 16.11854791096723
Epoch 6: loss 16.116612348561706
Epoch 7: loss 16.114680182875468
Epoch 8: loss 16.112743455991595
Epoch 9: loss 16.110810030667015
Epoch 10: loss 16.10887550853918
Epoch 11: loss 16.106946750721768
Epoch 12: loss 16.105013512346392
Epoch 13: loss 16.103083142772714
Epoch 14: loss 16.101143491339464
Epoch 15: loss 16.099210893414213
Epoch 16: loss 16.097283813990998
Epoch 17: loss 16.095343693240043
Epoch 18: loss 16.09341478814503
Epoch 19: loss 16.09147271596588
-----------Time: 0:04:41.645479, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012392520904541-------------


Epoch 0: loss 16.128216942345855
Epoch 1: loss 16.12628425807652
Epoch 2: loss 16.12435080448916
Epoch 3: loss 16.12240804933607
Epoch 4: loss 16.12047440828092
Epoch 5: loss 16.118543920211003
Epoch 6: loss 16.11661031390092
Epoch 7: loss 16.11467549566214
Epoch 8: loss 16.11273788018613
Epoch 9: loss 16.110809560052854
Epoch 10: loss 16.1088769468294
Epoch 11: loss 16.10693402391468
Epoch 12: loss 16.105004258231016
Epoch 13: loss 16.103079989009608
Epoch 14: loss 16.10113531198756
Epoch 15: loss 16.099208615019375
Epoch 16: loss 16.09727259263064
Epoch 17: loss 16.09533996773907
Epoch 18: loss 16.093407951404764
Epoch 19: loss 16.091467262546026
-----------Time: 0:05:04.208734, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012392044067383-------------


Epoch 0: loss 16.127993618164975
Epoch 1: loss 16.12543885302064
Epoch 2: loss 16.122891588402936
Epoch 3: loss 16.120318065589377
Epoch 4: loss 16.117764071578108
Epoch 5: loss 16.11520965440303
Epoch 6: loss 16.11265485347646
Epoch 7: loss 16.110090693162146
Epoch 8: loss 16.107543035458455
Epoch 9: loss 16.104984579817504
Epoch 10: loss 16.1024291081037
Epoch 11: loss 16.099873600866953
Epoch 12: loss 16.09731631826097
Epoch 13: loss 16.09476495113238
Epoch 14: loss 16.092209990482218
Epoch 15: loss 16.08964929248795
Epoch 16: loss 16.087092890436068
Epoch 17: loss 16.084547321585095
Epoch 18: loss 16.08198867458698
Epoch 19: loss 16.079435696739292
-----------Time: 0:03:44.837866, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 20, rmse: 4.010844707489014-------------


Epoch 0: loss 16.127923219213024
Epoch 1: loss 16.12536726262402
Epoch 2: loss 16.122812070167214
Epoch 3: loss 16.12025568263566
Epoch 4: loss 16.1177003843879
Epoch 5: loss 16.115139525632873
Epoch 6: loss 16.112581240865055
Epoch 7: loss 16.110026580215212
Epoch 8: loss 16.107476748351885
Epoch 9: loss 16.1049136210551
Epoch 10: loss 16.102359584260725
Epoch 11: loss 16.099802522830434
Epoch 12: loss 16.097244211096296
Epoch 13: loss 16.094691117604572
Epoch 14: loss 16.092144818070015
Epoch 15: loss 16.089587596656838
Epoch 16: loss 16.08703090445771
Epoch 17: loss 16.084475404740417
Epoch 18: loss 16.081921606234975
Epoch 19: loss 16.079362924751088
-----------Time: 0:03:41.980050, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 50, rmse: 4.010850429534912-------------


Epoch 0: loss 16.12790410294359
Epoch 1: loss 16.125345353266024
Epoch 2: loss 16.12278758345122
Epoch 3: loss 16.120227996780535
Epoch 4: loss 16.117675271223522
Epoch 5: loss 16.115114612900868
Epoch 6: loss 16.11256766565602
Epoch 7: loss 16.110004664634218
Epoch 8: loss 16.107452528965478
Epoch 9: loss 16.104895838840687
Epoch 10: loss 16.10233394214159
Epoch 11: loss 16.0997833943743
Epoch 12: loss 16.097223816260232
Epoch 13: loss 16.094665863385597
Epoch 14: loss 16.092115579836392
Epoch 15: loss 16.089561102246382
Epoch 16: loss 16.086997122139696
Epoch 17: loss 16.084447318798436
Epoch 18: loss 16.08189625659671
Epoch 19: loss 16.079345297074436
-----------Time: 0:03:48.222846, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 100, rmse: 4.010851860046387-------------


Epoch 0: loss 16.127908972179586
Epoch 1: loss 16.1253542944165
Epoch 2: loss 16.122794920624177
Epoch 3: loss 16.12023570525899
Epoch 4: loss 16.11768580623915
Epoch 5: loss 16.11511696778671
Epoch 6: loss 16.112561609642604
Epoch 7: loss 16.110007080712872
Epoch 8: loss 16.107445093521026
Epoch 9: loss 16.10489784549798
Epoch 10: loss 16.10234165347225
Epoch 11: loss 16.099785126441844
Epoch 12: loss 16.097234320160876
Epoch 13: loss 16.094669768316322
Epoch 14: loss 16.092116886665796
Epoch 15: loss 16.089571574254162
Epoch 16: loss 16.087009790606185
Epoch 17: loss 16.08445419650677
Epoch 18: loss 16.081904341566492
Epoch 19: loss 16.079339732159216
-----------Time: 0:04:51.235297, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 150, rmse: 4.010849952697754-------------


Epoch 0: loss 16.127904659383255
Epoch 1: loss 16.12534213597649
Epoch 2: loss 16.12278853090254
Epoch 3: loss 16.120229963506933
Epoch 4: loss 16.117670997320925
Epoch 5: loss 16.115113074783398
Epoch 6: loss 16.112562877318986
Epoch 7: loss 16.109995980182376
Epoch 8: loss 16.107448601217097
Epoch 9: loss 16.104882267780315
Epoch 10: loss 16.102327654062243
Epoch 11: loss 16.0997775496835
Epoch 12: loss 16.0972204423586
Epoch 13: loss 16.09466574177786
Epoch 14: loss 16.09211344638553
Epoch 15: loss 16.08955169281537
Epoch 16: loss 16.086995940548107
Epoch 17: loss 16.084440286293052
Epoch 18: loss 16.081884483204647
Epoch 19: loss 16.07932525772709
-----------Time: 0:05:57.651469, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.3219411484660288e-07, embedding_dim: 200, rmse: 4.010848522186279-------------


Epoch 0: loss 16.127644483391002
Epoch 1: loss 16.12426463127655
Epoch 2: loss 16.120891277786182
Epoch 3: loss 16.117506158941787
Epoch 4: loss 16.11412514779411
Epoch 5: loss 16.11074525860096
Epoch 6: loss 16.107364587125215
Epoch 7: loss 16.10399459949844
Epoch 8: loss 16.100613414625425
Epoch 9: loss 16.097242948865034
Epoch 10: loss 16.09386785812129
Epoch 11: loss 16.09048568975543
Epoch 12: loss 16.087107866597343
Epoch 13: loss 16.083734355976297
Epoch 14: loss 16.08035932331386
Epoch 15: loss 16.076989995324787
Epoch 16: loss 16.073616824116378
Epoch 17: loss 16.07024223354603
Epoch 18: loss 16.06686543925681
Epoch 19: loss 16.06349091013856
-----------Time: 0:02:55.857502, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 20, rmse: 4.008809566497803-------------


Epoch 0: loss 16.127497930251884
Epoch 1: loss 16.1241190914488
Epoch 2: loss 16.12073947736399
Epoch 3: loss 16.117364776335442
Epoch 4: loss 16.113984564065024
Epoch 5: loss 16.11060636078553
Epoch 6: loss 16.107224204865663
Epoch 7: loss 16.103840774787127
Epoch 8: loss 16.100465366411235
Epoch 9: loss 16.097086409111913
Epoch 10: loss 16.093713289502524
Epoch 11: loss 16.090336584150304
Epoch 12: loss 16.08695673540664
Epoch 13: loss 16.083577867303614
Epoch 14: loss 16.080209092383413
Epoch 15: loss 16.076835834312334
Epoch 16: loss 16.073459200524585
Epoch 17: loss 16.070081487046824
Epoch 18: loss 16.066710024051137
Epoch 19: loss 16.06333167323475
-----------Time: 0:03:17.597849, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 50, rmse: 4.008815765380859-------------


Epoch 0: loss 16.127489063518084
Epoch 1: loss 16.124108777608868
Epoch 2: loss 16.120725535775993
Epoch 3: loss 16.117348001727986
Epoch 4: loss 16.113975057658973
Epoch 5: loss 16.110587386607865
Epoch 6: loss 16.107211894740093
Epoch 7: loss 16.103840558019392
Epoch 8: loss 16.100458001753374
Epoch 9: loss 16.097074090948308
Epoch 10: loss 16.093695697867396
Epoch 11: loss 16.090318586723903
Epoch 12: loss 16.086942874199025
Epoch 13: loss 16.083575576203486
Epoch 14: loss 16.08019272019644
Epoch 15: loss 16.076815418214366
Epoch 16: loss 16.07344583949037
Epoch 17: loss 16.070070272428048
Epoch 18: loss 16.06669330648782
Epoch 19: loss 16.063320842367467
-----------Time: 0:04:43.377192, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 100, rmse: 4.0088090896606445-------------


Epoch 0: loss 16.127498951342
Epoch 1: loss 16.124116588766775
Epoch 2: loss 16.120736814439784
Epoch 3: loss 16.117365393190035
Epoch 4: loss 16.113984767868182
Epoch 5: loss 16.110607463033904
Epoch 6: loss 16.10722367668878
Epoch 7: loss 16.103847836592458
Epoch 8: loss 16.10047115846585
Epoch 9: loss 16.097092792869844
Epoch 10: loss 16.093713893911126
Epoch 11: loss 16.090331583194203
Epoch 12: loss 16.08695946963602
Epoch 13: loss 16.08357522019619
Epoch 14: loss 16.08020721692764
Epoch 15: loss 16.076823301714615
Epoch 16: loss 16.0734532663782
Epoch 17: loss 16.070075667507304
Epoch 18: loss 16.066697875723495
Epoch 19: loss 16.063332674100128
-----------Time: 0:05:01.237876, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 150, rmse: 4.0088090896606445-------------


Epoch 0: loss 16.127491454186163
Epoch 1: loss 16.124112559894687
Epoch 2: loss 16.120728627827713
Epoch 3: loss 16.117352587039733
Epoch 4: loss 16.113967434228147
Epoch 5: loss 16.11059831626523
Epoch 6: loss 16.10720932075289
Epoch 7: loss 16.103836612379894
Epoch 8: loss 16.100460407201073
Epoch 9: loss 16.097087315335877
Epoch 10: loss 16.093705703409682
Epoch 11: loss 16.090323345501698
Epoch 12: loss 16.086951723819585
Epoch 13: loss 16.083567104630006
Epoch 14: loss 16.080193546299316
Epoch 15: loss 16.07681961095871
Epoch 16: loss 16.073446771902777
Epoch 17: loss 16.070067739668197
Epoch 18: loss 16.066692472346908
Epoch 19: loss 16.063317254550306
-----------Time: 0:04:54.112949, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.747528400007683e-07, embedding_dim: 200, rmse: 4.0088090896606445-------------


Epoch 0: loss 16.12696992683307
Epoch 1: loss 16.1224906907385
Epoch 2: loss 16.118023480067375
Epoch 3: loss 16.11356031071433
Epoch 4: loss 16.109094566077626
Epoch 5: loss 16.104629878572563
Epoch 6: loss 16.100161853207396
Epoch 7: loss 16.095697888347882
Epoch 8: loss 16.09123489660955
Epoch 9: loss 16.08676753736437
Epoch 10: loss 16.08230649835069
Epoch 11: loss 16.07783659104746
Epoch 12: loss 16.073382635878893
Epoch 13: loss 16.068919728410314
Epoch 14: loss 16.06445754099437
Epoch 15: loss 16.059997463952335
Epoch 16: loss 16.05553770583891
Epoch 17: loss 16.051068689461438
Epoch 18: loss 16.046614601017016
Epoch 19: loss 16.042160881285177
-----------Time: 0:03:25.257059, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 20, rmse: 4.006103992462158-------------


Epoch 0: loss 16.126982974643095
Epoch 1: loss 16.12250902135454
Epoch 2: loss 16.118045480177415
Epoch 3: loss 16.113575979702624
Epoch 4: loss 16.109110393233763
Epoch 5: loss 16.104642802959813
Epoch 6: loss 16.10018203283147
Epoch 7: loss 16.09570668273934
Epoch 8: loss 16.09124361736221
Epoch 9: loss 16.086786258992063
Epoch 10: loss 16.08232236102977
Epoch 11: loss 16.07785871665461
Epoch 12: loss 16.073395690689793
Epoch 13: loss 16.068942091528523
Epoch 14: loss 16.064481833241697
Epoch 15: loss 16.060021699414815
Epoch 16: loss 16.05556876352133
Epoch 17: loss 16.05109703806576
Epoch 18: loss 16.046643088601606
Epoch 19: loss 16.042184237749307
-----------Time: 0:03:52.649601, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 50, rmse: 4.006114959716797-------------


Epoch 0: loss 16.1269521770301
Epoch 1: loss 16.122482866616014
Epoch 2: loss 16.118012237963693
Epoch 3: loss 16.113545556506622
Epoch 4: loss 16.10909039978102
Epoch 5: loss 16.10461596032081
Epoch 6: loss 16.100153761755326
Epoch 7: loss 16.09567934692781
Epoch 8: loss 16.091220854935017
Epoch 9: loss 16.086759912637085
Epoch 10: loss 16.082298444755185
Epoch 11: loss 16.077835295367592
Epoch 12: loss 16.07337530518823
Epoch 13: loss 16.068917747941466
Epoch 14: loss 16.06444954210938
Epoch 15: loss 16.0599901721554
Epoch 16: loss 16.05553499442718
Epoch 17: loss 16.051073794134158
Epoch 18: loss 16.046622449253615
Epoch 19: loss 16.042155890700737
-----------Time: 0:04:03.619707, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 100, rmse: 4.006115913391113-------------


Epoch 0: loss 16.126961543418716
Epoch 1: loss 16.12249632773677
Epoch 2: loss 16.118026656129555
Epoch 3: loss 16.113554751503525
Epoch 4: loss 16.10908961283117
Epoch 5: loss 16.10462680415267
Epoch 6: loss 16.100156076969565
Epoch 7: loss 16.095691362498183
Epoch 8: loss 16.091224680781817
Epoch 9: loss 16.086755779270508
Epoch 10: loss 16.082300530496404
Epoch 11: loss 16.0778355712538
Epoch 12: loss 16.073377469494783
Epoch 13: loss 16.06891179668184
Epoch 14: loss 16.06445150287207
Epoch 15: loss 16.05999022064292
Epoch 16: loss 16.05552554506527
Epoch 17: loss 16.051067037774274
Epoch 18: loss 16.046610899630156
Epoch 19: loss 16.04214653390591
-----------Time: 0:04:37.499649, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 150, rmse: 4.006110668182373-------------


Epoch 0: loss 16.1269586997683
Epoch 1: loss 16.122488840693297
Epoch 2: loss 16.118017601927964
Epoch 3: loss 16.113559661966878
Epoch 4: loss 16.109085484909713
Epoch 5: loss 16.104621429557447
Epoch 6: loss 16.10015164801062
Epoch 7: loss 16.095687383410073
Epoch 8: loss 16.091211139539976
Epoch 9: loss 16.086756030005432
Epoch 10: loss 16.082285483807826
Epoch 11: loss 16.07781804807164
Epoch 12: loss 16.07335962686544
Epoch 13: loss 16.068894206861753
Epoch 14: loss 16.064424124277437
Epoch 15: loss 16.059958431239746
Epoch 16: loss 16.05548948020375
Epoch 17: loss 16.051030974987086
Epoch 18: loss 16.046561787477195
Epoch 19: loss 16.042090323387505
-----------Time: 0:05:49.836090, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.310129700083158e-07, embedding_dim: 200, rmse: 4.006103515625-------------


Epoch 0: loss 16.126452488766994
Epoch 1: loss 16.120550911733805
Epoch 2: loss 16.114644227953566
Epoch 3: loss 16.108732200952378
Epoch 4: loss 16.10282683359533
Epoch 5: loss 16.096929676964464
Epoch 6: loss 16.091027814191992
Epoch 7: loss 16.085126189189868
Epoch 8: loss 16.07923209090282
Epoch 9: loss 16.073335915690468
Epoch 10: loss 16.06743621540951
Epoch 11: loss 16.061546083505288
Epoch 12: loss 16.055647208808622
Epoch 13: loss 16.049759611997867
Epoch 14: loss 16.043861534882005
Epoch 15: loss 16.037974321563457
Epoch 16: loss 16.0320946834475
Epoch 17: loss 16.02620099898977
Epoch 18: loss 16.020313872533883
Epoch 19: loss 16.014434128367686
-----------Time: 0:03:14.277183, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 20, rmse: 4.002528190612793-------------


Epoch 0: loss 16.126269788311642
Epoch 1: loss 16.12036444014217
Epoch 2: loss 16.11445775843626
Epoch 3: loss 16.108549040513818
Epoch 4: loss 16.102643126570186
Epoch 5: loss 16.096741061550304
Epoch 6: loss 16.090830107756837
Epoch 7: loss 16.08494059322579
Epoch 8: loss 16.07903540944716
Epoch 9: loss 16.07314479940932
Epoch 10: loss 16.067243907942988
Epoch 11: loss 16.061350579751842
Epoch 12: loss 16.055456784576617
Epoch 13: loss 16.04955355041114
Epoch 14: loss 16.043660218849205
Epoch 15: loss 16.03777443318992
Epoch 16: loss 16.03187782107131
Epoch 17: loss 16.025995677161827
Epoch 18: loss 16.020103139550613
Epoch 19: loss 16.01421846288128
-----------Time: 0:03:07.879472, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 50, rmse: 4.002546787261963-------------


Epoch 0: loss 16.12623407297513
Epoch 1: loss 16.120330475801484
Epoch 2: loss 16.11441692235095
Epoch 3: loss 16.10851191592774
Epoch 4: loss 16.102603807859023
Epoch 5: loss 16.096703678450552
Epoch 6: loss 16.09080401939695
Epoch 7: loss 16.08490375774979
Epoch 8: loss 16.078998187367457
Epoch 9: loss 16.073101583805467
Epoch 10: loss 16.06720321783882
Epoch 11: loss 16.06130802171136
Epoch 12: loss 16.055406515983346
Epoch 13: loss 16.049512419770632
Epoch 14: loss 16.04362170549759
Epoch 15: loss 16.037727925879857
Epoch 16: loss 16.031834034766757
Epoch 17: loss 16.025945969156883
Epoch 18: loss 16.02005339654131
Epoch 19: loss 16.014168813994814
-----------Time: 0:04:31.174186, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 100, rmse: 4.002543926239014-------------


Epoch 0: loss 16.126241989664695
Epoch 1: loss 16.12033850580146
Epoch 2: loss 16.114433874054537
Epoch 3: loss 16.108529848790546
Epoch 4: loss 16.102614150221026
Epoch 5: loss 16.096714538358487
Epoch 6: loss 16.09079736493864
Epoch 7: loss 16.084912681787024
Epoch 8: loss 16.079008837767823
Epoch 9: loss 16.0731082621186
Epoch 10: loss 16.067207216114504
Epoch 11: loss 16.061313362339387
Epoch 12: loss 16.055414506311713
Epoch 13: loss 16.049517874746236
Epoch 14: loss 16.043614332283106
Epoch 15: loss 16.03771610037019
Epoch 16: loss 16.031827151872594
Epoch 17: loss 16.025937089458505
Epoch 18: loss 16.02003784682713
Epoch 19: loss 16.014144558566883
-----------Time: 0:05:28.704739, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 150, rmse: 4.002540111541748-------------


Epoch 0: loss 16.126234701757134
Epoch 1: loss 16.120336444174356
Epoch 2: loss 16.11442607274971
Epoch 3: loss 16.10851794445625
Epoch 4: loss 16.10261301737625
Epoch 5: loss 16.096710633427765
Epoch 6: loss 16.090806390877454
Epoch 7: loss 16.084903715225973
Epoch 8: loss 16.078996722110915
Epoch 9: loss 16.0730942225184
Epoch 10: loss 16.06719869786854
Epoch 11: loss 16.06129599499145
Epoch 12: loss 16.05538949530821
Epoch 13: loss 16.04949013469918
Epoch 14: loss 16.043584268465192
Epoch 15: loss 16.037688917281383
Epoch 16: loss 16.03178804629908
Epoch 17: loss 16.0258839992546
Epoch 18: loss 16.019974100259155
Epoch 19: loss 16.014061237561307
-----------Time: 0:04:46.477648, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 3.0538555088334124e-07, embedding_dim: 200, rmse: 4.002530097961426-------------


Epoch 0: loss 16.1254253607849
Epoch 1: loss 16.117628321821368
Epoch 2: loss 16.10981869995756
Epoch 3: loss 16.10201991466545
Epoch 4: loss 16.09421014526475
Epoch 5: loss 16.08641306948182
Epoch 6: loss 16.078614608304147
Epoch 7: loss 16.07081643312505
Epoch 8: loss 16.063030660897898
Epoch 9: loss 16.05523277819967
Epoch 10: loss 16.047440242093177
Epoch 11: loss 16.03964918861576
Epoch 12: loss 16.031862987779675
Epoch 13: loss 16.0240825180647
Epoch 14: loss 16.016303643511325
Epoch 15: loss 16.008528968184585
Epoch 16: loss 16.00075071359731
Epoch 17: loss 15.99297516264301
Epoch 18: loss 15.985203610742268
Epoch 19: loss 15.977439455132954
-----------Time: 0:03:13.518336, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 20, rmse: 3.9978411197662354-------------


Epoch 0: loss 16.125290383457685
Epoch 1: loss 16.11747575285903
Epoch 2: loss 16.10966692961319
Epoch 3: loss 16.101863944835156
Epoch 4: loss 16.094064868617927
Epoch 5: loss 16.086271088949164
Epoch 6: loss 16.078476046789852
Epoch 7: loss 16.070678379044317
Epoch 8: loss 16.06288114146346
Epoch 9: loss 16.055090451512797
Epoch 10: loss 16.04730031748321
Epoch 11: loss 16.039514673864666
Epoch 12: loss 16.03173269273925
Epoch 13: loss 16.02394788170587
Epoch 14: loss 16.016166596000385
Epoch 15: loss 16.008395313503563
Epoch 16: loss 16.0006161993648
Epoch 17: loss 15.992845627586112
Epoch 18: loss 15.985068642230965
Epoch 19: loss 15.97730235861591
-----------Time: 0:03:59.174394, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 50, rmse: 3.997842311859131-------------


Epoch 0: loss 16.125286749227335
Epoch 1: loss 16.117477563751205
Epoch 2: loss 16.10966658657047
Epoch 3: loss 16.10186438614937
Epoch 4: loss 16.094060644499304
Epoch 5: loss 16.08626182135063
Epoch 6: loss 16.07845837244072
Epoch 7: loss 16.07066854878312
Epoch 8: loss 16.06286766867459
Epoch 9: loss 16.055071578458836
Epoch 10: loss 16.047280126709598
Epoch 11: loss 16.03948298740024
Epoch 12: loss 16.03170038786448
Epoch 13: loss 16.023911069566104
Epoch 14: loss 16.016127450755267
Epoch 15: loss 16.008348023910894
Epoch 16: loss 16.000560145977076
Epoch 17: loss 15.9927815016746
Epoch 18: loss 15.984993185538062
Epoch 19: loss 15.977215088081463
-----------Time: 0:04:25.522184, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 100, rmse: 3.997828960418701-------------


Epoch 0: loss 16.12528748483746
Epoch 1: loss 16.117492846395088
Epoch 2: loss 16.109681137752897
Epoch 3: loss 16.101876178729007
Epoch 4: loss 16.09406933258123
Epoch 5: loss 16.086268805368494
Epoch 6: loss 16.078465729579126
Epoch 7: loss 16.07066482380073
Epoch 8: loss 16.062869465305727
Epoch 9: loss 16.055073275781314
Epoch 10: loss 16.047272688153647
Epoch 11: loss 16.03948588731692
Epoch 12: loss 16.03168660470362
Epoch 13: loss 16.023890571532007
Epoch 14: loss 16.01609824726671
Epoch 15: loss 16.008302390931934
Epoch 16: loss 16.000509586977273
Epoch 17: loss 15.992711958644051
Epoch 18: loss 15.984921817872886
Epoch 19: loss 15.977122975277343
-----------Time: 0:04:29.664886, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 150, rmse: 3.9978134632110596-------------


Epoch 0: loss 16.12530307655687
Epoch 1: loss 16.117499184777
Epoch 2: loss 16.10969719879001
Epoch 3: loss 16.101898806583886
Epoch 4: loss 16.09409205974477
Epoch 5: loss 16.08628562690772
Epoch 6: loss 16.078490440841602
Epoch 7: loss 16.070686409044292
Epoch 8: loss 16.062891871207043
Epoch 9: loss 16.05510334167356
Epoch 10: loss 16.047306979201682
Epoch 11: loss 16.03950269563228
Epoch 12: loss 16.031710811644032
Epoch 13: loss 16.023912981322695
Epoch 14: loss 16.01610906957737
Epoch 15: loss 16.008311505029873
Epoch 16: loss 16.000511461395877
Epoch 17: loss 15.992700466324028
Epoch 18: loss 15.984890778600166
Epoch 19: loss 15.977060718339315
-----------Time: 0:05:37.750730, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 4.0370172585965496e-07, embedding_dim: 200, rmse: 3.997804880142212-------------


Epoch 0: loss 16.124183833437034
Epoch 1: loss 16.11385046340254
Epoch 2: loss 16.10352008818543
Epoch 3: loss 16.093212598559
Epoch 4: loss 16.08289590123038
Epoch 5: loss 16.07257889197403
Epoch 6: loss 16.062277767177246
Epoch 7: loss 16.051977797783607
Epoch 8: loss 16.04167684282279
Epoch 9: loss 16.031382123045844
Epoch 10: loss 16.02108204060109
Epoch 11: loss 16.010799511934895
Epoch 12: loss 16.000511576780617
Epoch 13: loss 15.990230718211288
Epoch 14: loss 15.979952777449757
Epoch 15: loss 15.96967314144008
Epoch 16: loss 15.959402564558534
Epoch 17: loss 15.949133398482305
Epoch 18: loss 15.938874576323832
Epoch 19: loss 15.928623395487302
-----------Time: 0:03:44.793190, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 20, rmse: 3.99161434173584-------------


Epoch 0: loss 16.12405640538326
Epoch 1: loss 16.113741153864318
Epoch 2: loss 16.103422331122875
Epoch 3: loss 16.09310829153263
Epoch 4: loss 16.082801552555235
Epoch 5: loss 16.072500813324986
Epoch 6: loss 16.062193182125373
Epoch 7: loss 16.05190132933508
Epoch 8: loss 16.041605029694733
Epoch 9: loss 16.031310578803126
Epoch 10: loss 16.021029727493957
Epoch 11: loss 16.010747300988633
Epoch 12: loss 16.00046229971823
Epoch 13: loss 15.990191307192331
Epoch 14: loss 15.979913049576528
Epoch 15: loss 15.969646073476719
Epoch 16: loss 15.95937384853809
Epoch 17: loss 15.949119639694853
Epoch 18: loss 15.938860873543353
Epoch 19: loss 15.928601596587503
-----------Time: 0:03:01.520035, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 50, rmse: 3.9916131496429443-------------


Epoch 0: loss 16.12404610736011
Epoch 1: loss 16.113732034580547
Epoch 2: loss 16.10341765272546
Epoch 3: loss 16.093103649954617
Epoch 4: loss 16.08279286421402
Epoch 5: loss 16.07249068669355
Epoch 6: loss 16.062188752388554
Epoch 7: loss 16.051890226211672
Epoch 8: loss 16.04159364679574
Epoch 9: loss 16.031303146210877
Epoch 10: loss 16.02101141736195
Epoch 11: loss 16.010720587736113
Epoch 12: loss 16.000431793548934
Epoch 13: loss 15.990148519938117
Epoch 14: loss 15.979863619272834
Epoch 15: loss 15.969586120344102
Epoch 16: loss 15.959303433250772
Epoch 17: loss 15.94901889637153
Epoch 18: loss 15.93873683702304
Epoch 19: loss 15.928464978204078
-----------Time: 0:04:20.865505, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 100, rmse: 3.9915971755981445-------------


Epoch 0: loss 16.12405295939858
Epoch 1: loss 16.113740897165684
Epoch 2: loss 16.103423117554144
Epoch 3: loss 16.093111528269034
Epoch 4: loss 16.08279110829165
Epoch 5: loss 16.0724838709559
Epoch 6: loss 16.062179750823145
Epoch 7: loss 16.051861159888347
Epoch 8: loss 16.0415629946453
Epoch 9: loss 16.031258579179475
Epoch 10: loss 16.020959445741788
Epoch 11: loss 16.0106517860201
Epoch 12: loss 16.000349102881113
Epoch 13: loss 15.990048522596583
Epoch 14: loss 15.979741973939182
Epoch 15: loss 15.969433715772603
Epoch 16: loss 15.959112616710652
Epoch 17: loss 15.948792443578823
Epoch 18: loss 15.93846209792095
Epoch 19: loss 15.928111843339382
-----------Time: 0:05:26.985637, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 150, rmse: 3.9915430545806885-------------


Epoch 0: loss 16.12404701669557
Epoch 1: loss 16.11373396189463
Epoch 2: loss 16.103420551864268
Epoch 3: loss 16.093111743740312
Epoch 4: loss 16.08278902851414
Epoch 5: loss 16.072475318483452
Epoch 6: loss 16.06215231881445
Epoch 7: loss 16.051839108697873
Epoch 8: loss 16.04152022424504
Epoch 9: loss 16.031203490874894
Epoch 10: loss 16.020874125857663
Epoch 11: loss 16.01054392566235
Epoch 12: loss 16.00020139044948
Epoch 13: loss 15.989857987647085
Epoch 14: loss 15.979477924131194
Epoch 15: loss 15.969082830521385
Epoch 16: loss 15.958645709381601
Epoch 17: loss 15.948171424243423
Epoch 18: loss 15.937655892302123
Epoch 19: loss 15.927077814572009
-----------Time: 0:04:55.709476, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 5.336699231206302e-07, embedding_dim: 200, rmse: 3.9913992881774902-------------


Epoch 0: loss 16.122470613534585
Epoch 1: loss 16.108823824732635
Epoch 2: loss 16.095194275447895
Epoch 3: loss 16.081563621840445
Epoch 4: loss 16.067936130552724
Epoch 5: loss 16.054321038586863
Epoch 6: loss 16.040704014639676
Epoch 7: loss 16.02709424852223
Epoch 8: loss 16.01348912916863
Epoch 9: loss 15.999900668519162
Epoch 10: loss 15.986302379164247
Epoch 11: loss 15.972725266409412
Epoch 12: loss 15.959141392368153
Epoch 13: loss 15.945584452755105
Epoch 14: loss 15.93201744692555
Epoch 15: loss 15.918463733677244
Epoch 16: loss 15.904914554919543
Epoch 17: loss 15.891371092244034
Epoch 18: loss 15.877827507182916
Epoch 19: loss 15.864304252912843
-----------Time: 0:03:10.655339, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 20, rmse: 3.9833791255950928-------------


Epoch 0: loss 16.1223908824205
Epoch 1: loss 16.108752588528276
Epoch 2: loss 16.095124751864212
Epoch 3: loss 16.081495383824258
Epoch 4: loss 16.06786965857128
Epoch 5: loss 16.054252193309875
Epoch 6: loss 16.04065004673388
Epoch 7: loss 16.02704496419968
Epoch 8: loss 16.01345815368163
Epoch 9: loss 15.999858386364885
Epoch 10: loss 15.986273580429799
Epoch 11: loss 15.972691950297602
Epoch 12: loss 15.959119345326343
Epoch 13: loss 15.945550108293011
Epoch 14: loss 15.931988599703583
Epoch 15: loss 15.918443779118233
Epoch 16: loss 15.904889861029604
Epoch 17: loss 15.891341305608785
Epoch 18: loss 15.87780212072525
Epoch 19: loss 15.864263205764214
-----------Time: 0:03:55.900179, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 50, rmse: 3.983374834060669-------------


Epoch 0: loss 16.122408942336165
Epoch 1: loss 16.108769027353176
Epoch 2: loss 16.09513710710651
Epoch 3: loss 16.08151061435054
Epoch 4: loss 16.067881697737178
Epoch 5: loss 16.05427369895093
Epoch 6: loss 16.040662222027844
Epoch 7: loss 16.027055300338688
Epoch 8: loss 16.013453566036254
Epoch 9: loss 15.999836438631739
Epoch 10: loss 15.986256544715753
Epoch 11: loss 15.972669015182237
Epoch 12: loss 15.959084370526496
Epoch 13: loss 15.945485040634596
Epoch 14: loss 15.931907202641298
Epoch 15: loss 15.918330671995992
Epoch 16: loss 15.90474833114566
Epoch 17: loss 15.891170277162503
Epoch 18: loss 15.87757729343126
Epoch 19: loss 15.864000628732223
-----------Time: 0:04:39.920687, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 100, rmse: 3.983339548110962-------------


Epoch 0: loss 16.122406479066452
Epoch 1: loss 16.108775455709253
Epoch 2: loss 16.09513664167818
Epoch 3: loss 16.081503810281014
Epoch 4: loss 16.067877058233496
Epoch 5: loss 16.054264606892772
Epoch 6: loss 16.040647524345694
Epoch 7: loss 16.027023704626302
Epoch 8: loss 16.01341255829994
Epoch 9: loss 15.999796928563411
Epoch 10: loss 15.986192386133506
Epoch 11: loss 15.972574974804743
Epoch 12: loss 15.95893994772674
Epoch 13: loss 15.945311294035006
Epoch 14: loss 15.931673761163475
Epoch 15: loss 15.918006963869876
Epoch 16: loss 15.904306132226875
Epoch 17: loss 15.890588066511793
Epoch 18: loss 15.876827521062273
Epoch 19: loss 15.863014270197509
-----------Time: 0:04:25.587458, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 150, rmse: 3.9832000732421875-------------


Epoch 0: loss 16.122398184070516
Epoch 1: loss 16.108759699858293
Epoch 2: loss 16.09512685316292
Epoch 3: loss 16.081485776035212
Epoch 4: loss 16.06785286710987
Epoch 5: loss 16.05422092793492
Epoch 6: loss 16.040600060768398
Epoch 7: loss 16.026960307324043
Epoch 8: loss 16.01333055034677
Epoch 9: loss 15.999682906660585
Epoch 10: loss 15.98601183442722
Epoch 11: loss 15.9723097055486
Epoch 12: loss 15.958567487344332
Epoch 13: loss 15.944770212728345
Epoch 14: loss 15.930891764818682
Epoch 15: loss 15.916912642297957
Epoch 16: loss 15.902801051095752
Epoch 17: loss 15.88850820472929
Epoch 18: loss 15.87400056331815
Epoch 19: loss 15.859210983575071
-----------Time: 0:05:33.477915, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 7.05480231071866e-07, embedding_dim: 200, rmse: 3.9826509952545166-------------


Epoch 0: loss 16.120359862090584
Epoch 1: loss 16.102315025008068
Epoch 2: loss 16.084292255969977
Epoch 3: loss 16.066275550205464
Epoch 4: loss 16.048272736244968
Epoch 5: loss 16.030271945017844
Epoch 6: loss 16.012289500482837
Epoch 7: loss 15.994314960709987
Epoch 8: loss 15.976351520949049
Epoch 9: loss 15.958400113093846
Epoch 10: loss 15.940459520029853
Epoch 11: loss 15.922536248678485
Epoch 12: loss 15.904622026602167
Epoch 13: loss 15.8867199404075
Epoch 14: loss 15.868829661312798
Epoch 15: loss 15.850950702109241
Epoch 16: loss 15.833084175676156
Epoch 17: loss 15.815219701017096
Epoch 18: loss 15.797360350996208
Epoch 19: loss 15.779519580770538
-----------Time: 0:03:52.287625, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 20, rmse: 3.9724819660186768-------------


Epoch 0: loss 16.12023263161697
Epoch 1: loss 16.102205008122503
Epoch 2: loss 16.08419100754672
Epoch 3: loss 16.066199896969568
Epoch 4: loss 16.04821554845672
Epoch 5: loss 16.03022261946794
Epoch 6: loss 16.012254995259983
Epoch 7: loss 15.994294612287113
Epoch 8: loss 15.976346303484503
Epoch 9: loss 15.95840249598318
Epoch 10: loss 15.940469782270771
Epoch 11: loss 15.922542462081868
Epoch 12: loss 15.904643372001047
Epoch 13: loss 15.88674941070706
Epoch 14: loss 15.868853412159375
Epoch 15: loss 15.850970972226067
Epoch 16: loss 15.83309668571551
Epoch 17: loss 15.815233516070816
Epoch 18: loss 15.79737106777521
Epoch 19: loss 15.779526424511676
-----------Time: 0:03:00.885935, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 50, rmse: 3.972503185272217-------------


Epoch 0: loss 16.120229555900618
Epoch 1: loss 16.10219810837439
Epoch 2: loss 16.084180111856547
Epoch 3: loss 16.066172440068886
Epoch 4: loss 16.04817507019375
Epoch 5: loss 16.03018441026531
Epoch 6: loss 16.01219945060228
Epoch 7: loss 15.994222894578346
Epoch 8: loss 15.976260334852922
Epoch 9: loss 15.958293177110466
Epoch 10: loss 15.940326527838735
Epoch 11: loss 15.922367127840126
Epoch 12: loss 15.90440501772625
Epoch 13: loss 15.88644672101318
Epoch 14: loss 15.868475549437548
Epoch 15: loss 15.850489832124612
Epoch 16: loss 15.83248435212841
Epoch 17: loss 15.81446356329988
Epoch 18: loss 15.79641630624934
Epoch 19: loss 15.778312615171084
-----------Time: 0:04:14.849377, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 100, rmse: 3.972330093383789-------------


Epoch 0: loss 16.120214279479733
Epoch 1: loss 16.102193220469406
Epoch 2: loss 16.084173075980374
Epoch 3: loss 16.066153009538095
Epoch 4: loss 16.048139179057568
Epoch 5: loss 16.030128530862218
Epoch 6: loss 16.012131778619548
Epoch 7: loss 15.994138250148705
Epoch 8: loss 15.97612854733034
Epoch 9: loss 15.958101134380613
Epoch 10: loss 15.940041118370832
Epoch 11: loss 15.921945623498432
Epoch 12: loss 15.90379753600261
Epoch 13: loss 15.885568562508148
Epoch 14: loss 15.867245348722408
Epoch 15: loss 15.848775528124197
Epoch 16: loss 15.830119883126056
Epoch 17: loss 15.81122274020238
Epoch 18: loss 15.792005916209634
Epoch 19: loss 15.772418605043164
-----------Time: 0:05:25.340625, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 150, rmse: 3.971482515335083-------------


Epoch 0: loss 16.120216609992173
Epoch 1: loss 16.102190010180742
Epoch 2: loss 16.08416911322763
Epoch 3: loss 16.06615403970342
Epoch 4: loss 16.04814432392048
Epoch 5: loss 16.030139602092767
Epoch 6: loss 16.012120109203394
Epoch 7: loss 15.994078772038206
Epoch 8: loss 15.976014349368553
Epoch 9: loss 15.957885249533557
Epoch 10: loss 15.939680998451104
Epoch 11: loss 15.921349917500482
Epoch 12: loss 15.902843088219514
Epoch 13: loss 15.884058476012967
Epoch 14: loss 15.864929212183327
Epoch 15: loss 15.845334506281178
Epoch 16: loss 15.82514246240007
Epoch 17: loss 15.80419369713647
Epoch 18: loss 15.782343215434171
Epoch 19: loss 15.759429965607819
-----------Time: 0:05:02.332585, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 9.326033468832218e-07, embedding_dim: 200, rmse: 3.9696099758148193-------------


Epoch 0: loss 16.117426616332658
Epoch 1: loss 16.09360326924617
Epoch 2: loss 16.069795526065796
Epoch 3: loss 16.046002257058255
Epoch 4: loss 16.022225177177855
Epoch 5: loss 15.998471626709051
Epoch 6: loss 15.974740781364005
Epoch 7: loss 15.951027694119261
Epoch 8: loss 15.927337348299094
Epoch 9: loss 15.903652606806462
Epoch 10: loss 15.879997259640966
Epoch 11: loss 15.856350317411023
Epoch 12: loss 15.832740505212282
Epoch 13: loss 15.809140539350816
Epoch 14: loss 15.785572310515628
Epoch 15: loss 15.762023758797493
Epoch 16: loss 15.738484063960081
Epoch 17: loss 15.714969409426118
Epoch 18: loss 15.691470027164378
Epoch 19: loss 15.667981385795756
-----------Time: 0:03:08.387598, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 20, rmse: 3.9581432342529297-------------


Epoch 0: loss 16.117330031786487
Epoch 1: loss 16.093507498875464
Epoch 2: loss 16.06969155274765
Epoch 3: loss 16.045903961187864
Epoch 4: loss 16.02212617084862
Epoch 5: loss 15.998365455376447
Epoch 6: loss 15.974637605626665
Epoch 7: loss 15.95092572356904
Epoch 8: loss 15.927231455704906
Epoch 9: loss 15.903546960798536
Epoch 10: loss 15.879878616903449
Epoch 11: loss 15.856234452204577
Epoch 12: loss 15.832594390535172
Epoch 13: loss 15.80898365214702
Epoch 14: loss 15.785384657591695
Epoch 15: loss 15.761787952321454
Epoch 16: loss 15.738204221741022
Epoch 17: loss 15.714617397812413
Epoch 18: loss 15.691050431986358
Epoch 19: loss 15.667497779053797
-----------Time: 0:03:46.967460, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 50, rmse: 3.9580695629119873-------------


Epoch 0: loss 16.117343794722608
Epoch 1: loss 16.09351708929198
Epoch 2: loss 16.06970805795119
Epoch 3: loss 16.04590032592035
Epoch 4: loss 16.022126505075427
Epoch 5: loss 15.99835511275515
Epoch 6: loss 15.97458840116591
Epoch 7: loss 15.9508348600316
Epoch 8: loss 15.927086387269195
Epoch 9: loss 15.903341956862034
Epoch 10: loss 15.87960440494108
Epoch 11: loss 15.855843856452664
Epoch 12: loss 15.832068484007113
Epoch 13: loss 15.808245253342529
Epoch 14: loss 15.784377985638468
Epoch 15: loss 15.760426386728955
Epoch 16: loss 15.736384801465315
Epoch 17: loss 15.712202476638371
Epoch 18: loss 15.687848298558727
Epoch 19: loss 15.663265152817644
-----------Time: 0:04:52.220765, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 100, rmse: 3.9574668407440186-------------


Epoch 0: loss 16.117356246421362
Epoch 1: loss 16.093545884914928
Epoch 2: loss 16.069724131434295
Epoch 3: loss 16.045933338401756
Epoch 4: loss 16.02213221078596
Epoch 5: loss 15.998351891835531
Epoch 6: loss 15.974568509874038
Epoch 7: loss 15.950769155034557
Epoch 8: loss 15.926921693834032
Epoch 9: loss 15.903016598604502
Epoch 10: loss 15.879005895742196
Epoch 11: loss 15.854821094421668
Epoch 12: loss 15.83040432673294
Epoch 13: loss 15.805664527150455
Epoch 14: loss 15.780441472943934
Epoch 15: loss 15.75460770805612
Epoch 16: loss 15.727967363132478
Epoch 17: loss 15.700319499668705
Epoch 18: loss 15.671419506166343
Epoch 19: loss 15.64104438930053
-----------Time: 0:04:19.624103, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 150, rmse: 3.9542460441589355-------------


Epoch 0: loss 16.117333910269476
Epoch 1: loss 16.093509074071623
Epoch 2: loss 16.069684547467876
Epoch 3: loss 16.045875976369587
Epoch 4: loss 16.02205938175361
Epoch 5: loss 15.99820188285865
Epoch 6: loss 15.974289703265423
Epoch 7: loss 15.950249839956959
Epoch 8: loss 15.925988032935818
Epoch 9: loss 15.901349780480455
Epoch 10: loss 15.876140541327654
Epoch 11: loss 15.850076574291336
Epoch 12: loss 15.822806822987857
Epoch 13: loss 15.793914925086233
Epoch 14: loss 15.76300083495405
Epoch 15: loss 15.729634568119517
Epoch 16: loss 15.69342709495167
Epoch 17: loss 15.654043986137436
Epoch 18: loss 15.611143463263375
Epoch 19: loss 15.564507803105348
-----------Time: 0:05:28.828764, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.2328467394420685e-06, embedding_dim: 200, rmse: 3.9434635639190674-------------


Epoch 0: loss 16.113659021969784
Epoch 1: loss 16.082162705110818
Epoch 2: loss 16.05069508845551
Epoch 3: loss 16.019267074176316
Epoch 4: loss 15.987876487335736
Epoch 5: loss 15.956511639589328
Epoch 6: loss 15.925172449778836
Epoch 7: loss 15.893869001234012
Epoch 8: loss 15.862606483675203
Epoch 9: loss 15.831391506443989
Epoch 10: loss 15.800189321879916
Epoch 11: loss 15.769028088008016
Epoch 12: loss 15.737910043551521
Epoch 13: loss 15.706803674215594
Epoch 14: loss 15.675748965609262
Epoch 15: loss 15.644724670345852
Epoch 16: loss 15.61373725982384
Epoch 17: loss 15.582778533170119
Epoch 18: loss 15.551870739414616
Epoch 19: loss 15.520989834711305
-----------Time: 0:04:01.646503, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 20, rmse: 3.939152240753174-------------


Epoch 0: loss 16.11353678156203
Epoch 1: loss 16.082058996010762
Epoch 2: loss 16.050602443585127
Epoch 3: loss 16.019185224131697
Epoch 4: loss 15.987796164518338
Epoch 5: loss 15.956439677109337
Epoch 6: loss 15.925112960778092
Epoch 7: loss 15.893814917684185
Epoch 8: loss 15.86255400462148
Epoch 9: loss 15.831305178693613
Epoch 10: loss 15.800078679064553
Epoch 11: loss 15.768870384295134
Epoch 12: loss 15.737690736833898
Epoch 13: loss 15.706522290248985
Epoch 14: loss 15.675395204814768
Epoch 15: loss 15.644284164860172
Epoch 16: loss 15.613187368308934
Epoch 17: loss 15.582090963806518
Epoch 18: loss 15.550995999409363
Epoch 19: loss 15.51987610516696
-----------Time: 0:03:06.087361, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 50, rmse: 3.9389705657958984-------------


Epoch 0: loss 16.113529082936655
Epoch 1: loss 16.08205049980458
Epoch 2: loss 16.050598695525792
Epoch 3: loss 16.019181551785493
Epoch 4: loss 15.987781277553356
Epoch 5: loss 15.95639307723219
Epoch 6: loss 15.925017149698611
Epoch 7: loss 15.893655124600002
Epoch 8: loss 15.862283401478885
Epoch 9: loss 15.830893212650832
Epoch 10: loss 15.79944602039082
Epoch 11: loss 15.76793203929509
Epoch 12: loss 15.736259515419027
Epoch 13: loss 15.704404236117805
Epoch 14: loss 15.672262604305315
Epoch 15: loss 15.639709311117615
Epoch 16: loss 15.606647268464346
Epoch 17: loss 15.57288912886702
Epoch 18: loss 15.53824669774166
Epoch 19: loss 15.502519521459151
-----------Time: 0:04:09.700051, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 100, rmse: 3.936448097229004-------------


Epoch 0: loss 16.113536903429058
Epoch 1: loss 16.082056248038928
Epoch 2: loss 16.050585965866496
Epoch 3: loss 16.01914003194837
Epoch 4: loss 15.987700786974328
Epoch 5: loss 15.956259713735575
Epoch 6: loss 15.924755089175383
Epoch 7: loss 15.893151639012165
Epoch 8: loss 15.86134927981959
Epoch 9: loss 15.829213590710108
Epoch 10: loss 15.79649711342335
Epoch 11: loss 15.762918553448294
Epoch 12: loss 15.728081951846631
Epoch 13: loss 15.691488305403526
Epoch 14: loss 15.652656959152015
Epoch 15: loss 15.610989318067707
Epoch 16: loss 15.565933628663089
Epoch 17: loss 15.517010373483215
Epoch 18: loss 15.463756745117005
Epoch 19: loss 15.405833336890813
-----------Time: 0:05:24.489514, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 150, rmse: 3.9226205348968506-------------


Epoch 0: loss 16.113536158484436
Epoch 1: loss 16.08204406418834
Epoch 2: loss 16.05056491191132
Epoch 3: loss 16.01907056255662
Epoch 4: loss 15.987511014601276
Epoch 5: loss 15.955834653469582
Epoch 6: loss 15.92381721297446
Epoch 7: loss 15.891155243179213
Epoch 8: loss 15.857352437760403
Epoch 9: loss 15.821654882685136
Epoch 10: loss 15.783177784177646
Epoch 11: loss 15.740933875404926
Epoch 12: loss 15.694011270773029
Epoch 13: loss 15.641553388982445
Epoch 14: loss 15.582848260816249
Epoch 15: loss 15.517361637559386
Epoch 16: loss 15.444652554002257
Epoch 17: loss 15.36438074832769
Epoch 18: loss 15.27640147439937
Epoch 19: loss 15.18052240115524
-----------Time: 0:05:14.520038, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 1.6297508346206469e-06, embedding_dim: 200, rmse: 3.8913486003875732-------------


Epoch 0: loss 16.108646203280145
Epoch 1: loss 16.067038420427746
Epoch 2: loss 16.02548639407425
Epoch 3: loss 15.983997393198932
Epoch 4: loss 15.94255150187722
Epoch 5: loss 15.901145838602138
Epoch 6: loss 15.859825099791568
Epoch 7: loss 15.818556340898489
Epoch 8: loss 15.777347208689969
Epoch 9: loss 15.736204073181485
Epoch 10: loss 15.695123224429555
Epoch 11: loss 15.654097484206947
Epoch 12: loss 15.61312118258536
Epoch 13: loss 15.572197092687908
Epoch 14: loss 15.53133263855022
Epoch 15: loss 15.490522263035817
Epoch 16: loss 15.449766412126168
Epoch 17: loss 15.4090632287752
Epoch 18: loss 15.368398852553169
Epoch 19: loss 15.327805597928634
-----------Time: 0:03:04.558601, Loss: regression, n_iter: 20, l2: 1.917910261672485e-15, batch_size: 256, learning_rate: 2.1544346900318865e-06, embedding_dim: 20, rmse: 3.9139981269836426-------------


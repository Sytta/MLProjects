{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = \"./data/data_train.csv\"\n",
    "# ratings = load_data(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    \"\"\"read text file from path.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "\n",
    "def load_data(path_dataset):\n",
    "    \"\"\"Load data in text format, one rating per line, as in the kaggle competition.\"\"\"\n",
    "    data = read_txt(path_dataset)[1:]\n",
    "    return preprocess_data(data)\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"preprocessing the text data, conversion to numerical array format.\"\"\"\n",
    "    def deal_line(line):\n",
    "        pos, rating = line.split(',')\n",
    "        row, col = pos.split(\"_\")\n",
    "        row = row.replace(\"r\", \"\")\n",
    "        col = col.replace(\"c\", \"\")\n",
    "        return int(row), int(col), float(rating)\n",
    "\n",
    "    def statistics(data):\n",
    "        row = set([line[0] for line in data])\n",
    "        col = set([line[1] for line in data])\n",
    "        return min(row), max(row), min(col), max(col)\n",
    "\n",
    "    # parse each line\n",
    "    data = [deal_line(line) for line in data]\n",
    "\n",
    "    # do statistics on the dataset.\n",
    "    min_row, max_row, min_col, max_col = statistics(data)\n",
    "    print(\"number of items: {}, number of users: {}\".format(max_row, max_col))\n",
    "\n",
    "    # build rating matrix.\n",
    "    ratings = sp.lil_matrix((max_row, max_col))\n",
    "    for row, col, rating in data:\n",
    "        ratings[row - 1, col - 1] = rating\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def group_by(data, index):\n",
    "    \"\"\"group list of list by a specific index.\"\"\"\n",
    "    sorted_data = sorted(data, key=lambda x: x[index])\n",
    "    groupby_data = groupby(sorted_data, lambda x: x[index])\n",
    "    return groupby_data\n",
    "\n",
    "\n",
    "def build_index_groups(train):\n",
    "    \"\"\"build groups for nnz rows and cols.\"\"\"\n",
    "    # row : items; cols: users\n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "\n",
    "    grouped_nz_train_byrow = group_by(nz_train, index=0) # group by items \n",
    "#     for g, value in grouped_nz_train_byrow:\n",
    "#         print(\"{}, {}\".format(g, list(value))) #value for g=0: (0, 1) (0, 2) (0, 3) index of all the users that rated the item 0\n",
    "    nz_row_colindices = [(g, np.array([v[1] for v in value])) # indices of all the users that rated item g\n",
    "                         for g, value in grouped_nz_train_byrow]\n",
    "    \n",
    "#     print(nz_row_colindices)\n",
    "\n",
    "    grouped_nz_train_bycol = group_by(nz_train, index=1) # group by users\n",
    "    nz_col_rowindices = [(g, np.array([v[0] for v in value])) # indices of all the movies rated by user g\n",
    "                         for g, value in grouped_nz_train_bycol]\n",
    "    return nz_train, nz_row_colindices, nz_col_rowindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_per(ratings):\n",
    "    \"\"\"plot the statistics result on raw rating data.\"\"\"\n",
    "    # do statistics.\n",
    "    num_items_per_user = np.array((ratings != 0).sum(axis=0)).flatten()\n",
    "    num_users_per_item = np.array((ratings != 0).sum(axis=1).T).flatten()\n",
    "    sorted_num_movies_per_user = np.sort(num_items_per_user)[::-1]\n",
    "    sorted_num_users_per_movie = np.sort(num_users_per_item)[::-1]\n",
    "    return num_items_per_user, num_users_per_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_items_per_user,num_users_per_item = get_number_per(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "\n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split the data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        indices = np.random.permutation(row)\n",
    "        idx_split = int(np.floor(len(row)*p_test))\n",
    "        selects = indices[:idx_split]\n",
    "        residual = indices[idx_split:]\n",
    "#         selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "#         residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "    \n",
    "    \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:941957\n",
      "Total number of nonzero elements in test data:234995\n"
     ]
    }
   ],
   "source": [
    "# valid_ratings, train, test = split_data(\n",
    "#     ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.lil.lil_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features,weight=1.0):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    num_item,num_user = train.shape\n",
    "    \n",
    "    user_features = weight * np.random.rand(num_features,num_user)\n",
    "    item_features = weight * np.random.rand(num_features,num_item)\n",
    "    \n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "    \n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    mse = 0\n",
    "    for row,col in nz:\n",
    "        user = user_features[:,col]\n",
    "        item = item_features[:,row]\n",
    "        mse += ((data[row,col] - user.T.dot(item))**2)\n",
    "    \n",
    "    rmse = np.sqrt(1.0*mse/len(nz))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_user[user] * lambda_user\"\"\"\n",
    "    # update and return user feature.\n",
    "    num_users = nnz_items_per_user.shape[0]\n",
    "    num_features = item_features.shape[0]\n",
    "    lambda_I = lambda_user * sp.eye(num_features)\n",
    "    updated_user_features = np.zeros((num_features,num_users))\n",
    "    \n",
    "    for user,item in nz_user_itemindices:\n",
    "        M = item_features[:,item]\n",
    "        \n",
    "        V = M @ train[item,user]\n",
    "        A = M @ M.T + nnz_items_per_user[user] * lambda_I\n",
    "        Z_star = np.linalg.solve(A,V)\n",
    "        updated_user_features[:,user] = np.copy(Z_star.T)\n",
    "    return updated_user_features\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_item[item] * lambda_item\"\"\"\n",
    "    # update and return item feature.\n",
    "    num_items = nnz_users_per_item.shape[0]\n",
    "    num_features = user_features.shape[0]\n",
    "    lambda_I = lambda_item * sp.eye(num_features)\n",
    "    updated_item_features = np.zeros((num_features,num_items))\n",
    "    \n",
    "    for item,user in nz_item_userindices:\n",
    "        M = user_features[:,user]\n",
    "        \n",
    "        V = M @ train[item,user].T\n",
    "        A = M @ M.T + nnz_users_per_item[item] * lambda_I\n",
    "        W_star = np.linalg.solve(A,V)\n",
    "        updated_item_features[:,item] = np.copy(W_star.T)\n",
    "    return updated_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(train, test,num_features,lambda_user,lambda_item,max_weight=1.0,iterations=50):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    stop_criterion = 1e-5\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    it = 0\n",
    " \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    user_features_file_path = './data/user_features_%s_%s_%s_%s.npy' \\\n",
    "        % (iterations, num_features, lambda_user, lambda_item)\n",
    "\n",
    "    item_features_file_path = './data/item_features_%s_%s_%s_%s.npy' \\\n",
    "        % (iterations, num_features, lambda_user, lambda_item)\n",
    "    \n",
    "    if(os.path.exists(user_features_file_path) and os.path.exists(item_features_file_path)):\n",
    "        user_features = np.load(user_features_file_path)\n",
    "        item_features = np.load(item_features_file_path)\n",
    "\n",
    "        train_rmse = compute_error(train, user_features, item_features,nz_train)\n",
    "\n",
    "        test_rmse = compute_error(test, user_features, item_features,nz_test)\n",
    "        \n",
    "        print(\"Train RMSE: {tr_rmse}, test RMSE: {te_rmse}\" .format(tr_rmse=train_rmse, te_rmse=test_rmse))\n",
    "\n",
    "        return user_features, item_features\n",
    "    \n",
    "    # set seed\n",
    "#     np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features,max_weight)\n",
    "    \n",
    "    # get the number of non-zero ratings for each user and item\n",
    "    nnz_items_per_user,nnz_users_per_item = train.getnnz(axis=0),train.getnnz(axis=1)\n",
    "    \n",
    "    # group the indices by row or column index\n",
    "    _, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "    \n",
    "    # start ALS\n",
    "    while(it < iterations):\n",
    "        user_features = update_user_feature(train, item_features, lambda_user,\n",
    "                            nnz_items_per_user, nz_user_itemindices)\n",
    "        \n",
    "        item_features = update_item_feature(train, user_features, lambda_item,\n",
    "                            nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        train_rmse = compute_error(train,user_features,item_features,nz_train)\n",
    "        print(\"ALS training RMSE : {err}\".format(err=train_rmse))\n",
    "        error_list.append(train_rmse)\n",
    "        change = np.fabs(error_list[-1] - error_list[-2])\n",
    "        if (change > stop_criterion):\n",
    "            break;\n",
    "        it += 1\n",
    "        \n",
    "    \n",
    "    # evaluate the error in test set\n",
    "    \n",
    "    test_rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data after ALS: {}.\".format(test_rmse))   \n",
    "    \n",
    "    np.save(user_features_file_path, user_features)\n",
    "    np.save(item_features_file_path, item_features)\n",
    "    \n",
    "    return item_features,user_features,test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS training RMSE : 0.6804265713964545\n",
      "ALS training RMSE : 0.6160907382671245\n",
      "ALS training RMSE : 0.5672073485553909\n",
      "ALS training RMSE : 0.5497330414721812\n",
      "ALS training RMSE : 0.5415995602519763\n",
      "ALS training RMSE : 0.5368605368734932\n",
      "ALS training RMSE : 0.5337420789679976\n",
      "ALS training RMSE : 0.5315389413941832\n",
      "ALS training RMSE : 0.5299072035495632\n",
      "ALS training RMSE : 0.5286563915443282\n",
      "ALS training RMSE : 0.527671453213674\n",
      "ALS training RMSE : 0.5268784327757908\n",
      "ALS training RMSE : 0.5262276284246435\n",
      "ALS training RMSE : 0.5256845598481641\n",
      "ALS training RMSE : 0.5252247230577611\n",
      "ALS training RMSE : 0.5248303451995805\n",
      "ALS training RMSE : 0.5244882944256996\n",
      "ALS training RMSE : 0.524188703361501\n",
      "ALS training RMSE : 0.5239240474442701\n",
      "ALS training RMSE : 0.5236885153834951\n",
      "ALS training RMSE : 0.5234775679682886\n",
      "ALS training RMSE : 0.5232876201623021\n",
      "ALS training RMSE : 0.5231158065289976\n",
      "ALS training RMSE : 0.5229598055817284\n",
      "ALS training RMSE : 0.5228177077655134\n",
      "ALS training RMSE : 0.5226879168328483\n",
      "ALS training RMSE : 0.5225690770920017\n",
      "ALS training RMSE : 0.5224600205351942\n",
      "ALS training RMSE : 0.522359728881977\n",
      "ALS training RMSE : 0.5222673064451062\n",
      "RMSE on test data after ALS: 0.7155165793912632.\n"
     ]
    }
   ],
   "source": [
    "# num_features = 50\n",
    "# lambda_user = 0.2\n",
    "# lambda_item = 0.02\n",
    "# item_features,user_features,_ = ALS(train, test,num_features,lambda_user,lambda_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ALS_grid_search(train,test,seed=988):\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "#     lambda_users = np.linspace(0.01,0.21,num=21)\n",
    "#     lambda_items = np.linspace(0.01,0.21,num=21)\n",
    "#     nb_features = 50\n",
    "#     weights = np.linspace(1.0,3.0,num=30)\n",
    "    # for test\n",
    "    lambda_users = [0.01]\n",
    "    lambda_items = [0.2]\n",
    "    nb_features = 20\n",
    "    weights = [1.0]\n",
    "                    \n",
    "    best_weight = -1\n",
    "    best_lambda_item = -1\n",
    "    best_lambda_user = -1\n",
    "    best_num_feature = -1\n",
    "    best_rmse = 100\n",
    "    \n",
    "    newpath = r'./data' \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for i in range(20,nb_features+1):\n",
    "        num_features = i\n",
    "        for weight in weights:\n",
    "            for lambda_user in lambda_users:\n",
    "                for lambda_item in lambda_item:\n",
    "                    item_features,user_features,test_rmse = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "                    if(test_rmse < best_rmse):\n",
    "                        best_rmse = min_rmse\n",
    "                        bset_lambda_item = lambda_item\n",
    "                        best_lambda_user = lambda_user\n",
    "                        best_weight = weight\n",
    "                        best_num_feature = num_features\n",
    "                        best_rmse = test_rmse\n",
    "                        print(\"CHANGE=====>best rmse: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                              .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "    print(\"=======>>>>FINAL: BEST RMSE: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                              .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "    best_param = np.array([best_num_feature,best_weight,best_lambda_user,bset_lambda_item])\n",
    "    np.save(\"best_param_grid_search.npy\", best_param)\n",
    "    return best_num_feature,best_weight,best_lambda_user,bset_lambda_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ALS_random_search(train,test,seed=988):\n",
    "#     users_range = np.linspace(0.01,1,num=100)\n",
    "#     item_range = np.linspace(0.01,1,num=100)\n",
    "#     features_num_range = 60\n",
    "#     weight_range = np.linspace(1.0,3.0,num=60)\n",
    "    \n",
    "#     lambda_users = np.random.choice(users_range,60)\n",
    "#     lambda_items = np.random.choice(item_range,60)\n",
    "#     nb_features = np.random.choice(features_num_range,60)\n",
    "#     weights = np.random.choice(weight_range,60)\n",
    "    \n",
    "    # for test\n",
    "    lambda_users = [0.01]\n",
    "    lambda_items = [0.2]\n",
    "    nb_features = 20\n",
    "    weights = [1.0]\n",
    "    \n",
    "    best_weight = -1\n",
    "    best_lambda_item = -1\n",
    "    best_lambda_user = -1\n",
    "    best_num_feature = -1\n",
    "    best_rmse = 100\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    newpath = r'./data' \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for num_features,weight,lambda_user,lambda_item in zip(nb_features,weights,lambda_users,lambda_items):\n",
    "        item_features,user_features,test_rmse = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "        if(test_rmse < best_rmse):\n",
    "            best_rmse = min_rmse\n",
    "            bset_lambda_item = lambda_item\n",
    "            best_lambda_user = lambda_user\n",
    "            best_weight = weight\n",
    "            best_num_feature = num_features\n",
    "            best_rmse = test_rmse\n",
    "            print(\"CHANGE=====>best rmse: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                  .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "            \n",
    "    print(\"=======>>>> FINAL: BEST RMSE: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                              .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "    \n",
    "    best_param = np.array([best_num_feature,best_weight,best_lambda_user,bset_lambda_item])\n",
    "    np.save(\"best_param_random_search.npy\", best_param)\n",
    "    return best_num_feature,best_weight,best_lambda_user,bset_lambda_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ALS(train,test,num_features=None,lambda_user=None,lambda_item=None,weight=None,load_File=None):\n",
    "    if(load_File==1):\n",
    "        best_param = np.load(\"best_param_random_search.npy\")\n",
    "        num_features = best_param[0]\n",
    "        weight = best_param[1]\n",
    "        lambda_user = best_param[2]\n",
    "        lambda_item = best_param[3]\n",
    "    \n",
    "    item_features,user_features , _ = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "    predict_labels = item_features.T @ user_features\n",
    "    predict_labels[predict_labels > 5] = 5\n",
    "    predict_labels[predict_labels < 1] = 1\n",
    "    \n",
    "    predict = np.asarray(predict_labels)\n",
    "    np.save(\"predict.npy\",predict)\n",
    "    \n",
    "    return predict\n",
    "    # Generate the CSV submission file\n",
    "    #generate_submission(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(intest=0):\n",
    "    train_dataset = \"./data/data_train.csv\"\n",
    "    ratings = load_data(train_dataset)\n",
    "    num_items_per_user,num_users_per_item = get_number_per(ratings)\n",
    "    valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)\n",
    "    if(intest ==1):\n",
    "        num_features = 20\n",
    "        weight = 1.0\n",
    "        lambda_user = 0.2\n",
    "        lambda_item = 0.02\n",
    "    else:\n",
    "        num_features,weight,lambda_user,lambda_item = cv_ALS_random_search(train,test)\n",
    "#     num_features,weight,lambda_user,lambda_item = cv_ALS_grid_search(train,test)\n",
    "#     predict_ALS(train,test,load_File=1)\n",
    "#     np.random.seed(988)\n",
    "#     predict_ALS(train,test,num_features,lambda_user,lambda_item,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:941957\n",
      "Total number of nonzero elements in test data:234995\n",
      "ALS training RMSE : 0.9767730275311427\n",
      "RMSE on test data after ALS: 1.001994343538817.\n"
     ]
    }
   ],
   "source": [
    "#test function\n",
    "run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = \"./data/data_train.csv\"\n",
    "ratings = load_data(train_dataset)\n",
    "num_items_per_user,num_users_per_item = get_number_per(ratings)\n",
    "valid_ratings, train, test = split_data(\n",
    "ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)\n",
    "num_features,weight,lambda_user,lambda_item = cv_ALS_random_search(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features,weight,lambda_user,lambda_item = cv_ALS_grid_search(train,test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = \"./data/data_train.csv\"\n",
    "# ratings = load_data(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    \"\"\"read text file from path.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "\n",
    "def load_data(path_dataset):\n",
    "    \"\"\"Load data in text format, one rating per line, as in the kaggle competition.\"\"\"\n",
    "    data = read_txt(path_dataset)[1:]\n",
    "    return preprocess_data(data)\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"preprocessing the text data, conversion to numerical array format.\"\"\"\n",
    "    def deal_line(line):\n",
    "        pos, rating = line.split(',')\n",
    "        row, col = pos.split(\"_\")\n",
    "        row = row.replace(\"r\", \"\")\n",
    "        col = col.replace(\"c\", \"\")\n",
    "        return int(row), int(col), float(rating)\n",
    "\n",
    "    def statistics(data):\n",
    "        row = set([line[0] for line in data])\n",
    "        col = set([line[1] for line in data])\n",
    "        return min(row), max(row), min(col), max(col)\n",
    "\n",
    "    # parse each line\n",
    "    data = [deal_line(line) for line in data]\n",
    "\n",
    "    # do statistics on the dataset.\n",
    "    min_row, max_row, min_col, max_col = statistics(data)\n",
    "    print(\"number of items: {}, number of users: {}\".format(max_row, max_col))\n",
    "\n",
    "    # build rating matrix.\n",
    "    ratings = sp.lil_matrix((max_row, max_col))\n",
    "    for row, col, rating in data:\n",
    "        ratings[row - 1, col - 1] = rating\n",
    "    return ratings\n",
    "\n",
    "\n",
    "def group_by(data, index):\n",
    "    \"\"\"group list of list by a specific index.\"\"\"\n",
    "    sorted_data = sorted(data, key=lambda x: x[index])\n",
    "    groupby_data = groupby(sorted_data, lambda x: x[index])\n",
    "    return groupby_data\n",
    "\n",
    "\n",
    "def build_index_groups(train):\n",
    "    \"\"\"build groups for nnz rows and cols.\"\"\"\n",
    "    # row : items; cols: users\n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "\n",
    "    grouped_nz_train_byrow = group_by(nz_train, index=0) # group by items \n",
    "#     for g, value in grouped_nz_train_byrow:\n",
    "#         print(\"{}, {}\".format(g, list(value))) #value for g=0: (0, 1) (0, 2) (0, 3) index of all the users that rated the item 0\n",
    "    nz_row_colindices = [(g, np.array([v[1] for v in value])) # indices of all the users that rated item g\n",
    "                         for g, value in grouped_nz_train_byrow]\n",
    "    \n",
    "#     print(nz_row_colindices)\n",
    "\n",
    "    grouped_nz_train_bycol = group_by(nz_train, index=1) # group by users\n",
    "    nz_col_rowindices = [(g, np.array([v[0] for v in value])) # indices of all the movies rated by user g\n",
    "                         for g, value in grouped_nz_train_bycol]\n",
    "    return nz_train, nz_row_colindices, nz_col_rowindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_per(ratings):\n",
    "    \"\"\"plot the statistics result on raw rating data.\"\"\"\n",
    "    # do statistics.\n",
    "    num_items_per_user = np.array((ratings != 0).sum(axis=0)).flatten()\n",
    "    num_users_per_item = np.array((ratings != 0).sum(axis=1).T).flatten()\n",
    "    sorted_num_movies_per_user = np.sort(num_items_per_user)[::-1]\n",
    "    sorted_num_users_per_movie = np.sort(num_users_per_item)[::-1]\n",
    "    return num_items_per_user, num_users_per_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_items_per_user,num_users_per_item = get_number_per(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "#     # set seed\n",
    "#     np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "\n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split the data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "    \n",
    "    \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:941957\n",
      "Total number of nonzero elements in test data:234995\n"
     ]
    }
   ],
   "source": [
    "# valid_ratings, train, test = split_data(\n",
    "#     ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.lil.lil_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features,weight=1.0):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    num_item,num_user = train.shape\n",
    "    \n",
    "    user_features = weight * np.random.rand(num_features,num_user)\n",
    "    item_features = weight * np.random.rand(num_features,num_item)\n",
    "    \n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "    \n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    mse = 0\n",
    "    for row,col in nz:\n",
    "        user = user_features[:,col]\n",
    "        item = item_features[:,row]\n",
    "        mse += ((data[row,col] - user.T.dot(item))**2)\n",
    "    \n",
    "    rmse = np.sqrt(1.0*mse/len(nz))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_user[user] * lambda_user\"\"\"\n",
    "    # update and return user feature.\n",
    "    num_users = nnz_items_per_user.shape[0]\n",
    "    num_features = item_features.shape[0]\n",
    "    lambda_I = lambda_user * sp.eye(num_features)\n",
    "    updated_user_features = np.zeros((num_features,num_users))\n",
    "    \n",
    "    for user,item in nz_user_itemindices:\n",
    "        M = item_features[:,item]\n",
    "        \n",
    "        V = M @ train[item,user]\n",
    "        A = M @ M.T + nnz_items_per_user[user] * lambda_I\n",
    "        Z_star = np.linalg.solve(A,V)\n",
    "        updated_user_features[:,user] = np.copy(Z_star.T)\n",
    "    return updated_user_features\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_item[item] * lambda_item\"\"\"\n",
    "    # update and return item feature.\n",
    "    num_items = nnz_users_per_item.shape[0]\n",
    "    num_features = user_features.shape[0]\n",
    "    lambda_I = lambda_item * sp.eye(num_features)\n",
    "    updated_item_features = np.zeros((num_features,num_items))\n",
    "    \n",
    "    for item,user in nz_item_userindices:\n",
    "        M = user_features[:,user]\n",
    "        \n",
    "        V = M @ train[item,user].T\n",
    "        A = M @ M.T + nnz_users_per_item[item] * lambda_I\n",
    "        W_star = np.linalg.solve(A,V)\n",
    "        updated_item_features[:,item] = np.copy(W_star.T)\n",
    "    return updated_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(train, test,num_features,lambda_user,lambda_item,max_weight=1.0,iterations=50):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    stop_criterion = 1e-5\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    it = 0\n",
    " \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "#     user_features_file_path = './data/user_features_%s_%s_%s_%s.npy' \\\n",
    "#         % (iterations, num_features, lambda_user, lambda_item)\n",
    "\n",
    "#     item_features_file_path = './data/item_features_%s_%s_%s_%s.npy' \\\n",
    "#         % (iterations, num_features, lambda_user, lambda_item)\n",
    "    \n",
    "#     if(os.path.exists(user_features_file_path) and os.path.exists(item_features_file_path)):\n",
    "#         user_features = np.load(user_features_file_path)\n",
    "#         item_features = np.load(item_features_file_path)\n",
    "\n",
    "#         train_rmse = compute_error(train, user_features, item_features,nz_train)\n",
    "\n",
    "#         test_rmse = compute_error(test, user_features, item_features,nz_test)\n",
    "        \n",
    "#         print(\"Train RMSE: {tr_rmse}, test RMSE: {te_rmse}\" .format(tr_rmse=train_rmse, te_rmse=test_rmse))\n",
    "\n",
    "#         return user_features, item_features,test_rmse\n",
    "    \n",
    "    # set seed\n",
    "#     np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features,max_weight)\n",
    "    \n",
    "    # get the number of non-zero ratings for each user and item\n",
    "    nnz_items_per_user,nnz_users_per_item = train.getnnz(axis=0),train.getnnz(axis=1)\n",
    "    \n",
    "    # group the indices by row or column index\n",
    "    _, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "    \n",
    "    train_rmse = 0\n",
    "    # start ALS\n",
    "    while(it < iterations):\n",
    "        user_features = update_user_feature(train, item_features, lambda_user,\n",
    "                            nnz_items_per_user, nz_user_itemindices)\n",
    "        \n",
    "        item_features = update_item_feature(train, user_features, lambda_item,\n",
    "                            nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        train_rmse = compute_error(train,user_features,item_features,nz_train)\n",
    "#         print(\"ALS training RMSE : {err}\".format(err=train_rmse))\n",
    "        error_list.append(train_rmse)\n",
    "        change = np.fabs(error_list[-1] - error_list[-2])\n",
    "        if (change < stop_criterion):\n",
    "            print(\"Converge!\")\n",
    "            break;\n",
    "        it += 1\n",
    "        \n",
    "    print(\"ALS Final training RMSE : {err}\".format(err=train_rmse))\n",
    "    # evaluate the error in test set\n",
    "    \n",
    "    test_rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data after ALS: {}.\".format(test_rmse))   \n",
    "    \n",
    "#     np.save(user_features_file_path, user_features)\n",
    "#     np.save(item_features_file_path, item_features)\n",
    "    \n",
    "    return item_features,user_features,test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS training RMSE : 0.6804265713964545\n",
      "ALS training RMSE : 0.6160907382671245\n",
      "ALS training RMSE : 0.5672073485553909\n",
      "ALS training RMSE : 0.5497330414721812\n",
      "ALS training RMSE : 0.5415995602519763\n",
      "ALS training RMSE : 0.5368605368734932\n",
      "ALS training RMSE : 0.5337420789679976\n",
      "ALS training RMSE : 0.5315389413941832\n",
      "ALS training RMSE : 0.5299072035495632\n",
      "ALS training RMSE : 0.5286563915443282\n",
      "ALS training RMSE : 0.527671453213674\n",
      "ALS training RMSE : 0.5268784327757908\n",
      "ALS training RMSE : 0.5262276284246435\n",
      "ALS training RMSE : 0.5256845598481641\n",
      "ALS training RMSE : 0.5252247230577611\n",
      "ALS training RMSE : 0.5248303451995805\n",
      "ALS training RMSE : 0.5244882944256996\n",
      "ALS training RMSE : 0.524188703361501\n",
      "ALS training RMSE : 0.5239240474442701\n",
      "ALS training RMSE : 0.5236885153834951\n",
      "ALS training RMSE : 0.5234775679682886\n",
      "ALS training RMSE : 0.5232876201623021\n",
      "ALS training RMSE : 0.5231158065289976\n",
      "ALS training RMSE : 0.5229598055817284\n",
      "ALS training RMSE : 0.5228177077655134\n",
      "ALS training RMSE : 0.5226879168328483\n",
      "ALS training RMSE : 0.5225690770920017\n",
      "ALS training RMSE : 0.5224600205351942\n",
      "ALS training RMSE : 0.522359728881977\n",
      "ALS training RMSE : 0.5222673064451062\n",
      "RMSE on test data after ALS: 0.7155165793912632.\n"
     ]
    }
   ],
   "source": [
    "# num_features = 50\n",
    "# lambda_user = 0.2\n",
    "# lambda_item = 0.02\n",
    "# item_features,user_features,_ = ALS(train, test,num_features,lambda_user,lambda_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv_ALS_grid_search(train,test,seed=988):\n",
    "#     # set seed\n",
    "# #     np.random.seed(seed)\n",
    "    \n",
    "#     lambda_users = np.linspace(0.01,0.21,num=21)\n",
    "#     lambda_items = np.linspace(0.01,0.21,num=21)\n",
    "#     nb_features = 50\n",
    "#     weights = np.linspace(1.0,3.0,num=30)\n",
    "#     # for test\n",
    "# #     lambda_users = [0.01]\n",
    "# #     lambda_items = [0.2]\n",
    "# #     nb_features = 20\n",
    "# #     weights = [1.0]\n",
    "                    \n",
    "#     best_weight = -1\n",
    "#     best_lambda_item = -1\n",
    "#     best_lambda_user = -1\n",
    "#     best_num_feature = -1\n",
    "#     best_rmse = 100\n",
    "    \n",
    "#     newpath = r'./data' \n",
    "#     if not os.path.exists(newpath):\n",
    "#         os.makedirs(newpath)\n",
    "#     for i in range(20,nb_features+1):\n",
    "#         num_features = i\n",
    "#         for weight in weights:\n",
    "#             for lambda_user in lambda_users:\n",
    "#                 for lambda_item in lambda_items:\n",
    "#                     item_features,user_features,test_rmse = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "#                     if(test_rmse < best_rmse):\n",
    "#                         best_rmse = test_rmse\n",
    "#                         bset_lambda_item = lambda_item\n",
    "#                         best_lambda_user = lambda_user\n",
    "#                         best_weight = weight\n",
    "#                         best_num_feature = num_features\n",
    "#                         best_rmse = test_rmse\n",
    "#                         print(\"CHANGE=====>best rmse: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "#                               .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "#     print(\"=======>>>>FINAL: BEST RMSE: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "#                               .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "#     best_param = np.array([best_num_feature,best_weight,best_lambda_user,bset_lambda_item])\n",
    "#     np.save(\"best_param_grid_search.npy\", best_param)\n",
    "#     return best_num_feature,best_weight,best_lambda_user,bset_lambda_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ALS_random_search(train,test,seed=988):\n",
    "#     # set seed\n",
    "#     np.random.seed(seed)\n",
    "    users_range = np.linspace(0.01,1,num=100)\n",
    "    item_range = np.linspace(0.01,1,num=100)\n",
    "#     features_num_range = 60\n",
    "    features_num_range = np.linspace(1,60,num=60,dtype=np.int32)\n",
    "    weight_range = np.linspace(1.0,3.0,num=60)\n",
    "    \n",
    "    lambda_users = np.random.choice(users_range,60)\n",
    "    lambda_items = np.random.choice(item_range,60)\n",
    "    nb_features = np.random.choice(features_num_range,60)\n",
    "    weights = np.random.choice(weight_range,60)\n",
    "    \n",
    "    # for test\n",
    "#     lambda_users = [0.01]\n",
    "#     lambda_items = [0.2]\n",
    "#     nb_features = [20]\n",
    "#     weights = [1.0]\n",
    "    \n",
    "    best_weight = -1\n",
    "    best_lambda_item = -1\n",
    "    best_lambda_user = -1\n",
    "    best_num_feature = -1\n",
    "    best_rmse = 100\n",
    "    \n",
    "    k_fold = 5\n",
    "#     # set seed\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "    newpath = r'./data' \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    train_tr_list, test_tr_list = split_for_cv(train,p_test=0.2)\n",
    "    for num_features,weight,lambda_user,lambda_item in zip(nb_features,weights,lambda_users,lambda_items):\n",
    "        rmse_list = []\n",
    "        for train_tr,test_tr in zip(train_tr_list, test_tr_list): # 5-fold cv\n",
    "            item_features,user_features,test_tr_rmse = ALS(train_tr, test_tr,num_features,lambda_user,lambda_item,weight)\n",
    "#             print(\"test RMSE: {te_rmse}\" .format(te_rmse=test_tr_rmse))\n",
    "            rmse_list.append(test_tr_rmse)\n",
    "        test_rmse = np.mean(rmse_list)\n",
    "        if(test_rmse < best_rmse):\n",
    "            best_rmse = test_rmse\n",
    "            bset_lambda_item = lambda_item\n",
    "            best_lambda_user = lambda_user\n",
    "            best_weight = weight\n",
    "            best_num_feature = num_features\n",
    "            best_rmse = test_rmse\n",
    "            print(\"CHANGE=====>best rmse: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                  .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "            \n",
    "    print(\"=======>>>> FINAL: BEST RMSE: {},lambda_item :{},lambda_user:{},weight:{},num_feature:{}\"\\\n",
    "                              .format(best_rmse,bset_lambda_item,best_lambda_user,best_weight,best_num_feature))\n",
    "    \n",
    "    best_param = np.array([best_num_feature,best_weight,best_lambda_user,bset_lambda_item])\n",
    "    np.save(\"best_param_random_search.npy\", best_param)\n",
    "    return best_num_feature,best_weight,best_lambda_user,bset_lambda_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_for_cv(train,p_test=0.2,k_fold=5):\n",
    "    # init\n",
    "    num_rows, num_cols = train.shape\n",
    "    nz_items, nz_users = train.nonzero()\n",
    "    train_tr_list=[]\n",
    "    test_tr_list = []\n",
    "    # split the data\n",
    "    for k in range(k_fold):\n",
    "        train_tr = sp.lil_matrix((num_rows, num_cols))\n",
    "        test_tr = sp.lil_matrix((num_rows, num_cols))\n",
    "        for user in set(nz_users):\n",
    "            # randomly select a subset of ratings\n",
    "            row, col = train[:, user].nonzero()\n",
    "            selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "            residual = list(set(row) - set(selects))\n",
    "\n",
    "            # add to train set\n",
    "            train_tr[residual, user] = train[residual, user]\n",
    "\n",
    "            # add to test set\n",
    "            test_tr[selects, user] = train[selects, user]\n",
    "            \n",
    "        train_tr_list.append(train_tr)\n",
    "        test_tr_list.append(test_tr)\n",
    "        \n",
    "    return train_tr_list, test_tr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ALS(train,test,num_features=None,lambda_user=None,lambda_item=None,weight=None,load_File=None):\n",
    "    if(load_File==1):\n",
    "        best_param = np.load(\"best_param_random_search.npy\")\n",
    "        num_features = best_param[0]\n",
    "        weight = best_param[1]\n",
    "        lambda_user = best_param[2]\n",
    "        lambda_item = best_param[3]\n",
    "    \n",
    "    item_features,user_features , _ = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "    predict_labels = item_features.T @ user_features\n",
    "    predict_labels[predict_labels > 5] = 5\n",
    "    predict_labels[predict_labels < 1] = 1\n",
    "    \n",
    "    predict = np.asarray(predict_labels)\n",
    "    np.save(\"predict.npy\",predict)\n",
    "    \n",
    "    return predict\n",
    "    # Generate the CSV submission file\n",
    "    #generate_submission(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(intest=0,seed=988):\n",
    "    train_dataset = \"./data/data_train.csv\"\n",
    "    ratings = load_data(train_dataset)\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    num_items_per_user,num_users_per_item = get_number_per(ratings)\n",
    "    valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)\n",
    "    if(intest ==1):\n",
    "        num_features = 20\n",
    "        weight = 1.0\n",
    "        lambda_user = 0.2\n",
    "        lambda_item = 0.02\n",
    "    else:\n",
    "        num_features,weight,lambda_user,lambda_item = cv_ALS_random_search(train,test)\n",
    "#     num_features,weight,lambda_user,lambda_item = cv_ALS_grid_search(train,test)\n",
    "#     predict_ALS(train,test,load_File=1)\n",
    "#     np.random.seed(988)\n",
    "    predict_ALS(train,test,num_features,lambda_user,lambda_item,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:941957\n",
      "Total number of nonzero elements in test data:234995\n",
      "ALS training RMSE : 0.9767730275311427\n",
      "RMSE on test data after ALS: 1.001994343538817.\n"
     ]
    }
   ],
   "source": [
    "#test function\n",
    "run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:963742\n",
      "Total number of nonzero elements in test data:213210\n"
     ]
    }
   ],
   "source": [
    "train_dataset = \"./data/data_train.csv\"\n",
    "ratings = load_data(train_dataset)\n",
    "num_items_per_user,num_users_per_item = get_number_per(ratings)\n",
    "valid_ratings, train, test = split_data(\n",
    "ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS Final training RMSE : 0.8249949573586155\n",
      "RMSE on test data after ALS: 0.8242937198559628.\n",
      "test RMSE: 0.8242937198559628\n",
      "ALS Final training RMSE : 0.8252324088835039\n",
      "RMSE on test data after ALS: 0.8247501770710333.\n",
      "test RMSE: 0.8247501770710333\n",
      "ALS Final training RMSE : 0.8251761616150919\n",
      "RMSE on test data after ALS: 0.824538177162649.\n",
      "test RMSE: 0.824538177162649\n",
      "ALS Final training RMSE : 0.8253676057344289\n",
      "RMSE on test data after ALS: 0.8244818615498287.\n",
      "test RMSE: 0.8244818615498287\n",
      "ALS Final training RMSE : 0.8252385791149907\n",
      "RMSE on test data after ALS: 0.8243587954515158.\n",
      "test RMSE: 0.8243587954515158\n",
      "CHANGE=====>best rmse: 0.824484546218198,lambda_item :0.2,lambda_user:0.01,weight:1.0,num_feature:20\n",
      "=======>>>> FINAL: BEST RMSE: 0.824484546218198,lambda_item :0.2,lambda_user:0.01,weight:1.0,num_feature:20\n"
     ]
    }
   ],
   "source": [
    "num_features,weight,lambda_user,lambda_item = cv_ALS_random_search(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1.0085226256030653, test RMSE: 1.0327025362264135\n",
      "CHANGE=====>best rmse: 1.0327025362264135,lambda_item :0.2,lambda_user:0.01,weight:1.0,num_feature:20\n",
      "=======>>>>FINAL: BEST RMSE: 1.0327025362264135,lambda_item :0.2,lambda_user:0.01,weight:1.0,num_feature:20\n"
     ]
    }
   ],
   "source": [
    "num_features,weight,lambda_user,lambda_item = cv_ALS_grid_search(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:941957\n",
      "Total number of nonzero elements in test data:234995\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2844089848348634\n",
      "RMSE on test data after ALS: 1.2919185835400455.\n",
      "CHANGE=====>best rmse: 1.2919185835400455,lambda_item :0.9400000000000001,lambda_user:0.7100000000000001,weight:2.9661016949152543,num_feature:32\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0810079018281236\n",
      "RMSE on test data after ALS: 1.0906493535590387.\n",
      "CHANGE=====>best rmse: 1.0906493535590387,lambda_item :0.5700000000000001,lambda_user:0.33,weight:2.3220338983050848,num_feature:34\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1279576635076167\n",
      "RMSE on test data after ALS: 1.1370043252059976.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0872603753772792\n",
      "RMSE on test data after ALS: 1.096817235839338.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1142248310737406\n",
      "RMSE on test data after ALS: 1.1234365779761046.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0800060627212296\n",
      "RMSE on test data after ALS: 1.089661249357824.\n",
      "CHANGE=====>best rmse: 1.089661249357824,lambda_item :0.6,lambda_user:0.31,weight:1.5084745762711864,num_feature:2\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0087394937548997\n",
      "RMSE on test data after ALS: 1.019782961811328.\n",
      "CHANGE=====>best rmse: 1.019782961811328,lambda_item :0.98,lambda_user:0.04,weight:2.389830508474576,num_feature:32\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1337914596675172\n",
      "RMSE on test data after ALS: 1.1427699161495164.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0123368780246207\n",
      "RMSE on test data after ALS: 1.023097977330351.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.150098788648393\n",
      "RMSE on test data after ALS: 1.158892200599582.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1123284461819973\n",
      "RMSE on test data after ALS: 1.1215635173654905.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0215189117223804\n",
      "RMSE on test data after ALS: 1.0320978882668552.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1421259849367615\n",
      "RMSE on test data after ALS: 1.1510088665050233.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1609605416201598\n",
      "RMSE on test data after ALS: 1.169634977394835.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.312152602409695\n",
      "RMSE on test data after ALS: 1.3194380826721084.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.218916382768321\n",
      "RMSE on test data after ALS: 1.2270055145906829.\n",
      "ALS Final training RMSE : 0.8383804081611202\n",
      "RMSE on test data after ALS: 0.990812883994429.\n",
      "CHANGE=====>best rmse: 0.990812883994429,lambda_item :0.17,lambda_user:0.05,weight:1.1694915254237288,num_feature:55\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0275528068061304\n",
      "RMSE on test data after ALS: 1.0380207118134717.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0671224244193707\n",
      "RMSE on test data after ALS: 1.0769590007060428.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1013051365622633\n",
      "RMSE on test data after ALS: 1.1106785061115774.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0067786978459479\n",
      "RMSE on test data after ALS: 1.0176606383422235.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2002420111280576\n",
      "RMSE on test data after ALS: 1.2085113753205072.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0450494036510258\n",
      "RMSE on test data after ALS: 1.0552212541064658.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1303314210132194\n",
      "RMSE on test data after ALS: 1.1393501963968058.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3543705455969575\n",
      "RMSE on test data after ALS: 1.3613358444647463.\n",
      "ALS Final training RMSE : 0.8708284951009893\n",
      "RMSE on test data after ALS: 0.9901923370979466.\n",
      "CHANGE=====>best rmse: 0.9901923370979466,lambda_item :0.17,lambda_user:0.05,weight:1.8813559322033897,num_feature:31\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.231032143763445\n",
      "RMSE on test data after ALS: 1.2390081587117792.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1502722875668847\n",
      "RMSE on test data after ALS: 1.159063773148949.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0713462989099793\n",
      "RMSE on test data after ALS: 1.081122389144139.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2360830604732216\n",
      "RMSE on test data after ALS: 1.2440127607367832.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3217537480049832\n",
      "RMSE on test data after ALS: 1.3289642898094596.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0154541806427015\n",
      "RMSE on test data after ALS: 1.026151269694027.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1944586758272742\n",
      "RMSE on test data after ALS: 1.202785383484231.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.05675956806189\n",
      "RMSE on test data after ALS: 1.066749221348276.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0580419202540394\n",
      "RMSE on test data after ALS: 1.0680122459280805.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2129727148827962\n",
      "RMSE on test data after ALS: 1.2211184166652351.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1873802480496007\n",
      "RMSE on test data after ALS: 1.1957781717652018.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0991616726285085\n",
      "RMSE on test data after ALS: 1.1085624966137408.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1926914225322145\n",
      "RMSE on test data after ALS: 1.2010358029846453.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1171962840361267\n",
      "RMSE on test data after ALS: 1.1263717485649103.\n",
      "ALS Final training RMSE : 0.5700747599920404\n",
      "RMSE on test data after ALS: 1.2500563416824106.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3247140514769014\n",
      "RMSE on test data after ALS: 1.3319017476215746.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.031993284824586\n",
      "RMSE on test data after ALS: 1.0423827740744627.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2006189305907868\n",
      "RMSE on test data after ALS: 1.2088845832470796.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0794738969553124\n",
      "RMSE on test data after ALS: 1.0891363977820991.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1304648289146695\n",
      "RMSE on test data after ALS: 1.1394820425662306.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.032524791224025\n",
      "RMSE on test data after ALS: 1.0429050618091644.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1171931481610595\n",
      "RMSE on test data after ALS: 1.1263686509832365.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1862393543889134\n",
      "RMSE on test data after ALS: 1.19464886523835.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2471166622039218\n",
      "RMSE on test data after ALS: 1.2549468500371.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.297391640091953\n",
      "RMSE on test data after ALS: 1.3047949228097588.\n",
      "ALS Final training RMSE : 0.9585648638486614\n",
      "RMSE on test data after ALS: 0.9954155612185944.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2880813079251154\n",
      "RMSE on test data after ALS: 1.295560569626203.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1643169556513309\n",
      "RMSE on test data after ALS: 1.1729552774078305.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0309220129189307\n",
      "RMSE on test data after ALS: 1.0413301880846526.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2223663740680366\n",
      "RMSE on test data after ALS: 1.2304229994548794.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.301718422232984\n",
      "RMSE on test data after ALS: 1.3090868416618842.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0764025497247127\n",
      "RMSE on test data after ALS: 1.0861075586877298.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9851429551933328\n",
      "RMSE on test data after ALS: 1.0024703092684057.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1184535111018934\n",
      "RMSE on test data after ALS: 1.127613720185237.\n",
      "=======>>>> FINAL: BEST RMSE: 0.9901923370979466,lambda_item :0.17,lambda_user:0.05,weight:1.8813559322033897,num_feature:31\n",
      "Train RMSE: 0.8708284951009893, test RMSE: 0.9901923370979466\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After modifying cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:963742\n",
      "Total number of nonzero elements in test data:213210\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3717441129343997\n",
      "RMSE on test data after ALS: 1.3787216172274093.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3719836828840728\n",
      "RMSE on test data after ALS: 1.3771756192963431.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3716211007834207\n",
      "RMSE on test data after ALS: 1.3810476885851843.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.371656746775496\n",
      "RMSE on test data after ALS: 1.3804117114326881.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3717704761187886\n",
      "RMSE on test data after ALS: 1.3810379518481368.\n",
      "CHANGE=====>best rmse: 1.3796789176779523,lambda_item :0.9,lambda_user:1.0,weight:1.2033898305084745,num_feature:46\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.030223332824094\n",
      "RMSE on test data after ALS: 1.0418586548831528.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0305550112821185\n",
      "RMSE on test data after ALS: 1.0401050490159807.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0300694079755695\n",
      "RMSE on test data after ALS: 1.043178191202724.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.030112045109483\n",
      "RMSE on test data after ALS: 1.042673896152528.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0302550625636477\n",
      "RMSE on test data after ALS: 1.0426641126670781.\n",
      "CHANGE=====>best rmse: 1.0420959807842927,lambda_item :0.92,lambda_user:0.09,weight:2.7627118644067794,num_feature:59\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2073314163040743\n",
      "RMSE on test data after ALS: 1.2160246008632873.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2076087965731899\n",
      "RMSE on test data after ALS: 1.2143545410774432.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2071963390047322\n",
      "RMSE on test data after ALS: 1.218063067011129.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2072349927270623\n",
      "RMSE on test data after ALS: 1.2174360411505887.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2073604528610644\n",
      "RMSE on test data after ALS: 1.2178638173921976.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0859598868165994\n",
      "RMSE on test data after ALS: 1.096443841839059.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.086272553739793\n",
      "RMSE on test data after ALS: 1.094698535121059.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0858127720326656\n",
      "RMSE on test data after ALS: 1.0980918817564862.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.085854009593324\n",
      "RMSE on test data after ALS: 1.0975191627626488.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.085990928338378\n",
      "RMSE on test data after ALS: 1.0977007319463459.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0965475537460538\n",
      "RMSE on test data after ALS: 1.106845776526823.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0968568149025784\n",
      "RMSE on test data after ALS: 1.1051051905582985.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0964016045687852\n",
      "RMSE on test data after ALS: 1.1085400011683957.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0964426378499632\n",
      "RMSE on test data after ALS: 1.1079591330798728.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0965784050418066\n",
      "RMSE on test data after ALS: 1.1081686403437956.\n",
      "ALS Final training RMSE : 0.8432458264482428\n",
      "RMSE on test data after ALS: 0.9964832383146077.\n",
      "ALS Final training RMSE : 0.8437543794807405\n",
      "RMSE on test data after ALS: 0.994757821256834.\n",
      "ALS Final training RMSE : 0.8432983202600488\n",
      "RMSE on test data after ALS: 0.99668992585385.\n",
      "ALS Final training RMSE : 0.843625068762086\n",
      "RMSE on test data after ALS: 0.9979007242134388.\n",
      "ALS Final training RMSE : 0.8433621870508415\n",
      "RMSE on test data after ALS: 0.9970501719567648.\n",
      "CHANGE=====>best rmse: 0.996576376319099,lambda_item :0.09999999999999999,lambda_user:0.09,weight:2.016949152542373,num_feature:33\n",
      "ALS Final training RMSE : 0.8811196986966934\n",
      "RMSE on test data after ALS: 0.9942879168934942.\n",
      "ALS Final training RMSE : 0.8814315987228748\n",
      "RMSE on test data after ALS: 0.9925672376348489.\n",
      "ALS Final training RMSE : 0.8810763783854448\n",
      "RMSE on test data after ALS: 0.9949749120232075.\n",
      "ALS Final training RMSE : 0.8812091333086953\n",
      "RMSE on test data after ALS: 0.9952195937871587.\n",
      "ALS Final training RMSE : 0.881300104860349\n",
      "RMSE on test data after ALS: 0.9945861844017719.\n",
      "CHANGE=====>best rmse: 0.9943271689480963,lambda_item :0.03,lambda_user:0.4,weight:2.830508474576271,num_feature:35\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0803050035416175\n",
      "RMSE on test data after ALS: 1.090891461959826.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0806195027291068\n",
      "RMSE on test data after ALS: 1.0891438906638298.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0801572340278238\n",
      "RMSE on test data after ALS: 1.0925133004590009.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0801986048965913\n",
      "RMSE on test data after ALS: 1.0919453981864964.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0803361224998684\n",
      "RMSE on test data after ALS: 1.092111227538543.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1396609264659034\n",
      "RMSE on test data after ALS: 1.1492712762056334.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1399570543609447\n",
      "RMSE on test data after ALS: 1.1475548518601462.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.139519529645155\n",
      "RMSE on test data after ALS: 1.1511240497419415.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1395595766008964\n",
      "RMSE on test data after ALS: 1.1505183294659393.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1396910700353489\n",
      "RMSE on test data after ALS: 1.1508261138651168.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.232915984111116\n",
      "RMSE on test data after ALS: 1.241301281366645.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2331868189809523\n",
      "RMSE on test data after ALS: 1.2396500485347501.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.232783064965901\n",
      "RMSE on test data after ALS: 1.2433965557882054.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2328212153956415\n",
      "RMSE on test data after ALS: 1.2427650755804605.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2329445950888143\n",
      "RMSE on test data after ALS: 1.2432307410186036.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.068525311515355\n",
      "RMSE on test data after ALS: 1.0793335867544822.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0688437130983652\n",
      "RMSE on test data after ALS: 1.0775820502395166.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0683761854851517\n",
      "RMSE on test data after ALS: 1.0808969616717983.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.068417809402478\n",
      "RMSE on test data after ALS: 1.0803402171822711.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0685566146832737\n",
      "RMSE on test data after ALS: 1.080471242662569.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1316124861227252\n",
      "RMSE on test data after ALS: 1.1413439907193548.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1319109857668614\n",
      "RMSE on test data after ALS: 1.1396225989869337.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1314702800746776\n",
      "RMSE on test data after ALS: 1.1431701689913298.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1315105067570643\n",
      "RMSE on test data after ALS: 1.1425682366588903.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1316427824303321\n",
      "RMSE on test data after ALS: 1.1428592888430769.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0728382079237557\n",
      "RMSE on test data after ALS: 1.0835638924529905.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0731551752097235\n",
      "RMSE on test data after ALS: 1.0818136870320672.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0726896013541574\n",
      "RMSE on test data after ALS: 1.0851493507965948.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0727311357025116\n",
      "RMSE on test data after ALS: 1.0845883300531916.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0728694669530612\n",
      "RMSE on test data after ALS: 1.0847324418866304.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2727662858412672\n",
      "RMSE on test data after ALS: 1.280706035801407.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2730273281978428\n",
      "RMSE on test data after ALS: 1.2790846431617544.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2726363409860402\n",
      "RMSE on test data after ALS: 1.2828790567525123.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2726738633024692\n",
      "RMSE on test data after ALS: 1.282243508474793.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.272794158615388\n",
      "RMSE on test data after ALS: 1.282762086582667.\n",
      "ALS Final training RMSE : 0.8825894126878895\n",
      "RMSE on test data after ALS: 0.9940058716033706.\n",
      "ALS Final training RMSE : 0.8829060497754935\n",
      "RMSE on test data after ALS: 0.9926714045068703.\n",
      "ALS Final training RMSE : 0.8824470605405601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data after ALS: 0.994391042085989.\n",
      "ALS Final training RMSE : 0.8823597221463976\n",
      "RMSE on test data after ALS: 0.9953297193337383.\n",
      "ALS Final training RMSE : 0.882561263025589\n",
      "RMSE on test data after ALS: 0.9946929362172725.\n",
      "CHANGE=====>best rmse: 0.9942181947494481,lambda_item :0.47000000000000003,lambda_user:0.02,weight:2.1864406779661016,num_feature:20\n",
      "ALS Final training RMSE : 0.7888157640965784\n",
      "RMSE on test data after ALS: 1.0024889438794549.\n",
      "ALS Final training RMSE : 0.7892524800546797\n",
      "RMSE on test data after ALS: 1.0009379081076568.\n",
      "ALS Final training RMSE : 0.7888493387029238\n",
      "RMSE on test data after ALS: 1.003050101822153.\n",
      "ALS Final training RMSE : 0.7890895452166216\n",
      "RMSE on test data after ALS: 1.0037604482509932.\n",
      "ALS Final training RMSE : 0.7890850012752084\n",
      "RMSE on test data after ALS: 1.0035713010534035.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1598784543255796\n",
      "RMSE on test data after ALS: 1.1691969296706228.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1601687327851786\n",
      "RMSE on test data after ALS: 1.1674935931312982.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.159739030424066\n",
      "RMSE on test data after ALS: 1.1711116763791876.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1597786571119073\n",
      "RMSE on test data after ALS: 1.1704978149968646.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1599082646312793\n",
      "RMSE on test data after ALS: 1.1708450331249276.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0398382277369502\n",
      "RMSE on test data after ALS: 1.051246353637571.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0401665027386633\n",
      "RMSE on test data after ALS: 1.0494910953151255.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0396856107584365\n",
      "RMSE on test data after ALS: 1.0526375238101675.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0397279047224095\n",
      "RMSE on test data after ALS: 1.0521168573982753.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0398699852306557\n",
      "RMSE on test data after ALS: 1.052147830254519.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.089561026768887\n",
      "RMSE on test data after ALS: 1.0999809365822428.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0898724843004886\n",
      "RMSE on test data after ALS: 1.0982371229002523.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0894145480597721\n",
      "RMSE on test data after ALS: 1.1016453143403266.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.089460908513281\n",
      "RMSE on test data after ALS: 1.101074827188275.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0895919722322074\n",
      "RMSE on test data after ALS: 1.1012607233328933.\n",
      "ALS Final training RMSE : 0.9291105872949267\n",
      "RMSE on test data after ALS: 0.9951275074937787.\n",
      "ALS Final training RMSE : 0.9294093484866862\n",
      "RMSE on test data after ALS: 0.9937839990274078.\n",
      "ALS Final training RMSE : 0.9290733639846575\n",
      "RMSE on test data after ALS: 0.9959339488477307.\n",
      "ALS Final training RMSE : 0.9289777550110704\n",
      "RMSE on test data after ALS: 0.9961292025015185.\n",
      "ALS Final training RMSE : 0.9293582909912877\n",
      "RMSE on test data after ALS: 0.9956793031373903.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2658488169909141\n",
      "RMSE on test data after ALS: 1.2738631802715783.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2661115694313994\n",
      "RMSE on test data after ALS: 1.2722366248429433.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2657185108420184\n",
      "RMSE on test data after ALS: 1.2760236546364077.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.265755968137067\n",
      "RMSE on test data after ALS: 1.2753884345910635.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2658768337309185\n",
      "RMSE on test data after ALS: 1.2758983505082955.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.039549086841185\n",
      "RMSE on test data after ALS: 1.050963809082924.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0398774619639166\n",
      "RMSE on test data after ALS: 1.0492085712052082.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0393964202835264\n",
      "RMSE on test data after ALS: 1.0523529430200744.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0394387197370123\n",
      "RMSE on test data after ALS: 1.0518327252971449.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.039580824607852\n",
      "RMSE on test data after ALS: 1.051862533468006.\n",
      "ALS Final training RMSE : 0.9792590545265343\n",
      "RMSE on test data after ALS: 1.003970401336038.\n",
      "ALS Final training RMSE : 0.9794878271596881\n",
      "RMSE on test data after ALS: 1.002334800802896.\n",
      "ALS Final training RMSE : 0.979270068387588\n",
      "RMSE on test data after ALS: 1.0049407111639288.\n",
      "ALS Final training RMSE : 0.9792475292206249\n",
      "RMSE on test data after ALS: 1.00455164978122.\n",
      "ALS Final training RMSE : 0.9794082463191688\n",
      "RMSE on test data after ALS: 1.0044153549195944.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0550916730916078\n",
      "RMSE on test data after ALS: 1.0661687588054454.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0554146273805027\n",
      "RMSE on test data after ALS: 1.0644142047911662.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0549409560856589\n",
      "RMSE on test data after ALS: 1.067657707349487.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.054982754131547\n",
      "RMSE on test data after ALS: 1.06711586758629.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0551231099958243\n",
      "RMSE on test data after ALS: 1.0672032459158811.\n",
      "ALS Final training RMSE : 0.8240999610288249\n",
      "RMSE on test data after ALS: 1.0094318284579733.\n",
      "ALS Final training RMSE : 0.8242261484316795\n",
      "RMSE on test data after ALS: 1.0084497950047544.\n",
      "ALS Final training RMSE : 0.8239600628655154\n",
      "RMSE on test data after ALS: 1.0100304927595851.\n",
      "ALS Final training RMSE : 0.8241815532022585\n",
      "RMSE on test data after ALS: 1.0117420026005866.\n",
      "ALS Final training RMSE : 0.8241380810091462\n",
      "RMSE on test data after ALS: 1.0108679924405257.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0823479490575454\n",
      "RMSE on test data after ALS: 1.09289709549109.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0826617915151846\n",
      "RMSE on test data after ALS: 1.0911503258298232.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.082200425160982\n",
      "RMSE on test data after ALS: 1.0945285382549277.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0822417413678453\n",
      "RMSE on test data after ALS: 1.0939588513376064.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0823790453143063\n",
      "RMSE on test data after ALS: 1.0941304384569233.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9749947853963775\n",
      "RMSE on test data after ALS: 1.0028003778153756.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.975349002505029\n",
      "RMSE on test data after ALS: 1.0011669491505415.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9750656614243095\n",
      "RMSE on test data after ALS: 1.0037479258669615.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9750355940002089\n",
      "RMSE on test data after ALS: 1.0033976373765985.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9750944957530087\n",
      "RMSE on test data after ALS: 1.0032217503658782.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.219738871653936\n",
      "RMSE on test data after ALS: 1.2282804371042761.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2200130570872902\n",
      "RMSE on test data after ALS: 1.2266194669093622.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2196048383516962\n",
      "RMSE on test data after ALS: 1.2303471787697187.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2196432813627205\n",
      "RMSE on test data after ALS: 1.229717832278493.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2197677120269566\n",
      "RMSE on test data after ALS: 1.2301643975596575.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3323441532411326\n",
      "RMSE on test data after ALS: 1.3396829420988239.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3325920487307277\n",
      "RMSE on test data after ALS: 1.3381071219196046.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3322180299508224\n",
      "RMSE on test data after ALS: 1.3419533296137613.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.332255059104836\n",
      "RMSE on test data after ALS: 1.341316782630253.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.332371296778746\n",
      "RMSE on test data after ALS: 1.3419036553884132.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1635945138033676\n",
      "RMSE on test data after ALS: 1.1728611451702415.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1638837534503408\n",
      "RMSE on test data after ALS: 1.1711603133588935.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1634554569504953\n",
      "RMSE on test data after ALS: 1.174786616522483.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1634950063340002\n",
      "RMSE on test data after ALS: 1.17417144588758.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1636242709624776\n",
      "RMSE on test data after ALS: 1.1745255364658653.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2942997769735989\n",
      "RMSE on test data after ALS: 1.302014011758711.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2945559478032431\n",
      "RMSE on test data after ALS: 1.3004090831264206.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2941714950842278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data after ALS: 1.3042247345856814.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2942085124200797\n",
      "RMSE on test data after ALS: 1.3035879434992563.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2943273868802276\n",
      "RMSE on test data after ALS: 1.304132661810086.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0571311020863674\n",
      "RMSE on test data after ALS: 1.0681661531921085.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0574533586252757\n",
      "RMSE on test data after ALS: 1.0664119351746668.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0569806137212305\n",
      "RMSE on test data after ALS: 1.0696669999167603.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0570225082000355\n",
      "RMSE on test data after ALS: 1.0691228355193025.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0571625980808363\n",
      "RMSE on test data after ALS: 1.0692171303931108.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1258296840318454\n",
      "RMSE on test data after ALS: 1.1356501314016338.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.126129896336141\n",
      "RMSE on test data after ALS: 1.133925266389459.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1256868816207422\n",
      "RMSE on test data after ALS: 1.137456437510769.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1257272444581117\n",
      "RMSE on test data after ALS: 1.1368574461373389.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1258600695178906\n",
      "RMSE on test data after ALS: 1.137136047784755.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.309907842385734\n",
      "RMSE on test data after ALS: 1.3174646559610186.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.310160528549767\n",
      "RMSE on test data after ALS: 1.3158716359525964.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3097807760569866\n",
      "RMSE on test data after ALS: 1.3197010727039402.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3098174795453348\n",
      "RMSE on test data after ALS: 1.319063845722375.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.3099352003209528\n",
      "RMSE on test data after ALS: 1.3196263538634985.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1322346030852115\n",
      "RMSE on test data after ALS: 1.1419566352269097.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.132532933595386\n",
      "RMSE on test data after ALS: 1.1402356366050392.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.132092647979252\n",
      "RMSE on test data after ALS: 1.1437850975510553.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1321326506157028\n",
      "RMSE on test data after ALS: 1.143182652851508.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1322650678691775\n",
      "RMSE on test data after ALS: 1.1434752215602857.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9954013191919425\n",
      "RMSE on test data after ALS: 1.0115857103050605.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.995623105019458\n",
      "RMSE on test data after ALS: 1.0098765158221417.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9952781746911222\n",
      "RMSE on test data after ALS: 1.012634869584437.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9954083930473439\n",
      "RMSE on test data after ALS: 1.012220007554071.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9954689776501553\n",
      "RMSE on test data after ALS: 1.0120866594486955.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0970032858414769\n",
      "RMSE on test data after ALS: 1.1072936840459797.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.097312429803796\n",
      "RMSE on test data after ALS: 1.1055533420987196.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0968574059371217\n",
      "RMSE on test data after ALS: 1.1089898397517612.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0968983934910028\n",
      "RMSE on test data after ALS: 1.1084086066014092.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.09703416562531\n",
      "RMSE on test data after ALS: 1.1086193302090934.\n",
      "ALS Final training RMSE : 0.9792371298949081\n",
      "RMSE on test data after ALS: 1.0039799221437737.\n",
      "ALS Final training RMSE : 0.9795423036838705\n",
      "RMSE on test data after ALS: 1.0023442081467329.\n",
      "ALS Final training RMSE : 0.9792452365228556\n",
      "RMSE on test data after ALS: 1.0049340135718487.\n",
      "ALS Final training RMSE : 0.9790936837305159\n",
      "RMSE on test data after ALS: 1.00454611517839.\n",
      "ALS Final training RMSE : 0.9793218316994299\n",
      "RMSE on test data after ALS: 1.0044114546625853.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9999781335300286\n",
      "RMSE on test data after ALS: 1.0146889328053132.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0002517597852356\n",
      "RMSE on test data after ALS: 1.0129863550182654.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9998562446614366\n",
      "RMSE on test data after ALS: 1.0157719046320817.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0000164418098207\n",
      "RMSE on test data after ALS: 1.0153809406701348.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0000685913014773\n",
      "RMSE on test data after ALS: 1.0152377912278732.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0793519866429249\n",
      "RMSE on test data after ALS: 1.0899559634462546.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0796665466437931\n",
      "RMSE on test data after ALS: 1.0882077861576394.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0792044790315647\n",
      "RMSE on test data after ALS: 1.091573641752454.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.079246585676032\n",
      "RMSE on test data after ALS: 1.0910072868961052.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0793830022509507\n",
      "RMSE on test data after ALS: 1.0911692263011996.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0895191240318378\n",
      "RMSE on test data after ALS: 1.0999397735741707.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.089830636811318\n",
      "RMSE on test data after ALS: 1.0981959825543688.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0893724012884916\n",
      "RMSE on test data after ALS: 1.1016037300146129.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0894135596819403\n",
      "RMSE on test data after ALS: 1.101028146950277.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0895500913251472\n",
      "RMSE on test data after ALS: 1.1012193160342052.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0602177976383715\n",
      "RMSE on test data after ALS: 1.0711901019290344.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0605390336441538\n",
      "RMSE on test data after ALS: 1.0694365113897495.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0600676966334648\n",
      "RMSE on test data after ALS: 1.0727085570910453.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0601095269216743\n",
      "RMSE on test data after ALS: 1.072160799159871.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.060249232568586\n",
      "RMSE on test data after ALS: 1.0722653474793833.\n",
      "ALS Final training RMSE : 0.9625110827312108\n",
      "RMSE on test data after ALS: 1.0001137788348842.\n",
      "ALS Final training RMSE : 0.9626762954574937\n",
      "RMSE on test data after ALS: 0.9984908181102328.\n",
      "ALS Final training RMSE : 0.9624261624920817\n",
      "RMSE on test data after ALS: 1.0010197639894882.\n",
      "ALS Final training RMSE : 0.9625652604758745\n",
      "RMSE on test data after ALS: 1.0007995769018818.\n",
      "ALS Final training RMSE : 0.9626591442878228\n",
      "RMSE on test data after ALS: 1.000539891624108.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2648971801776687\n",
      "RMSE on test data after ALS: 1.2729218937586764.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2651602112633933\n",
      "RMSE on test data after ALS: 1.2712946705633001.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2647668144251873\n",
      "RMSE on test data after ALS: 1.2750806078270105.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2648043515435237\n",
      "RMSE on test data after ALS: 1.2744455271499946.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2649252733376346\n",
      "RMSE on test data after ALS: 1.2749542143050994.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0476812896636192\n",
      "RMSE on test data after ALS: 1.0589152924731857.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0480068139320213\n",
      "RMSE on test data after ALS: 1.0571599709221065.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.047529654667296\n",
      "RMSE on test data after ALS: 1.060358784462828.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0475717615587758\n",
      "RMSE on test data after ALS: 1.0598266826189036.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.047712929708927\n",
      "RMSE on test data after ALS: 1.0598876833581767.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0136085033117954\n",
      "RMSE on test data after ALS: 1.0256875769398524.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.013946196788018\n",
      "RMSE on test data after ALS: 1.02394326673386.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0134526361043548\n",
      "RMSE on test data after ALS: 1.0268549631402657.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.013495239230229\n",
      "RMSE on test data after ALS: 1.0263871383970722.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0136406085312144\n",
      "RMSE on test data after ALS: 1.026293183336326.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0430185638574283\n",
      "RMSE on test data after ALS: 1.0543549965170445.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0433457212255342\n",
      "RMSE on test data after ALS: 1.052599592580178.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0428663422866447\n",
      "RMSE on test data after ALS: 1.0557679557258006.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge!\n",
      "ALS Final training RMSE : 1.0429085613609677\n",
      "RMSE on test data after ALS: 1.0552424793556556.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0430502747878714\n",
      "RMSE on test data after ALS: 1.055285927012656.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2252335771956464\n",
      "RMSE on test data after ALS: 1.2337094017332895.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2255061943281227\n",
      "RMSE on test data after ALS: 1.232052318672762.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2250999964542377\n",
      "RMSE on test data after ALS: 1.235788210304894.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.22513825452332\n",
      "RMSE on test data after ALS: 1.235157863897624.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.2252621923450455\n",
      "RMSE on test data after ALS: 1.235612448486516.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0857791782090034\n",
      "RMSE on test data after ALS: 1.0962663715737886.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.086091922712234\n",
      "RMSE on test data after ALS: 1.0945210083022094.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0856320466134841\n",
      "RMSE on test data after ALS: 1.0979135951495236.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0856733030783818\n",
      "RMSE on test data after ALS: 1.0973410394881642.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0858102396871867\n",
      "RMSE on test data after ALS: 1.0975221123483745.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0160878407745737\n",
      "RMSE on test data after ALS: 1.0280951430626082.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0164246908213466\n",
      "RMSE on test data after ALS: 1.026348765782595.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0159321385325377\n",
      "RMSE on test data after ALS: 1.0292882902807083.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0159749552817374\n",
      "RMSE on test data after ALS: 1.0288142227122516.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0161199086776014\n",
      "RMSE on test data after ALS: 1.0287343448769206.\n",
      "ALS Final training RMSE : 0.8144529206816368\n",
      "RMSE on test data after ALS: 0.9967638870493938.\n",
      "ALS Final training RMSE : 0.8150286104201329\n",
      "RMSE on test data after ALS: 0.995121409315461.\n",
      "ALS Final training RMSE : 0.814377475011575\n",
      "RMSE on test data after ALS: 0.9974627936835347.\n",
      "ALS Final training RMSE : 0.81473688317033\n",
      "RMSE on test data after ALS: 0.9981198808700614.\n",
      "ALS Final training RMSE : 0.8147880104812008\n",
      "RMSE on test data after ALS: 0.9976075818912172.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0283694925514066\n",
      "RMSE on test data after ALS: 1.0400506904130737.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.028701887589444\n",
      "RMSE on test data after ALS: 1.0382977060700205.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0282154616467636\n",
      "RMSE on test data after ALS: 1.0413554202587751.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.028257990306522\n",
      "RMSE on test data after ALS: 1.0408544566040092.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0284014213247556\n",
      "RMSE on test data after ALS: 1.0408364660654201.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1446733387018715\n",
      "RMSE on test data after ALS: 1.1542097141453334.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1449680353273277\n",
      "RMSE on test data after ALS: 1.152496491870707.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1445324704327882\n",
      "RMSE on test data after ALS: 1.1560785007947385.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1445724334915954\n",
      "RMSE on test data after ALS: 1.1554706112697544.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.144703426345845\n",
      "RMSE on test data after ALS: 1.1557884824730664.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0393068380058061\n",
      "RMSE on test data after ALS: 1.0507270972627398.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0396354748570047\n",
      "RMSE on test data after ALS: 1.0489720515260041.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.03915435663673\n",
      "RMSE on test data after ALS: 1.0521147427032176.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0391967682915666\n",
      "RMSE on test data after ALS: 1.0515950079810563.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0393387989278693\n",
      "RMSE on test data after ALS: 1.0516237459351339.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.044365581349457\n",
      "RMSE on test data after ALS: 1.0556721069844943.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0446923070167957\n",
      "RMSE on test data after ALS: 1.053916732236508.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0442135314847276\n",
      "RMSE on test data after ALS: 1.0570940554471913.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0442556856763971\n",
      "RMSE on test data after ALS: 1.0565665828939939.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0443972661823553\n",
      "RMSE on test data after ALS: 1.0566152114010998.\n",
      "ALS Final training RMSE : 0.9888323016486019\n",
      "RMSE on test data after ALS: 1.0079956459988244.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.989166624483649\n",
      "RMSE on test data after ALS: 1.0063257172574025.\n",
      "ALS Final training RMSE : 0.9887373530321737\n",
      "RMSE on test data after ALS: 1.009007203343302.\n",
      "ALS Final training RMSE : 0.9889476280576063\n",
      "RMSE on test data after ALS: 1.0086058965421667.\n",
      "ALS Final training RMSE : 0.9889093127312858\n",
      "RMSE on test data after ALS: 1.0084561485106631.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0405131197688773\n",
      "RMSE on test data after ALS: 1.051905900795438.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0408411654804883\n",
      "RMSE on test data after ALS: 1.050150604819232.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0403606003249541\n",
      "RMSE on test data after ALS: 1.0533017781185892.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.040402860608257\n",
      "RMSE on test data after ALS: 1.052780052338867.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0405448771018777\n",
      "RMSE on test data after ALS: 1.0528137221013874.\n",
      "ALS Final training RMSE : 0.9933256806697471\n",
      "RMSE on test data after ALS: 1.0102084542007912.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9936780253724568\n",
      "RMSE on test data after ALS: 1.0085594572567358.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9933383559656572\n",
      "RMSE on test data after ALS: 1.0113111030680642.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9935285835573027\n",
      "RMSE on test data after ALS: 1.0109041989566148.\n",
      "Converge!\n",
      "ALS Final training RMSE : 0.9934550086880293\n",
      "RMSE on test data after ALS: 1.0107253007559285.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1827640388699003\n",
      "RMSE on test data after ALS: 1.191771357310744.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1830479633003521\n",
      "RMSE on test data after ALS: 1.1900837209816253.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1826267760364315\n",
      "RMSE on test data after ALS: 1.1937491460823177.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1826659130813395\n",
      "RMSE on test data after ALS: 1.1931280110917668.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.1827934860856308\n",
      "RMSE on test data after ALS: 1.1935159644920599.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0528968088468538\n",
      "RMSE on test data after ALS: 1.0640196696560238.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0532205221296949\n",
      "RMSE on test data after ALS: 1.0622648150749925.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0527458023393441\n",
      "RMSE on test data after ALS: 1.065495497925001.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0527877953618279\n",
      "RMSE on test data after ALS: 1.0649565358423587.\n",
      "Converge!\n",
      "ALS Final training RMSE : 1.0529283677299868\n",
      "RMSE on test data after ALS: 1.0650362644963733.\n",
      "=======>>>> FINAL: BEST RMSE: 0.9942181947494481,lambda_item :0.47000000000000003,lambda_user:0.02,weight:2.1864406779661016,num_feature:20\n",
      "ALS Final training RMSE : 0.9068718750680539\n",
      "RMSE on test data after ALS: 0.9866228580655622.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ALS(num_features=None,lambda_user=None,lambda_item=None,weight=None,load_File=None):\n",
    "    seed = 988\n",
    "    train_dataset = \"./data/data_train.csv\"\n",
    "    ratings = load_data(train_dataset)\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    num_items_per_user,num_users_per_item = get_number_per(ratings)\n",
    "    valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.1)\n",
    "    \n",
    "    if(load_File==1):\n",
    "        best_param = np.load(\"best_param_random_search.npy\")\n",
    "        num_features = best_param[0]\n",
    "        weight = best_param[1]\n",
    "        lambda_user = best_param[2]\n",
    "        lambda_item = best_param[3]\n",
    "#     else:\n",
    "#         num_features = 20\n",
    "#         weight = 2.18644068\n",
    "#         lambda_user = 0.02\n",
    "#         lambda_item = 0.47\n",
    "    item_features,user_features , _ = ALS(train, test,num_features,lambda_user,lambda_item,weight)\n",
    "    predict_labels = item_features.T @ user_features\n",
    "    predict = np.asarray(predict_labels.T)\n",
    "    moive_user_predict = pd.DataFrame(data=predict)\n",
    "    moive_user_predict.reset_index(inplace=True)\n",
    "    moive_user_predict.rename(columns={\"index\":\"Movie\"},inplace=True)\n",
    "    moive_user_predict_melt = pd.melt(moive_user_predict,id_vars=[\"Movie\"],var_name=\"User\",value_name =\"Rating\")\n",
    "    moive_user_predict_melt[\"Movie\"] = moive_user_predict_melt[\"Movie\"].values +1\n",
    "    moive_user_predict_melt[\"User\"] = moive_user_predict_melt[\"User\"].values +1\n",
    "    \n",
    "    sample = pd.read_csv(\"./data/sampleSubmission.csv\")\n",
    "    moive_user_predict_melt['Id'] = moive_user_predict_melt.apply(lambda x: 'r{}_c{}'.format(int(x.User), int(x.Movie)), axis=1)\n",
    "    prediction = moive_user_predict_melt[moive_user_predict_melt.Id.isin(sample.Id.values)]\n",
    "    prediction = prediction[[\"User\",\"Movie\",\"Rating\"]]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"./data/sampleSubmission.csv\")\n",
    "moive_user_predict_melt['Id'] = moive_user_predict_melt.apply(lambda x: 'r{}_c{}'.format(int(x.User), int(x.Movie)), axis=1)\n",
    "prediction = moive_user_predict_melt[moive_user_predict_melt.Id.isin(sample.Id.values)]\n",
    "prediction = prediction[[\"User\",\"Movie\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1065327\n",
      "Total number of nonzero elements in test data:111625\n",
      "ALS Final training RMSE : 0.9167933066275361\n",
      "RMSE on test data after ALS: 0.9874095570030827.\n"
     ]
    }
   ],
   "source": [
    "moive_user_predict_melt = predict_ALS(num_features = 20, weight = 2.18644068,lambda_user = 0.02,lambda_item = 0.47)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

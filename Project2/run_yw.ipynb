{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"./datas/data_train.csv\"\n",
    "test_dataset = \"./datas/sampleSubmission.csv\"\n",
    "\n",
    "train_df_path = \"./datas/train.csv\"\n",
    "test_df_path = \"./datas/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, name):\n",
    "    \"\"\"Load dataset as a (User, Movie, Rating) pandas dataframe\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    parsed_df = pd.DataFrame()\n",
    "    # Get all pairs of (r44_c1) -> (44, 1) (user, movie)\n",
    "    user_movie_indices = df.Id.apply(lambda x: x.split('_'))\n",
    "    parsed_df['User'] =  [int(i[0][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Movie'] = [int(i[1][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Rating'] = df['Prediction']\n",
    "    num_items = parsed_df.Movie.nunique()\n",
    "    num_users = parsed_df.User.nunique()\n",
    "    \n",
    "    # save to csv for later use\n",
    "    parsed_df.to_csv(name, index=False, header=False)\n",
    "    \n",
    "    print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))\n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 10000 ITEMS: 1000\n",
      "USERS: 10000 ITEMS: 1000\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset(train_dataset, train_df_path)\n",
    "test_df = load_dataset(test_dataset, test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, p_test=0.2, min_num_ratings = 0):\n",
    "    \"\"\" split dataframe into train and test set \"\"\"\n",
    "    # select user and item based on the condition.\n",
    "    user_counts = df.User.value_counts()\n",
    "    valid_users = user_counts[user_counts > min_num_ratings].index.values\n",
    "    movie_counts = df.Movie.value_counts()\n",
    "    valid_movies = movie_counts[movie_counts > min_num_ratings].index.values\n",
    "\n",
    "    valid_ratings = df[df.User.isin(valid_users) & df.Movie.isin(valid_movies)].reset_index(drop=True)\n",
    "\n",
    "    # Split data\n",
    "    size = df.shape[0]\n",
    "    indexes = list(range(size))\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    test_ind = indexes[:int(size*p_test)]\n",
    "    train_ind = indexes[int(size*p_test):]\n",
    "    \n",
    "    test = valid_ratings.loc[test_ind]\n",
    "    train = valid_ratings.loc[train_ind]\n",
    "\n",
    "    print(\"Train: {}, Test: {}\".format(test.shape, train.shape))\n",
    "    \n",
    "    # Test that the sum of nb rows of splitted dataframes = nb rows of original\n",
    "    if (train.shape[0] + test.shape[0] == df.shape[0]):\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "    else:\n",
    "        raise Exception(\"[Error] Train: {} + Test {} != Original: {} !!\".format(train_tr.shape[0], test_tr.shape[0], df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (235390, 3), Test: (941562, 3)\n"
     ]
    }
   ],
   "source": [
    "train_tr, test_tr = split_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred, truth):\n",
    "    \"\"\" compute RMSE for pandas dataframes \"\"\"\n",
    "    truth_sorted = truth.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "    pred_sorted = pred.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "\n",
    "    truth_sorted['square_error'] = np.square(truth_sorted['Rating'] - prediction_sorted['Rating'])\n",
    "\n",
    "    mse = truth_sorted['square_error'].mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d565fdd18ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2d565fdd18ff>\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Normalize in [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "def standardize(df):\n",
    "    # Normalize in [0, 1]\n",
    "    r = df['Rating'].values.astype(float)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    df_normalized = pd.DataFrame(x_scaled)\n",
    "    df['Rating'] = df_normalized\n",
    "    return df\n",
    "\n",
    "train_tr = standardize(train_tr)\n",
    "test_tr = standardize(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000\n"
     ]
    }
   ],
   "source": [
    "def df_to_matrix(df):\n",
    "    # Convert DataFrame in user-item matrix\n",
    "    matrix = df.pivot(index='User', columns='Movie', values='Rating')\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    # Convert to numpy matrix\n",
    "    users = matrix.index.tolist()\n",
    "    items = matrix.columns.tolist()\n",
    "\n",
    "    matrix = matrix.as_matrix()\n",
    "    return users, items, matrix\n",
    "\n",
    "users, items, matrix_tr = df_to_matrix(train_tr)\n",
    "num_users = len(users)\n",
    "num_items = len(items)\n",
    "\n",
    "print(num_users, num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIDN'T WORK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://vitobellini.github.io/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "num_input = num_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "\n",
    "# Targets are the input data.\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 1. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0.5]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = matrix_tr.copy()\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.33865233287215235\n",
      "Epoch: 2 Loss: 0.33617846965789794\n",
      "Epoch: 3 Loss: 0.31951463147997855\n",
      "Epoch: 4 Loss: 0.2501043420284986\n",
      "Epoch: 5 Loss: 0.1083067674189806\n",
      "Epoch: 6 Loss: 0.051446060091257094\n",
      "Epoch: 7 Loss: 0.04880251819267869\n",
      "Epoch: 8 Loss: 0.04830874148756266\n",
      "Epoch: 9 Loss: 0.04717384213581681\n",
      "Epoch: 10 Loss: 0.04601081358268857\n",
      "Epoch: 11 Loss: 0.045704937912523745\n",
      "Epoch: 12 Loss: 0.0455327108502388\n",
      "Epoch: 13 Loss: 0.045409477315843105\n",
      "Epoch: 14 Loss: 0.045302273239940405\n",
      "Epoch: 15 Loss: 0.04522054763510823\n",
      "Epoch: 16 Loss: 0.045146352518349885\n",
      "Epoch: 17 Loss: 0.04511649133637548\n",
      "Epoch: 18 Loss: 0.0450407731346786\n",
      "Epoch: 19 Loss: 0.04499190943315625\n",
      "Epoch: 20 Loss: 0.044984130375087264\n",
      "Epoch: 21 Loss: 0.04493501167744398\n",
      "Epoch: 22 Loss: 0.04489314304664731\n",
      "Epoch: 23 Loss: 0.044886811077594756\n",
      "Epoch: 24 Loss: 0.04482487924396992\n",
      "Epoch: 25 Loss: 0.04484245739877224\n",
      "Epoch: 26 Loss: 0.04480467587709427\n",
      "Epoch: 27 Loss: 0.044766230136156084\n",
      "Epoch: 28 Loss: 0.044736027158796786\n",
      "Epoch: 29 Loss: 0.04477565037086606\n",
      "Epoch: 30 Loss: 0.04470333745703101\n",
      "Epoch: 31 Loss: 0.04472107756882906\n",
      "Epoch: 32 Loss: 0.04468234172090888\n",
      "Epoch: 33 Loss: 0.04466209216043353\n",
      "Epoch: 34 Loss: 0.04466738793998957\n",
      "Epoch: 35 Loss: 0.044618974532932044\n",
      "Epoch: 36 Loss: 0.04464649436995387\n",
      "Epoch: 37 Loss: 0.04460823470726609\n",
      "Epoch: 38 Loss: 0.04458524528890848\n",
      "Epoch: 39 Loss: 0.04458816070109606\n",
      "Epoch: 40 Loss: 0.04457164946943522\n",
      "Epoch: 41 Loss: 0.04455391857773065\n",
      "Epoch: 42 Loss: 0.0445655208081007\n",
      "Epoch: 43 Loss: 0.044530633557587865\n",
      "Epoch: 44 Loss: 0.04452999262139201\n",
      "Epoch: 45 Loss: 0.044500865135341884\n",
      "Epoch: 46 Loss: 0.04450207781046629\n",
      "Epoch: 47 Loss: 0.04448799081146717\n",
      "Epoch: 48 Loss: 0.0444696668535471\n",
      "Epoch: 49 Loss: 0.04448767984285951\n",
      "Epoch: 50 Loss: 0.044455155916512015\n",
      "Epoch: 51 Loss: 0.04445903664454818\n",
      "Epoch: 52 Loss: 0.04443043079227209\n",
      "Epoch: 53 Loss: 0.0444074934348464\n",
      "Epoch: 54 Loss: 0.044445902667939664\n",
      "Epoch: 55 Loss: 0.04441371327266097\n",
      "Epoch: 56 Loss: 0.044391130283474925\n",
      "Epoch: 57 Loss: 0.044410888012498616\n",
      "Epoch: 58 Loss: 0.04437300069257617\n",
      "Epoch: 59 Loss: 0.0443827249109745\n",
      "Epoch: 60 Loss: 0.044358399044722316\n",
      "Epoch: 61 Loss: 0.04433533139526844\n",
      "Epoch: 62 Loss: 0.04435667209327221\n",
      "Epoch: 63 Loss: 0.044338295422494414\n",
      "Epoch: 64 Loss: 0.0443241068162024\n",
      "Epoch: 65 Loss: 0.044301016815006734\n",
      "Epoch: 66 Loss: 0.04431061614304781\n",
      "Epoch: 67 Loss: 0.04429475627839565\n",
      "Epoch: 68 Loss: 0.04426982682198286\n",
      "Epoch: 69 Loss: 0.0442534577101469\n",
      "Epoch: 70 Loss: 0.044303194340318444\n",
      "Epoch: 71 Loss: 0.044235889427363874\n",
      "Epoch: 72 Loss: 0.044267417211085555\n",
      "Epoch: 73 Loss: 0.04423698522150517\n",
      "Epoch: 74 Loss: 0.04421449145302177\n",
      "Epoch: 75 Loss: 0.04423879766836762\n",
      "Epoch: 76 Loss: 0.04420651812106371\n",
      "Epoch: 77 Loss: 0.04420016659423709\n",
      "Epoch: 78 Loss: 0.044193525519222024\n",
      "Epoch: 79 Loss: 0.04418654423207045\n",
      "Epoch: 80 Loss: 0.04418487306684256\n",
      "Epoch: 81 Loss: 0.044166828598827125\n",
      "Epoch: 82 Loss: 0.044170218612998725\n",
      "Epoch: 83 Loss: 0.04414757704362273\n",
      "Epoch: 84 Loss: 0.044145706482231616\n",
      "Epoch: 85 Loss: 0.04413016336038709\n",
      "Epoch: 86 Loss: 0.04411067208275199\n",
      "Epoch: 87 Loss: 0.044128856901079413\n",
      "Epoch: 88 Loss: 0.044125120900571344\n",
      "Epoch: 89 Loss: 0.044094237685203555\n",
      "Epoch: 90 Loss: 0.044107076991349456\n",
      "Epoch: 91 Loss: 0.04408998629078269\n",
      "Epoch: 92 Loss: 0.044079163763672116\n",
      "Epoch: 93 Loss: 0.04409637292847037\n",
      "Epoch: 94 Loss: 0.04408256001770496\n",
      "Epoch: 95 Loss: 0.04406386138871312\n",
      "Epoch: 96 Loss: 0.04406011076644063\n",
      "Epoch: 97 Loss: 0.04404389569535851\n",
      "Epoch: 98 Loss: 0.04405580749735236\n",
      "Epoch: 99 Loss: 0.044033596850931646\n",
      "Epoch: 100 Loss: 0.04402221208438277\n",
      "Predictions...\n",
      "Filtering out items in training set\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 250\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "    \n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='Rating')\n",
    "    predictions.columns = ['User', 'Movie', 'Rating']\n",
    "    predictions['User'] = predictions['User'].map(lambda value: users[value])\n",
    "    predictions['Movie'] = predictions['Movie'].map(lambda value: items[value])\n",
    "    \n",
    "    print(\"Filtering out items in training set\")\n",
    "\n",
    "    keys = ['User', 'Movie']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    df = train_tr.copy()\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    #recs = predictions[~i1.isin(i2)]\n",
    "    recs = predictions[i1.isin(i2)]\n",
    "    recs = recs.sort_values(['User', 'Rating'], ascending=[True, False])\n",
    "    recs = recs.groupby('User').head(10)\n",
    "   # recs.to_csv('recs.tsv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = recs.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_tr.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predictions.set_index(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = t.merge(r, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "from: https://github.com/NicolasHug/Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import *\n",
    "from surprise.model_selection import KFold, PredefinedKFold\n",
    "from surprise import accuracy\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_cv_algo(train_tr, test_tr, test, algo, k_fold=5, verbose=True):\n",
    "    train_full = train_tr.append(test_tr)\n",
    "    # reader with rating scale\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    # load data from df\n",
    "    data = Dataset.load_from_df(train_full, reader)\n",
    "    kf = KFold(n_splits=k_fold)\n",
    "    \n",
    "    rmses = dict()\n",
    "    \n",
    "    print(\"Start cv training...\")\n",
    "    i = 0\n",
    "    for trainset, testset in kf.split(data):\n",
    "        # train and test algorithm.\n",
    "        print(\"Split {}...\".format(i))\n",
    "        i = i + 1\n",
    "        model = algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Compute and print RMSE\n",
    "        rmse_ = accuracy.rmse(predictions, verbose=verbose)\n",
    "        rmses[rmse_] = model\n",
    "\n",
    "    # Find the model with least RMSE\n",
    "    lowest_rmse = min(rmses.keys())\n",
    "    best_model = rmses[lowest_rmse]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Best model rmse: {}. Starting predictions.\".format(lowest_rmse))\n",
    "    \n",
    "    predictions = test.copy()\n",
    "    # For printing progress\n",
    "    progress = np.linspace(0, 100, 11) / 100 * len(predictions)\n",
    "\n",
    "    perc = 0\n",
    "    for index, row in predictions.iterrows():\n",
    "        if verbose and index in progress:\n",
    "            print(\"Predicting row {} ...\".format(perc))\n",
    "            perc = perc + 10\n",
    "            \n",
    "        user = row.User\n",
    "        movie = row.Movie\n",
    "        rating = best_model.predict(user, movie).est\n",
    "        if rating > 5:\n",
    "            row.Rating = 5\n",
    "        elif rating < 1:\n",
    "            row.Rating = 1\n",
    "        else:\n",
    "            row.Rating = rating\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_algo(algo, train_path=\"datas/train.csv\", test_path=\"datas/test.csv\", verbose=True):\n",
    "    # reader with rating scale\n",
    "    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))\n",
    "    \n",
    "    # Specify the training and test dataset\n",
    "    folds_files = [(train_path, test_path)]\n",
    "\n",
    "    data = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "    pkf = PredefinedKFold()\n",
    "    \n",
    "    print(\"Start prediction...\")\n",
    "    i = 0\n",
    "    for trainset, testset in pkf.split(data):\n",
    "        # train and predict algorithm.\n",
    "        model = algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "    \n",
    "    pred = pd.read_csv(test_path, names = [\"User\", \"Movie\", \"Rating\"])\n",
    "    \n",
    "    print(\"Postprocessing predictions...\")\n",
    "    for index, row in pred.iterrows():\n",
    "        rating = round(predictions[index].est)\n",
    "        if rating > 5:\n",
    "            rating = 5\n",
    "        elif rating < 1:\n",
    "            rating = 1\n",
    "        row.Rating = rating\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(original_df, col_userID, col_movie, col_rate):\n",
    "    \"\"\" return table according with Kaggle convention \"\"\"\n",
    "    print(\"Creating submission file...\")\n",
    "    def id(row):\n",
    "        return 'r' + str(round(row[col_userID])) + '_c' + str(round(row[col_movie]))\n",
    "\n",
    "    def pred(row):\n",
    "        return row[col_rate]\n",
    "\n",
    "    df = pd.DataFrame.copy(original_df)\n",
    "    df['Id'] = df.apply(id, axis=1)\n",
    "    df['Prediction'] = df.apply(pred, axis=1)\n",
    "\n",
    "    return df[['Id', 'Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    import time\n",
    "    import datetime \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.t = 0\n",
    "        \n",
    "    def start(self):\n",
    "        self.t = time.time()\n",
    "        \n",
    "    def stop(self):\n",
    "        print(\"Time taken: {}\".format(datetime.timedelta(seconds=time.time() - self.t).__str__()))\n",
    "        self.t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:03:33.136205\n",
      "Creating submission file...\n",
      "Time taken: 0:00:44.869094\n"
     ]
    }
   ],
   "source": [
    "# svd\n",
    "t.start()\n",
    "algo = SVD(n_epochs=30, lr_all=0.001, reg_all=0.001)\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "t.start()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_svd.csv\")\n",
    "t.stop() # 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Postprocessing predictions...\n",
      "Time taken: 1:36:23.325838\n",
      "Creating submission file...\n",
      "Time taken: 0:00:44.045763\n"
     ]
    }
   ],
   "source": [
    "#svd++\n",
    "t.start()\n",
    "algo = SVDpp(n_epochs=30, lr_all=0.001, reg_all=0.001)\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "t.start()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_svd++.csv\")\n",
    "t.stop() #1.025 slightly better than knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:10:54.984091\n",
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "t.start()\n",
    "algo = KNNBaseline(k=60, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_knnBaseline.csv\") #1.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

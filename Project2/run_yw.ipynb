{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"./datas/data_train.csv\"\n",
    "test_dataset = \"./datas/sampleSubmission.csv\"\n",
    "\n",
    "train_df_path = \"./datas/train.csv\"\n",
    "test_df_path = \"./datas/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, name):\n",
    "    \"\"\"Load dataset as a (User, Movie, Rating) pandas dataframe\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    parsed_df = pd.DataFrame()\n",
    "    # Get all pairs of (r44_c1) -> (44, 1) (user, movie)\n",
    "    user_movie_indices = df.Id.apply(lambda x: x.split('_'))\n",
    "    parsed_df['User'] =  [int(i[0][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Movie'] = [int(i[1][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Rating'] = df['Prediction']\n",
    "    num_items = parsed_df.Movie.nunique()\n",
    "    num_users = parsed_df.User.nunique()\n",
    "    \n",
    "    # save to csv for later use\n",
    "    parsed_df.to_csv(name, index=False, header=False)\n",
    "    \n",
    "    print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))\n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 10000 ITEMS: 1000\n",
      "USERS: 10000 ITEMS: 1000\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset(train_dataset, train_df_path)\n",
    "test_df = load_dataset(test_dataset, test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, p_test=0.2, min_num_ratings = 0):\n",
    "    \"\"\" split dataframe into train and test set \"\"\"\n",
    "    # select user and item based on the condition.\n",
    "    user_counts = df.User.value_counts()\n",
    "    valid_users = user_counts[user_counts > min_num_ratings].index.values\n",
    "    movie_counts = df.Movie.value_counts()\n",
    "    valid_movies = movie_counts[movie_counts > min_num_ratings].index.values\n",
    "\n",
    "    valid_ratings = df[df.User.isin(valid_users) & df.Movie.isin(valid_movies)].reset_index(drop=True)\n",
    "\n",
    "    # Split data\n",
    "    size = df.shape[0]\n",
    "    indexes = list(range(size))\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    test_ind = indexes[:int(size*p_test)]\n",
    "    train_ind = indexes[int(size*p_test):]\n",
    "    \n",
    "    test = valid_ratings.loc[test_ind]\n",
    "    train = valid_ratings.loc[train_ind]\n",
    "\n",
    "    print(\"Train: {}, Test: {}\".format(test.shape, train.shape))\n",
    "    \n",
    "    # Test that the sum of nb rows of splitted dataframes = nb rows of original\n",
    "    if (train.shape[0] + test.shape[0] == df.shape[0]):\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "    else:\n",
    "        raise Exception(\"[Error] Train: {} + Test {} != Original: {} !!\".format(train_tr.shape[0], test_tr.shape[0], df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (235390, 3), Test: (941562, 3)\n"
     ]
    }
   ],
   "source": [
    "train_tr, test_tr = split_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred, truth):\n",
    "    \"\"\" compute RMSE for pandas dataframes \"\"\"\n",
    "    truth_sorted = truth.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "    pred_sorted = pred.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "\n",
    "    truth_sorted['square_error'] = np.square(truth_sorted['Rating'] - prediction_sorted['Rating'])\n",
    "\n",
    "    mse = truth_sorted['square_error'].mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5637, 6994, 6523, ...,  730, 1482, 5217], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tr.User.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d565fdd18ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2d565fdd18ff>\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Normalize in [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "def standardize(df):\n",
    "    # Normalize in [0, 1]\n",
    "    r = df['Rating'].values.astype(float)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    df_normalized = pd.DataFrame(x_scaled)\n",
    "    df['Rating'] = df_normalized\n",
    "    return df\n",
    "\n",
    "train_tr = standardize(train_tr)\n",
    "test_tr = standardize(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000\n"
     ]
    }
   ],
   "source": [
    "def df_to_matrix(df):\n",
    "    # Convert DataFrame in user-item matrix\n",
    "    matrix = df.pivot(index='User', columns='Movie', values='Rating')\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    # Convert to numpy matrix\n",
    "    users = matrix.index.tolist()\n",
    "    items = matrix.columns.tolist()\n",
    "\n",
    "    matrix = matrix.as_matrix()\n",
    "    return users, items, matrix\n",
    "\n",
    "users, items, matrix_tr = df_to_matrix(train_tr)\n",
    "num_users = len(users)\n",
    "num_items = len(items)\n",
    "\n",
    "print(num_users, num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIDN'T WORK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://vitobellini.github.io/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "num_input = num_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "\n",
    "# Targets are the input data.\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 1. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0.5]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = matrix_tr.copy()\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.33865233287215235\n",
      "Epoch: 2 Loss: 0.33617846965789794\n",
      "Epoch: 3 Loss: 0.31951463147997855\n",
      "Epoch: 4 Loss: 0.2501043420284986\n",
      "Epoch: 5 Loss: 0.1083067674189806\n",
      "Epoch: 6 Loss: 0.051446060091257094\n",
      "Epoch: 7 Loss: 0.04880251819267869\n",
      "Epoch: 8 Loss: 0.04830874148756266\n",
      "Epoch: 9 Loss: 0.04717384213581681\n",
      "Epoch: 10 Loss: 0.04601081358268857\n",
      "Epoch: 11 Loss: 0.045704937912523745\n",
      "Epoch: 12 Loss: 0.0455327108502388\n",
      "Epoch: 13 Loss: 0.045409477315843105\n",
      "Epoch: 14 Loss: 0.045302273239940405\n",
      "Epoch: 15 Loss: 0.04522054763510823\n",
      "Epoch: 16 Loss: 0.045146352518349885\n",
      "Epoch: 17 Loss: 0.04511649133637548\n",
      "Epoch: 18 Loss: 0.0450407731346786\n",
      "Epoch: 19 Loss: 0.04499190943315625\n",
      "Epoch: 20 Loss: 0.044984130375087264\n",
      "Epoch: 21 Loss: 0.04493501167744398\n",
      "Epoch: 22 Loss: 0.04489314304664731\n",
      "Epoch: 23 Loss: 0.044886811077594756\n",
      "Epoch: 24 Loss: 0.04482487924396992\n",
      "Epoch: 25 Loss: 0.04484245739877224\n",
      "Epoch: 26 Loss: 0.04480467587709427\n",
      "Epoch: 27 Loss: 0.044766230136156084\n",
      "Epoch: 28 Loss: 0.044736027158796786\n",
      "Epoch: 29 Loss: 0.04477565037086606\n",
      "Epoch: 30 Loss: 0.04470333745703101\n",
      "Epoch: 31 Loss: 0.04472107756882906\n",
      "Epoch: 32 Loss: 0.04468234172090888\n",
      "Epoch: 33 Loss: 0.04466209216043353\n",
      "Epoch: 34 Loss: 0.04466738793998957\n",
      "Epoch: 35 Loss: 0.044618974532932044\n",
      "Epoch: 36 Loss: 0.04464649436995387\n",
      "Epoch: 37 Loss: 0.04460823470726609\n",
      "Epoch: 38 Loss: 0.04458524528890848\n",
      "Epoch: 39 Loss: 0.04458816070109606\n",
      "Epoch: 40 Loss: 0.04457164946943522\n",
      "Epoch: 41 Loss: 0.04455391857773065\n",
      "Epoch: 42 Loss: 0.0445655208081007\n",
      "Epoch: 43 Loss: 0.044530633557587865\n",
      "Epoch: 44 Loss: 0.04452999262139201\n",
      "Epoch: 45 Loss: 0.044500865135341884\n",
      "Epoch: 46 Loss: 0.04450207781046629\n",
      "Epoch: 47 Loss: 0.04448799081146717\n",
      "Epoch: 48 Loss: 0.0444696668535471\n",
      "Epoch: 49 Loss: 0.04448767984285951\n",
      "Epoch: 50 Loss: 0.044455155916512015\n",
      "Epoch: 51 Loss: 0.04445903664454818\n",
      "Epoch: 52 Loss: 0.04443043079227209\n",
      "Epoch: 53 Loss: 0.0444074934348464\n",
      "Epoch: 54 Loss: 0.044445902667939664\n",
      "Epoch: 55 Loss: 0.04441371327266097\n",
      "Epoch: 56 Loss: 0.044391130283474925\n",
      "Epoch: 57 Loss: 0.044410888012498616\n",
      "Epoch: 58 Loss: 0.04437300069257617\n",
      "Epoch: 59 Loss: 0.0443827249109745\n",
      "Epoch: 60 Loss: 0.044358399044722316\n",
      "Epoch: 61 Loss: 0.04433533139526844\n",
      "Epoch: 62 Loss: 0.04435667209327221\n",
      "Epoch: 63 Loss: 0.044338295422494414\n",
      "Epoch: 64 Loss: 0.0443241068162024\n",
      "Epoch: 65 Loss: 0.044301016815006734\n",
      "Epoch: 66 Loss: 0.04431061614304781\n",
      "Epoch: 67 Loss: 0.04429475627839565\n",
      "Epoch: 68 Loss: 0.04426982682198286\n",
      "Epoch: 69 Loss: 0.0442534577101469\n",
      "Epoch: 70 Loss: 0.044303194340318444\n",
      "Epoch: 71 Loss: 0.044235889427363874\n",
      "Epoch: 72 Loss: 0.044267417211085555\n",
      "Epoch: 73 Loss: 0.04423698522150517\n",
      "Epoch: 74 Loss: 0.04421449145302177\n",
      "Epoch: 75 Loss: 0.04423879766836762\n",
      "Epoch: 76 Loss: 0.04420651812106371\n",
      "Epoch: 77 Loss: 0.04420016659423709\n",
      "Epoch: 78 Loss: 0.044193525519222024\n",
      "Epoch: 79 Loss: 0.04418654423207045\n",
      "Epoch: 80 Loss: 0.04418487306684256\n",
      "Epoch: 81 Loss: 0.044166828598827125\n",
      "Epoch: 82 Loss: 0.044170218612998725\n",
      "Epoch: 83 Loss: 0.04414757704362273\n",
      "Epoch: 84 Loss: 0.044145706482231616\n",
      "Epoch: 85 Loss: 0.04413016336038709\n",
      "Epoch: 86 Loss: 0.04411067208275199\n",
      "Epoch: 87 Loss: 0.044128856901079413\n",
      "Epoch: 88 Loss: 0.044125120900571344\n",
      "Epoch: 89 Loss: 0.044094237685203555\n",
      "Epoch: 90 Loss: 0.044107076991349456\n",
      "Epoch: 91 Loss: 0.04408998629078269\n",
      "Epoch: 92 Loss: 0.044079163763672116\n",
      "Epoch: 93 Loss: 0.04409637292847037\n",
      "Epoch: 94 Loss: 0.04408256001770496\n",
      "Epoch: 95 Loss: 0.04406386138871312\n",
      "Epoch: 96 Loss: 0.04406011076644063\n",
      "Epoch: 97 Loss: 0.04404389569535851\n",
      "Epoch: 98 Loss: 0.04405580749735236\n",
      "Epoch: 99 Loss: 0.044033596850931646\n",
      "Epoch: 100 Loss: 0.04402221208438277\n",
      "Predictions...\n",
      "Filtering out items in training set\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 250\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "    \n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='Rating')\n",
    "    predictions.columns = ['User', 'Movie', 'Rating']\n",
    "    predictions['User'] = predictions['User'].map(lambda value: users[value])\n",
    "    predictions['Movie'] = predictions['Movie'].map(lambda value: items[value])\n",
    "    \n",
    "    print(\"Filtering out items in training set\")\n",
    "\n",
    "    keys = ['User', 'Movie']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    df = train_tr.copy()\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    #recs = predictions[~i1.isin(i2)]\n",
    "    recs = predictions[i1.isin(i2)]\n",
    "    recs = recs.sort_values(['User', 'Rating'], ascending=[True, False])\n",
    "    recs = recs.groupby('User').head(10)\n",
    "   # recs.to_csv('recs.tsv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = recs.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_tr.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predictions.set_index(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = t.merge(r, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "from: https://github.com/NicolasHug/Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import *\n",
    "from surprise.model_selection import KFold, PredefinedKFold\n",
    "from surprise import accuracy\n",
    "from itertools import islice\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_cv_algo(data, algo, k_fold=5, verbose=True):\n",
    "    # Split into folds\n",
    "    kf = KFold(n_splits=k_fold)\n",
    "    rmse_ = 0\n",
    "        \n",
    "    for trainset, testset in kf.split(data):\n",
    "        # train and test algorithm.\n",
    "        model = algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Compute and print RMSE\n",
    "        rmse_ += accuracy.rmse(predictions, verbose=verbose)\n",
    "    \n",
    "    rmse_mean = rmse_/k_fold\n",
    "    return rmse_mean\n",
    "    \n",
    "    \n",
    "def surprise_knn_best_params(train_path=\"datas/train.csv\", test_path=\"datas/test.csv\", verbose=True, t = Timer()):\n",
    "    # reader with rating scale\n",
    "    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))\n",
    "    # load data from df\n",
    "    data = Dataset.load_from_file(train_path, reader)\n",
    "    \n",
    "    #knn parameters\n",
    "    ks = np.linspace(40, 200, 9, dtype = np.int64)\n",
    "    names = ['pearson_baseline', 'pearson', 'msd', 'cosine']\n",
    "    user_baseds = [True, False]\n",
    "    params = dict()\n",
    "    rmses = dict()\n",
    "    \n",
    "    for k in ks:\n",
    "        params['k'] = k\n",
    "        for name in names:\n",
    "            params['name'] = name\n",
    "            for user_based in user_baseds:\n",
    "                params['user_based'] = user_based\n",
    "                algo = KNNBaseline(k=k, sim_options={'name': name, 'user_based': user_based})\n",
    "                rmse = surprise_cv_algo(data, algo)\n",
    "                print(\"------Time:{}, rmse: {}, k: {}, name: {}, user_based: {}------\\n\\n\".format(t.now(), rmse, k, name, user_based))\n",
    "                rmses[rmse] = params\n",
    "    \n",
    "    # Find the model with least RMSE\n",
    "    lowest_rmse = min(rmses.keys())\n",
    "    best_params = rmses[lowest_rmse]\n",
    "    \n",
    "    print(\"Best knn rmse: {}. Params: k: {}, name: {}, user_based: {}\".format(lowest_rmse, best_params['k'], best_params['name'], best_params['user_based']))\n",
    "\n",
    "    \n",
    "def surprise_svd_best_params(train_path=\"datas/train.csv\", test_path=\"datas/test.csv\", verbose=True, t = Timer()):\n",
    "    # reader with rating scale\n",
    "    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))\n",
    "    # load data from df\n",
    "    data = Dataset.load_from_file(train_path, reader)\n",
    "    \n",
    "    #svd parameters\n",
    "    n_epochss = np.linspace(200, 40, 9, dtype=np.int32)\n",
    "    reg_alls = np.logspace(-2, -5, 4)\n",
    "    lr_bus = np.logspace(-10, -2, 9)\n",
    "    lr_qis = np.logspace(-10, -2, 9)\n",
    "    params = dict()\n",
    "    rmses = dict()\n",
    "    \n",
    "    t.start()\n",
    "    \n",
    "    for n_epoch in n_epochss:\n",
    "        params['n_epoch'] = k\n",
    "        for reg_all in reg_alls:\n",
    "            params['reg_all'] = reg_all\n",
    "            for lr_bu in lr_bus:\n",
    "                params['lr_bu'] = lr_bu\n",
    "                for lr_qi in lr_qis:\n",
    "                    params['lr_qi'] = lr_qi\n",
    "                    algo = SVD(n_epoch = n_epoch, reg_all = reg_all, lr_bu = lr_bu, lr_qi = lr_qi)\n",
    "                    rmse = surprise_cv_algo(data, algo)\n",
    "                    print(\"------Time:{}, rmse: {}, k: {}, name: {}, user_based: {}------\\n\\n\".format(t.now(), rmse, k, name, user_based))\n",
    "                    rmses[rmse] = params\n",
    "    \n",
    "    # Find the model with least RMSE\n",
    "    lowest_rmse = min(rmses.keys())\n",
    "    best_params = rmses[lowest_rmse]\n",
    "    \n",
    "    print(\"Best svd rmse: {}. Params: k: {}, name: {}, user_based: {}\".format(lowest_rmse, best_params['k'], best_params['name'], best_params['user_based']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_algo(algo, train_path=\"datas/train.csv\", test_path=\"datas/test.csv\", verbose=True):\n",
    "    # reader with rating scale\n",
    "    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 5))\n",
    "    \n",
    "    # Specify the training and test dataset\n",
    "    folds_files = [(train_path, test_path)]\n",
    "\n",
    "    data = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "    pkf = PredefinedKFold()\n",
    "    \n",
    "    print(\"Start prediction...\")\n",
    "    for trainset, testset in pkf.split(data):\n",
    "        # train and predict algorithm.\n",
    "        model = algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "    \n",
    "    pred = pd.read_csv(test_path, names = [\"User\", \"Movie\", \"Rating\"])\n",
    "    \n",
    "    print(\"Postprocessing predictions...\")\n",
    "    for index, row in pred.iterrows():\n",
    "        rating = round(predictions[index].est)\n",
    "        if rating > 5:\n",
    "            rating = 5\n",
    "        elif rating < 1:\n",
    "            rating = 1\n",
    "        row.Rating = rating\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    import time\n",
    "    import datetime \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.t = 0\n",
    "        \n",
    "    def start(self):\n",
    "        self.t = time.time()\n",
    "        \n",
    "    def stop(self):\n",
    "        print(\"Time taken: {}\".format(datetime.timedelta(seconds=time.time() - self.t).__str__()))\n",
    "        self.t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:03:33.136205\n",
      "Creating submission file...\n",
      "Time taken: 0:00:44.869094\n"
     ]
    }
   ],
   "source": [
    "# svd\n",
    "t.start()\n",
    "algo = SVD(n_epochs=30, lr_all=0.001, reg_all=0.001)\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "t.start()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_svd.csv\")\n",
    "t.stop() # 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Postprocessing predictions...\n"
     ]
    }
   ],
   "source": [
    "# ------rmse: 1.0665431544988566, n_factor:50, n_epoch: 200, reg_all: 0.01, lr_bu: 1e-09, lr_qi: 1e-05------\n",
    "# svd\n",
    "t.start()\n",
    "algo = SVD(n_factors=50, n_epochs=200, lr_bu=(1*10**-9) , lr_qi= (1*10**-5), reg_all=0.01)\n",
    "predictions_2 = surprise_algo(algo)\n",
    "t.stop()\n",
    "t.start()\n",
    "submission_2 = create_csv_submission(predictions)\n",
    "submission_2.to_csv(\"surprise_svd_better_param.csv\")\n",
    "t.stop() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Postprocessing predictions...\n",
      "Time taken: 1:36:23.325838\n",
      "Creating submission file...\n",
      "Time taken: 0:00:44.045763\n"
     ]
    }
   ],
   "source": [
    "#svd++\n",
    "t.start()\n",
    "algo = SVDpp(n_epochs=30, lr_all=0.001, reg_all=0.001)\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "t.start()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_svd++.csv\")\n",
    "t.stop() #1.025 slightly better than knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:10:54.984091\n",
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "t.start()\n",
    "algo = KNNBaseline(k=60, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "submission = create_csv_submission(predictions, 'User', 'Movie', 'Rating')\n",
    "submission.to_csv(\"suprise_knnBaseline.csv\") #1.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:07:57.490047\n",
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "#knn best params - Best knn rmse: 0.9902320847559991. Params: k: 200, name: cosine, user_based: False\n",
    "t.start()\n",
    "algo = KNNBaseline(k=200, sim_options={'name': 'cosine', 'user_based': False})\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "submission = create_csv_submission(predictions)\n",
    "submission.to_csv(\"suprise_knnBaseline_best_params.csv\") #1.04 WTF -- got the wrong one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r37_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r73_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r156_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r160_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r248_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Prediction\n",
       "0   r37_c1           3\n",
       "1   r73_c1           3\n",
       "2  r156_c1           4\n",
       "3  r160_c1           3\n",
       "4  r248_c1           3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Postprocessing predictions...\n",
      "Time taken: 0:06:55.763423\n",
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "# ------Time:17872 days, 12:09:10.825712, rmse: 0.9902320847559991, k: 100, name: pearson_baseline, user_based: False------\n",
    "t.start()\n",
    "algo = KNNBaseline(k=100, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "predictions = surprise_algo(algo)\n",
    "t.stop()\n",
    "submission = create_csv_submission(predictions)\n",
    "submission.to_csv(\"suprise_knnBaseline_best_params.csv\") # still 1.025 ... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r37_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r73_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r156_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r160_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r248_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Prediction\n",
       "0   r37_c1           3\n",
       "1   r73_c1           3\n",
       "2  r156_c1           4\n",
       "3  r160_c1           3\n",
       "4  r248_c1           3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyFM -  1.028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfm import pylibfm\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPyFMData(df):\n",
    "    data = []\n",
    "    users = set(df.User.unique())\n",
    "    movies = set(df.Movie.unique())\n",
    "    ratings = df.Rating.astype(float).tolist()\n",
    "    for row in df.iterrows():\n",
    "        data.append({\"user_id\": str(row[1].User), \"movie_id\": str(row[1].Movie)})\n",
    "    return (data, np.array(ratings), users, movies)\n",
    "\n",
    "(train_data, y_train, train_users, train_items) = toPyFMData(train_tr)\n",
    "(test_data, y_test, test_users, test_items) = toPyFMData(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_data)\n",
    "X_test = v.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.54774\n",
      "-- Epoch 2\n",
      "Training MSE: 0.51386\n",
      "-- Epoch 3\n",
      "Training MSE: 0.50497\n",
      "-- Epoch 4\n",
      "Training MSE: 0.50006\n",
      "-- Epoch 5\n",
      "Training MSE: 0.49690\n",
      "-- Epoch 6\n",
      "Training MSE: 0.49467\n",
      "-- Epoch 7\n",
      "Training MSE: 0.49310\n",
      "-- Epoch 8\n",
      "Training MSE: 0.49181\n",
      "-- Epoch 9\n",
      "Training MSE: 0.49084\n",
      "-- Epoch 10\n",
      "Training MSE: 0.49001\n",
      "-- Epoch 11\n",
      "Training MSE: 0.48929\n",
      "-- Epoch 12\n",
      "Training MSE: 0.48865\n",
      "-- Epoch 13\n",
      "Training MSE: 0.48797\n",
      "-- Epoch 14\n",
      "Training MSE: 0.48735\n",
      "-- Epoch 15\n",
      "Training MSE: 0.48667\n",
      "-- Epoch 16\n",
      "Training MSE: 0.48603\n",
      "-- Epoch 17\n",
      "Training MSE: 0.48531\n",
      "-- Epoch 18\n",
      "Training MSE: 0.48459\n",
      "-- Epoch 19\n",
      "Training MSE: 0.48387\n",
      "-- Epoch 20\n",
      "Training MSE: 0.48313\n",
      "-- Epoch 21\n",
      "Training MSE: 0.48236\n",
      "-- Epoch 22\n",
      "Training MSE: 0.48157\n",
      "-- Epoch 23\n",
      "Training MSE: 0.48083\n",
      "-- Epoch 24\n",
      "Training MSE: 0.48006\n",
      "-- Epoch 25\n",
      "Training MSE: 0.47930\n",
      "-- Epoch 26\n",
      "Training MSE: 0.47859\n",
      "-- Epoch 27\n",
      "Training MSE: 0.47789\n",
      "-- Epoch 28\n",
      "Training MSE: 0.47727\n",
      "-- Epoch 29\n",
      "Training MSE: 0.47660\n",
      "-- Epoch 30\n",
      "Training MSE: 0.47608\n",
      "-- Epoch 31\n",
      "Training MSE: 0.47553\n",
      "-- Epoch 32\n",
      "Training MSE: 0.47500\n",
      "-- Epoch 33\n",
      "Training MSE: 0.47460\n",
      "-- Epoch 34\n",
      "Training MSE: 0.47414\n",
      "-- Epoch 35\n",
      "Training MSE: 0.47373\n",
      "-- Epoch 36\n",
      "Training MSE: 0.47334\n",
      "-- Epoch 37\n",
      "Training MSE: 0.47293\n",
      "-- Epoch 38\n",
      "Training MSE: 0.47257\n",
      "-- Epoch 39\n",
      "Training MSE: 0.47224\n",
      "-- Epoch 40\n",
      "Training MSE: 0.47183\n",
      "-- Epoch 41\n",
      "Training MSE: 0.47149\n",
      "-- Epoch 42\n",
      "Training MSE: 0.47115\n",
      "-- Epoch 43\n",
      "Training MSE: 0.47079\n",
      "-- Epoch 44\n",
      "Training MSE: 0.47038\n",
      "-- Epoch 45\n",
      "Training MSE: 0.46999\n",
      "-- Epoch 46\n",
      "Training MSE: 0.46961\n",
      "-- Epoch 47\n",
      "Training MSE: 0.46923\n",
      "-- Epoch 48\n",
      "Training MSE: 0.46876\n",
      "-- Epoch 49\n",
      "Training MSE: 0.46832\n",
      "-- Epoch 50\n",
      "Training MSE: 0.46786\n",
      "-- Epoch 51\n",
      "Training MSE: 0.46741\n",
      "-- Epoch 52\n",
      "Training MSE: 0.46693\n",
      "-- Epoch 53\n",
      "Training MSE: 0.46640\n",
      "-- Epoch 54\n",
      "Training MSE: 0.46590\n",
      "-- Epoch 55\n",
      "Training MSE: 0.46535\n",
      "-- Epoch 56\n",
      "Training MSE: 0.46482\n",
      "-- Epoch 57\n",
      "Training MSE: 0.46423\n",
      "-- Epoch 58\n",
      "Training MSE: 0.46368\n",
      "-- Epoch 59\n",
      "Training MSE: 0.46311\n",
      "-- Epoch 60\n",
      "Training MSE: 0.46252\n",
      "-- Epoch 61\n",
      "Training MSE: 0.46197\n",
      "-- Epoch 62\n",
      "Training MSE: 0.46143\n",
      "-- Epoch 63\n",
      "Training MSE: 0.46089\n",
      "-- Epoch 64\n",
      "Training MSE: 0.46039\n",
      "-- Epoch 65\n",
      "Training MSE: 0.45989\n",
      "-- Epoch 66\n",
      "Training MSE: 0.45944\n",
      "-- Epoch 67\n",
      "Training MSE: 0.45896\n",
      "-- Epoch 68\n",
      "Training MSE: 0.45851\n",
      "-- Epoch 69\n",
      "Training MSE: 0.45800\n",
      "-- Epoch 70\n",
      "Training MSE: 0.45761\n",
      "-- Epoch 71\n",
      "Training MSE: 0.45719\n",
      "-- Epoch 72\n",
      "Training MSE: 0.45679\n",
      "-- Epoch 73\n",
      "Training MSE: 0.45636\n",
      "-- Epoch 74\n",
      "Training MSE: 0.45593\n",
      "-- Epoch 75\n",
      "Training MSE: 0.45556\n",
      "-- Epoch 76\n",
      "Training MSE: 0.45517\n",
      "-- Epoch 77\n",
      "Training MSE: 0.45484\n",
      "-- Epoch 78\n",
      "Training MSE: 0.45443\n",
      "-- Epoch 79\n",
      "Training MSE: 0.45413\n",
      "-- Epoch 80\n",
      "Training MSE: 0.45375\n",
      "-- Epoch 81\n",
      "Training MSE: 0.45347\n",
      "-- Epoch 82\n",
      "Training MSE: 0.45316\n",
      "-- Epoch 83\n",
      "Training MSE: 0.45285\n",
      "-- Epoch 84\n",
      "Training MSE: 0.45253\n",
      "-- Epoch 85\n",
      "Training MSE: 0.45227\n",
      "-- Epoch 86\n",
      "Training MSE: 0.45203\n",
      "-- Epoch 87\n",
      "Training MSE: 0.45172\n",
      "-- Epoch 88\n",
      "Training MSE: 0.45144\n",
      "-- Epoch 89\n",
      "Training MSE: 0.45116\n",
      "-- Epoch 90\n",
      "Training MSE: 0.45096\n",
      "-- Epoch 91\n",
      "Training MSE: 0.45070\n",
      "-- Epoch 92\n",
      "Training MSE: 0.45044\n",
      "-- Epoch 93\n",
      "Training MSE: 0.45019\n",
      "-- Epoch 94\n",
      "Training MSE: 0.44995\n",
      "-- Epoch 95\n",
      "Training MSE: 0.44969\n",
      "-- Epoch 96\n",
      "Training MSE: 0.44948\n",
      "-- Epoch 97\n",
      "Training MSE: 0.44929\n",
      "-- Epoch 98\n",
      "Training MSE: 0.44905\n",
      "-- Epoch 99\n",
      "Training MSE: 0.44890\n",
      "-- Epoch 100\n",
      "Training MSE: 0.44868\n"
     ]
    }
   ],
   "source": [
    "# Build and train a Factorization Machine\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=100, verbose=True, task=\"regression\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "\n",
    "fm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM MSE: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "preds = fm.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"FM MSE: %.4f\" % mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission, toPyFMData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, y_train, train_users, train_items) = toPyFMData(train_df)\n",
    "(test_data, y_test, test_users, test_items) = toPyFMData(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_data)\n",
    "X_test = v.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.54224\n",
      "-- Epoch 2\n",
      "Training MSE: 0.51051\n",
      "-- Epoch 3\n",
      "Training MSE: 0.50252\n",
      "-- Epoch 4\n",
      "Training MSE: 0.49830\n",
      "-- Epoch 5\n",
      "Training MSE: 0.49567\n",
      "-- Epoch 6\n",
      "Training MSE: 0.49396\n",
      "-- Epoch 7\n",
      "Training MSE: 0.49266\n",
      "-- Epoch 8\n",
      "Training MSE: 0.49170\n",
      "-- Epoch 9\n",
      "Training MSE: 0.49086\n",
      "-- Epoch 10\n",
      "Training MSE: 0.49015\n",
      "-- Epoch 11\n",
      "Training MSE: 0.48952\n",
      "-- Epoch 12\n",
      "Training MSE: 0.48890\n",
      "-- Epoch 13\n",
      "Training MSE: 0.48837\n",
      "-- Epoch 14\n",
      "Training MSE: 0.48779\n",
      "-- Epoch 15\n",
      "Training MSE: 0.48724\n",
      "-- Epoch 16\n",
      "Training MSE: 0.48668\n",
      "-- Epoch 17\n",
      "Training MSE: 0.48607\n",
      "-- Epoch 18\n",
      "Training MSE: 0.48552\n",
      "-- Epoch 19\n",
      "Training MSE: 0.48493\n",
      "-- Epoch 20\n",
      "Training MSE: 0.48434\n",
      "-- Epoch 21\n",
      "Training MSE: 0.48374\n",
      "-- Epoch 22\n",
      "Training MSE: 0.48314\n",
      "-- Epoch 23\n",
      "Training MSE: 0.48253\n",
      "-- Epoch 24\n",
      "Training MSE: 0.48189\n",
      "-- Epoch 25\n",
      "Training MSE: 0.48128\n",
      "-- Epoch 26\n",
      "Training MSE: 0.48065\n",
      "-- Epoch 27\n",
      "Training MSE: 0.47999\n",
      "-- Epoch 28\n",
      "Training MSE: 0.47930\n",
      "-- Epoch 29\n",
      "Training MSE: 0.47863\n",
      "-- Epoch 30\n",
      "Training MSE: 0.47790\n",
      "-- Epoch 31\n",
      "Training MSE: 0.47711\n",
      "-- Epoch 32\n",
      "Training MSE: 0.47635\n",
      "-- Epoch 33\n",
      "Training MSE: 0.47544\n",
      "-- Epoch 34\n",
      "Training MSE: 0.47449\n",
      "-- Epoch 35\n",
      "Training MSE: 0.47345\n",
      "-- Epoch 36\n",
      "Training MSE: 0.47246\n",
      "-- Epoch 37\n",
      "Training MSE: 0.47146\n",
      "-- Epoch 38\n",
      "Training MSE: 0.47044\n",
      "-- Epoch 39\n",
      "Training MSE: 0.46956\n",
      "-- Epoch 40\n",
      "Training MSE: 0.46874\n",
      "-- Epoch 41\n",
      "Training MSE: 0.46802\n",
      "-- Epoch 42\n",
      "Training MSE: 0.46732\n",
      "-- Epoch 43\n",
      "Training MSE: 0.46669\n",
      "-- Epoch 44\n",
      "Training MSE: 0.46605\n",
      "-- Epoch 45\n",
      "Training MSE: 0.46543\n",
      "-- Epoch 46\n",
      "Training MSE: 0.46475\n",
      "-- Epoch 47\n",
      "Training MSE: 0.46408\n",
      "-- Epoch 48\n",
      "Training MSE: 0.46344\n",
      "-- Epoch 49\n",
      "Training MSE: 0.46282\n",
      "-- Epoch 50\n",
      "Training MSE: 0.46224\n",
      "-- Epoch 51\n",
      "Training MSE: 0.46171\n",
      "-- Epoch 52\n",
      "Training MSE: 0.46121\n",
      "-- Epoch 53\n",
      "Training MSE: 0.46073\n",
      "-- Epoch 54\n",
      "Training MSE: 0.46029\n",
      "-- Epoch 55\n",
      "Training MSE: 0.45984\n",
      "-- Epoch 56\n",
      "Training MSE: 0.45943\n",
      "-- Epoch 57\n",
      "Training MSE: 0.45902\n",
      "-- Epoch 58\n",
      "Training MSE: 0.45858\n",
      "-- Epoch 59\n",
      "Training MSE: 0.45820\n",
      "-- Epoch 60\n",
      "Training MSE: 0.45787\n",
      "-- Epoch 61\n",
      "Training MSE: 0.45753\n",
      "-- Epoch 62\n",
      "Training MSE: 0.45717\n",
      "-- Epoch 63\n",
      "Training MSE: 0.45687\n",
      "-- Epoch 64\n",
      "Training MSE: 0.45651\n",
      "-- Epoch 65\n",
      "Training MSE: 0.45621\n",
      "-- Epoch 66\n",
      "Training MSE: 0.45588\n",
      "-- Epoch 67\n",
      "Training MSE: 0.45558\n",
      "-- Epoch 68\n",
      "Training MSE: 0.45528\n",
      "-- Epoch 69\n",
      "Training MSE: 0.45499\n",
      "-- Epoch 70\n",
      "Training MSE: 0.45470\n",
      "-- Epoch 71\n",
      "Training MSE: 0.45442\n",
      "-- Epoch 72\n",
      "Training MSE: 0.45412\n",
      "-- Epoch 73\n",
      "Training MSE: 0.45384\n",
      "-- Epoch 74\n",
      "Training MSE: 0.45357\n",
      "-- Epoch 75\n",
      "Training MSE: 0.45334\n",
      "-- Epoch 76\n",
      "Training MSE: 0.45305\n",
      "-- Epoch 77\n",
      "Training MSE: 0.45278\n",
      "-- Epoch 78\n",
      "Training MSE: 0.45253\n",
      "-- Epoch 79\n",
      "Training MSE: 0.45232\n",
      "-- Epoch 80\n",
      "Training MSE: 0.45210\n",
      "-- Epoch 81\n",
      "Training MSE: 0.45189\n",
      "-- Epoch 82\n",
      "Training MSE: 0.45167\n",
      "-- Epoch 83\n",
      "Training MSE: 0.45149\n",
      "-- Epoch 84\n",
      "Training MSE: 0.45129\n",
      "-- Epoch 85\n",
      "Training MSE: 0.45110\n",
      "-- Epoch 86\n",
      "Training MSE: 0.45095\n",
      "-- Epoch 87\n",
      "Training MSE: 0.45080\n",
      "-- Epoch 88\n",
      "Training MSE: 0.45066\n",
      "-- Epoch 89\n",
      "Training MSE: 0.45048\n",
      "-- Epoch 90\n",
      "Training MSE: 0.45035\n",
      "-- Epoch 91\n",
      "Training MSE: 0.45023\n",
      "-- Epoch 92\n",
      "Training MSE: 0.45009\n",
      "-- Epoch 93\n",
      "Training MSE: 0.44999\n",
      "-- Epoch 94\n",
      "Training MSE: 0.44981\n",
      "-- Epoch 95\n",
      "Training MSE: 0.44970\n",
      "-- Epoch 96\n",
      "Training MSE: 0.44958\n",
      "-- Epoch 97\n",
      "Training MSE: 0.44941\n",
      "-- Epoch 98\n",
      "Training MSE: 0.44932\n",
      "-- Epoch 99\n",
      "Training MSE: 0.44920\n",
      "-- Epoch 100\n",
      "Training MSE: 0.44903\n"
     ]
    }
   ],
   "source": [
    "# Build and train a Factorization Machine\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=100, verbose=True, task=\"regression\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "\n",
    "fm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_df.copy()\n",
    "predictions.Rating = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3.292142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>3.157120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>3.888872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>3.408896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>3.667776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Movie    Rating\n",
       "0    37      1  3.292142\n",
       "1    73      1  3.157120\n",
       "2   156      1  3.888872\n",
       "3   160      1  3.408896\n",
       "4   248      1  3.667776"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(predictions):\n",
    "    \"\"\"Create submission file \"\"\"\n",
    "    print(\"Creating submission file...\")\n",
    "    predictions['Id'] = predictions.apply(lambda x: 'r{}_c{}'.format(int(x.User), int(x.Movie)), axis=1)\n",
    "    predictions['Prediction'] = predictions.Rating.apply(lambda x: round(x))\n",
    "    return predictions[['Id', 'Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "submission = create_csv_submission(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"pyFM.csv\") #1.028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r37_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r73_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r156_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r160_c1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r248_c1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Prediction\n",
       "0   r37_c1           3\n",
       "1   r73_c1           3\n",
       "2  r156_c1           4\n",
       "3  r160_c1           3\n",
       "4  r248_c1           4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

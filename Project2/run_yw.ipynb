{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"./datas/data_train.csv\"\n",
    "test_dataset = \"./datas/sampleSubmission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"Load dataset as a (User, Movie, Rating) pandas dataframe\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    parsed_df = pd.DataFrame()\n",
    "    # Get all pairs of (r44_c1) -> (44, 1) (user, movie)\n",
    "    user_movie_indices = df.Id.apply(lambda x: x.split('_'))\n",
    "    parsed_df['User'] =  [int(i[0][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Movie'] = [int(i[1][1:]) for i in user_movie_indices]\n",
    "    parsed_df['Rating'] = df['Prediction']\n",
    "    num_items = parsed_df.Movie.nunique()\n",
    "    num_users = parsed_df.User.nunique()\n",
    "\n",
    "    print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))\n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 10000 ITEMS: 1000\n",
      "USERS: 10000 ITEMS: 1000\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset(train_dataset)\n",
    "test_df = load_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, p_test=0.2, min_num_ratings = 0):\n",
    "    \"\"\" split dataframe into train and test set \"\"\"\n",
    "    # select user and item based on the condition.\n",
    "    user_counts = df.User.value_counts()\n",
    "    valid_users = user_counts[user_counts > min_num_ratings].index.values\n",
    "    movie_counts = df.Movie.value_counts()\n",
    "    valid_movies = movie_counts[movie_counts > min_num_ratings].index.values\n",
    "\n",
    "    valid_ratings = df[df.User.isin(valid_users) & df.Movie.isin(valid_movies)].reset_index(drop=True)\n",
    "\n",
    "    # Split data\n",
    "    size = df.shape[0]\n",
    "    indexes = list(range(size))\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    test_ind = indexes[:int(size*p_test)]\n",
    "    train_ind = indexes[int(size*p_test):]\n",
    "    \n",
    "    test = valid_ratings.loc[test_ind]\n",
    "    train = valid_ratings.loc[train_ind]\n",
    "\n",
    "    print(\"Train: {}, Test: {}\".format(test.shape, train.shape))\n",
    "    \n",
    "    # Test that the sum of nb rows of splitted dataframes = nb rows of original\n",
    "    if (train.shape[0] + test.shape[0] == df.shape[0]):\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "    else:\n",
    "        raise Exception(\"[Error] Train: {} + Test {} != Original: {} !!\".format(train_tr.shape[0], test_tr.shape[0], df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (235390, 3), Test: (941562, 3)\n"
     ]
    }
   ],
   "source": [
    "train_tr, test_tr = split_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred, truth):\n",
    "    \"\"\" compute RMSE for pandas dataframes \"\"\"\n",
    "    truth_sorted = truth.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "    pred_sorted = pred.sort_values(['User', 'Movie']).reset_index(drop=True)\n",
    "\n",
    "    truth_sorted['square_error'] = np.square(truth_sorted['Rating'] - prediction_sorted['Rating'])\n",
    "\n",
    "    mse = truth_sorted['square_error'].mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5637, 6994, 6523, ...,  730, 1482, 5217], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tr.User.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    # Normalize in [0, 1]\n",
    "    r = df['Rating'].values.astype(float)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    df_normalized = pd.DataFrame(x_scaled)\n",
    "    df['Rating'] = df_normalized\n",
    "    return df\n",
    "\n",
    "train_tr = standardize(train_tr)\n",
    "test_tr = standardize(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000\n"
     ]
    }
   ],
   "source": [
    "def df_to_matrix(df):\n",
    "    # Convert DataFrame in user-item matrix\n",
    "    matrix = df.pivot(index='User', columns='Movie', values='Rating')\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    # Convert to numpy matrix\n",
    "    users = matrix.index.tolist()\n",
    "    items = matrix.columns.tolist()\n",
    "\n",
    "    matrix = matrix.as_matrix()\n",
    "    return users, items, matrix\n",
    "\n",
    "users, items, matrix_tr = df_to_matrix(train_tr)\n",
    "num_users = len(users)\n",
    "num_items = len(items)\n",
    "\n",
    "print(num_users, num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIDN'T WORK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://vitobellini.github.io/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "num_input = num_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "\n",
    "# Targets are the input data.\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 1. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0.5]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = matrix_tr.copy()\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.33865233287215235\n",
      "Epoch: 2 Loss: 0.33617846965789794\n",
      "Epoch: 3 Loss: 0.31951463147997855\n",
      "Epoch: 4 Loss: 0.2501043420284986\n",
      "Epoch: 5 Loss: 0.1083067674189806\n",
      "Epoch: 6 Loss: 0.051446060091257094\n",
      "Epoch: 7 Loss: 0.04880251819267869\n",
      "Epoch: 8 Loss: 0.04830874148756266\n",
      "Epoch: 9 Loss: 0.04717384213581681\n",
      "Epoch: 10 Loss: 0.04601081358268857\n",
      "Epoch: 11 Loss: 0.045704937912523745\n",
      "Epoch: 12 Loss: 0.0455327108502388\n",
      "Epoch: 13 Loss: 0.045409477315843105\n",
      "Epoch: 14 Loss: 0.045302273239940405\n",
      "Epoch: 15 Loss: 0.04522054763510823\n",
      "Epoch: 16 Loss: 0.045146352518349885\n",
      "Epoch: 17 Loss: 0.04511649133637548\n",
      "Epoch: 18 Loss: 0.0450407731346786\n",
      "Epoch: 19 Loss: 0.04499190943315625\n",
      "Epoch: 20 Loss: 0.044984130375087264\n",
      "Epoch: 21 Loss: 0.04493501167744398\n",
      "Epoch: 22 Loss: 0.04489314304664731\n",
      "Epoch: 23 Loss: 0.044886811077594756\n",
      "Epoch: 24 Loss: 0.04482487924396992\n",
      "Epoch: 25 Loss: 0.04484245739877224\n",
      "Epoch: 26 Loss: 0.04480467587709427\n",
      "Epoch: 27 Loss: 0.044766230136156084\n",
      "Epoch: 28 Loss: 0.044736027158796786\n",
      "Epoch: 29 Loss: 0.04477565037086606\n",
      "Epoch: 30 Loss: 0.04470333745703101\n",
      "Epoch: 31 Loss: 0.04472107756882906\n",
      "Epoch: 32 Loss: 0.04468234172090888\n",
      "Epoch: 33 Loss: 0.04466209216043353\n",
      "Epoch: 34 Loss: 0.04466738793998957\n",
      "Epoch: 35 Loss: 0.044618974532932044\n",
      "Epoch: 36 Loss: 0.04464649436995387\n",
      "Epoch: 37 Loss: 0.04460823470726609\n",
      "Epoch: 38 Loss: 0.04458524528890848\n",
      "Epoch: 39 Loss: 0.04458816070109606\n",
      "Epoch: 40 Loss: 0.04457164946943522\n",
      "Epoch: 41 Loss: 0.04455391857773065\n",
      "Epoch: 42 Loss: 0.0445655208081007\n",
      "Epoch: 43 Loss: 0.044530633557587865\n",
      "Epoch: 44 Loss: 0.04452999262139201\n",
      "Epoch: 45 Loss: 0.044500865135341884\n",
      "Epoch: 46 Loss: 0.04450207781046629\n",
      "Epoch: 47 Loss: 0.04448799081146717\n",
      "Epoch: 48 Loss: 0.0444696668535471\n",
      "Epoch: 49 Loss: 0.04448767984285951\n",
      "Epoch: 50 Loss: 0.044455155916512015\n",
      "Epoch: 51 Loss: 0.04445903664454818\n",
      "Epoch: 52 Loss: 0.04443043079227209\n",
      "Epoch: 53 Loss: 0.0444074934348464\n",
      "Epoch: 54 Loss: 0.044445902667939664\n",
      "Epoch: 55 Loss: 0.04441371327266097\n",
      "Epoch: 56 Loss: 0.044391130283474925\n",
      "Epoch: 57 Loss: 0.044410888012498616\n",
      "Epoch: 58 Loss: 0.04437300069257617\n",
      "Epoch: 59 Loss: 0.0443827249109745\n",
      "Epoch: 60 Loss: 0.044358399044722316\n",
      "Epoch: 61 Loss: 0.04433533139526844\n",
      "Epoch: 62 Loss: 0.04435667209327221\n",
      "Epoch: 63 Loss: 0.044338295422494414\n",
      "Epoch: 64 Loss: 0.0443241068162024\n",
      "Epoch: 65 Loss: 0.044301016815006734\n",
      "Epoch: 66 Loss: 0.04431061614304781\n",
      "Epoch: 67 Loss: 0.04429475627839565\n",
      "Epoch: 68 Loss: 0.04426982682198286\n",
      "Epoch: 69 Loss: 0.0442534577101469\n",
      "Epoch: 70 Loss: 0.044303194340318444\n",
      "Epoch: 71 Loss: 0.044235889427363874\n",
      "Epoch: 72 Loss: 0.044267417211085555\n",
      "Epoch: 73 Loss: 0.04423698522150517\n",
      "Epoch: 74 Loss: 0.04421449145302177\n",
      "Epoch: 75 Loss: 0.04423879766836762\n",
      "Epoch: 76 Loss: 0.04420651812106371\n",
      "Epoch: 77 Loss: 0.04420016659423709\n",
      "Epoch: 78 Loss: 0.044193525519222024\n",
      "Epoch: 79 Loss: 0.04418654423207045\n",
      "Epoch: 80 Loss: 0.04418487306684256\n",
      "Epoch: 81 Loss: 0.044166828598827125\n",
      "Epoch: 82 Loss: 0.044170218612998725\n",
      "Epoch: 83 Loss: 0.04414757704362273\n",
      "Epoch: 84 Loss: 0.044145706482231616\n",
      "Epoch: 85 Loss: 0.04413016336038709\n",
      "Epoch: 86 Loss: 0.04411067208275199\n",
      "Epoch: 87 Loss: 0.044128856901079413\n",
      "Epoch: 88 Loss: 0.044125120900571344\n",
      "Epoch: 89 Loss: 0.044094237685203555\n",
      "Epoch: 90 Loss: 0.044107076991349456\n",
      "Epoch: 91 Loss: 0.04408998629078269\n",
      "Epoch: 92 Loss: 0.044079163763672116\n",
      "Epoch: 93 Loss: 0.04409637292847037\n",
      "Epoch: 94 Loss: 0.04408256001770496\n",
      "Epoch: 95 Loss: 0.04406386138871312\n",
      "Epoch: 96 Loss: 0.04406011076644063\n",
      "Epoch: 97 Loss: 0.04404389569535851\n",
      "Epoch: 98 Loss: 0.04405580749735236\n",
      "Epoch: 99 Loss: 0.044033596850931646\n",
      "Epoch: 100 Loss: 0.04402221208438277\n",
      "Predictions...\n",
      "Filtering out items in training set\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 250\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "    \n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='Rating')\n",
    "    predictions.columns = ['User', 'Movie', 'Rating']\n",
    "    predictions['User'] = predictions['User'].map(lambda value: users[value])\n",
    "    predictions['Movie'] = predictions['Movie'].map(lambda value: items[value])\n",
    "    \n",
    "    print(\"Filtering out items in training set\")\n",
    "\n",
    "    keys = ['User', 'Movie']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    df = train_tr.copy()\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    #recs = predictions[~i1.isin(i2)]\n",
    "    recs = predictions[i1.isin(i2)]\n",
    "    recs = recs.sort_values(['User', 'Rating'], ascending=[True, False])\n",
    "    recs = recs.groupby('User').head(10)\n",
    "   # recs.to_csv('recs.tsv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = recs.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_tr.sort_values(by=['User', 'Movie']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predictions.set_index(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = t.merge(r, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "from: https://github.com/NicolasHug/Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b08cb1e38fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise import *\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train, test, **kwargs):\n",
    "    \"\"\"\n",
    "    K Nearest Neighbors with Baseline from library Surprise\n",
    "    Args:\n",
    "        train (pandas.DataFrame): train set\n",
    "        test (pandas.DataFrame): test set\n",
    "        **kwargs: Arbitrary keyword arguments.\n",
    "            k (int): Number of nearest neighbor for the algorithm\n",
    "            sim_options (dict): Dictionary specific for the kNN algorithms in Surprise\n",
    "    Returns:\n",
    "        pandas.DataFrame: predictions, sorted by (Movie, User)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get parameters\n",
    "    k = kwargs['k']\n",
    "    sim_options = kwargs['sim_options']\n",
    "\n",
    "    # First, we need to dump the pandas DF into files\n",
    "    train_file = 'tmp_train.csv'\n",
    "    test_file = 'tmp_test.csv'\n",
    "    train.to_csv(train_file, index=False, header=False)\n",
    "    test.to_csv(test_file, index=False, header=False)\n",
    "\n",
    "    # Create Reader\n",
    "    reader = Reader(line_format='user item rating', sep=',')\n",
    "\n",
    "    # Train and test set for Surprise\n",
    "    fold = [(train_file, test_file)]\n",
    "\n",
    "    # Load the data\n",
    "    data = Dataset.load_from_folds(fold, reader=reader)\n",
    "\n",
    "    # Algorithm\n",
    "    algo = KNNBaseline(k=k, sim_options=sim_options)\n",
    "\n",
    "    # Go through 1 fold\n",
    "    for trainset, testset in data.folds():\n",
    "        # Train\n",
    "        algo.train(trainset)\n",
    "\n",
    "        # Predict\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "    # Postprocess the predictions\n",
    "    pred = np.zeros(len(predictions))\n",
    "    for i in range(len(predictions)):\n",
    "        val = predictions[i].est\n",
    "        if val > 5:\n",
    "            pred[i] = 5\n",
    "        elif val < 1:\n",
    "            pred[i] = 1\n",
    "        else:\n",
    "            pred[i] = val\n",
    "\n",
    "    # Copy the test\n",
    "    df_return = test.copy()\n",
    "    df_return.Rating = pred\n",
    "\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176952, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0283\n",
      "RMSE: 1.0264\n",
      "RMSE: 1.0278\n",
      "RMSE: 1.0265\n",
      "RMSE: 1.0270\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'SVD' and 'SVD'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-0c06c6d74e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Find the model with least RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mlowest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlowest_rmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'SVD' and 'SVD'"
     ]
    }
   ],
   "source": [
    "\n",
    "k_fold = 5\n",
    "verbose = True\n",
    "\n",
    "train_full = train_tr.append(test_tr)\n",
    "\n",
    "# reader with rating scale\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# load data from df\n",
    "data = Dataset.load_from_df(train_full, reader)\n",
    "\n",
    "kf = KFold(n_splits=k_fold)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "rmses = dict()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    model = algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print RMSE\n",
    "    rmse_ = accuracy.rmse(predictions, verbose=verbose)\n",
    "\n",
    "    rmses[rmse_] = model\n",
    "\n",
    "# Find the model with least RMSE\n",
    "lowest_rmse = min(rmses.keys())\n",
    "best_model = rmses[lowest_rmse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Movie  Rating\n",
       "0    44      1       4\n",
       "1    61      1       3\n",
       "2    67      1       4\n",
       "3    72      1       3\n",
       "4    86      1       5"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4701499652786496"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(72, 1, None).est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-684b4577c859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmovie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMovie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   3637\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3639\u001b[0;31m                 \u001b[0mexisting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3640\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3641\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;34m\"\"\" return the dtype object of the underlying data \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4479\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4483\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = test_df.copy()\n",
    "for index, row in test.iterrows():\n",
    "    if verbose and (row % 50 == 0):\n",
    "        print(\"Processing row {}..\".format(index))\n",
    "    user = row.User\n",
    "    movie = row.Movie\n",
    "    row.Rating = best_model.predict(user, movie).est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

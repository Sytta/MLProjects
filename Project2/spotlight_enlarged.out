Epoch 0: loss 16.12921304298264
Epoch 1: loss 16.12921509190446
Epoch 2: loss 16.129210626644696
Epoch 3: loss 16.12921447116049
Epoch 4: loss 16.129198656191054
Epoch 5: loss 16.12920899492298
Epoch 6: loss 16.129200524645956
Epoch 7: loss 16.12921106381025
Epoch 8: loss 16.129198261030737
Epoch 9: loss 16.12919823873166
Epoch 10: loss 16.129195255841687
Epoch 11: loss 16.129192022994655
Epoch 12: loss 16.129195635444514
Epoch 13: loss 16.1291933316391
Epoch 14: loss 16.12918606888282
Epoch 15: loss 16.1291850509042
Epoch 16: loss 16.12918551477678
Epoch 17: loss 16.129180523673757
Epoch 18: loss 16.12917894173602
Epoch 19: loss 16.129178687889592
Epoch 20: loss 16.12917681424886
Epoch 21: loss 16.129176755130388
Epoch 22: loss 16.12917108157201
Epoch 23: loss 16.129167603953757
Epoch 24: loss 16.129166980616873
Epoch 25: loss 16.12916542305254
Epoch 26: loss 16.129168730834827
Epoch 27: loss 16.129160339123143
Epoch 28: loss 16.12916134647081
Epoch 29: loss 16.129159870064733
Epoch 30: loss 16.12915656954261
Epoch 31: loss 16.1291516362616
Epoch 32: loss 16.129147629170006
Epoch 33: loss 16.129151598664325
Epoch 34: loss 16.12914939935341
Epoch 35: loss 16.12914229450568
Epoch 36: loss 16.129148933147203
Epoch 37: loss 16.12913680841509
Epoch 38: loss 16.129146977829635
Epoch 39: loss 16.129136192597663
Epoch 40: loss 16.129139338841316
Epoch 41: loss 16.12913239682868
Epoch 42: loss 16.1291359859423
Epoch 43: loss 16.12913439259573
Epoch 44: loss 16.129135141429725
Epoch 45: loss 16.129123833725284
Epoch 46: loss 16.129127199329584
Epoch 47: loss 16.129125958878817
Epoch 48: loss 16.129127545743096
Epoch 49: loss 16.12912246025795
-----------Time: 0:08:32.625514, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017199993133545-------------


Epoch 0: loss 16.129190796286338
Epoch 1: loss 16.12919780467761
Epoch 2: loss 16.129183064212352
Epoch 3: loss 16.12918384234629
Epoch 4: loss 16.129191387730362
Epoch 5: loss 16.129179483396065
Epoch 6: loss 16.129183306649953
Epoch 7: loss 16.129184236988028
Epoch 8: loss 16.129180111400196
Epoch 9: loss 16.129172301020162
Epoch 10: loss 16.129175887281573
Epoch 11: loss 16.129172376474003
Epoch 12: loss 16.129171085461383
Epoch 13: loss 16.12916707836979
Epoch 14: loss 16.12916307075961
Epoch 15: loss 16.12916319262664
Epoch 16: loss 16.129161171449017
Epoch 17: loss 16.129161892798106
Epoch 18: loss 16.129161756151458
Epoch 19: loss 16.129156656145984
Epoch 20: loss 16.129153086738523
Epoch 21: loss 16.129151656486343
Epoch 22: loss 16.12915463963561
Epoch 23: loss 16.129144961059968
Epoch 24: loss 16.129144193816277
Epoch 25: loss 16.129142817756026
Epoch 26: loss 16.129148346889014
Epoch 27: loss 16.12914699494288
Epoch 28: loss 16.129141089577853
Epoch 29: loss 16.12913474523224
Epoch 30: loss 16.12913655197575
Epoch 31: loss 16.12913534808509
Epoch 32: loss 16.129135669865903
Epoch 33: loss 16.129134119043147
Epoch 34: loss 16.129125146518394
Epoch 35: loss 16.129125029318615
Epoch 36: loss 16.129125153778556
Epoch 37: loss 16.129122280568904
Epoch 38: loss 16.12911767892179
Epoch 39: loss 16.1291122296506
Epoch 40: loss 16.129120192494064
Epoch 41: loss 16.129115823172175
Epoch 42: loss 16.12910929135877
Epoch 43: loss 16.129113509254392
Epoch 44: loss 16.129103778561152
Epoch 45: loss 16.129106210197296
Epoch 46: loss 16.12910540354129
Epoch 47: loss 16.12910744831444
Epoch 48: loss 16.129103618318975
Epoch 49: loss 16.129095867057416
-----------Time: 0:08:08.552798, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12917612401476
Epoch 1: loss 16.129172979326857
Epoch 2: loss 16.12918102436574
Epoch 3: loss 16.12917349220554
Epoch 4: loss 16.129170218909028
Epoch 5: loss 16.12916681441099
Epoch 6: loss 16.12916277257433
Epoch 7: loss 16.129166841377312
Epoch 8: loss 16.129161199711795
Epoch 9: loss 16.12915835787642
Epoch 10: loss 16.129159815094923
Epoch 11: loss 16.12915421750897
Epoch 12: loss 16.129148752161704
Epoch 13: loss 16.129149385351667
Epoch 14: loss 16.129149519405395
Epoch 15: loss 16.12914960730523
Epoch 16: loss 16.12915004265574
Epoch 17: loss 16.12914281879319
Epoch 18: loss 16.129144691396757
Epoch 19: loss 16.129135821032875
Epoch 20: loss 16.129132210138767
Epoch 21: loss 16.12913287211009
Epoch 22: loss 16.129129415235163
Epoch 23: loss 16.129133125956518
Epoch 24: loss 16.129128933990046
Epoch 25: loss 16.129131917139315
Epoch 26: loss 16.129131324917417
Epoch 27: loss 16.129127499329908
Epoch 28: loss 16.12912260390547
Epoch 29: loss 16.129125508489395
Epoch 30: loss 16.129122602349717
Epoch 31: loss 16.129117982811483
Epoch 32: loss 16.12911619784846
Epoch 33: loss 16.129114192746915
Epoch 34: loss 16.129115333111148
Epoch 35: loss 16.129109572949393
Epoch 36: loss 16.12910562886564
Epoch 37: loss 16.129102051420144
Epoch 38: loss 16.12910990121249
Epoch 39: loss 16.12910509757726
Epoch 40: loss 16.129097908719068
Epoch 41: loss 16.12909542937328
Epoch 42: loss 16.129097231449546
Epoch 43: loss 16.129098996447116
Epoch 44: loss 16.129089464889784
Epoch 45: loss 16.129088556332196
Epoch 46: loss 16.129084604728995
Epoch 47: loss 16.12909110802033
Epoch 48: loss 16.129085191246478
Epoch 49: loss 16.129082274216554
-----------Time: 0:11:05.814390, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017174243927002-------------


Epoch 0: loss 16.12917578252779
Epoch 1: loss 16.129180308202482
Epoch 2: loss 16.12917485711625
Epoch 3: loss 16.129174098169887
Epoch 4: loss 16.12916425598128
Epoch 5: loss 16.12917217837526
Epoch 6: loss 16.129167938180565
Epoch 7: loss 16.129161604984485
Epoch 8: loss 16.129158477928407
Epoch 9: loss 16.12916191535647
Epoch 10: loss 16.129160193660585
Epoch 11: loss 16.1291569901135
Epoch 12: loss 16.129165955637383
Epoch 13: loss 16.12914692000762
Epoch 14: loss 16.129149925974545
Epoch 15: loss 16.12914896918873
Epoch 16: loss 16.129143675233177
Epoch 17: loss 16.12914567774181
Epoch 18: loss 16.129141270304064
Epoch 19: loss 16.129139728297222
Epoch 20: loss 16.12914220738372
Epoch 21: loss 16.129136678510026
Epoch 22: loss 16.12913458006352
Epoch 23: loss 16.129131872022587
Epoch 24: loss 16.12912968256475
Epoch 25: loss 16.129126964670736
Epoch 26: loss 16.12913091368102
Epoch 27: loss 16.129125155075016
Epoch 28: loss 16.129127875561945
Epoch 29: loss 16.129125610131684
Epoch 30: loss 16.1291191820349
Epoch 31: loss 16.129122116955937
Epoch 32: loss 16.129122623611625
Epoch 33: loss 16.12911730398621
Epoch 34: loss 16.12911502196129
Epoch 35: loss 16.12911502066483
Epoch 36: loss 16.1291053387184
Epoch 37: loss 16.12910672359456
Epoch 38: loss 16.129112355666294
Epoch 39: loss 16.129104436124518
Epoch 40: loss 16.129102401204445
Epoch 41: loss 16.129100335947257
Epoch 42: loss 16.129096336375117
Epoch 43: loss 16.129090121156693
Epoch 44: loss 16.129097764034384
Epoch 45: loss 16.12908680611424
Epoch 46: loss 16.129090553136415
Epoch 47: loss 16.12908914673905
Epoch 48: loss 16.129080972313687
Epoch 49: loss 16.129088086236617
-----------Time: 0:10:58.978907, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017177104949951-------------


Epoch 0: loss 16.129173929889674
Epoch 1: loss 16.129168897559293
Epoch 2: loss 16.129170953481985
Epoch 3: loss 16.12916535122878
Epoch 4: loss 16.129160314490445
Epoch 5: loss 16.129163999541937
Epoch 6: loss 16.129159313625067
Epoch 7: loss 16.12916323618762
Epoch 8: loss 16.129156800312085
Epoch 9: loss 16.12916140014416
Epoch 10: loss 16.129153852167175
Epoch 11: loss 16.129149099612377
Epoch 12: loss 16.12914695371552
Epoch 13: loss 16.129148908255214
Epoch 14: loss 16.129146589670185
Epoch 15: loss 16.129140001849805
Epoch 16: loss 16.129141056129242
Epoch 17: loss 16.129139933137544
Epoch 18: loss 16.129142839536517
Epoch 19: loss 16.12913831982553
Epoch 20: loss 16.12913795811382
Epoch 21: loss 16.129133780926967
Epoch 22: loss 16.12913141566946
Epoch 23: loss 16.129135029156483
Epoch 24: loss 16.12913193658618
Epoch 25: loss 16.129124763544777
Epoch 26: loss 16.129126173312926
Epoch 27: loss 16.129122404510266
Epoch 28: loss 16.129126080486554
Epoch 29: loss 16.12912246570307
Epoch 30: loss 16.1291131838435
Epoch 31: loss 16.129115393526078
Epoch 32: loss 16.12911604175495
Epoch 33: loss 16.129110602596132
Epoch 34: loss 16.129110216511016
Epoch 35: loss 16.12910701451968
Epoch 36: loss 16.12910151961318
Epoch 37: loss 16.129105552374636
Epoch 38: loss 16.129102526960846
Epoch 39: loss 16.12909523983116
Epoch 40: loss 16.12909468079858
Epoch 41: loss 16.12909260361398
Epoch 42: loss 16.129097955391547
Epoch 43: loss 16.129090567397448
Epoch 44: loss 16.12909224345802
Epoch 45: loss 16.1290926368033
Epoch 46: loss 16.12908901864903
Epoch 47: loss 16.129082223654702
Epoch 48: loss 16.129080926937664
Epoch 49: loss 16.129086153995996
-----------Time: 0:13:20.036641, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129099756171353
Epoch 1: loss 16.129078422699887
Epoch 2: loss 16.129065780681135
Epoch 3: loss 16.129039095950684
Epoch 4: loss 16.129023702589308
Epoch 5: loss 16.12900307205723
Epoch 6: loss 16.128987260458583
Epoch 7: loss 16.128968017654874
Epoch 8: loss 16.128943130852022
Epoch 9: loss 16.12892444137648
Epoch 10: loss 16.128905872730797
Epoch 11: loss 16.12888723329852
Epoch 12: loss 16.128873959386265
Epoch 13: loss 16.12884825322212
Epoch 14: loss 16.1288330053233
Epoch 15: loss 16.128811054219366
Epoch 16: loss 16.12879147044727
Epoch 17: loss 16.128771701800297
Epoch 18: loss 16.128756699709868
Epoch 19: loss 16.12873255162864
Epoch 20: loss 16.12871532922468
Epoch 21: loss 16.128698300511505
Epoch 22: loss 16.128666743174268
Epoch 23: loss 16.128656263647034
Epoch 24: loss 16.128636816003
Epoch 25: loss 16.128623337509712
Epoch 26: loss 16.128595007833674
Epoch 27: loss 16.1285831672851
Epoch 28: loss 16.128561771583662
Epoch 29: loss 16.128537122291867
Epoch 30: loss 16.128522683641975
Epoch 31: loss 16.128500966419015
Epoch 32: loss 16.128479296127825
Epoch 33: loss 16.128465151255256
Epoch 34: loss 16.128440641980898
Epoch 35: loss 16.128423342308057
Epoch 36: loss 16.12840554790694
Epoch 37: loss 16.128388341579054
Epoch 38: loss 16.128370976824034
Epoch 39: loss 16.12834439373587
Epoch 40: loss 16.128325262686815
Epoch 41: loss 16.128310497848155
Epoch 42: loss 16.12828768019186
Epoch 43: loss 16.12827383324528
Epoch 44: loss 16.1282463027058
Epoch 45: loss 16.128232836917796
Epoch 46: loss 16.128217194636534
Epoch 47: loss 16.128197316568528
Epoch 48: loss 16.12818184567898
Epoch 49: loss 16.128157592843966
-----------Time: 0:07:15.348531, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017088890075684-------------


Epoch 0: loss 16.129163388132465
Epoch 1: loss 16.129144154922542
Epoch 2: loss 16.129125567089286
Epoch 3: loss 16.129111205708274
Epoch 4: loss 16.129090389004862
Epoch 5: loss 16.12907204775787
Epoch 6: loss 16.129047325086567
Epoch 7: loss 16.129028892568947
Epoch 8: loss 16.129010443197377
Epoch 9: loss 16.128998092622325
Epoch 10: loss 16.128973177556695
Epoch 11: loss 16.128951632503323
Epoch 12: loss 16.128932205343325
Epoch 13: loss 16.12891111016079
Epoch 14: loss 16.128903135649207
Epoch 15: loss 16.12887735714272
Epoch 16: loss 16.1288643184079
Epoch 17: loss 16.12883298354281
Epoch 18: loss 16.12881803486644
Epoch 19: loss 16.128802056802623
Epoch 20: loss 16.128787394902705
Epoch 21: loss 16.12876290429734
Epoch 22: loss 16.128741583012577
Epoch 23: loss 16.12871436673445
Epoch 24: loss 16.128708602164735
Epoch 25: loss 16.12868543394627
Epoch 26: loss 16.128668081637244
Epoch 27: loss 16.12864591687707
Epoch 28: loss 16.128624565773777
Epoch 29: loss 16.128612053919383
Epoch 30: loss 16.128588175760655
Epoch 31: loss 16.128559438219014
Epoch 32: loss 16.12854772187109
Epoch 33: loss 16.128533649859445
Epoch 34: loss 16.12851338933641
Epoch 35: loss 16.128491474273996
Epoch 36: loss 16.128467890151885
Epoch 37: loss 16.128452238536127
Epoch 38: loss 16.12843244810867
Epoch 39: loss 16.128418307903345
Epoch 40: loss 16.12839005549619
Epoch 41: loss 16.128374646836612
Epoch 42: loss 16.128358403776833
Epoch 43: loss 16.128335958981786
Epoch 44: loss 16.128317350145913
Epoch 45: loss 16.128302870268662
Epoch 46: loss 16.128280261342066
Epoch 47: loss 16.128259571950807
Epoch 48: loss 16.128236461554355
Epoch 49: loss 16.128214920131068
-----------Time: 0:09:34.999483, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017075061798096-------------


Epoch 0: loss 16.12917285279258
Epoch 1: loss 16.129150630988267
Epoch 2: loss 16.129127361386804
Epoch 3: loss 16.129109163268744
Epoch 4: loss 16.129092815455177
Epoch 5: loss 16.129069130727945
Epoch 6: loss 16.129053697954255
Epoch 7: loss 16.129030464135024
Epoch 8: loss 16.12900740144822
Epoch 9: loss 16.12899191889055
Epoch 10: loss 16.128982328214743
Epoch 11: loss 16.12895161357504
Epoch 12: loss 16.128931810701587
Epoch 13: loss 16.128919004551285
Epoch 14: loss 16.128899358030633
Epoch 15: loss 16.12887369594605
Epoch 16: loss 16.128849304130767
Epoch 17: loss 16.128829014307783
Epoch 18: loss 16.12881817747679
Epoch 19: loss 16.12880166682813
Epoch 20: loss 16.128777695065157
Epoch 21: loss 16.128757884412956
Epoch 22: loss 16.128739857168025
Epoch 23: loss 16.128724660349643
Epoch 24: loss 16.128699891524448
Epoch 25: loss 16.128676555285058
Epoch 26: loss 16.128668197799858
Epoch 27: loss 16.128646305814392
Epoch 28: loss 16.128628917204548
Epoch 29: loss 16.128603505595603
Epoch 30: loss 16.128587748448183
Epoch 31: loss 16.12856318109252
Epoch 32: loss 16.12855148185784
Epoch 33: loss 16.128522844402735
Epoch 34: loss 16.128507189675478
Epoch 35: loss 16.128491498906694
Epoch 36: loss 16.12847915792543
Epoch 37: loss 16.1284558719886
Epoch 38: loss 16.128434622009014
Epoch 39: loss 16.128417130201132
Epoch 40: loss 16.128389686265024
Epoch 41: loss 16.128371180108605
Epoch 42: loss 16.12835105026851
Epoch 43: loss 16.12833817462807
Epoch 44: loss 16.128311212455664
Epoch 45: loss 16.128298209762367
Epoch 46: loss 16.128271692274964
Epoch 47: loss 16.128251196315198
Epoch 48: loss 16.128242778415068
Epoch 49: loss 16.12822153336202
-----------Time: 0:09:39.422290, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017065048217773-------------


Epoch 0: loss 16.12916502815151
Epoch 1: loss 16.12914629381853
Epoch 2: loss 16.129124617822924
Epoch 3: loss 16.129103817973466
Epoch 4: loss 16.12908581458336
Epoch 5: loss 16.129065484310896
Epoch 6: loss 16.12904158514955
Epoch 7: loss 16.129029021436846
Epoch 8: loss 16.129010859360314
Epoch 9: loss 16.12899061672839
Epoch 10: loss 16.12896957910858
Epoch 11: loss 16.128946642178178
Epoch 12: loss 16.12893539514796
Epoch 13: loss 16.128918219157182
Epoch 14: loss 16.1288951497288
Epoch 15: loss 16.12887634123843
Epoch 16: loss 16.128853922891125
Epoch 17: loss 16.12883347230738
Epoch 18: loss 16.12881746027637
Epoch 19: loss 16.128795573217445
Epoch 20: loss 16.128774106988704
Epoch 21: loss 16.12876189824613
Epoch 22: loss 16.12873491377465
Epoch 23: loss 16.128719432254147
Epoch 24: loss 16.12870118072203
Epoch 25: loss 16.128677817775607
Epoch 26: loss 16.128658349906836
Epoch 27: loss 16.12864067685416
Epoch 28: loss 16.12862450432168
Epoch 29: loss 16.128603793927805
Epoch 30: loss 16.128581587940275
Epoch 31: loss 16.128568107631946
Epoch 32: loss 16.12854900743859
Epoch 33: loss 16.12852649315341
Epoch 34: loss 16.128505123381125
Epoch 35: loss 16.128483787316743
Epoch 36: loss 16.12846750795614
Epoch 37: loss 16.128454081580458
Epoch 38: loss 16.128433652258618
Epoch 39: loss 16.12841124480156
Epoch 40: loss 16.128393239077827
Epoch 41: loss 16.128374670691436
Epoch 42: loss 16.1283472736871
Epoch 43: loss 16.128333656472833
Epoch 44: loss 16.128308919021915
Epoch 45: loss 16.12829379195296
Epoch 46: loss 16.128278663328253
Epoch 47: loss 16.12825117634975
Epoch 48: loss 16.128232449536224
Epoch 49: loss 16.128217997143878
-----------Time: 0:12:18.153542, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017070770263672-------------


Epoch 0: loss 16.129169150627845
Epoch 1: loss 16.129145494422684
Epoch 2: loss 16.12912844729981
Epoch 3: loss 16.12911123319318
Epoch 4: loss 16.129090852618152
Epoch 5: loss 16.129067064952178
Epoch 6: loss 16.129041761208516
Epoch 7: loss 16.12903504374236
Epoch 8: loss 16.129013028852704
Epoch 9: loss 16.12898022302666
Epoch 10: loss 16.128970617830525
Epoch 11: loss 16.12895317684379
Epoch 12: loss 16.128930576214522
Epoch 13: loss 16.128917865483512
Epoch 14: loss 16.12889741541835
Epoch 15: loss 16.128870128872215
Epoch 16: loss 16.128849906465035
Epoch 17: loss 16.128840389168737
Epoch 18: loss 16.12881823918818
Epoch 19: loss 16.128800426377364
Epoch 20: loss 16.128783715296336
Epoch 21: loss 16.128763375430083
Epoch 22: loss 16.12873649934247
Epoch 23: loss 16.128722182559603
Epoch 24: loss 16.128706214608155
Epoch 25: loss 16.12868642988511
Epoch 26: loss 16.128670920879703
Epoch 27: loss 16.12864255853293
Epoch 28: loss 16.128625158254966
Epoch 29: loss 16.128602424090552
Epoch 30: loss 16.12858169865777
Epoch 31: loss 16.128561391980835
Epoch 32: loss 16.128558448762462
Epoch 33: loss 16.12852864786618
Epoch 34: loss 16.128507152596786
Epoch 35: loss 16.128483392415713
Epoch 36: loss 16.12846865039471
Epoch 37: loss 16.12845268762909
Epoch 38: loss 16.128430225980093
Epoch 39: loss 16.12841191610738
Epoch 40: loss 16.128390521443105
Epoch 41: loss 16.128369101627552
Epoch 42: loss 16.128355762633117
Epoch 43: loss 16.128332108242994
Epoch 44: loss 16.128321942717825
Epoch 45: loss 16.1282963923879
Epoch 46: loss 16.128271735835945
Epoch 47: loss 16.128255687763406
Epoch 48: loss 16.128235877629788
Epoch 49: loss 16.128218859028983
-----------Time: 0:12:12.296894, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017068862915039-------------


Epoch 0: loss 16.129293278418785
Epoch 1: loss 16.129097983135743
Epoch 2: loss 16.128899519050684
Epoch 3: loss 16.12871155082823
Epoch 4: loss 16.12851222096869
Epoch 5: loss 16.12831820840094
Epoch 6: loss 16.128131221078412
Epoch 7: loss 16.127935272380668
Epoch 8: loss 16.127746079264938
Epoch 9: loss 16.1275464086963
Epoch 10: loss 16.127360079715018
Epoch 11: loss 16.12715703705979
Epoch 12: loss 16.126963199513835
Epoch 13: loss 16.126777050221595
Epoch 14: loss 16.12658393869198
Epoch 15: loss 16.126393853354028
Epoch 16: loss 16.126196601456957
Epoch 17: loss 16.126004946886553
Epoch 18: loss 16.12580782307951
Epoch 19: loss 16.12560866072231
Epoch 20: loss 16.125420784807645
Epoch 21: loss 16.125225559792614
Epoch 22: loss 16.12503899952327
Epoch 23: loss 16.12484057718415
Epoch 24: loss 16.124645340241706
Epoch 25: loss 16.124443251001505
Epoch 26: loss 16.12425777742304
Epoch 27: loss 16.12407750108438
Epoch 28: loss 16.123877517550845
Epoch 29: loss 16.123686242323974
Epoch 30: loss 16.123488388870513
Epoch 31: loss 16.12329435426298
Epoch 32: loss 16.123097335469016
Epoch 33: loss 16.122907046587198
Epoch 34: loss 16.12270098837126
Epoch 35: loss 16.122516622227
Epoch 36: loss 16.12232374820844
Epoch 37: loss 16.122130902971243
Epoch 38: loss 16.12193824727617
Epoch 39: loss 16.121745879394712
Epoch 40: loss 16.121556827333585
Epoch 41: loss 16.121355254602147
Epoch 42: loss 16.1211666817118
Epoch 43: loss 16.120972542350483
Epoch 44: loss 16.120780032117967
Epoch 45: loss 16.120587954124463
Epoch 46: loss 16.120391944752498
Epoch 47: loss 16.120201567192968
Epoch 48: loss 16.120010743651978
Epoch 49: loss 16.119811061155932
-----------Time: 0:08:35.394101, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016026973724365-------------


Epoch 0: loss 16.129088636193995
Epoch 1: loss 16.12889386467988
Epoch 2: loss 16.128704570959027
Epoch 3: loss 16.128497725533947
Epoch 4: loss 16.128314432338115
Epoch 5: loss 16.12811822720103
Epoch 6: loss 16.127918172880904
Epoch 7: loss 16.127731530156844
Epoch 8: loss 16.127528356559385
Epoch 9: loss 16.127352518773456
Epoch 10: loss 16.12714818562419
Epoch 11: loss 16.126960624748758
Epoch 12: loss 16.12676111597805
Epoch 13: loss 16.126570108080767
Epoch 14: loss 16.12637523414649
Epoch 15: loss 16.12617686807364
Epoch 16: loss 16.125992265196196
Epoch 17: loss 16.125795350896723
Epoch 18: loss 16.1256016105851
Epoch 19: loss 16.12541201427101
Epoch 20: loss 16.125219155809944
Epoch 21: loss 16.125023360353506
Epoch 22: loss 16.124829620819757
Epoch 23: loss 16.124643084923814
Epoch 24: loss 16.124438328092157
Epoch 25: loss 16.12423552294799
Epoch 26: loss 16.12405302707367
Epoch 27: loss 16.123866183917247
Epoch 28: loss 16.12366644593281
Epoch 29: loss 16.123473941664
Epoch 30: loss 16.123283542842564
Epoch 31: loss 16.12309261013983
Epoch 32: loss 16.122886048120414
Epoch 33: loss 16.12269663927411
Epoch 34: loss 16.12250426024312
Epoch 35: loss 16.12231384378986
Epoch 36: loss 16.12212560694136
Epoch 37: loss 16.12192433006158
Epoch 38: loss 16.121730562005762
Epoch 39: loss 16.121542214958353
Epoch 40: loss 16.12134335675014
Epoch 41: loss 16.121148389470907
Epoch 42: loss 16.120961809754697
Epoch 43: loss 16.120769422426374
Epoch 44: loss 16.120570758946116
Epoch 45: loss 16.120387655551696
Epoch 46: loss 16.120188908579557
Epoch 47: loss 16.119987357110027
Epoch 48: loss 16.11980056192254
Epoch 49: loss 16.11960458055406
-----------Time: 0:07:49.857687, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015985012054443-------------


Epoch 0: loss 16.12908648718564
Epoch 1: loss 16.12889102828963
Epoch 2: loss 16.12869785297429
Epoch 3: loss 16.12850561344215
Epoch 4: loss 16.128311017987002
Epoch 5: loss 16.12811481829504
Epoch 6: loss 16.12792124315213
Epoch 7: loss 16.127725120210467
Epoch 8: loss 16.127536741010907
Epoch 9: loss 16.127346144868604
Epoch 10: loss 16.127149718296533
Epoch 11: loss 16.126953670290128
Epoch 12: loss 16.12676286101017
Epoch 13: loss 16.126567903324727
Epoch 14: loss 16.12637397969398
Epoch 15: loss 16.12618058683303
Epoch 16: loss 16.125999753795416
Epoch 17: loss 16.125794887542725
Epoch 18: loss 16.12559725604283
Epoch 19: loss 16.125409459212086
Epoch 20: loss 16.125205432804723
Epoch 21: loss 16.125023648945
Epoch 22: loss 16.124827192813694
Epoch 23: loss 16.124636455357496
Epoch 24: loss 16.124445458350458
Epoch 25: loss 16.12424527957039
Epoch 26: loss 16.124060816451088
Epoch 27: loss 16.12386200232244
Epoch 28: loss 16.123663682922064
Epoch 29: loss 16.123473693262696
Epoch 30: loss 16.123279144739314
Epoch 31: loss 16.123084638739748
Epoch 32: loss 16.122887875347956
Epoch 33: loss 16.122694853792506
Epoch 34: loss 16.122505619190125
Epoch 35: loss 16.1223154656585
Epoch 36: loss 16.122112443487303
Epoch 37: loss 16.121921838269795
Epoch 38: loss 16.12172614056627
Epoch 39: loss 16.12153966456668
Epoch 40: loss 16.121338932717734
Epoch 41: loss 16.121153683945042
Epoch 42: loss 16.12094908035469
Epoch 43: loss 16.12076439242962
Epoch 44: loss 16.120571969319062
Epoch 45: loss 16.120378081470548
Epoch 46: loss 16.120184226811354
Epoch 47: loss 16.119995469824005
Epoch 48: loss 16.119794994673697
Epoch 49: loss 16.119604656526484
-----------Time: 0:11:07.803592, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015987873077393-------------


Epoch 0: loss 16.129076214573058
Epoch 1: loss 16.128879386617673
Epoch 2: loss 16.128691137582468
Epoch 3: loss 16.128497013260063
Epoch 4: loss 16.128298148310268
Epoch 5: loss 16.128108241883485
Epoch 6: loss 16.12791472871126
Epoch 7: loss 16.127722539740972
Epoch 8: loss 16.127524795449254
Epoch 9: loss 16.127337550131635
Epoch 10: loss 16.127145561334423
Epoch 11: loss 16.12695115801431
Epoch 12: loss 16.126756453916002
Epoch 13: loss 16.126558417921288
Epoch 14: loss 16.126366687637752
Epoch 15: loss 16.126170048187323
Epoch 16: loss 16.125976166043223
Epoch 17: loss 16.125787322193204
Epoch 18: loss 16.12558878861801
Epoch 19: loss 16.125398878561146
Epoch 20: loss 16.12521004637925
Epoch 21: loss 16.12501201194029
Epoch 22: loss 16.124815046560382
Epoch 23: loss 16.12462039769117
Epoch 24: loss 16.124433492823357
Epoch 25: loss 16.124239864784975
Epoch 26: loss 16.12404985645661
Epoch 27: loss 16.123854242244967
Epoch 28: loss 16.123656410312705
Epoch 29: loss 16.123461217449826
Epoch 30: loss 16.123270819406265
Epoch 31: loss 16.123083203561023
Epoch 32: loss 16.12287786098976
Epoch 33: loss 16.122691378507877
Epoch 34: loss 16.122501880724577
Epoch 35: loss 16.122298562442435
Epoch 36: loss 16.12211184193091
Epoch 37: loss 16.12191885019399
Epoch 38: loss 16.121719818260438
Epoch 39: loss 16.121533986341053
Epoch 40: loss 16.12133808353791
Epoch 41: loss 16.121145634498202
Epoch 42: loss 16.120950212940176
Epoch 43: loss 16.12075450797649
Epoch 44: loss 16.120567250472167
Epoch 45: loss 16.120373496936676
Epoch 46: loss 16.12017634408898
Epoch 47: loss 16.11998578191387
Epoch 48: loss 16.11979402284897
Epoch 49: loss 16.119594883050137
-----------Time: 0:11:12.719606, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015988826751709-------------


Epoch 0: loss 16.129081857535034
Epoch 1: loss 16.12888875196912
Epoch 2: loss 16.12869265754953
Epoch 3: loss 16.128497487763596
Epoch 4: loss 16.128298001032668
Epoch 5: loss 16.128110312585793
Epoch 6: loss 16.127917497426417
Epoch 7: loss 16.12773070094247
Epoch 8: loss 16.127528693379105
Epoch 9: loss 16.12733764295801
Epoch 10: loss 16.12714307317272
Epoch 11: loss 16.126953940731212
Epoch 12: loss 16.12675928797263
Epoch 13: loss 16.12656471766876
Epoch 14: loss 16.126367776662256
Epoch 15: loss 16.12617603963714
Epoch 16: loss 16.125984370027826
Epoch 17: loss 16.125790001452778
Epoch 18: loss 16.125597004011443
Epoch 19: loss 16.125402763267132
Epoch 20: loss 16.125215481130112
Epoch 21: loss 16.12501740494521
Epoch 22: loss 16.124826370081607
Epoch 23: loss 16.124625013080742
Epoch 24: loss 16.124434121346074
Epoch 25: loss 16.124239438250378
Epoch 26: loss 16.124052900798688
Epoch 27: loss 16.123852597817965
Epoch 28: loss 16.123659067013914
Epoch 29: loss 16.123465733012146
Epoch 30: loss 16.123271040322667
Epoch 31: loss 16.123082724390244
Epoch 32: loss 16.12289513991928
Epoch 33: loss 16.122698443424706
Epoch 34: loss 16.12249544718267
Epoch 35: loss 16.122307305494168
Epoch 36: loss 16.12211843467783
Epoch 37: loss 16.121923346050153
Epoch 38: loss 16.121722595532216
Epoch 39: loss 16.121530797573584
Epoch 40: loss 16.121344885792404
Epoch 41: loss 16.12114498134279
Epoch 42: loss 16.12095159314909
Epoch 43: loss 16.120758729502192
Epoch 44: loss 16.120563774928247
Epoch 45: loss 16.120368409636487
Epoch 46: loss 16.120173612711806
Epoch 47: loss 16.11999190430592
Epoch 48: loss 16.119794013514475
Epoch 49: loss 16.119601571475638
-----------Time: 0:13:17.653681, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015990257263184-------------


Epoch 0: loss 16.12834026399937
Epoch 1: loss 16.126406837896916
Epoch 2: loss 16.12447035370993
Epoch 3: loss 16.12253554195344
Epoch 4: loss 16.120608943256755
Epoch 5: loss 16.11867659736289
Epoch 6: loss 16.116735781969876
Epoch 7: loss 16.114798349035116
Epoch 8: loss 16.112875244551343
Epoch 9: loss 16.11092985758903
Epoch 10: loss 16.10900156338491
Epoch 11: loss 16.107065656640206
Epoch 12: loss 16.105139954314406
Epoch 13: loss 16.103205734261227
Epoch 14: loss 16.101270322763384
Epoch 15: loss 16.09933420884473
Epoch 16: loss 16.097404945149506
Epoch 17: loss 16.09546497608411
Epoch 18: loss 16.093534046181393
Epoch 19: loss 16.091601686545076
Epoch 20: loss 16.089669032612854
Epoch 21: loss 16.087736953529994
Epoch 22: loss 16.0858109532782
Epoch 23: loss 16.083869708498383
Epoch 24: loss 16.08194790176881
Epoch 25: loss 16.08001458197589
Epoch 26: loss 16.078084380682427
Epoch 27: loss 16.0761508340094
Epoch 28: loss 16.074217962531566
Epoch 29: loss 16.07228884248386
Epoch 30: loss 16.07035943540041
Epoch 31: loss 16.068426141536644
Epoch 32: loss 16.066505453131523
Epoch 33: loss 16.064564769458617
Epoch 34: loss 16.062637264278674
Epoch 35: loss 16.060712421245068
Epoch 36: loss 16.058773488049408
Epoch 37: loss 16.05684789514464
Epoch 38: loss 16.054915480797806
Epoch 39: loss 16.052982002318455
Epoch 40: loss 16.051052613903998
Epoch 41: loss 16.049129533534337
Epoch 42: loss 16.04719277009036
Epoch 43: loss 16.04526944261585
Epoch 44: loss 16.043342303036994
Epoch 45: loss 16.04141163268512
Epoch 46: loss 16.039478366306255
Epoch 47: loss 16.037552355682806
Epoch 48: loss 16.03563144791703
Epoch 49: loss 16.033692195274185
-----------Time: 0:07:44.094977, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005220413208008-------------


Epoch 0: loss 16.12822418721102
Epoch 1: loss 16.126290059465635
Epoch 2: loss 16.12436304616176
Epoch 3: loss 16.122416685300394
Epoch 4: loss 16.12048781002391
Epoch 5: loss 16.118550375533403
Epoch 6: loss 16.116614987371797
Epoch 7: loss 16.114683317969583
Epoch 8: loss 16.112749892904294
Epoch 9: loss 16.11081367863981
Epoch 10: loss 16.10888356291256
Epoch 11: loss 16.10695085893707
Epoch 12: loss 16.10502162531966
Epoch 13: loss 16.10307881804897
Epoch 14: loss 16.101145969648087
Epoch 15: loss 16.099213322716736
Epoch 16: loss 16.0972791104423
Epoch 17: loss 16.09534335486457
Epoch 18: loss 16.09341430923354
Epoch 19: loss 16.091474335760186
Epoch 20: loss 16.089550459624764
Epoch 21: loss 16.08761614181867
Epoch 22: loss 16.08568212582794
Epoch 23: loss 16.0837517274729
Epoch 24: loss 16.081817568093943
Epoch 25: loss 16.07988699679144
Epoch 26: loss 16.07795764519638
Epoch 27: loss 16.076023258418736
Epoch 28: loss 16.074088686766217
Epoch 29: loss 16.072158625230898
Epoch 30: loss 16.070222052106914
Epoch 31: loss 16.06829765323973
Epoch 32: loss 16.066360790227794
Epoch 33: loss 16.064436159813255
Epoch 34: loss 16.062503581853456
Epoch 35: loss 16.06056683137405
Epoch 36: loss 16.05863457130569
Epoch 37: loss 16.05670379957082
Epoch 38: loss 16.054781453514824
Epoch 39: loss 16.05285064418268
Epoch 40: loss 16.05092370036894
Epoch 41: loss 16.04899418204942
Epoch 42: loss 16.04706605456976
Epoch 43: loss 16.045133418787945
Epoch 44: loss 16.043202324235097
Epoch 45: loss 16.04127410470694
Epoch 46: loss 16.03934682096198
Epoch 47: loss 16.037414900565548
Epoch 48: loss 16.035480311799788
Epoch 49: loss 16.033560045262014
-----------Time: 0:09:26.956089, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.0051960945129395-------------


Epoch 0: loss 16.128213115202595
Epoch 1: loss 16.126276994023783
Epoch 2: loss 16.12434282582891
Epoch 3: loss 16.122416593251895
Epoch 4: loss 16.120469369727545
Epoch 5: loss 16.118535946218003
Epoch 6: loss 16.116606171977722
Epoch 7: loss 16.11467578192001
Epoch 8: loss 16.11273915304834
Epoch 9: loss 16.110804713634515
Epoch 10: loss 16.10887634216151
Epoch 11: loss 16.1069400705936
Epoch 12: loss 16.105011092119078
Epoch 13: loss 16.103070012767265
Epoch 14: loss 16.101140569901585
Epoch 15: loss 16.099204553476554
Epoch 16: loss 16.09727069591296
Epoch 17: loss 16.095339134894616
Epoch 18: loss 16.093410134639605
Epoch 19: loss 16.091473851662865
Epoch 20: loss 16.089542869901837
Epoch 21: loss 16.087610872495816
Epoch 22: loss 16.085671066784094
Epoch 23: loss 16.083744371630953
Epoch 24: loss 16.081810330488942
Epoch 25: loss 16.079882283908248
Epoch 26: loss 16.077946299894663
Epoch 27: loss 16.076015201971025
Epoch 28: loss 16.074084318481496
Epoch 29: loss 16.072151718481912
Epoch 30: loss 16.070220599037075
Epoch 31: loss 16.0682906036211
Epoch 32: loss 16.06636306006601
Epoch 33: loss 16.064425414771474
Epoch 34: loss 16.062500187727082
Epoch 35: loss 16.06057016767841
Epoch 36: loss 16.058644729570698
Epoch 37: loss 16.056705580385177
Epoch 38: loss 16.054775882376607
Epoch 39: loss 16.052848296556995
Epoch 40: loss 16.050918184459825
Epoch 41: loss 16.04898591220476
Epoch 42: loss 16.04705642448164
Epoch 43: loss 16.045117655158236
Epoch 44: loss 16.043195910658635
Epoch 45: loss 16.04127075491942
Epoch 46: loss 16.03933231697062
Epoch 47: loss 16.037407562355426
Epoch 48: loss 16.03547901456417
Epoch 49: loss 16.03354737083182
-----------Time: 0:10:02.832136, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.005199432373047-------------


Epoch 0: loss 16.128214960061968
Epoch 1: loss 16.12628512022092
Epoch 2: loss 16.124349642344438
Epoch 3: loss 16.122419200169126
Epoch 4: loss 16.120480506558856
Epoch 5: loss 16.11854860197921
Epoch 6: loss 16.116613604310675
Epoch 7: loss 16.114676155818422
Epoch 8: loss 16.11274211130562
Epoch 9: loss 16.11081375694586
Epoch 10: loss 16.108869567391924
Epoch 11: loss 16.10693829678011
Epoch 12: loss 16.105008028070845
Epoch 13: loss 16.103078680624453
Epoch 14: loss 16.101147442942665
Epoch 15: loss 16.099207863592465
Epoch 16: loss 16.097281409061882
Epoch 17: loss 16.09534401605802
Epoch 18: loss 16.093407855726184
Epoch 19: loss 16.091480682698716
Epoch 20: loss 16.089554996967575
Epoch 21: loss 16.08761247854767
Epoch 22: loss 16.08568399739434
Epoch 23: loss 16.083752438191034
Epoch 24: loss 16.081824422984617
Epoch 25: loss 16.079888159195452
Epoch 26: loss 16.07795827475626
Epoch 27: loss 16.076024319958336
Epoch 28: loss 16.07409677692183
Epoch 29: loss 16.072160911145193
Epoch 30: loss 16.07022707717713
Epoch 31: loss 16.068290730414667
Epoch 32: loss 16.06637038894164
Epoch 33: loss 16.06443722524223
Epoch 34: loss 16.062505486609172
Epoch 35: loss 16.060571212882643
Epoch 36: loss 16.058638930255917
Epoch 37: loss 16.056706799314746
Epoch 38: loss 16.054786030788534
Epoch 39: loss 16.05284613536194
Epoch 40: loss 16.05092281125822
Epoch 41: loss 16.048984695608816
Epoch 42: loss 16.04706297081537
Epoch 43: loss 16.045128039525473
Epoch 44: loss 16.04320381464292
Epoch 45: loss 16.04126812285091
Epoch 46: loss 16.03934182700675
Epoch 47: loss 16.03740549009737
Epoch 48: loss 16.03547956192863
Epoch 49: loss 16.033552238512065
-----------Time: 0:12:01.695901, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005198955535889-------------


Epoch 0: loss 16.128215497314056
Epoch 1: loss 16.126286722642693
Epoch 2: loss 16.12433982686286
Epoch 3: loss 16.122405776645646
Epoch 4: loss 16.120474080795695
Epoch 5: loss 16.118538161086413
Epoch 6: loss 16.1166086546943
Epoch 7: loss 16.114675392723395
Epoch 8: loss 16.112732595824365
Epoch 9: loss 16.11080599712768
Epoch 10: loss 16.10887274293552
Epoch 11: loss 16.106934361771568
Epoch 12: loss 16.105000955634562
Epoch 13: loss 16.103071383382392
Epoch 14: loss 16.101139814844593
Epoch 15: loss 16.099205875863454
Epoch 16: loss 16.09726715995411
Epoch 17: loss 16.095339888395856
Epoch 18: loss 16.093404060216493
Epoch 19: loss 16.0914720679963
Epoch 20: loss 16.089539470071053
Epoch 21: loss 16.087610525823017
Epoch 22: loss 16.08567263005284
Epoch 23: loss 16.083737115616252
Epoch 24: loss 16.081810013116087
Epoch 25: loss 16.079884567488918
Epoch 26: loss 16.077943711646423
Epoch 27: loss 16.076009429622566
Epoch 28: loss 16.07408080715534
Epoch 29: loss 16.072148129627585
Epoch 30: loss 16.070213604647545
Epoch 31: loss 16.068282886586026
Epoch 32: loss 16.066351050200055
Epoch 33: loss 16.064416254001056
Epoch 34: loss 16.062488142338182
Epoch 35: loss 16.060553651584627
Epoch 36: loss 16.058619519431282
Epoch 37: loss 16.056690740611256
Epoch 38: loss 16.054753488402707
Epoch 39: loss 16.05282493127695
Epoch 40: loss 16.050888261955805
Epoch 41: loss 16.04896208175568
Epoch 42: loss 16.047026094889887
Epoch 43: loss 16.04508811692429
Epoch 44: loss 16.043160555478085
Epoch 45: loss 16.041229391694394
Epoch 46: loss 16.03929704787486
Epoch 47: loss 16.037363678816547
Epoch 48: loss 16.035433791265856
Epoch 49: loss 16.03349735167702
-----------Time: 0:12:44.596715, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005194187164307-------------


Epoch 0: loss 16.11978467383289
Epoch 1: loss 16.1004615753095
Epoch 2: loss 16.081133710748155
Epoch 3: loss 16.061806069177543
Epoch 4: loss 16.042510788488674
Epoch 5: loss 16.023225417663507
Epoch 6: loss 16.003960411596584
Epoch 7: loss 15.984706216950077
Epoch 8: loss 15.96545960295129
Epoch 9: loss 15.946218107626969
Epoch 10: loss 15.92700664578345
Epoch 11: loss 15.907789227494472
Epoch 12: loss 15.888591207324843
Epoch 13: loss 15.869410925549357
Epoch 14: loss 15.850239794431918
Epoch 15: loss 15.831085496262535
Epoch 16: loss 15.811942146419506
Epoch 17: loss 15.792815489766388
Epoch 18: loss 15.77369334789772
Epoch 19: loss 15.754593805621148
Epoch 20: loss 15.735497157816138
Epoch 21: loss 15.716421576412298
Epoch 22: loss 15.697356839877488
Epoch 23: loss 15.678308926956236
Epoch 24: loss 15.659272220097138
Epoch 25: loss 15.64024003165257
Epoch 26: loss 15.621233552742938
Epoch 27: loss 15.602233073839745
Epoch 28: loss 15.583241224418586
Epoch 29: loss 15.564257663770368
Epoch 30: loss 15.545287203827652
Epoch 31: loss 15.526336431892233
Epoch 32: loss 15.507395452361447
Epoch 33: loss 15.488468080969723
Epoch 34: loss 15.469539567398726
Epoch 35: loss 15.450628420397793
Epoch 36: loss 15.43174140980997
Epoch 37: loss 15.412870319463948
Epoch 38: loss 15.393994427297736
Epoch 39: loss 15.37513803178166
Epoch 40: loss 15.356285264532225
Epoch 41: loss 15.33744972616904
Epoch 42: loss 15.318619211320323
Epoch 43: loss 15.299813194337132
Epoch 44: loss 15.281011451515832
Epoch 45: loss 15.26222412789756
Epoch 46: loss 15.243453351488057
Epoch 47: loss 15.224677722177926
Epoch 48: loss 15.205921190727528
Epoch 49: loss 15.187179199050728
-----------Time: 0:08:31.902666, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.897545337677002-------------


Epoch 0: loss 16.11950652997347
Epoch 1: loss 16.10017976428971
Epoch 2: loss 16.080848439223356
Epoch 3: loss 16.06155286916512
Epoch 4: loss 16.04225159287777
Epoch 5: loss 16.022963366474777
Epoch 6: loss 16.003696576481996
Epoch 7: loss 15.984429266868952
Epoch 8: loss 15.965182398764448
Epoch 9: loss 15.945936055984621
Epoch 10: loss 15.926705046669824
Epoch 11: loss 15.90748330625002
Epoch 12: loss 15.888259694263818
Epoch 13: loss 15.869054963628905
Epoch 14: loss 15.849876049616338
Epoch 15: loss 15.830681846477598
Epoch 16: loss 15.81150118950724
Epoch 17: loss 15.792339756930891
Epoch 18: loss 15.773180569819356
Epoch 19: loss 15.754026295764087
Epoch 20: loss 15.734884343502507
Epoch 21: loss 15.715741974559409
Epoch 22: loss 15.696606145986337
Epoch 23: loss 15.677470579038436
Epoch 24: loss 15.658316716997438
Epoch 25: loss 15.639185960685642
Epoch 26: loss 15.620042326140494
Epoch 27: loss 15.600910602930513
Epoch 28: loss 15.58176683355376
Epoch 29: loss 15.562616298741915
Epoch 30: loss 15.543453759509237
Epoch 31: loss 15.524283586992654
Epoch 32: loss 15.505102865458701
Epoch 33: loss 15.485883155032433
Epoch 34: loss 15.466659617722197
Epoch 35: loss 15.447419972183784
Epoch 36: loss 15.428135040079919
Epoch 37: loss 15.408830710115998
Epoch 38: loss 15.389480854779109
Epoch 39: loss 15.37009691842034
Epoch 40: loss 15.350664642078842
Epoch 41: loss 15.331167378718598
Epoch 42: loss 15.31162217157311
Epoch 43: loss 15.29200052501975
Epoch 44: loss 15.272314407178538
Epoch 45: loss 15.252554185627725
Epoch 46: loss 15.232699952480779
Epoch 47: loss 15.212759406363077
Epoch 48: loss 15.192724192641624
Epoch 49: loss 15.172569692102446
-----------Time: 0:08:15.232203, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.89554762840271-------------


Epoch 0: loss 16.11959354406858
Epoch 1: loss 16.100264516584637
Epoch 2: loss 16.080942902505367
Epoch 3: loss 16.061627372702286
Epoch 4: loss 16.042332133500068
Epoch 5: loss 16.023041667336678
Epoch 6: loss 16.003765181655012
Epoch 7: loss 15.98449042207719
Epoch 8: loss 15.965245035305303
Epoch 9: loss 15.945971868036882
Epoch 10: loss 15.926719856884507
Epoch 11: loss 15.90747131738668
Epoch 12: loss 15.888222530265422
Epoch 13: loss 15.868958338373961
Epoch 14: loss 15.84967870722652
Epoch 15: loss 15.830375716503449
Epoch 16: loss 15.811054177359436
Epoch 17: loss 15.791705747088898
Epoch 18: loss 15.772311329387854
Epoch 19: loss 15.752851719568449
Epoch 20: loss 15.733338798369449
Epoch 21: loss 15.713713489063672
Epoch 22: loss 15.693985432370193
Epoch 23: loss 15.674125873634127
Epoch 24: loss 15.654099432782935
Epoch 25: loss 15.633894229337143
Epoch 26: loss 15.613437380072472
Epoch 27: loss 15.592713273563353
Epoch 28: loss 15.571663906031034
Epoch 29: loss 15.55027039612422
Epoch 30: loss 15.528463601158519
Epoch 31: loss 15.506213268292475
Epoch 32: loss 15.483441060104079
Epoch 33: loss 15.460099532203612
Epoch 34: loss 15.436144511682823
Epoch 35: loss 15.411512092768724
Epoch 36: loss 15.386151329339231
Epoch 37: loss 15.360005287193228
Epoch 38: loss 15.333025219519545
Epoch 39: loss 15.305169835435494
Epoch 40: loss 15.276391016393333
Epoch 41: loss 15.24662897339199
Epoch 42: loss 15.215862524386267
Epoch 43: loss 15.184035525236395
Epoch 44: loss 15.151102061632082
Epoch 45: loss 15.117052259751155
Epoch 46: loss 15.081839124556142
Epoch 47: loss 15.045439796126232
Epoch 48: loss 15.007821283241924
Epoch 49: loss 14.96895707956535
-----------Time: 0:10:53.209072, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.8681399822235107-------------


Epoch 0: loss 16.11958850370016
Epoch 1: loss 16.100262389097477
Epoch 2: loss 16.080943583923556
Epoch 3: loss 16.061638799680843
Epoch 4: loss 16.042334849319747
Epoch 5: loss 16.023045205888444
Epoch 6: loss 16.003756176718817
Epoch 7: loss 15.984460681336545
Epoch 8: loss 15.965153854913591
Epoch 9: loss 15.945836803069122
Epoch 10: loss 15.926493523106618
Epoch 11: loss 15.907122895923758
Epoch 12: loss 15.887691034708029
Epoch 13: loss 15.868172546778768
Epoch 14: loss 15.848533195279876
Epoch 15: loss 15.82875014777544
Epoch 16: loss 15.80873657815673
Epoch 17: loss 15.788469127625989
Epoch 18: loss 15.767825914116383
Epoch 19: loss 15.746749322267899
Epoch 20: loss 15.725129402870067
Epoch 21: loss 15.70283826347795
Epoch 22: loss 15.679795076174214
Epoch 23: loss 15.655851931465651
Epoch 24: loss 15.630913051721388
Epoch 25: loss 15.604820853280529
Epoch 26: loss 15.57748583077477
Epoch 27: loss 15.548785285887478
Epoch 28: loss 15.518627434336926
Epoch 29: loss 15.486923163322212
Epoch 30: loss 15.453543004165077
Epoch 31: loss 15.418431777290316
Epoch 32: loss 15.381522038632466
Epoch 33: loss 15.342730381101159
Epoch 34: loss 15.30202890388858
Epoch 35: loss 15.259341178736394
Epoch 36: loss 15.214646478396775
Epoch 37: loss 15.1678963306483
Epoch 38: loss 15.119115757384723
Epoch 39: loss 15.068236681609909
Epoch 40: loss 15.015259052502714
Epoch 41: loss 14.960158350157828
Epoch 42: loss 14.902959061550536
Epoch 43: loss 14.843613835117491
Epoch 44: loss 14.782136399827618
Epoch 45: loss 14.718556052255138
Epoch 46: loss 14.652861155654634
Epoch 47: loss 14.585074026471833
Epoch 48: loss 14.515194918812456
Epoch 49: loss 14.44320563455844
-----------Time: 0:11:12.534610, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.797692060470581-------------


Epoch 0: loss 16.119573031773445
Epoch 1: loss 16.10023550393466
Epoch 2: loss 16.080912934106742
Epoch 3: loss 16.061581964529097
Epoch 4: loss 16.042265269729093
Epoch 5: loss 16.02293853464173
Epoch 6: loss 16.00356137823838
Epoch 7: loss 15.984155375650229
Epoch 8: loss 15.964664986368234
Epoch 9: loss 15.945049168170309
Epoch 10: loss 15.92523467586117
Epoch 11: loss 15.905156497270793
Epoch 12: loss 15.884684530530954
Epoch 13: loss 15.863685207564005
Epoch 14: loss 15.842004555084577
Epoch 15: loss 15.819444247210006
Epoch 16: loss 15.795819736810532
Epoch 17: loss 15.77093284372534
Epoch 18: loss 15.744557895110702
Epoch 19: loss 15.716523876262787
Epoch 20: loss 15.686646650510356
Epoch 21: loss 15.654731837869534
Epoch 22: loss 15.620641645106886
Epoch 23: loss 15.584238298954427
Epoch 24: loss 15.545368421978766
Epoch 25: loss 15.50395656552504
Epoch 26: loss 15.45990987547459
Epoch 27: loss 15.413143228486286
Epoch 28: loss 15.363598294333313
Epoch 29: loss 15.311227568211018
Epoch 30: loss 15.256018210261201
Epoch 31: loss 15.19791216847688
Epoch 32: loss 15.136899047081986
Epoch 33: loss 15.07295116851873
Epoch 34: loss 15.006094702305257
Epoch 35: loss 14.93633900601427
Epoch 36: loss 14.863690466515198
Epoch 37: loss 14.788164216840702
Epoch 38: loss 14.709761228037632
Epoch 39: loss 14.62851192148695
Epoch 40: loss 14.544403601496553
Epoch 41: loss 14.45748828972987
Epoch 42: loss 14.3677449361211
Epoch 43: loss 14.275296422565289
Epoch 44: loss 14.180126285345546
Epoch 45: loss 14.082281781643093
Epoch 46: loss 13.981822912300197
Epoch 47: loss 13.878719431994854
Epoch 48: loss 13.773052410044833
Epoch 49: loss 13.664866399479793
-----------Time: 0:13:10.678352, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.6918113231658936-------------


Epoch 0: loss 16.033634721746648
Epoch 1: loss 15.841885502407122
Epoch 2: loss 15.651346688947319
Epoch 3: loss 15.462168838486456
Epoch 4: loss 15.274058669724498
Epoch 5: loss 15.08717934472072
Epoch 6: loss 14.901535633143165
Epoch 7: loss 14.716933881283584
Epoch 8: loss 14.53323805468312
Epoch 9: loss 14.350282736483186
Epoch 10: loss 14.167749823403787
Epoch 11: loss 13.98521385379561
Epoch 12: loss 13.802223161486843
Epoch 13: loss 13.617890732388188
Epoch 14: loss 13.431356651488693
Epoch 15: loss 13.2414944510281
Epoch 16: loss 13.04702744813248
Epoch 17: loss 12.846403265336468
Epoch 18: loss 12.638265590294345
Epoch 19: loss 12.421025109200325
Epoch 20: loss 12.193138124373634
Epoch 21: loss 11.953302206586015
Epoch 22: loss 11.700393700353343
Epoch 23: loss 11.433324006149599
Epoch 24: loss 11.151317762122847
Epoch 25: loss 10.854021761066049
Epoch 26: loss 10.541276126662437
Epoch 27: loss 10.213112849525423
Epoch 28: loss 9.87005552952545
Epoch 29: loss 9.512990444363817
Epoch 30: loss 9.142823523966886
Epoch 31: loss 8.760830978390443
Epoch 32: loss 8.368508587459171
Epoch 33: loss 7.967833813666779
Epoch 34: loss 7.5607046423689575
Epoch 35: loss 7.149118937507409
Epoch 36: loss 6.735527179499176
Epoch 37: loss 6.322229870251173
Epoch 38: loss 5.9117500105262
Epoch 39: loss 5.506675430845475
Epoch 40: loss 5.109488341392156
Epoch 41: loss 4.723068083479976
Epoch 42: loss 4.350226387956857
Epoch 43: loss 3.993550657577784
Epoch 44: loss 3.655735922417218
Epoch 45: loss 3.3391641946251203
Epoch 46: loss 3.0463406717472585
Epoch 47: loss 2.7791956956131165
Epoch 48: loss 2.5391415791903067
Epoch 49: loss 2.3267382912301318
-----------Time: 0:07:53.745734, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.504428505897522-------------


Epoch 0: loss 16.03351180458795
Epoch 1: loss 15.841588555761755
Epoch 2: loss 15.65053322212796
Epoch 3: loss 15.45957989197441
Epoch 4: loss 15.266475085268336
Epoch 5: loss 15.065990899943216
Epoch 6: loss 14.848764780748791
Epoch 7: loss 14.602354405949205
Epoch 8: loss 14.313830626147023
Epoch 9: loss 13.973986234659732
Epoch 10: loss 13.577960569745759
Epoch 11: loss 13.12473442166833
Epoch 12: loss 12.615767329072355
Epoch 13: loss 12.053979364149333
Epoch 14: loss 11.443395612549692
Epoch 15: loss 10.788943858040358
Epoch 16: loss 10.097303548670775
Epoch 17: loss 9.376326824414335
Epoch 18: loss 8.634832836092004
Epoch 19: loss 7.881826366011769
Epoch 20: loss 7.127781525067884
Epoch 21: loss 6.384154251569506
Epoch 22: loss 5.6616628167420515
Epoch 23: loss 4.972404344370989
Epoch 24: loss 4.328196963063396
Epoch 25: loss 3.7405072586117654
Epoch 26: loss 3.2193814355954973
Epoch 27: loss 2.7730269941575765
Epoch 28: loss 2.405310611870057
Epoch 29: loss 2.1140501392218263
Epoch 30: loss 1.8907360562557367
Epoch 31: loss 1.7225474654947563
Epoch 32: loss 1.595474459834563
Epoch 33: loss 1.4977473066253724
Epoch 34: loss 1.4206499520885743
Epoch 35: loss 1.358293950768383
Epoch 36: loss 1.306784116067985
Epoch 37: loss 1.2635716294354236
Epoch 38: loss 1.2269364724410754
Epoch 39: loss 1.1956736611697647
Epoch 40: loss 1.1687970844185047
Epoch 41: loss 1.145616211130802
Epoch 42: loss 1.1255943903526318
Epoch 43: loss 1.1082345626644623
Epoch 44: loss 1.0931434457136147
Epoch 45: loss 1.0800209765268318
Epoch 46: loss 1.0685733326950302
Epoch 47: loss 1.0585761342831705
Epoch 48: loss 1.0498359207714167
Epoch 49: loss 1.0421700577589401
-----------Time: 0:09:05.298379, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.0292662382125854-------------


Epoch 0: loss 16.03345212760083
Epoch 1: loss 15.840969319833627
Epoch 2: loss 15.643827011819894
Epoch 3: loss 15.416832889144093
Epoch 4: loss 15.10539018984655
Epoch 5: loss 14.660602843391954
Epoch 6: loss 14.06681343444733
Epoch 7: loss 13.332127438607976
Epoch 8: loss 12.4737068456823
Epoch 9: loss 11.511029678041874
Epoch 10: loss 10.467383186346556
Epoch 11: loss 9.369039870527143
Epoch 12: loss 8.2424455068888
Epoch 13: loss 7.118969109733834
Epoch 14: loss 6.030886314302894
Epoch 15: loss 5.0122353292924675
Epoch 16: loss 4.096192857699267
Epoch 17: loss 3.313553094280486
Epoch 18: loss 2.6868051262638244
Epoch 19: loss 2.220578398926223
Epoch 20: loss 1.89586596232254
Epoch 21: loss 1.6766818287646141
Epoch 22: loss 1.525956832208732
Epoch 23: loss 1.4172730588472169
Epoch 24: loss 1.3352806811879807
Epoch 25: loss 1.2713042198639322
Epoch 26: loss 1.2203733101527674
Epoch 27: loss 1.1793138096357183
Epoch 28: loss 1.1459647939378894
Epoch 29: loss 1.1187910535952654
Epoch 30: loss 1.0964648560410677
Epoch 31: loss 1.0781505006917993
Epoch 32: loss 1.0630385018743336
Epoch 33: loss 1.0504946153147057
Epoch 34: loss 1.0400954507789644
Epoch 35: loss 1.0314266958140756
Epoch 36: loss 1.024170413686245
Epoch 37: loss 1.0180755474601122
Epoch 38: loss 1.0129491740299865
Epoch 39: loss 1.0086219927874385
Epoch 40: loss 1.004951259910315
Epoch 41: loss 1.0018248463565076
Epoch 42: loss 0.9991583686247022
Epoch 43: loss 0.996883720078502
Epoch 44: loss 0.9949319178381324
Epoch 45: loss 0.9932500236283314
Epoch 46: loss 0.9918130957574673
Epoch 47: loss 0.9905609826538601
Epoch 48: loss 0.9894652622688588
Epoch 49: loss 0.9885264312268082
-----------Time: 0:09:48.008649, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0034828186035156-------------


Epoch 0: loss 16.03337517583286
Epoch 1: loss 15.838322893052467
Epoch 2: loss 15.603364507551747
Epoch 3: loss 15.211229004115758
Epoch 4: loss 14.579848530630887
Epoch 5: loss 13.717879009350543
Epoch 6: loss 12.660163273170891
Epoch 7: loss 11.445935849329258
Epoch 8: loss 10.120199907208475
Epoch 9: loss 8.730964947161693
Epoch 10: loss 7.333881028778466
Epoch 11: loss 5.987377717224005
Epoch 12: loss 4.753319846799414
Epoch 13: loss 3.6925970055345223
Epoch 14: loss 2.8533081142652157
Epoch 15: loss 2.2540786098435626
Epoch 16: loss 1.8656024762701249
Epoch 17: loss 1.6232640626250303
Epoch 18: loss 1.4657600662058756
Epoch 19: loss 1.3560204402689962
Epoch 20: loss 1.2752319437255673
Epoch 21: loss 1.2139304287897756
Epoch 22: loss 1.1666783450932008
Epoch 23: loss 1.1298995181034932
Epoch 24: loss 1.1010498988557083
Epoch 25: loss 1.0783403712229083
Epoch 26: loss 1.0603747870884492
Epoch 27: loss 1.0460986360996687
Epoch 28: loss 1.0346637362044036
Epoch 29: loss 1.0254794950878057
Epoch 30: loss 1.0180585174666856
Epoch 31: loss 1.0120447147080534
Epoch 32: loss 1.0071353569440442
Epoch 33: loss 1.0031249648919243
Epoch 34: loss 0.9998223685971934
Epoch 35: loss 0.9970934671056602
Epoch 36: loss 0.9948259675515742
Epoch 37: loss 0.9929403881458824
Epoch 38: loss 0.9913633046686942
Epoch 39: loss 0.9900307245329711
Epoch 40: loss 0.9889196677497056
Epoch 41: loss 0.9879760848334977
Epoch 42: loss 0.9871745821961636
Epoch 43: loss 0.9864838691007968
Epoch 44: loss 0.9858952983849201
Epoch 45: loss 0.9853981897324048
Epoch 46: loss 0.9849675303247067
Epoch 47: loss 0.9845770942801817
Epoch 48: loss 0.9842505606378531
Epoch 49: loss 0.9839615164048131
-----------Time: 0:12:10.398126, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.001043677330017-------------


Epoch 0: loss 16.03323602520817
Epoch 1: loss 15.82638989355721
Epoch 2: loss 15.465670769988616
Epoch 3: loss 14.768599738628726
Epoch 4: loss 13.746227921449082
Epoch 5: loss 12.462685649326277
Epoch 6: loss 10.985568222369494
Epoch 7: loss 9.38891849095177
Epoch 8: loss 7.7541831824492995
Epoch 9: loss 6.170651196137014
Epoch 10: loss 4.733558633652895
Epoch 11: loss 3.5328795460010753
Epoch 12: loss 2.637043576256097
Epoch 13: loss 2.0523417957325614
Epoch 14: loss 1.706342581499524
Epoch 15: loss 1.5001367353563273
Epoch 16: loss 1.3662418489906047
Epoch 17: loss 1.2725643153226913
Epoch 18: loss 1.2041717784569406
Epoch 19: loss 1.1533605071466342
Epoch 20: loss 1.1151243157040625
Epoch 21: loss 1.0861607265738187
Epoch 22: loss 1.0641094409395262
Epoch 23: loss 1.0471874633646971
Epoch 24: loss 1.0341036259771494
Epoch 25: loss 1.0239310217266695
Epoch 26: loss 1.015966819527358
Epoch 27: loss 1.0096855825091782
Epoch 28: loss 1.004713572403735
Epoch 29: loss 1.000743278898319
Epoch 30: loss 0.9975607680716937
Epoch 31: loss 0.9949924349784851
Epoch 32: loss 0.9929207781529285
Epoch 33: loss 0.9912109018798235
Epoch 34: loss 0.9898313589398144
Epoch 35: loss 0.9886857280269662
Epoch 36: loss 0.9877366722764497
Epoch 37: loss 0.9869615246803621
Epoch 38: loss 0.9862966142218031
Epoch 39: loss 0.9857499941883688
Epoch 40: loss 0.9852797419821328
Epoch 41: loss 0.9848785431181496
Epoch 42: loss 0.9845463556560113
Epoch 43: loss 0.9842567234631788
Epoch 44: loss 0.9840054955898646
Epoch 45: loss 0.9837920964088305
Epoch 46: loss 0.9836078788386278
Epoch 47: loss 0.9834417476156213
Epoch 48: loss 0.9832971189383776
Epoch 49: loss 0.9831762951051755
-----------Time: 0:12:25.455237, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0004485845565796-------------


Epoch 0: loss 15.196244345857373
Epoch 1: loss 13.309660665233087
Epoch 2: loss 10.472519832567523
Epoch 3: loss 6.225829290526921
Epoch 4: loss 2.770860605715668
Epoch 5: loss 1.4669990958144574
Epoch 6: loss 1.1693939574877161
Epoch 7: loss 1.0678544137191357
Epoch 8: loss 1.0249851250240114
Epoch 9: loss 1.0054749448121794
Epoch 10: loss 0.9959534698202922
Epoch 11: loss 0.9910007889436729
Epoch 12: loss 0.9882913959486579
Epoch 13: loss 0.9865806900074198
Epoch 14: loss 0.9855136760490754
Epoch 15: loss 0.9847121125268443
Epoch 16: loss 0.9840692462740935
Epoch 17: loss 0.9835264388960039
Epoch 18: loss 0.9830217441668518
Epoch 19: loss 0.9825678133750623
Epoch 20: loss 0.9820931227619457
Epoch 21: loss 0.9815991552644867
Epoch 22: loss 0.9810966511761903
Epoch 23: loss 0.9805787405350599
Epoch 24: loss 0.9800637278335129
Epoch 25: loss 0.9794758872664319
Epoch 26: loss 0.9788875351657421
Epoch 27: loss 0.9782408312198064
Epoch 28: loss 0.977552198793267
Epoch 29: loss 0.9768511523138641
Epoch 30: loss 0.9760628047198171
Epoch 31: loss 0.9752504153864873
Epoch 32: loss 0.9743638670820461
Epoch 33: loss 0.9734136164350442
Epoch 34: loss 0.9724263541234324
Epoch 35: loss 0.9713313150755928
Epoch 36: loss 0.9702025314528635
Epoch 37: loss 0.9689663190631649
Epoch 38: loss 0.9676969694728239
Epoch 39: loss 0.966291384894022
Epoch 40: loss 0.9648433306607428
Epoch 41: loss 0.9632739574770489
Epoch 42: loss 0.9616619362787037
Epoch 43: loss 0.9599292838683655
Epoch 44: loss 0.9580644919471678
Epoch 45: loss 0.956198899544342
Epoch 46: loss 0.9541621927724966
Epoch 47: loss 0.9520888423517776
Epoch 48: loss 0.949908439541978
Epoch 49: loss 0.9476743115880945
-----------Time: 0:08:46.039260, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9957643151283264-------------


Epoch 0: loss 15.094347853191266
Epoch 1: loss 9.691329747713928
Epoch 2: loss 3.0114619624297085
Epoch 3: loss 1.3069551457459023
Epoch 4: loss 1.0770485933673584
Epoch 5: loss 1.0188134418291006
Epoch 6: loss 1.0011179767237077
Epoch 7: loss 0.9948775189735761
Epoch 8: loss 0.9923815267963731
Epoch 9: loss 0.9911638399613428
Epoch 10: loss 0.9904030366654367
Epoch 11: loss 0.9899384030801825
Epoch 12: loss 0.9895198179237735
Epoch 13: loss 0.989233300211379
Epoch 14: loss 0.9887866924125387
Epoch 15: loss 0.9884241389281857
Epoch 16: loss 0.9880342762583038
Epoch 17: loss 0.9875610130613689
Epoch 18: loss 0.9871414808263722
Epoch 19: loss 0.9866142670313517
Epoch 20: loss 0.9860854848465757
Epoch 21: loss 0.9854094798374332
Epoch 22: loss 0.9847338308679986
Epoch 23: loss 0.9839049227021193
Epoch 24: loss 0.9830746546330953
Epoch 25: loss 0.9821260416066925
Epoch 26: loss 0.981087642788174
Epoch 27: loss 0.9799053207274816
Epoch 28: loss 0.9785060527987166
Epoch 29: loss 0.9770494024671894
Epoch 30: loss 0.9753709862483722
Epoch 31: loss 0.9735211768410917
Epoch 32: loss 0.9715083266419519
Epoch 33: loss 0.9693376919282007
Epoch 34: loss 0.9669137836928728
Epoch 35: loss 0.9643265307597845
Epoch 36: loss 0.9615007471850283
Epoch 37: loss 0.9585280278702675
Epoch 38: loss 0.9553477815501776
Epoch 39: loss 0.9520601272064425
Epoch 40: loss 0.9485014535409722
Epoch 41: loss 0.9448900063314017
Epoch 42: loss 0.9410280706669858
Epoch 43: loss 0.9370968931067437
Epoch 44: loss 0.9329105755309166
Epoch 45: loss 0.9285311406562353
Epoch 46: loss 0.9240158731487538
Epoch 47: loss 0.9192831731315797
Epoch 48: loss 0.9143168545242494
Epoch 49: loss 0.9090922482477835
-----------Time: 0:08:11.828573, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9908030033111572-------------


Epoch 0: loss 13.934059076428479
Epoch 1: loss 4.060656434624917
Epoch 2: loss 1.2521085135634662
Epoch 3: loss 1.0424254310072731
Epoch 4: loss 1.0068790126650666
Epoch 5: loss 0.9988154501873492
Epoch 6: loss 0.9964420005573793
Epoch 7: loss 0.9954846203003065
Epoch 8: loss 0.9949635267743862
Epoch 9: loss 0.9945648376257151
Epoch 10: loss 0.9942404935870759
Epoch 11: loss 0.9939662076942035
Epoch 12: loss 0.99356185894028
Epoch 13: loss 0.9930954470849672
Epoch 14: loss 0.9926498272517507
Epoch 15: loss 0.9921177480614918
Epoch 16: loss 0.9915742598162065
Epoch 17: loss 0.9908125922529512
Epoch 18: loss 0.9899318035754774
Epoch 19: loss 0.988952239030337
Epoch 20: loss 0.987871874441849
Epoch 21: loss 0.9866560038792951
Epoch 22: loss 0.9849988119377916
Epoch 23: loss 0.9831568628565781
Epoch 24: loss 0.9811036038683963
Epoch 25: loss 0.978729158688525
Epoch 26: loss 0.9760455207503186
Epoch 27: loss 0.9730821318312661
Epoch 28: loss 0.969895381198881
Epoch 29: loss 0.966354850665456
Epoch 30: loss 0.9626539829731506
Epoch 31: loss 0.9585984951617472
Epoch 32: loss 0.9543774845549566
Epoch 33: loss 0.9499221702773264
Epoch 34: loss 0.9450938786547621
Epoch 35: loss 0.9399453839930326
Epoch 36: loss 0.9344865231004469
Epoch 37: loss 0.9285969041073173
Epoch 38: loss 0.9222386439997843
Epoch 39: loss 0.9153538411761705
Epoch 40: loss 0.9078998614453051
Epoch 41: loss 0.8998258106892624
Epoch 42: loss 0.8910809390696577
Epoch 43: loss 0.8817838022992817
Epoch 44: loss 0.871738839661576
Epoch 45: loss 0.8609780741165746
Epoch 46: loss 0.8496354924005423
Epoch 47: loss 0.8375004233874466
Epoch 48: loss 0.8247179692857223
Epoch 49: loss 0.8112191023743626
-----------Time: 0:11:04.097155, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9863123297691345-------------


Epoch 0: loss 12.2669452106648
Epoch 1: loss 2.1626504582351416
Epoch 2: loss 1.0900987539847562
Epoch 3: loss 1.013788425319541
Epoch 4: loss 1.0021129308867027
Epoch 5: loss 0.9995864315840652
Epoch 6: loss 0.9986475995372598
Epoch 7: loss 0.9981797651598932
Epoch 8: loss 0.9978166939837574
Epoch 9: loss 0.9974907430991846
Epoch 10: loss 0.9972429189622369
Epoch 11: loss 0.9967928718230593
Epoch 12: loss 0.9964112271233185
Epoch 13: loss 0.995882516324747
Epoch 14: loss 0.9953075815893633
Epoch 15: loss 0.9945433970506324
Epoch 16: loss 0.9936817242212436
Epoch 17: loss 0.9926744083984835
Epoch 18: loss 0.9915159790978735
Epoch 19: loss 0.9900065876823071
Epoch 20: loss 0.9881605372894322
Epoch 21: loss 0.9859913642183473
Epoch 22: loss 0.9834991485415236
Epoch 23: loss 0.9805075134221077
Epoch 24: loss 0.9770051387582803
Epoch 25: loss 0.9730729628607525
Epoch 26: loss 0.9687843272840283
Epoch 27: loss 0.9640494701070199
Epoch 28: loss 0.9588108219939642
Epoch 29: loss 0.9533182601036748
Epoch 30: loss 0.9474159162643488
Epoch 31: loss 0.941013551280186
Epoch 32: loss 0.933975498837709
Epoch 33: loss 0.926274354964122
Epoch 34: loss 0.917948074756335
Epoch 35: loss 0.908850227606692
Epoch 36: loss 0.8987543436740649
Epoch 37: loss 0.8877552537214114
Epoch 38: loss 0.8756728258678743
Epoch 39: loss 0.8626710165934176
Epoch 40: loss 0.8484879205770114
Epoch 41: loss 0.8332075883085927
Epoch 42: loss 0.816690573797983
Epoch 43: loss 0.7990589192893727
Epoch 44: loss 0.7802552021736033
Epoch 45: loss 0.7602971173972782
Epoch 46: loss 0.7392461883626339
Epoch 47: loss 0.7172638700992404
Epoch 48: loss 0.6944891309984487
Epoch 49: loss 0.6709682565869165
-----------Time: 0:11:07.640084, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9890126585960388-------------


Epoch 0: loss 10.860744167048365
Epoch 1: loss 1.5516504776289828
Epoch 2: loss 1.0451043966252107
Epoch 3: loss 1.0081509158558142
Epoch 4: loss 1.0027769349430358
Epoch 5: loss 1.0013831501554185
Epoch 6: loss 1.0010145797944705
Epoch 7: loss 1.0005947310941383
Epoch 8: loss 1.0003172812123478
Epoch 9: loss 0.9999814917854539
Epoch 10: loss 0.9996445836937901
Epoch 11: loss 0.9990915541839444
Epoch 12: loss 0.9986239442072076
Epoch 13: loss 0.9980659717253332
Epoch 14: loss 0.9974109764135421
Epoch 15: loss 0.9964540171000931
Epoch 16: loss 0.9952728876508793
Epoch 17: loss 0.9941120048364004
Epoch 18: loss 0.9924712922341024
Epoch 19: loss 0.9903678101421115
Epoch 20: loss 0.9879413406410704
Epoch 21: loss 0.9847670561143275
Epoch 22: loss 0.9813874651550792
Epoch 23: loss 0.9772398359364566
Epoch 24: loss 0.9723002660235612
Epoch 25: loss 0.966905818742666
Epoch 26: loss 0.9610453966261577
Epoch 27: loss 0.9545252605616366
Epoch 28: loss 0.947251083087247
Epoch 29: loss 0.939572660721405
Epoch 30: loss 0.9308632117143593
Epoch 31: loss 0.921235746407781
Epoch 32: loss 0.9102093893882696
Epoch 33: loss 0.898149270007246
Epoch 34: loss 0.8845584100682558
Epoch 35: loss 0.869412508086707
Epoch 36: loss 0.8527898781483688
Epoch 37: loss 0.8345044394949974
Epoch 38: loss 0.8146031115805733
Epoch 39: loss 0.7931424824195819
Epoch 40: loss 0.7700593176258849
Epoch 41: loss 0.74541909675873
Epoch 42: loss 0.7194890035209998
Epoch 43: loss 0.6921621362692381
Epoch 44: loss 0.6639585959775218
Epoch 45: loss 0.6348567424405847
Epoch 46: loss 0.6051792663650191
Epoch 47: loss 0.5751495512527509
Epoch 48: loss 0.5452007520224751
Epoch 49: loss 0.5152927107011059
-----------Time: 0:13:21.956429, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9989083409309387-------------


Epoch 0: loss 5.310604952417553
Epoch 1: loss 1.0271364940353682
Epoch 2: loss 1.0127093811894707
Epoch 3: loss 0.996739687220826
Epoch 4: loss 0.9809087958589983
Epoch 5: loss 0.962651067563799
Epoch 6: loss 0.9416677907084952
Epoch 7: loss 0.9188962702585213
Epoch 8: loss 0.8947792937244781
Epoch 9: loss 0.8695845117771217
Epoch 10: loss 0.845708437268017
Epoch 11: loss 0.8243413867835054
Epoch 12: loss 0.8060487942438919
Epoch 13: loss 0.791338138780625
Epoch 14: loss 0.7795578994590215
Epoch 15: loss 0.7702873018268919
Epoch 16: loss 0.7622748182771516
Epoch 17: loss 0.7558881940402176
Epoch 18: loss 0.7504708400850001
Epoch 19: loss 0.7454489116000377
Epoch 20: loss 0.741300333463478
Epoch 21: loss 0.7375019769781371
Epoch 22: loss 0.7341501040423675
Epoch 23: loss 0.730922163837367
Epoch 24: loss 0.7281999173308275
Epoch 25: loss 0.7255401111381607
Epoch 26: loss 0.7230275191322106
Epoch 27: loss 0.7207450797581815
Epoch 28: loss 0.7185157663378785
Epoch 29: loss 0.7166742481100877
Epoch 30: loss 0.7146025837812818
Epoch 31: loss 0.7128002792353731
Epoch 32: loss 0.7112300035388266
Epoch 33: loss 0.7094495809146281
Epoch 34: loss 0.7078066232850722
Epoch 35: loss 0.7065248361840852
Epoch 36: loss 0.7049459078637331
Epoch 37: loss 0.7035317373800822
Epoch 38: loss 0.702427253013658
Epoch 39: loss 0.7010916742197906
Epoch 40: loss 0.6998096640470431
Epoch 41: loss 0.6985048878251242
Epoch 42: loss 0.6975160266732054
Epoch 43: loss 0.696370479600659
Epoch 44: loss 0.6951425490316455
Epoch 45: loss 0.6940419922596479
Epoch 46: loss 0.6931711987527963
Epoch 47: loss 0.6921048703856154
Epoch 48: loss 0.6911067800751324
Epoch 49: loss 0.6902526435708144
-----------Time: 0:07:42.328683, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.132291316986084-------------


Epoch 0: loss 3.680194104865169
Epoch 1: loss 1.0478515860955309
Epoch 2: loss 1.0259117889883986
Epoch 3: loss 0.999597577312318
Epoch 4: loss 0.9638679021075084
Epoch 5: loss 0.9137943894270647
Epoch 6: loss 0.8497770507903512
Epoch 7: loss 0.7803536823840813
Epoch 8: loss 0.7138187481599894
Epoch 9: loss 0.6567974500013734
Epoch 10: loss 0.6117177008205034
Epoch 11: loss 0.5771279763825371
Epoch 12: loss 0.5511362189229511
Epoch 13: loss 0.5310701333176772
Epoch 14: loss 0.5146369355294678
Epoch 15: loss 0.5017218277336398
Epoch 16: loss 0.4902793307580528
Epoch 17: loss 0.48073356142019436
Epoch 18: loss 0.4723408860055696
Epoch 19: loss 0.4652890676289165
Epoch 20: loss 0.45833294172206607
Epoch 21: loss 0.452616642161904
Epoch 22: loss 0.44757956412254696
Epoch 23: loss 0.44259367910755143
Epoch 24: loss 0.43784672403510594
Epoch 25: loss 0.4340052799873212
Epoch 26: loss 0.4303319009751195
Epoch 27: loss 0.4266199990418244
Epoch 28: loss 0.423357130547397
Epoch 29: loss 0.4204011400062601
Epoch 30: loss 0.41755789233945906
Epoch 31: loss 0.4147534993653909
Epoch 32: loss 0.4122524585751103
Epoch 33: loss 0.4099006146524768
Epoch 34: loss 0.4075797132625082
Epoch 35: loss 0.40527159296474485
Epoch 36: loss 0.4032269909556564
Epoch 37: loss 0.40118434731891583
Epoch 38: loss 0.39931571895642537
Epoch 39: loss 0.397470152377272
Epoch 40: loss 0.39567505597841357
Epoch 41: loss 0.39415974125403824
Epoch 42: loss 0.3924311748895715
Epoch 43: loss 0.39117411139981784
Epoch 44: loss 0.3894260880649252
Epoch 45: loss 0.38799281125405094
Epoch 46: loss 0.38652544093073676
Epoch 47: loss 0.3852279891278706
Epoch 48: loss 0.38392367057174104
Epoch 49: loss 0.38279681231840723
-----------Time: 0:09:27.298810, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4247838258743286-------------


Epoch 0: loss 2.9414057321137483
Epoch 1: loss 1.0678599137952627
Epoch 2: loss 1.0335046372347516
Epoch 3: loss 0.9782158615084819
Epoch 4: loss 0.8896490791007318
Epoch 5: loss 0.7712957247111252
Epoch 6: loss 0.638031076637865
Epoch 7: loss 0.5197266944729161
Epoch 8: loss 0.4303920836050657
Epoch 9: loss 0.367331503209308
Epoch 10: loss 0.3229355589487165
Epoch 11: loss 0.29066007791766657
Epoch 12: loss 0.26594792214455715
Epoch 13: loss 0.2469614768494984
Epoch 14: loss 0.2313718588748271
Epoch 15: loss 0.21852502610094848
Epoch 16: loss 0.20765828012877213
Epoch 17: loss 0.1985930821586394
Epoch 18: loss 0.19046574293991805
Epoch 19: loss 0.1835485765910006
Epoch 20: loss 0.1775000216221602
Epoch 21: loss 0.17194501038533375
Epoch 22: loss 0.1667720333426429
Epoch 23: loss 0.16246411965601856
Epoch 24: loss 0.15827437980853065
Epoch 25: loss 0.15466741000491033
Epoch 26: loss 0.1512549242319751
Epoch 27: loss 0.14820576271588157
Epoch 28: loss 0.14518174033383108
Epoch 29: loss 0.1424673103938581
Epoch 30: loss 0.14001801866245958
Epoch 31: loss 0.1375144887259792
Epoch 32: loss 0.13531184678997535
Epoch 33: loss 0.13328393676752237
Epoch 34: loss 0.13143227332843394
Epoch 35: loss 0.12962384312907402
Epoch 36: loss 0.12775839941779707
Epoch 37: loss 0.12620947820343617
Epoch 38: loss 0.12450230907590056
Epoch 39: loss 0.12312523612490227
Epoch 40: loss 0.1215900521079213
Epoch 41: loss 0.12041171450753325
Epoch 42: loss 0.11900280308446332
Epoch 43: loss 0.11779681677246885
Epoch 44: loss 0.11656129112656574
Epoch 45: loss 0.11548961009961908
Epoch 46: loss 0.11438606888695928
Epoch 47: loss 0.11331217414828634
Epoch 48: loss 0.11231483236084884
Epoch 49: loss 0.11131860115051334
-----------Time: 0:09:58.145057, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.6316291093826294-------------


Epoch 0: loss 2.6833149179808142
Epoch 1: loss 1.085918887047874
Epoch 2: loss 1.038268545323445
Epoch 3: loss 0.939301414260273
Epoch 4: loss 0.7763586001204303
Epoch 5: loss 0.588478011385522
Epoch 6: loss 0.4273256993831792
Epoch 7: loss 0.31352565854447506
Epoch 8: loss 0.24024552228805357
Epoch 9: loss 0.19336425427138773
Epoch 10: loss 0.16162136618236345
Epoch 11: loss 0.13977347917008037
Epoch 12: loss 0.1235120549753446
Epoch 13: loss 0.11096258023211883
Epoch 14: loss 0.1011693485905662
Epoch 15: loss 0.09345232574000768
Epoch 16: loss 0.08686262854821108
Epoch 17: loss 0.08154095921416267
Epoch 18: loss 0.07712171308680064
Epoch 19: loss 0.07319754111424064
Epoch 20: loss 0.06982772949376043
Epoch 21: loss 0.06697083853059582
Epoch 22: loss 0.06428323467244364
Epoch 23: loss 0.06221608491538497
Epoch 24: loss 0.06014467332291337
Epoch 25: loss 0.05820016918451712
Epoch 26: loss 0.056570868474259656
Epoch 27: loss 0.05502251714423437
Epoch 28: loss 0.05369766543114425
Epoch 29: loss 0.052346166676034936
Epoch 30: loss 0.051190111912734294
Epoch 31: loss 0.05011835940133592
Epoch 32: loss 0.04916676668609255
Epoch 33: loss 0.04808441886922437
Epoch 34: loss 0.04729176157059441
Epoch 35: loss 0.046412941046128
Epoch 36: loss 0.04567020772457479
Epoch 37: loss 0.045099546896853866
Epoch 38: loss 0.04417793313978223
Epoch 39: loss 0.04362716340440132
Epoch 40: loss 0.04299166922616028
Epoch 41: loss 0.04246333285522014
Epoch 42: loss 0.04183078719715245
Epoch 43: loss 0.04129588930012254
Epoch 44: loss 0.040874425628206826
Epoch 45: loss 0.04035330902533874
Epoch 46: loss 0.03988755877925333
Epoch 47: loss 0.03952651465278222
Epoch 48: loss 0.039006009872632065
Epoch 49: loss 0.038702904351578256
-----------Time: 0:12:12.128978, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4540722370147705-------------


Epoch 0: loss 2.540772703110102
Epoch 1: loss 1.0765269496247716
Epoch 2: loss 0.982732676500338
Epoch 3: loss 0.825907309558354
Epoch 4: loss 0.618420157964428
Epoch 5: loss 0.4209000699347682
Epoch 6: loss 0.27737664647274785
Epoch 7: loss 0.19003106417086799
Epoch 8: loss 0.13991166002007202
Epoch 9: loss 0.11063102793197647
Epoch 10: loss 0.09218440510032778
Epoch 11: loss 0.0801659412287042
Epoch 12: loss 0.07207538687349047
Epoch 13: loss 0.06593241525663021
Epoch 14: loss 0.06143620697849305
Epoch 15: loss 0.05827889648276772
Epoch 16: loss 0.0554417533948896
Epoch 17: loss 0.053245169182002644
Epoch 18: loss 0.05142320490676693
Epoch 19: loss 0.04975160352618329
Epoch 20: loss 0.04850355441955006
Epoch 21: loss 0.047246021011468506
Epoch 22: loss 0.046093892872276804
Epoch 23: loss 0.04518708856923221
Epoch 24: loss 0.044359044041211125
Epoch 25: loss 0.04360303352118478
Epoch 26: loss 0.04275944530194126
Epoch 27: loss 0.04209119849800334
Epoch 28: loss 0.04149037573887658
Epoch 29: loss 0.04097763484707502
Epoch 30: loss 0.040399084491589396
Epoch 31: loss 0.03986472170192256
Epoch 32: loss 0.039372531895674134
Epoch 33: loss 0.03891995777955857
Epoch 34: loss 0.038630905407194764
Epoch 35: loss 0.03816792304876254
Epoch 36: loss 0.03769568910051488
Epoch 37: loss 0.037435252565329424
Epoch 38: loss 0.03709049795689937
Epoch 39: loss 0.036791087329274226
Epoch 40: loss 0.036453743862284956
Epoch 41: loss 0.036175451034810666
Epoch 42: loss 0.03590712413270798
Epoch 43: loss 0.035634928906074094
Epoch 44: loss 0.03537750373583828
Epoch 45: loss 0.03514827380352869
Epoch 46: loss 0.034838553053155225
Epoch 47: loss 0.034788697516386635
Epoch 48: loss 0.0344518798313804
Epoch 49: loss 0.0343470117421603
-----------Time: 0:12:24.526128, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2530732154846191-------------


Epoch 0: loss 16.129368544013815
Epoch 1: loss 16.129367387314215
Epoch 2: loss 16.129370831483858
Epoch 3: loss 16.12936641393374
Epoch 4: loss 16.129367130874876
Epoch 5: loss 16.129356704243115
Epoch 6: loss 16.129357739075687
Epoch 7: loss 16.12935811997497
Epoch 8: loss 16.129353149874564
Epoch 9: loss 16.129354183151385
Epoch 10: loss 16.129353436910307
Epoch 11: loss 16.12934864364674
Epoch 12: loss 16.12934893042319
Epoch 13: loss 16.12935392593417
Epoch 14: loss 16.129348026792144
Epoch 15: loss 16.129337210704477
Epoch 16: loss 16.12933801969411
Epoch 17: loss 16.12934096965406
Epoch 18: loss 16.129334635939397
Epoch 19: loss 16.129337995839286
Epoch 20: loss 16.12933422444371
Epoch 21: loss 16.129330633515053
Epoch 22: loss 16.129329824006835
Epoch 23: loss 16.129330476643663
Epoch 24: loss 16.129328673789527
Epoch 25: loss 16.1293257445729
Epoch 26: loss 16.129326390208856
Epoch 27: loss 16.129322290031595
Epoch 28: loss 16.12931623194385
Epoch 29: loss 16.129315019496566
Epoch 30: loss 16.12932518087307
Epoch 31: loss 16.129314438164915
Epoch 32: loss 16.129305243945886
Epoch 33: loss 16.129307600646772
Epoch 34: loss 16.129306647491042
Epoch 35: loss 16.129300262955233
Epoch 36: loss 16.129306057343474
Epoch 37: loss 16.129304813781207
Epoch 38: loss 16.12930175725243
Epoch 39: loss 16.129302206604684
Epoch 40: loss 16.129300846361218
Epoch 41: loss 16.129295225698318
Epoch 42: loss 16.129291032953972
Epoch 43: loss 16.129291773490635
Epoch 44: loss 16.12928654746947
Epoch 45: loss 16.12928392888412
Epoch 46: loss 16.129282123437065
Epoch 47: loss 16.129291053697298
Epoch 48: loss 16.12928104089485
Epoch 49: loss 16.129278346596365
-----------Time: 0:08:52.419155, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.0172200202941895-------------


Epoch 0: loss 16.12918970311317
Epoch 1: loss 16.12918763915244
Epoch 2: loss 16.129187042522585
Epoch 3: loss 16.1291835659415
Epoch 4: loss 16.129182789363313
Epoch 5: loss 16.129181225057398
Epoch 6: loss 16.12918036446875
Epoch 7: loss 16.12918032946439
Epoch 8: loss 16.12917852635096
Epoch 9: loss 16.129177856082304
Epoch 10: loss 16.129172148297442
Epoch 11: loss 16.129164993665736
Epoch 12: loss 16.129168934119402
Epoch 13: loss 16.129164733855603
Epoch 14: loss 16.12916424794324
Epoch 15: loss 16.12916531881734
Epoch 16: loss 16.129160652347334
Epoch 17: loss 16.129157586484062
Epoch 18: loss 16.129152874638038
Epoch 19: loss 16.12915594879864
Epoch 20: loss 16.129154772133592
Epoch 21: loss 16.129154793914083
Epoch 22: loss 16.129147757000737
Epoch 23: loss 16.1291412477457
Epoch 24: loss 16.129139205565462
Epoch 25: loss 16.129143755094976
Epoch 26: loss 16.129137553878294
Epoch 27: loss 16.1291353600125
Epoch 28: loss 16.129136253012593
Epoch 29: loss 16.129137413342274
Epoch 30: loss 16.129133174703327
Epoch 31: loss 16.12913298101254
Epoch 32: loss 16.12912123329034
Epoch 33: loss 16.12912626121276
Epoch 34: loss 16.12912647331325
Epoch 35: loss 16.12912358350894
Epoch 36: loss 16.129125654211247
Epoch 37: loss 16.129115029740035
Epoch 38: loss 16.129118869847872
Epoch 39: loss 16.129116616863605
Epoch 40: loss 16.12911820606151
Epoch 41: loss 16.129103764300115
Epoch 42: loss 16.129102335085097
Epoch 43: loss 16.129112893696256
Epoch 44: loss 16.129105962314576
Epoch 45: loss 16.12910112808294
Epoch 46: loss 16.129102663088908
Epoch 47: loss 16.129092656509457
Epoch 48: loss 16.129090869990687
Epoch 49: loss 16.129085253217156
-----------Time: 0:08:08.317385, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129186361622978
Epoch 1: loss 16.12918097432247
Epoch 2: loss 16.129171480621704
Epoch 3: loss 16.12917305400282
Epoch 4: loss 16.129174481143504
Epoch 5: loss 16.129174082871685
Epoch 6: loss 16.129170141121563
Epoch 7: loss 16.12916910473324
Epoch 8: loss 16.129166980616873
Epoch 9: loss 16.12916751397959
Epoch 10: loss 16.129159396857656
Epoch 11: loss 16.129162720456726
Epoch 12: loss 16.129158150702473
Epoch 13: loss 16.129155668245183
Epoch 14: loss 16.129147639282376
Epoch 15: loss 16.129153299098302
Epoch 16: loss 16.129147886646514
Epoch 17: loss 16.129145422339633
Epoch 18: loss 16.12914658500294
Epoch 19: loss 16.129146066419842
Epoch 20: loss 16.129145114560565
Epoch 21: loss 16.129147334874098
Epoch 22: loss 16.12914174195539
Epoch 23: loss 16.129134722933166
Epoch 24: loss 16.129136369175207
Epoch 25: loss 16.12913205508242
Epoch 26: loss 16.129129676601043
Epoch 27: loss 16.12912606855914
Epoch 28: loss 16.12912250926405
Epoch 29: loss 16.129134773495018
Epoch 30: loss 16.129121527067664
Epoch 31: loss 16.129123898288878
Epoch 32: loss 16.129122589903723
Epoch 33: loss 16.129119359131025
Epoch 34: loss 16.129118989121984
Epoch 35: loss 16.129114688253065
Epoch 36: loss 16.12911370994605
Epoch 37: loss 16.12910849481513
Epoch 38: loss 16.129114767336986
Epoch 39: loss 16.129111608647342
Epoch 40: loss 16.12909851338696
Epoch 41: loss 16.12910595609158
Epoch 42: loss 16.1291034150344
Epoch 43: loss 16.12910209420325
Epoch 44: loss 16.129100119698105
Epoch 45: loss 16.129094304825834
Epoch 46: loss 16.129095468526305
Epoch 47: loss 16.129089823489995
Epoch 48: loss 16.12909157396724
Epoch 49: loss 16.12908752564829
-----------Time: 0:11:08.326588, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017177104949951-------------


Epoch 0: loss 16.129170702747057
Epoch 1: loss 16.129175110962677
Epoch 2: loss 16.12917725452591
Epoch 3: loss 16.12917287794386
Epoch 4: loss 16.12916817906241
Epoch 5: loss 16.129161487784703
Epoch 6: loss 16.129161176116263
Epoch 7: loss 16.12916788243288
Epoch 8: loss 16.12915896721156
Epoch 9: loss 16.129164568168303
Epoch 10: loss 16.12914870678568
Epoch 11: loss 16.129148787943937
Epoch 12: loss 16.129158089250375
Epoch 13: loss 16.12914482726553
Epoch 14: loss 16.129144874456593
Epoch 15: loss 16.129144100212027
Epoch 16: loss 16.12914050954266
Epoch 17: loss 16.12913792466521
Epoch 18: loss 16.129143949045055
Epoch 19: loss 16.12913504523256
Epoch 20: loss 16.129133430624087
Epoch 21: loss 16.12913649985815
Epoch 22: loss 16.129129041077455
Epoch 23: loss 16.129127594749196
Epoch 24: loss 16.129125534418552
Epoch 25: loss 16.129123951962228
Epoch 26: loss 16.129125193709456
Epoch 27: loss 16.129120941328058
Epoch 28: loss 16.129121008225276
Epoch 29: loss 16.129125003389458
Epoch 30: loss 16.12912248411277
Epoch 31: loss 16.12911712792725
Epoch 32: loss 16.129120264577114
Epoch 33: loss 16.129117212974876
Epoch 34: loss 16.1291092197943
Epoch 35: loss 16.12911008842099
Epoch 36: loss 16.129103882796354
Epoch 37: loss 16.129101819354208
Epoch 38: loss 16.129106495936583
Epoch 39: loss 16.12909580430886
Epoch 40: loss 16.12909401960513
Epoch 41: loss 16.12910454865705
Epoch 42: loss 16.129095418483036
Epoch 43: loss 16.129092587278613
Epoch 44: loss 16.129087067739416
Epoch 45: loss 16.129088516919882
Epoch 46: loss 16.129085920633603
Epoch 47: loss 16.129083165660898
Epoch 48: loss 16.129084286837553
Epoch 49: loss 16.129079269805377
-----------Time: 0:10:57.556481, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017178535461426-------------


Epoch 0: loss 16.12917538114447
Epoch 1: loss 16.129171368607754
Epoch 2: loss 16.12917046575458
Epoch 3: loss 16.12916265744888
Epoch 4: loss 16.129163001528767
Epoch 5: loss 16.129155222004428
Epoch 6: loss 16.12915496945446
Epoch 7: loss 16.12915830757386
Epoch 8: loss 16.129160379572625
Epoch 9: loss 16.129161478709502
Epoch 10: loss 16.129157490286897
Epoch 11: loss 16.129153274984187
Epoch 12: loss 16.129146414907684
Epoch 13: loss 16.129151560289177
Epoch 14: loss 16.129144109287232
Epoch 15: loss 16.129150265646473
Epoch 16: loss 16.129143240401252
Epoch 17: loss 16.12914379139579
Epoch 18: loss 16.129140285774053
Epoch 19: loss 16.12913820936733
Epoch 20: loss 16.129137300031868
Epoch 21: loss 16.12913029682643
Epoch 22: loss 16.129131667700847
Epoch 23: loss 16.129127805293937
Epoch 24: loss 16.129127075128935
Epoch 25: loss 16.12912993848551
Epoch 26: loss 16.129130164328448
Epoch 27: loss 16.12912597443631
Epoch 28: loss 16.129119553340395
Epoch 29: loss 16.129118891628362
Epoch 30: loss 16.129118193615515
Epoch 31: loss 16.129112067593383
Epoch 32: loss 16.129109831463065
Epoch 33: loss 16.129111133625223
Epoch 34: loss 16.129106096368304
Epoch 35: loss 16.129105486514582
Epoch 36: loss 16.129106784268785
Epoch 37: loss 16.129102338974473
Epoch 38: loss 16.129101871990393
Epoch 39: loss 16.129098567319602
Epoch 40: loss 16.129093390823122
Epoch 41: loss 16.12909172435634
Epoch 42: loss 16.129091074312427
Epoch 43: loss 16.12908332486591
Epoch 44: loss 16.12909222790053
Epoch 45: loss 16.129089009055242
Epoch 46: loss 16.129085800063034
Epoch 47: loss 16.129085659267723
Epoch 48: loss 16.129081111034665
Epoch 49: loss 16.1290776430102
-----------Time: 0:13:23.495130, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129323028493925
Epoch 1: loss 16.129293927684824
Epoch 2: loss 16.12927265151679
Epoch 3: loss 16.129259770949812
Epoch 4: loss 16.129242163238608
Epoch 5: loss 16.12921640936482
Epoch 6: loss 16.129197966216246
Epoch 7: loss 16.12916879643559
Epoch 8: loss 16.129159701784516
Epoch 9: loss 16.12914396019459
Epoch 10: loss 16.129119679096796
Epoch 11: loss 16.12910366265783
Epoch 12: loss 16.12908207274702
Epoch 13: loss 16.129067013093866
Epoch 14: loss 16.129047482476537
Epoch 15: loss 16.129026067328233
Epoch 16: loss 16.128999881474723
Epoch 17: loss 16.12897938629283
Epoch 18: loss 16.128966103823952
Epoch 19: loss 16.128948079949815
Epoch 20: loss 16.128930825912285
Epoch 21: loss 16.12890979918272
Epoch 22: loss 16.12889102828963
Epoch 23: loss 16.12886493474391
Epoch 24: loss 16.128847006807643
Epoch 25: loss 16.128831164094013
Epoch 26: loss 16.128806234508055
Epoch 27: loss 16.128796402431817
Epoch 28: loss 16.12877932471254
Epoch 29: loss 16.12875726755836
Epoch 30: loss 16.128741710584016
Epoch 31: loss 16.128710891968403
Epoch 32: loss 16.128689992291697
Epoch 33: loss 16.12867508717631
Epoch 34: loss 16.128656668660433
Epoch 35: loss 16.128640056888067
Epoch 36: loss 16.12861381969483
Epoch 37: loss 16.128595077064517
Epoch 38: loss 16.128581370653958
Epoch 39: loss 16.128553377797644
Epoch 40: loss 16.128544672602477
Epoch 41: loss 16.12851978009521
Epoch 42: loss 16.12851067585035
Epoch 43: loss 16.128483852139627
Epoch 44: loss 16.128463833276314
Epoch 45: loss 16.12844595382757
Epoch 46: loss 16.128423764953283
Epoch 47: loss 16.128411957594025
Epoch 48: loss 16.12839095445999
Epoch 49: loss 16.128366173188798
-----------Time: 0:07:36.016224, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.0171098709106445-------------


Epoch 0: loss 16.129134462863743
Epoch 1: loss 16.129111483150233
Epoch 2: loss 16.129094622976563
Epoch 3: loss 16.129066466507282
Epoch 4: loss 16.12905988724352
Epoch 5: loss 16.129033573040694
Epoch 6: loss 16.129018820388737
Epoch 7: loss 16.129000926678955
Epoch 8: loss 16.128979518012937
Epoch 9: loss 16.12896058843342
Epoch 10: loss 16.128938438712158
Epoch 11: loss 16.12892318121955
Epoch 12: loss 16.12889495785305
Epoch 13: loss 16.128883449975532
Epoch 14: loss 16.128863526272216
Epoch 15: loss 16.1288455231414
Epoch 16: loss 16.128825476793182
Epoch 17: loss 16.128799339686484
Epoch 18: loss 16.128789124377334
Epoch 19: loss 16.12876339306191
Epoch 20: loss 16.128742457084385
Epoch 21: loss 16.12872547348794
Epoch 22: loss 16.1287033323233
Epoch 23: loss 16.128683549156
Epoch 24: loss 16.128672925721958
Epoch 25: loss 16.128646942375145
Epoch 26: loss 16.12862957010067
Epoch 27: loss 16.12861692497042
Epoch 28: loss 16.12859288553235
Epoch 29: loss 16.128565865278635
Epoch 30: loss 16.128556120583646
Epoch 31: loss 16.12853953862981
Epoch 32: loss 16.128516194870965
Epoch 33: loss 16.128498069873125
Epoch 34: loss 16.12847833571193
Epoch 35: loss 16.12845326221916
Epoch 36: loss 16.12844240490414
Epoch 37: loss 16.128418947057014
Epoch 38: loss 16.128401457582758
Epoch 39: loss 16.12837823569094
Epoch 40: loss 16.12836553870238
Epoch 41: loss 16.128342527873883
Epoch 42: loss 16.128318330008675
Epoch 43: loss 16.128298780463066
Epoch 44: loss 16.128276445089053
Epoch 45: loss 16.128260901079287
Epoch 46: loss 16.128247380580767
Epoch 47: loss 16.12822177450316
Epoch 48: loss 16.128203178891155
Epoch 49: loss 16.12819064473769
-----------Time: 0:09:26.111042, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017070293426514-------------


Epoch 0: loss 16.129168576815648
Epoch 1: loss 16.129140628298188
Epoch 2: loss 16.129124150838848
Epoch 3: loss 16.129109335438333
Epoch 4: loss 16.12908744837941
Epoch 5: loss 16.129069295637372
Epoch 6: loss 16.129042817562283
Epoch 7: loss 16.129024338112895
Epoch 8: loss 16.12901663144948
Epoch 9: loss 16.128990972217107
Epoch 10: loss 16.12897651126814
Epoch 11: loss 16.12895819491313
Epoch 12: loss 16.128935458155805
Epoch 13: loss 16.128913420967073
Epoch 14: loss 16.12889498819016
Epoch 15: loss 16.12887444581721
Epoch 16: loss 16.128851197218363
Epoch 17: loss 16.128841341546593
Epoch 18: loss 16.128816135555844
Epoch 19: loss 16.128796278231167
Epoch 20: loss 16.128783698183096
Epoch 21: loss 16.128763459959128
Epoch 22: loss 16.128741460626966
Epoch 23: loss 16.1287161078772
Epoch 24: loss 16.12870078245021
Epoch 25: loss 16.1286772473342
Epoch 26: loss 16.128660668232573
Epoch 27: loss 16.128640178236513
Epoch 28: loss 16.128622143212837
Epoch 29: loss 16.128608293932633
Epoch 30: loss 16.128582871433444
Epoch 31: loss 16.12856747547915
Epoch 32: loss 16.128545781592432
Epoch 33: loss 16.128522144834136
Epoch 34: loss 16.128505411454036
Epoch 35: loss 16.12848310045343
Epoch 36: loss 16.128466737600952
Epoch 37: loss 16.12844173152399
Epoch 38: loss 16.128429360464903
Epoch 39: loss 16.1284141766111
Epoch 40: loss 16.128396854639185
Epoch 41: loss 16.12836932902624
Epoch 42: loss 16.128354135837938
Epoch 43: loss 16.128333705478937
Epoch 44: loss 16.12831731280793
Epoch 45: loss 16.128296229293515
Epoch 46: loss 16.128270605843376
Epoch 47: loss 16.128259959591674
Epoch 48: loss 16.128235634155025
Epoch 49: loss 16.128217137333102
-----------Time: 0:09:44.127158, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0170722007751465-------------


Epoch 0: loss 16.12916194906437
Epoch 1: loss 16.129146788806096
Epoch 2: loss 16.12912892154405
Epoch 3: loss 16.129105272599055
Epoch 4: loss 16.129090923664037
Epoch 5: loss 16.129072694949574
Epoch 6: loss 16.129051732783605
Epoch 7: loss 16.12902909377919
Epoch 8: loss 16.129013300849536
Epoch 9: loss 16.12899792382353
Epoch 10: loss 16.128973976174667
Epoch 11: loss 16.1289535753749
Epoch 12: loss 16.12894183283853
Epoch 13: loss 16.128917473953273
Epoch 14: loss 16.12889303442834
Epoch 15: loss 16.128878297333877
Epoch 16: loss 16.128864432236888
Epoch 17: loss 16.128836224168587
Epoch 18: loss 16.12882202640054
Epoch 19: loss 16.128806736755784
Epoch 20: loss 16.12878446957545
Epoch 21: loss 16.12876066064757
Epoch 22: loss 16.128743873075535
Epoch 23: loss 16.12872738135516
Epoch 24: loss 16.128701925148064
Epoch 25: loss 16.128688193067642
Epoch 26: loss 16.128662808425016
Epoch 27: loss 16.128640258876185
Epoch 28: loss 16.12861785764212
Epoch 29: loss 16.128604576988284
Epoch 30: loss 16.12859082183091
Epoch 31: loss 16.1285680998532
Epoch 32: loss 16.12855055126047
Epoch 33: loss 16.128529725222563
Epoch 34: loss 16.1285133006587
Epoch 35: loss 16.128487617830793
Epoch 36: loss 16.128469660853874
Epoch 37: loss 16.128451998950737
Epoch 38: loss 16.12843627084397
Epoch 39: loss 16.128408210571852
Epoch 40: loss 16.128388820490542
Epoch 41: loss 16.128374426438796
Epoch 42: loss 16.12836052426312
Epoch 43: loss 16.12833416494356
Epoch 44: loss 16.128324898122898
Epoch 45: loss 16.1283107265433
Epoch 46: loss 16.128282467135275
Epoch 47: loss 16.128258692952457
Epoch 48: loss 16.128243896220933
Epoch 49: loss 16.12822877744931
-----------Time: 0:12:21.703727, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017068862915039-------------


Epoch 0: loss 16.129165532992157
Epoch 1: loss 16.1291471629638
Epoch 2: loss 16.12912651868927
Epoch 3: loss 16.12911099568212
Epoch 4: loss 16.12909355288034
Epoch 5: loss 16.12907760904301
Epoch 6: loss 16.129043885843465
Epoch 7: loss 16.129028701211784
Epoch 8: loss 16.129004426077696
Epoch 9: loss 16.12898746918828
Epoch 10: loss 16.1289692778118
Epoch 11: loss 16.128950277704984
Epoch 12: loss 16.1289351208175
Epoch 13: loss 16.12891600843744
Epoch 14: loss 16.128894393893933
Epoch 15: loss 16.128871602166793
Epoch 16: loss 16.128851191254657
Epoch 17: loss 16.128837031861764
Epoch 18: loss 16.128816948953435
Epoch 19: loss 16.12878964581264
Epoch 20: loss 16.128779331454123
Epoch 21: loss 16.128757315268007
Epoch 22: loss 16.12873808646604
Epoch 23: loss 16.128726104603572
Epoch 24: loss 16.128698224279788
Epoch 25: loss 16.128678346471077
Epoch 26: loss 16.12866670168762
Epoch 27: loss 16.128640191719672
Epoch 28: loss 16.128623387812272
Epoch 29: loss 16.12860075269723
Epoch 30: loss 16.128592895126136
Epoch 31: loss 16.128569024227573
Epoch 32: loss 16.128545379431237
Epoch 33: loss 16.128527665929084
Epoch 34: loss 16.128512002126623
Epoch 35: loss 16.12848776329335
Epoch 36: loss 16.128464071046665
Epoch 37: loss 16.128453268442158
Epoch 38: loss 16.128424622689725
Epoch 39: loss 16.128402771672327
Epoch 40: loss 16.128395646599856
Epoch 41: loss 16.12837336204699
Epoch 42: loss 16.12835511051487
Epoch 43: loss 16.128338378431227
Epoch 44: loss 16.128310301045868
Epoch 45: loss 16.12828881537026
Epoch 46: loss 16.128273623997003
Epoch 47: loss 16.128256491826498
Epoch 48: loss 16.128240707194177
Epoch 49: loss 16.128216104834152
-----------Time: 0:12:13.208715, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0170698165893555-------------


Epoch 0: loss 16.129145528389877
Epoch 1: loss 16.128955005886375
Epoch 2: loss 16.12875236694809
Epoch 3: loss 16.128562566052967
Epoch 4: loss 16.128369238014905
Epoch 5: loss 16.128171995193043
Epoch 6: loss 16.127985776151373
Epoch 7: loss 16.127791068941566
Epoch 8: loss 16.12760231765863
Epoch 9: loss 16.127393011816043
Epoch 10: loss 16.127211871258652
Epoch 11: loss 16.127015223770183
Epoch 12: loss 16.126824008439662
Epoch 13: loss 16.12663285300549
Epoch 14: loss 16.12643838045453
Epoch 15: loss 16.126248099610752
Epoch 16: loss 16.126061718252576
Epoch 17: loss 16.125861947597404
Epoch 18: loss 16.12566466665968
Epoch 19: loss 16.12546665088971
Epoch 20: loss 16.12527708335698
Epoch 21: loss 16.12507842454397
Epoch 22: loss 16.124888467555333
Epoch 23: loss 16.12469410909266
Epoch 24: loss 16.124496763591342
Epoch 25: loss 16.124309302802445
Epoch 26: loss 16.12411054105069
Epoch 27: loss 16.12392030506435
Epoch 28: loss 16.123728402870515
Epoch 29: loss 16.123536243459466
Epoch 30: loss 16.123344641525243
Epoch 31: loss 16.12315176646952
Epoch 32: loss 16.122954696335825
Epoch 33: loss 16.12275949102695
Epoch 34: loss 16.122566879930023
Epoch 35: loss 16.122376431583906
Epoch 36: loss 16.12218113085574
Epoch 37: loss 16.121996300579607
Epoch 38: loss 16.121788147287955
Epoch 39: loss 16.121604506641447
Epoch 40: loss 16.121403605993706
Epoch 41: loss 16.121218972260568
Epoch 42: loss 16.121019569799394
Epoch 43: loss 16.120821643225714
Epoch 44: loss 16.120636029370523
Epoch 45: loss 16.120440505651626
Epoch 46: loss 16.12024399014256
Epoch 47: loss 16.120054895816903
Epoch 48: loss 16.11987082163493
Epoch 49: loss 16.119660683985053
-----------Time: 0:09:03.205850, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016030311584473-------------


Epoch 0: loss 16.129126356891344
Epoch 1: loss 16.128932690737102
Epoch 2: loss 16.128743796065944
Epoch 3: loss 16.128547479174202
Epoch 4: loss 16.128345827877425
Epoch 5: loss 16.128160424048385
Epoch 6: loss 16.127970924450043
Epoch 7: loss 16.12776840815666
Epoch 8: loss 16.127578069750157
Epoch 9: loss 16.127383177406184
Epoch 10: loss 16.127190166740977
Epoch 11: loss 16.12699433835451
Epoch 12: loss 16.126801969954474
Epoch 13: loss 16.126607353756
Epoch 14: loss 16.126412178784236
Epoch 15: loss 16.126218387132887
Epoch 16: loss 16.12603896904925
Epoch 17: loss 16.1258370193079
Epoch 18: loss 16.125643533101996
Epoch 19: loss 16.125450864701637
Epoch 20: loss 16.125258539343996
Epoch 21: loss 16.125064032825847
Epoch 22: loss 16.12486776908887
Epoch 23: loss 16.124678244339247
Epoch 24: loss 16.124477618799837
Epoch 25: loss 16.124292759742342
Epoch 26: loss 16.12409107733058
Epoch 27: loss 16.123908658725142
Epoch 28: loss 16.12370743655588
Epoch 29: loss 16.1235163918392
Epoch 30: loss 16.12332017295966
Epoch 31: loss 16.123138327388546
Epoch 32: loss 16.12294145146422
Epoch 33: loss 16.122745024892154
Epoch 34: loss 16.1225535160436
Epoch 35: loss 16.12235514737783
Epoch 36: loss 16.122164289610357
Epoch 37: loss 16.121964180320422
Epoch 38: loss 16.121774993168394
Epoch 39: loss 16.121580758128495
Epoch 40: loss 16.121385176328292
Epoch 41: loss 16.12119698718944
Epoch 42: loss 16.12100137608929
Epoch 43: loss 16.120811090837556
Epoch 44: loss 16.12061788077715
Epoch 45: loss 16.120423543057797
Epoch 46: loss 16.12022973325604
Epoch 47: loss 16.120037407898398
Epoch 48: loss 16.119838733527892
Epoch 49: loss 16.11965038933269
-----------Time: 0:08:00.959398, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015987396240234-------------


Epoch 0: loss 16.129067111365362
Epoch 1: loss 16.128875194651204
Epoch 2: loss 16.12868576169079
Epoch 3: loss 16.12848756571319
Epoch 4: loss 16.128297275275624
Epoch 5: loss 16.128103786476803
Epoch 6: loss 16.127913407102234
Epoch 7: loss 16.127719908709626
Epoch 8: loss 16.127522458713816
Epoch 9: loss 16.127332505614554
Epoch 10: loss 16.12713866132702
Epoch 11: loss 16.12694282412464
Epoch 12: loss 16.12674861838469
Epoch 13: loss 16.12655319138154
Epoch 14: loss 16.126363527392357
Epoch 15: loss 16.126174105322185
Epoch 16: loss 16.125974827580247
Epoch 17: loss 16.125780250534795
Epoch 18: loss 16.125591117574704
Epoch 19: loss 16.125389797393236
Epoch 20: loss 16.12519864351481
Epoch 21: loss 16.125007032764678
Epoch 22: loss 16.124821569817165
Epoch 23: loss 16.124627480499118
Epoch 24: loss 16.124424493591572
Epoch 25: loss 16.124233323637075
Epoch 26: loss 16.12403737156854
Epoch 27: loss 16.123839966430165
Epoch 28: loss 16.12365715473874
Epoch 29: loss 16.123471019448242
Epoch 30: loss 16.12326939045054
Epoch 31: loss 16.123070827056818
Epoch 32: loss 16.122885022622338
Epoch 33: loss 16.12268379786016
Epoch 34: loss 16.122487815973095
Epoch 35: loss 16.12229742181891
Epoch 36: loss 16.122106755149307
Epoch 37: loss 16.121907837304036
Epoch 38: loss 16.12171599993309
Epoch 39: loss 16.12151926324833
Epoch 40: loss 16.12133077047907
Epoch 41: loss 16.121139064050357
Epoch 42: loss 16.12094630800945
Epoch 43: loss 16.120749375559573
Epoch 44: loss 16.120559486764613
Epoch 45: loss 16.12036269096138
Epoch 46: loss 16.120166714000856
Epoch 47: loss 16.119974777321247
Epoch 48: loss 16.11978409483486
Epoch 49: loss 16.11958421424007
-----------Time: 0:11:14.227926, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015988349914551-------------


Epoch 0: loss 16.129077899708832
Epoch 1: loss 16.128884619121127
Epoch 2: loss 16.12869612505541
Epoch 3: loss 16.12849825811879
Epoch 4: loss 16.128309011329705
Epoch 5: loss 16.12811490256479
Epoch 6: loss 16.127913857750947
Epoch 7: loss 16.127719780101017
Epoch 8: loss 16.12752859510761
Epoch 9: loss 16.12734050709246
Epoch 10: loss 16.12714454802305
Epoch 11: loss 16.12694906345718
Epoch 12: loss 16.126751804818532
Epoch 13: loss 16.126563138064643
Epoch 14: loss 16.12636906611913
Epoch 15: loss 16.126173907223443
Epoch 16: loss 16.12598427720145
Epoch 17: loss 16.12579285599344
Epoch 18: loss 16.12559132811944
Epoch 19: loss 16.125401877267908
Epoch 20: loss 16.125210381383933
Epoch 21: loss 16.125015791633196
Epoch 22: loss 16.124823481314465
Epoch 23: loss 16.12463120444434
Epoch 24: loss 16.124437147019155
Epoch 25: loss 16.124238120012144
Epoch 26: loss 16.12404741963464
Epoch 27: loss 16.12385500093204
Epoch 28: loss 16.12366330020774
Epoch 29: loss 16.123465818578357
Epoch 30: loss 16.123278060382056
Epoch 31: loss 16.123082935194272
Epoch 32: loss 16.122889480362645
Epoch 33: loss 16.122687320854432
Epoch 34: loss 16.122499935260088
Epoch 35: loss 16.122307860118788
Epoch 36: loss 16.12211889829112
Epoch 37: loss 16.12192015287473
Epoch 38: loss 16.12173185975996
Epoch 39: loss 16.121534813481095
Epoch 40: loss 16.121337158385668
Epoch 41: loss 16.12114982776113
Epoch 42: loss 16.12094806730261
Epoch 43: loss 16.120762547310957
Epoch 44: loss 16.12057538885601
Epoch 45: loss 16.120372807221155
Epoch 46: loss 16.12017729335534
Epoch 47: loss 16.1199817219268
Epoch 48: loss 16.11978977902419
Epoch 49: loss 16.119593921597072
-----------Time: 0:11:04.126197, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015987873077393-------------


Epoch 0: loss 16.129081530568392
Epoch 1: loss 16.128881364493605
Epoch 2: loss 16.128695352885178
Epoch 3: loss 16.128503019230205
Epoch 4: loss 16.128302056611783
Epoch 5: loss 16.12811393099936
Epoch 6: loss 16.12791750598304
Epoch 7: loss 16.12772591623552
Epoch 8: loss 16.12753342000475
Epoch 9: loss 16.12733722938799
Epoch 10: loss 16.12714305761523
Epoch 11: loss 16.126947016868986
Epoch 12: loss 16.12675931986549
Epoch 13: loss 16.126565178170548
Epoch 14: loss 16.126365999477983
Epoch 15: loss 16.126176266517245
Epoch 16: loss 16.125991897780068
Epoch 17: loss 16.125786393670168
Epoch 18: loss 16.125592773929117
Epoch 19: loss 16.125402995073777
Epoch 20: loss 16.12521395857014
Epoch 21: loss 16.125015378581757
Epoch 22: loss 16.12481876920915
Epoch 23: loss 16.12462844687872
Epoch 24: loss 16.124437376751466
Epoch 25: loss 16.124242632203675
Epoch 26: loss 16.124042867252918
Epoch 27: loss 16.12385319600357
Epoch 28: loss 16.12366287367314
Epoch 29: loss 16.12346730042956
Epoch 30: loss 16.12327648337086
Epoch 31: loss 16.123078609433364
Epoch 32: loss 16.122886917784268
Epoch 33: loss 16.12269217064356
Epoch 34: loss 16.122496151159222
Epoch 35: loss 16.122306103418545
Epoch 36: loss 16.122114971320613
Epoch 37: loss 16.12192704199189
Epoch 38: loss 16.121721157501288
Epoch 39: loss 16.121533558509995
Epoch 40: loss 16.121338743694196
Epoch 41: loss 16.12114575973602
Epoch 42: loss 16.120948838435677
Epoch 43: loss 16.120760047481138
Epoch 44: loss 16.120565632752196
Epoch 45: loss 16.12037221188776
Epoch 46: loss 16.120179911681397
Epoch 47: loss 16.119987872322337
Epoch 48: loss 16.119791597695116
Epoch 49: loss 16.119598008031883
-----------Time: 0:13:34.892720, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015989780426025-------------


Epoch 0: loss 16.128466893435174
Epoch 1: loss 16.12654083095341
Epoch 2: loss 16.124609906236525
Epoch 3: loss 16.122672355583404
Epoch 4: loss 16.120740787304893
Epoch 5: loss 16.118804534665262
Epoch 6: loss 16.11686781711589
Epoch 7: loss 16.11493883189979
Epoch 8: loss 16.113010434238337
Epoch 9: loss 16.111073253853544
Epoch 10: loss 16.10913124053358
Epoch 11: loss 16.107197933445942
Epoch 12: loss 16.10527461556522
Epoch 13: loss 16.10333834062652
Epoch 14: loss 16.101402840191675
Epoch 15: loss 16.099470617461296
Epoch 16: loss 16.09754330571285
Epoch 17: loss 16.095608927232536
Epoch 18: loss 16.093678138903005
Epoch 19: loss 16.0917489268068
Epoch 20: loss 16.089817268554125
Epoch 21: loss 16.087885687311037
Epoch 22: loss 16.085946877538152
Epoch 23: loss 16.084013696984794
Epoch 24: loss 16.082089554297657
Epoch 25: loss 16.080150875207714
Epoch 26: loss 16.07821547071074
Epoch 27: loss 16.076285502779672
Epoch 28: loss 16.074357600105788
Epoch 29: loss 16.072425845396655
Epoch 30: loss 16.070494645571436
Epoch 31: loss 16.068567923711264
Epoch 32: loss 16.066633534081415
Epoch 33: loss 16.06470161679648
Epoch 34: loss 16.062779591743425
Epoch 35: loss 16.06084101844443
Epoch 36: loss 16.05891726080525
Epoch 37: loss 16.056984262274554
Epoch 38: loss 16.055050140752165
Epoch 39: loss 16.053121125717535
Epoch 40: loss 16.051200469723856
Epoch 41: loss 16.0492676695512
Epoch 42: loss 16.047340932133537
Epoch 43: loss 16.045406747862593
Epoch 44: loss 16.043485229724514
Epoch 45: loss 16.04155226956897
Epoch 46: loss 16.039622958164102
Epoch 47: loss 16.037702488082463
Epoch 48: loss 16.03576947736507
Epoch 49: loss 16.033840882382748
-----------Time: 0:07:33.815804, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005213260650635-------------


Epoch 0: loss 16.128211166367315
Epoch 1: loss 16.126278897742335
Epoch 2: loss 16.12434510733525
Epoch 3: loss 16.12241127492294
Epoch 4: loss 16.120478122632356
Epoch 5: loss 16.118540715367462
Epoch 6: loss 16.116607436024022
Epoch 7: loss 16.114667499370068
Epoch 8: loss 16.112743599379822
Epoch 9: loss 16.110802382862783
Epoch 10: loss 16.108875009143656
Epoch 11: loss 16.106935332818416
Epoch 12: loss 16.10500769565908
Epoch 13: loss 16.103073033254518
Epoch 14: loss 16.101145277598942
Epoch 15: loss 16.099208568346896
Epoch 16: loss 16.097277206205167
Epoch 17: loss 16.095341235934036
Epoch 18: loss 16.093411662903993
Epoch 19: loss 16.091473080270507
Epoch 20: loss 16.089545773967185
Epoch 21: loss 16.087616564204602
Epoch 22: loss 16.08567828701656
Epoch 23: loss 16.083746623059472
Epoch 24: loss 16.081815917443944
Epoch 25: loss 16.0798942079487
Epoch 26: loss 16.077953098778366
Epoch 27: loss 16.076023274754103
Epoch 28: loss 16.074091642949163
Epoch 29: loss 16.07215651096761
Epoch 30: loss 16.07023311244722
Epoch 31: loss 16.068301626364132
Epoch 32: loss 16.066372648667485
Epoch 33: loss 16.064437514611598
Epoch 34: loss 16.062505681077834
Epoch 35: loss 16.06057285679106
Epoch 36: loss 16.058652906329687
Epoch 37: loss 16.05671486405979
Epoch 38: loss 16.054788517135197
Epoch 39: loss 16.05285611730869
Epoch 40: loss 16.050925224225374
Epoch 41: loss 16.048996766148992
Epoch 42: loss 16.04706894385549
Epoch 43: loss 16.045140786038722
Epoch 44: loss 16.04321113962917
Epoch 45: loss 16.041286136612676
Epoch 46: loss 16.039354666605664
Epoch 47: loss 16.03742255640782
Epoch 48: loss 16.03549882003054
Epoch 49: loss 16.033568807760616
-----------Time: 0:09:34.983597, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.005211353302002-------------


Epoch 0: loss 16.128207491168904
Epoch 1: loss 16.12627125279031
Epoch 2: loss 16.124336236712075
Epoch 3: loss 16.12241070603728
Epoch 4: loss 16.12046280576199
Epoch 5: loss 16.118536753651888
Epoch 6: loss 16.11660613801053
Epoch 7: loss 16.114665266091958
Epoch 8: loss 16.11273098951322
Epoch 9: loss 16.110794139465863
Epoch 10: loss 16.108861392447974
Epoch 11: loss 16.1069319431
Epoch 12: loss 16.104998021232102
Epoch 13: loss 16.103071377418686
Epoch 14: loss 16.101132132554586
Epoch 15: loss 16.09919483756293
Epoch 16: loss 16.097266256841646
Epoch 17: loss 16.095331115784887
Epoch 18: loss 16.093400330048272
Epoch 19: loss 16.091468648718646
Epoch 20: loss 16.08954047404793
Epoch 21: loss 16.087595804286043
Epoch 22: loss 16.085664190890803
Epoch 23: loss 16.083746951841153
Epoch 24: loss 16.08180346626377
Epoch 25: loss 16.079872644226338
Epoch 26: loss 16.07793586289124
Epoch 27: loss 16.076009365836843
Epoch 28: loss 16.0740773476875
Epoch 29: loss 16.07213797940062
Epoch 30: loss 16.070214265840995
Epoch 31: loss 16.068272812331482
Epoch 32: loss 16.06635042815963
Epoch 33: loss 16.06442368270393
Epoch 34: loss 16.06248975435374
Epoch 35: loss 16.060558072764824
Epoch 36: loss 16.058628471731296
Epoch 37: loss 16.056693332489576
Epoch 38: loss 16.05476536914147
Epoch 39: loss 16.052833990923666
Epoch 40: loss 16.050908615305218
Epoch 41: loss 16.04897606275599
Epoch 42: loss 16.047039577791132
Epoch 43: loss 16.045121517305855
Epoch 44: loss 16.04318815447054
Epoch 45: loss 16.04125539578453
Epoch 46: loss 16.039324966314503
Epoch 47: loss 16.03739467997341
Epoch 48: loss 16.035463395878438
Epoch 49: loss 16.03354331940137
-----------Time: 0:09:40.336514, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.005202770233154-------------


Epoch 0: loss 16.12821467276693
Epoch 1: loss 16.126280860838648
Epoch 2: loss 16.124353607949384
Epoch 3: loss 16.12241340344726
Epoch 4: loss 16.120476663858103
Epoch 5: loss 16.118541288401783
Epoch 6: loss 16.116611583910927
Epoch 7: loss 16.114673080620655
Epoch 8: loss 16.112740167396883
Epoch 9: loss 16.11081078183463
Epoch 10: loss 16.10887841130807
Epoch 11: loss 16.10694127163205
Epoch 12: loss 16.105015006125004
Epoch 13: loss 16.10307876411633
Epoch 14: loss 16.10114197863257
Epoch 15: loss 16.09921036082937
Epoch 16: loss 16.097276958063155
Epoch 17: loss 16.095345416491675
Epoch 18: loss 16.093416079416944
Epoch 19: loss 16.091475883212148
Epoch 20: loss 16.089549661525375
Epoch 21: loss 16.087610793152603
Epoch 22: loss 16.085685534733933
Epoch 23: loss 16.083752166972076
Epoch 24: loss 16.081813602229705
Epoch 25: loss 16.079885503012825
Epoch 26: loss 16.07795382323895
Epoch 27: loss 16.076024787460998
Epoch 28: loss 16.074095448571228
Epoch 29: loss 16.072155317448612
Epoch 30: loss 16.070226994463127
Epoch 31: loss 16.068296251250324
Epoch 32: loss 16.06636922575975
Epoch 33: loss 16.064424139834927
Epoch 34: loss 16.062505363445688
Epoch 35: loss 16.06057264883924
Epoch 36: loss 16.058635924548284
Epoch 37: loss 16.05670930977552
Epoch 38: loss 16.05477270683301
Epoch 39: loss 16.052847702260767
Epoch 40: loss 16.050914280047685
Epoch 41: loss 16.048983968555312
Epoch 42: loss 16.047063501844463
Epoch 43: loss 16.045124269426356
Epoch 44: loss 16.04319475421833
Epoch 45: loss 16.041265691474056
Epoch 46: loss 16.039334530023986
Epoch 47: loss 16.037406446623894
Epoch 48: loss 16.035468031233457
Epoch 49: loss 16.033542183445096
-----------Time: 0:12:16.142591, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005198955535889-------------


Epoch 0: loss 16.12821553465204
Epoch 1: loss 16.126276820298443
Epoch 2: loss 16.124340383821107
Epoch 3: loss 16.122408918740636
Epoch 4: loss 16.12047626973495
Epoch 5: loss 16.118540252272755
Epoch 6: loss 16.11660686454545
Epoch 7: loss 16.114683948307338
Epoch 8: loss 16.112740192548163
Epoch 9: loss 16.11080246428033
Epoch 10: loss 16.108872182865774
Epoch 11: loss 16.1069446450151
Epoch 12: loss 16.10500290058047
Epoch 13: loss 16.103076806205845
Epoch 14: loss 16.1011404988557
Epoch 15: loss 16.099203829015966
Epoch 16: loss 16.09726888242787
Epoch 17: loss 16.09533738467666
Epoch 18: loss 16.093397345343252
Epoch 19: loss 16.091462293482785
Epoch 20: loss 16.0895385910727
Epoch 21: loss 16.087606576034855
Epoch 22: loss 16.0856712874412
Epoch 23: loss 16.083738219679667
Epoch 24: loss 16.081812392893923
Epoch 25: loss 16.079878035416222
Epoch 26: loss 16.07794683507242
Epoch 27: loss 16.07601530672481
Epoch 28: loss 16.074079079236462
Epoch 29: loss 16.07214132503947
Epoch 30: loss 16.070207969723608
Epoch 31: loss 16.068287904136785
Epoch 32: loss 16.066342935671035
Epoch 33: loss 16.064417535160597
Epoch 34: loss 16.06248431130555
Epoch 35: loss 16.06054837189011
Epoch 36: loss 16.058614750022535
Epoch 37: loss 16.056682846998637
Epoch 38: loss 16.05474921450011
Epoch 39: loss 16.052811000579204
Epoch 40: loss 16.05087561449193
Epoch 41: loss 16.04895373567931
Epoch 42: loss 16.047017727033026
Epoch 43: loss 16.04508514155377
Epoch 44: loss 16.043154737494316
Epoch 45: loss 16.041219456160828
Epoch 46: loss 16.03928630931537
Epoch 47: loss 16.037359382614873
Epoch 48: loss 16.03542703983251
Epoch 49: loss 16.033484448033096
-----------Time: 0:12:11.389119, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005192279815674-------------


Epoch 0: loss 16.11968159895996
Epoch 1: loss 16.100360181942268
Epoch 2: loss 16.08105082379663
Epoch 3: loss 16.061757306549588
Epoch 4: loss 16.04246188921407
Epoch 5: loss 16.023187541909813
Epoch 6: loss 16.00391622805738
Epoch 7: loss 15.98467093774195
Epoch 8: loss 15.965431201970079
Epoch 9: loss 15.946214306672156
Epoch 10: loss 15.927006817953037
Epoch 11: loss 15.907801212727732
Epoch 12: loss 15.888632032001324
Epoch 13: loss 15.869464604085785
Epoch 14: loss 15.850303395537335
Epoch 15: loss 15.831162877676602
Epoch 16: loss 15.81202611332033
Epoch 17: loss 15.792906344228623
Epoch 18: loss 15.773805124854315
Epoch 19: loss 15.754712506439972
Epoch 20: loss 15.735628802728368
Epoch 21: loss 15.716558218131965
Epoch 22: loss 15.697502793793836
Epoch 23: loss 15.678467329200549
Epoch 24: loss 15.659426160971064
Epoch 25: loss 15.640398359739502
Epoch 26: loss 15.621396272969415
Epoch 27: loss 15.602394985595172
Epoch 28: loss 15.583414372054698
Epoch 29: loss 15.564453116961962
Epoch 30: loss 15.545488636800943
Epoch 31: loss 15.52655313091475
Epoch 32: loss 15.50763253014395
Epoch 33: loss 15.488718208377293
Epoch 34: loss 15.469814226379727
Epoch 35: loss 15.450910561495725
Epoch 36: loss 15.432027500696062
Epoch 37: loss 15.413157390212806
Epoch 38: loss 15.39431528399469
Epoch 39: loss 15.375478690314901
Epoch 40: loss 15.356648772096037
Epoch 41: loss 15.337840823650101
Epoch 42: loss 15.319035865872888
Epoch 43: loss 15.300240354346089
Epoch 44: loss 15.281452978350924
Epoch 45: loss 15.262682122079623
Epoch 46: loss 15.24392844594776
Epoch 47: loss 15.225185976655927
Epoch 48: loss 15.206466500301476
Epoch 49: loss 15.18774013949711
-----------Time: 0:08:58.645977, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.897514581680298-------------


Epoch 0: loss 16.119620596474444
Epoch 1: loss 16.100295006159275
Epoch 2: loss 16.08096498403295
Epoch 3: loss 16.061658477057186
Epoch 4: loss 16.0423614020708
Epoch 5: loss 16.023080633411336
Epoch 6: loss 16.00381535249537
Epoch 7: loss 15.984561092766684
Epoch 8: loss 15.96531578507872
Epoch 9: loss 15.946081859252581
Epoch 10: loss 15.92685822911597
Epoch 11: loss 15.907646482051748
Epoch 12: loss 15.888450120310864
Epoch 13: loss 15.869251643010234
Epoch 14: loss 15.850073999526256
Epoch 15: loss 15.83091114084639
Epoch 16: loss 15.811745797634902
Epoch 17: loss 15.792599210018306
Epoch 18: loss 15.77346052820103
Epoch 19: loss 15.754326291159485
Epoch 20: loss 15.735211848434773
Epoch 21: loss 15.716102540201309
Epoch 22: loss 15.697006606485223
Epoch 23: loss 15.677904429547226
Epoch 24: loss 15.658812044495276
Epoch 25: loss 15.63972077491557
Epoch 26: loss 15.620632989954988
Epoch 27: loss 15.60155536118508
Epoch 28: loss 15.58247584036474
Epoch 29: loss 15.56339079144858
Epoch 30: loss 15.544309110729639
Epoch 31: loss 15.525225199584796
Epoch 32: loss 15.506129149446803
Epoch 33: loss 15.487033533103574
Epoch 34: loss 15.467926129625827
Epoch 35: loss 15.44880797099393
Epoch 36: loss 15.4296674132023
Epoch 37: loss 15.410534192065041
Epoch 38: loss 15.391349748919488
Epoch 39: loss 15.372159199976545
Epoch 40: loss 15.352936869409177
Epoch 41: loss 15.333684456354902
Epoch 42: loss 15.31442032954562
Epoch 43: loss 15.295088725740849
Epoch 44: loss 15.275727550512816
Epoch 45: loss 15.25631068723885
Epoch 46: loss 15.236847455635091
Epoch 47: loss 15.217305570273636
Epoch 48: loss 15.19770574725277
Epoch 49: loss 15.178031913607972
-----------Time: 0:07:55.352693, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.8962669372558594-------------


Epoch 0: loss 16.119578859869588
Epoch 1: loss 16.100257823491887
Epoch 2: loss 16.08092901251639
Epoch 3: loss 16.061618902425263
Epoch 4: loss 16.042312731750638
Epoch 5: loss 16.023020425395124
Epoch 6: loss 16.00373342933053
Epoch 7: loss 15.984467238041939
Epoch 8: loss 15.965211281509358
Epoch 9: loss 15.945943596701444
Epoch 10: loss 15.926682023654624
Epoch 11: loss 15.90742382139794
Epoch 12: loss 15.888159285426594
Epoch 13: loss 15.8688916828141
Epoch 14: loss 15.849598072999973
Epoch 15: loss 15.830286133088386
Epoch 16: loss 15.810948564540828
Epoch 17: loss 15.791579090946067
Epoch 18: loss 15.772151770962523
Epoch 19: loss 15.75265635790677
Epoch 20: loss 15.733085253758558
Epoch 21: loss 15.713416113290792
Epoch 22: loss 15.693617904750209
Epoch 23: loss 15.67367129561822
Epoch 24: loss 15.653546108247925
Epoch 25: loss 15.633206502583572
Epoch 26: loss 15.61259789775415
Epoch 27: loss 15.591717879496041
Epoch 28: loss 15.570492186450387
Epoch 29: loss 15.548895692228946
Epoch 30: loss 15.526876670760652
Epoch 31: loss 15.504368738741768
Epoch 32: loss 15.481345442211323
Epoch 33: loss 15.45774936105584
Epoch 34: loss 15.433533196348156
Epoch 35: loss 15.408622986989023
Epoch 36: loss 15.382998478159818
Epoch 37: loss 15.356613556155049
Epoch 38: loss 15.329405194200078
Epoch 39: loss 15.301341389235496
Epoch 40: loss 15.272387267844453
Epoch 41: loss 15.242487170762377
Epoch 42: loss 15.21161048239894
Epoch 43: loss 15.179703278150035
Epoch 44: loss 15.146778814814672
Epoch 45: loss 15.11278326839387
Epoch 46: loss 15.07769529271087
Epoch 47: loss 15.041474320047119
Epoch 48: loss 15.004109599915713
Epoch 49: loss 14.965588821413208
-----------Time: 0:11:12.213332, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.8677456378936768-------------


Epoch 0: loss 16.119584309140777
Epoch 1: loss 16.100251754254604
Epoch 2: loss 16.080937322551236
Epoch 3: loss 16.061608977263358
Epoch 4: loss 16.0423059033077
Epoch 5: loss 16.0230134769002
Epoch 6: loss 16.003719024647832
Epoch 7: loss 15.984415412921502
Epoch 8: loss 15.965115295408081
Epoch 9: loss 15.94578917950894
Epoch 10: loss 15.926421468059546
Epoch 11: loss 15.907013786817906
Epoch 12: loss 15.887537154508324
Epoch 13: loss 15.8679822226333
Epoch 14: loss 15.848265885399242
Epoch 15: loss 15.828383522490045
Epoch 16: loss 15.808257399855131
Epoch 17: loss 15.787820872047014
Epoch 18: loss 15.766994042264306
Epoch 19: loss 15.745684429508891
Epoch 20: loss 15.723793637564546
Epoch 21: loss 15.701206182642693
Epoch 22: loss 15.67781495607178
Epoch 23: loss 15.653496559966577
Epoch 24: loss 15.628136150988631
Epoch 25: loss 15.601614943530782
Epoch 26: loss 15.573794281437321
Epoch 27: loss 15.544609047006562
Epoch 28: loss 15.513904986918265
Epoch 29: loss 15.481604669455798
Epoch 30: loss 15.44761266003101
Epoch 31: loss 15.411870814069838
Epoch 32: loss 15.374289568641252
Epoch 33: loss 15.334774239806134
Epoch 34: loss 15.293305217344647
Epoch 35: loss 15.249842149009519
Epoch 36: loss 15.204317429455939
Epoch 37: loss 15.156714432391219
Epoch 38: loss 15.106999787511613
Epoch 39: loss 15.05515878365181
Epoch 40: loss 15.00118344292944
Epoch 41: loss 14.945063959197418
Epoch 42: loss 14.886796105484914
Epoch 43: loss 14.826371044617362
Epoch 44: loss 14.763813960805026
Epoch 45: loss 14.69910226456817
Epoch 46: loss 14.632257651089974
Epoch 47: loss 14.56330550760598
Epoch 48: loss 14.4922404001432
Epoch 49: loss 14.419102432547605
-----------Time: 0:11:01.706311, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7944395542144775-------------


Epoch 0: loss 16.119585847517534
Epoch 1: loss 16.100260024099263
Epoch 2: loss 16.080940009330263
Epoch 3: loss 16.061617084791507
Epoch 4: loss 16.042287865950403
Epoch 5: loss 16.022964137607843
Epoch 6: loss 16.003614140179184
Epoch 7: loss 15.984220606143738
Epoch 8: loss 15.964759057082839
Epoch 9: loss 15.945208485973081
Epoch 10: loss 15.92546721785661
Epoch 11: loss 15.905473334430157
Epoch 12: loss 15.88513588918299
Epoch 13: loss 15.864290767159392
Epoch 14: loss 15.842752046206517
Epoch 15: loss 15.820354869016946
Epoch 16: loss 15.796865376134875
Epoch 17: loss 15.772051508106442
Epoch 18: loss 15.745717503701687
Epoch 19: loss 15.717610818397228
Epoch 20: loss 15.687532736854491
Epoch 21: loss 15.655274899904335
Epoch 22: loss 15.620697664786707
Epoch 23: loss 15.58362495464887
Epoch 24: loss 15.543953785629231
Epoch 25: loss 15.501600818571804
Epoch 26: loss 15.456501903710254
Epoch 27: loss 15.408532765976043
Epoch 28: loss 15.357695953427221
Epoch 29: loss 15.303952815028362
Epoch 30: loss 15.247256661706025
Epoch 31: loss 15.187639681913076
Epoch 32: loss 15.125079942695987
Epoch 33: loss 15.059585114417354
Epoch 34: loss 14.991142165083934
Epoch 35: loss 14.919773589534044
Epoch 36: loss 14.845480134268051
Epoch 37: loss 14.768302290513503
Epoch 38: loss 14.68822563491871
Epoch 39: loss 14.605300404702664
Epoch 40: loss 14.519548803000687
Epoch 41: loss 14.4309763516856
Epoch 42: loss 14.33966974877611
Epoch 43: loss 14.245566763263348
Epoch 44: loss 14.148747157894963
Epoch 45: loss 14.049259813017272
Epoch 46: loss 13.94715840638883
Epoch 47: loss 13.842455899216286
Epoch 48: loss 13.73520512293058
Epoch 49: loss 13.62543667834242
-----------Time: 0:13:23.049865, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.6863696575164795-------------


Epoch 0: loss 16.033522223959544
Epoch 1: loss 15.841595722839456
Epoch 2: loss 15.651047469611528
Epoch 3: loss 15.46169458568712
Epoch 4: loss 15.273495195999685
Epoch 5: loss 15.086584985742366
Epoch 6: loss 14.900675427466387
Epoch 7: loss 14.715677249165836
Epoch 8: loss 14.531318379070786
Epoch 9: loss 14.347386770367688
Epoch 10: loss 14.163333928863791
Epoch 11: loss 13.978538556744574
Epoch 12: loss 13.792091813536038
Epoch 13: loss 13.602895500998319
Epoch 14: loss 13.40950853271028
Epoch 15: loss 13.210501475072283
Epoch 16: loss 13.004231399268544
Epoch 17: loss 12.789132006729732
Epoch 18: loss 12.563681776462657
Epoch 19: loss 12.326414127463941
Epoch 20: loss 12.076139558975692
Epoch 21: loss 11.811995921412391
Epoch 22: loss 11.533258587462283
Epoch 23: loss 11.239293779608605
Epoch 24: loss 10.929939500270427
Epoch 25: loss 10.605277172179894
Epoch 26: loss 10.265541478562056
Epoch 27: loss 9.911479021428136
Epoch 28: loss 9.544138968801162
Epoch 29: loss 9.164382673463724
Epoch 30: loss 8.773717661666248
Epoch 31: loss 8.373726973914788
Epoch 32: loss 7.96616346515865
Epoch 33: loss 7.5528825369135335
Epoch 34: loss 7.135848805203005
Epoch 35: loss 6.71742264896453
Epoch 36: loss 6.300001895706442
Epoch 37: loss 5.885983781622699
Epoch 38: loss 5.477826225297356
Epoch 39: loss 5.078422006801006
Epoch 40: loss 4.6903576996094385
Epoch 41: loss 4.316318637135886
Epoch 42: loss 3.9593192211890624
Epoch 43: loss 3.6215827436535304
Epoch 44: loss 3.3059806209209497
Epoch 45: loss 3.0146393164011887
Epoch 46: loss 2.7494621819423033
Epoch 47: loss 2.511701627520519
Epoch 48: loss 2.3019716308906455
Epoch 49: loss 2.120028170053566
-----------Time: 0:07:44.334276, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.4388483762741089-------------


Epoch 0: loss 16.033398950274968
Epoch 1: loss 15.841465625379188
Epoch 2: loss 15.650523170172487
Epoch 3: loss 15.459894892053153
Epoch 4: loss 15.267659803912197
Epoch 5: loss 15.068983883621772
Epoch 6: loss 14.853788593140811
Epoch 7: loss 14.607353082877777
Epoch 8: loss 14.313884996991886
Epoch 9: loss 13.961469421054826
Epoch 10: loss 13.544187912155326
Epoch 11: loss 13.061794579450432
Epoch 12: loss 12.517506616798803
Epoch 13: loss 11.916943394275643
Epoch 14: loss 11.26650013464699
Epoch 15: loss 10.57390806945917
Epoch 16: loss 9.84783866238244
Epoch 17: loss 9.096930383015353
Epoch 18: loss 8.331229446762732
Epoch 19: loss 7.560265886841941
Epoch 20: loss 6.795315012327674
Epoch 21: loss 6.047682982673458
Epoch 22: loss 5.329069952785871
Epoch 23: loss 4.651569704765726
Epoch 24: loss 4.026966940714912
Epoch 25: loss 3.4663265826197796
Epoch 26: loss 2.9788395746561935
Epoch 27: loss 2.570347610020391
Epoch 28: loss 2.2416048752606854
Epoch 29: loss 1.9866462964736749
Epoch 30: loss 1.7939668533722948
Epoch 31: loss 1.6492352449680037
Epoch 32: loss 1.5390909237988686
Epoch 33: loss 1.4532885000458096
Epoch 34: loss 1.3846548370299876
Epoch 35: loss 1.3285244792100201
Epoch 36: loss 1.2817529314169747
Epoch 37: loss 1.2422859398096135
Epoch 38: loss 1.2086714866240431
Epoch 39: loss 1.1798795395697375
Epoch 40: loss 1.1551219873968968
Epoch 41: loss 1.1337474892650239
Epoch 42: loss 1.1152473640156675
Epoch 43: loss 1.0992113966829042
Epoch 44: loss 1.0852626223581001
Epoch 45: loss 1.0731137472854613
Epoch 46: loss 1.0625250313092731
Epoch 47: loss 1.0532777844044487
Epoch 48: loss 1.0451795273238125
Epoch 49: loss 1.0380774900406324
-----------Time: 0:09:36.574090, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.0273703336715698-------------


Epoch 0: loss 16.03343481496341
Epoch 1: loss 15.84084814874422
Epoch 2: loss 15.643621596646996
Epoch 3: loss 15.41732948717052
Epoch 4: loss 15.11128178234007
Epoch 5: loss 14.680289990361324
Epoch 6: loss 14.106920365732348
Epoch 7: loss 13.393487229692086
Epoch 8: loss 12.553684193651076
Epoch 9: loss 11.608010633280902
Epoch 10: loss 10.579111300712697
Epoch 11: loss 9.49165426900427
Epoch 12: loss 8.37305558473277
Epoch 13: loss 7.252992129701839
Epoch 14: loss 6.16287563610751
Epoch 15: loss 5.137044460116164
Epoch 16: loss 4.208301895075741
Epoch 17: loss 3.4087502977133703
Epoch 18: loss 2.7615882591944536
Epoch 19: loss 2.2751185272036847
Epoch 20: loss 1.9333870615759512
Epoch 21: loss 1.702168440805822
Epoch 22: loss 1.5439886921381678
Epoch 23: loss 1.4307340889472298
Epoch 24: loss 1.345695425856042
Epoch 25: loss 1.2796033135148868
Epoch 26: loss 1.2270440120875932
Epoch 27: loss 1.184736557633025
Epoch 28: loss 1.1504175879178713
Epoch 29: loss 1.1224332312127052
Epoch 30: loss 1.099516980232526
Epoch 31: loss 1.080668685340959
Epoch 32: loss 1.0651021764905897
Epoch 33: loss 1.0522336391828317
Epoch 34: loss 1.0415189780412386
Epoch 35: loss 1.0326084243077958
Epoch 36: loss 1.0251614188227465
Epoch 37: loss 1.0189137283779475
Epoch 38: loss 1.0136497128651543
Epoch 39: loss 1.0092049236852492
Epoch 40: loss 1.0054389796806718
Epoch 41: loss 1.0022289505498834
Epoch 42: loss 0.9995003623149523
Epoch 43: loss 0.9971665929782125
Epoch 44: loss 0.9951626914003869
Epoch 45: loss 0.9934445418308065
Epoch 46: loss 0.9919634588455752
Epoch 47: loss 0.9906793530218104
Epoch 48: loss 0.9895685841656042
Epoch 49: loss 0.9886011244519499
-----------Time: 0:09:37.110400, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0035306215286255-------------


Epoch 0: loss 16.03329481593677
Epoch 1: loss 15.83760023531932
Epoch 2: loss 15.597714099759573
Epoch 3: loss 15.196190008201826
Epoch 4: loss 14.55693376343039
Epoch 5: loss 13.687289359843103
Epoch 6: loss 12.621514387270755
Epoch 7: loss 11.399812913316952
Epoch 8: loss 10.06788779523724
Epoch 9: loss 8.675869844088158
Epoch 10: loss 7.278268864389474
Epoch 11: loss 5.934030701246775
Epoch 12: loss 4.705511723218112
Epoch 13: loss 3.6524535727410163
Epoch 14: loss 2.8228392440252423
Epoch 15: loss 2.233325459601893
Epoch 16: loss 1.8526071865251363
Epoch 17: loss 1.6148341865303595
Epoch 18: loss 1.4600380654403735
Epoch 19: loss 1.3518271921541847
Epoch 20: loss 1.272088259555776
Epoch 21: loss 1.2115067453926318
Epoch 22: loss 1.1647870932797364
Epoch 23: loss 1.1283830239631483
Epoch 24: loss 1.099861375772804
Epoch 25: loss 1.07739262426269
Epoch 26: loss 1.05959718614055
Epoch 27: loss 1.0454556987524162
Epoch 28: loss 1.0341513077158717
Epoch 29: loss 1.0250576113539587
Epoch 30: loss 1.0177338896982693
Epoch 31: loss 1.0117773268292818
Epoch 32: loss 1.006915361580478
Epoch 33: loss 1.0029360470951998
Epoch 34: loss 0.9996715785369333
Epoch 35: loss 0.9969635930574737
Epoch 36: loss 0.9947203952148598
Epoch 37: loss 0.9928489255153206
Epoch 38: loss 0.9912836920022317
Epoch 39: loss 0.9899787403047571
Epoch 40: loss 0.98886165863927
Epoch 41: loss 0.987925980258338
Epoch 42: loss 0.9871256576568423
Epoch 43: loss 0.9864443203656164
Epoch 44: loss 0.9858698416430384
Epoch 45: loss 0.9853702675816545
Epoch 46: loss 0.9849368085868985
Epoch 47: loss 0.9845623251956206
Epoch 48: loss 0.9842360945840839
Epoch 49: loss 0.9839483908235281
-----------Time: 0:12:21.648034, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0010261535644531-------------


Epoch 0: loss 16.03324473325554
Epoch 1: loss 15.826281669931998
Epoch 2: loss 15.466060783112418
Epoch 3: loss 14.77746506525551
Epoch 4: loss 13.769247591787217
Epoch 5: loss 12.499220468480669
Epoch 6: loss 11.032549791455335
Epoch 7: loss 9.442746428836617
Epoch 8: loss 7.811086370609194
Epoch 9: loss 6.226981380829804
Epoch 10: loss 4.783611656985508
Epoch 11: loss 3.5737614693234576
Epoch 12: loss 2.6663336558728323
Epoch 13: loss 2.0705593063690015
Epoch 14: loss 1.7172698710131995
Epoch 15: loss 1.5071048154138624
Epoch 16: loss 1.3710592392834067
Epoch 17: loss 1.276077282710721
Epoch 18: loss 1.2068203767724113
Epoch 19: loss 1.1553483055686744
Epoch 20: loss 1.1166382553364804
Epoch 21: loss 1.0873334492724378
Epoch 22: loss 1.0650254136003054
Epoch 23: loss 1.0478802714709552
Epoch 24: loss 1.0346642099624745
Epoch 25: loss 1.0243569536498216
Epoch 26: loss 1.0163145454569056
Epoch 27: loss 1.0099719765232982
Epoch 28: loss 1.0049166102134515
Epoch 29: loss 1.000925636294745
Epoch 30: loss 0.9977003168547652
Epoch 31: loss 0.9950963350143557
Epoch 32: loss 0.9930025579295384
Epoch 33: loss 0.9912956188462752
Epoch 34: loss 0.989876217836268
Epoch 35: loss 0.9887328559837631
Epoch 36: loss 0.9877794206369306
Epoch 37: loss 0.9869715260253645
Epoch 38: loss 0.9863190336062767
Epoch 39: loss 0.9857571528715048
Epoch 40: loss 0.9852816905419162
Epoch 41: loss 0.9848791778962724
Epoch 42: loss 0.9845416961058513
Epoch 43: loss 0.9842544479529581
Epoch 44: loss 0.9839972977634355
Epoch 45: loss 0.9837821591574579
Epoch 46: loss 0.9835893781435366
Epoch 47: loss 0.9834241518804469
Epoch 48: loss 0.9832697115785859
Epoch 49: loss 0.9831417574384668
-----------Time: 0:12:07.042270, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.000457525253296-------------


Epoch 0: loss 15.19583502640343
Epoch 1: loss 13.316876854567246
Epoch 2: loss 10.489432501754013
Epoch 3: loss 6.274297160970057
Epoch 4: loss 2.8361606875666463
Epoch 5: loss 1.492392566263708
Epoch 6: loss 1.1789426558072182
Epoch 7: loss 1.072357594059368
Epoch 8: loss 1.0268851225631532
Epoch 9: loss 1.0058527633896206
Epoch 10: loss 0.9954253571262951
Epoch 11: loss 0.9897187978045456
Epoch 12: loss 0.9863437659361882
Epoch 13: loss 0.9840188634304847
Epoch 14: loss 0.9822524195783096
Epoch 15: loss 0.9808084031464679
Epoch 16: loss 0.9794628441722708
Epoch 17: loss 0.9781646985376315
Epoch 18: loss 0.9769124737117781
Epoch 19: loss 0.9756748530286495
Epoch 20: loss 0.9743863541902311
Epoch 21: loss 0.9731361396031383
Epoch 22: loss 0.9718546078314011
Epoch 23: loss 0.970577741849027
Epoch 24: loss 0.9693406018600184
Epoch 25: loss 0.9680440045038341
Epoch 26: loss 0.9667903451798985
Epoch 27: loss 0.9655412217888772
Epoch 28: loss 0.9642942435330447
Epoch 29: loss 0.9630632038638928
Epoch 30: loss 0.9618309551344545
Epoch 31: loss 0.9606308541977256
Epoch 32: loss 0.9593944349906054
Epoch 33: loss 0.9581639690850338
Epoch 34: loss 0.9569634693903143
Epoch 35: loss 0.9556997616437546
Epoch 36: loss 0.9545040310822861
Epoch 37: loss 0.9532223427632763
Epoch 38: loss 0.9519664247830709
Epoch 39: loss 0.9506751512657889
Epoch 40: loss 0.9493630181485508
Epoch 41: loss 0.9480077876644072
Epoch 42: loss 0.9466105363594571
Epoch 43: loss 0.9451920925566137
Epoch 44: loss 0.9436856773218556
Epoch 45: loss 0.9421959425231048
Epoch 46: loss 0.9405873786696537
Epoch 47: loss 0.9389378009865373
Epoch 48: loss 0.9372585732013782
Epoch 49: loss 0.9354673870527724
-----------Time: 0:09:11.028372, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9954467415809631-------------


Epoch 0: loss 15.08272183103494
Epoch 1: loss 9.6940332069677
Epoch 2: loss 3.031936388290596
Epoch 3: loss 1.3099967610453573
Epoch 4: loss 1.0779936565466066
Epoch 5: loss 1.0191242511892138
Epoch 6: loss 1.0011521238325212
Epoch 7: loss 0.9948709712785635
Epoch 8: loss 0.9923348897757123
Epoch 9: loss 0.9910688517597981
Epoch 10: loss 0.9903210985265133
Epoch 11: loss 0.9898100262624275
Epoch 12: loss 0.9893339465920726
Epoch 13: loss 0.9890074459445937
Epoch 14: loss 0.9885125446896503
Epoch 15: loss 0.9881782758132215
Epoch 16: loss 0.9876698220372524
Epoch 17: loss 0.9872424519373192
Epoch 18: loss 0.9867412388778498
Epoch 19: loss 0.9861335936546844
Epoch 20: loss 0.9854622474016218
Epoch 21: loss 0.9847676556774169
Epoch 22: loss 0.9839030258872057
Epoch 23: loss 0.9830345461522839
Epoch 24: loss 0.9820304335362889
Epoch 25: loss 0.9809286544477246
Epoch 26: loss 0.9796562726371894
Epoch 27: loss 0.9782584105385848
Epoch 28: loss 0.9766796270487683
Epoch 29: loss 0.9749635597199704
Epoch 30: loss 0.9730645124357378
Epoch 31: loss 0.970922473674238
Epoch 32: loss 0.9687101613606623
Epoch 33: loss 0.9661835357506292
Epoch 34: loss 0.9636011567045775
Epoch 35: loss 0.96064057651261
Epoch 36: loss 0.9576867931462164
Epoch 37: loss 0.9545714265305818
Epoch 38: loss 0.951213644956233
Epoch 39: loss 0.947682993284965
Epoch 40: loss 0.9440769100234585
Epoch 41: loss 0.9403167343159096
Epoch 42: loss 0.9363623346207646
Epoch 43: loss 0.9322638322374884
Epoch 44: loss 0.927971570727746
Epoch 45: loss 0.9234739467461126
Epoch 46: loss 0.9188292022954516
Epoch 47: loss 0.9139109723363122
Epoch 48: loss 0.9087574845944623
Epoch 49: loss 0.9033498032835918
-----------Time: 0:07:48.940651, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9901518225669861-------------


Epoch 0: loss 13.85724524196689
Epoch 1: loss 3.9661176556021447
Epoch 2: loss 1.2460942499303118
Epoch 3: loss 1.0415747726559963
Epoch 4: loss 1.0067154470833182
Epoch 5: loss 0.9988400334894431
Epoch 6: loss 0.996444813206249
Epoch 7: loss 0.9954658101407519
Epoch 8: loss 0.9950093534441342
Epoch 9: loss 0.9945900447670697
Epoch 10: loss 0.9943211268250743
Epoch 11: loss 0.9939673068148731
Epoch 12: loss 0.9937314498418047
Epoch 13: loss 0.9932762341145396
Epoch 14: loss 0.9928591135016208
Epoch 15: loss 0.9923838050615664
Epoch 16: loss 0.991871031067046
Epoch 17: loss 0.9912552651242759
Epoch 18: loss 0.9905708698619637
Epoch 19: loss 0.9897316287009855
Epoch 20: loss 0.988782892105954
Epoch 21: loss 0.9875966909302519
Epoch 22: loss 0.986361952207865
Epoch 23: loss 0.9848092542905532
Epoch 24: loss 0.9830557714991753
Epoch 25: loss 0.9809446139884034
Epoch 26: loss 0.9785587708220137
Epoch 27: loss 0.9758878441171973
Epoch 28: loss 0.972887364595464
Epoch 29: loss 0.9695012816240633
Epoch 30: loss 0.9658357572011029
Epoch 31: loss 0.9619038647902667
Epoch 32: loss 0.9575483627362119
Epoch 33: loss 0.95285170226722
Epoch 34: loss 0.9479590541095173
Epoch 35: loss 0.9426991629043048
Epoch 36: loss 0.9370574162925567
Epoch 37: loss 0.9311270330540832
Epoch 38: loss 0.924617568311515
Epoch 39: loss 0.9176645574198915
Epoch 40: loss 0.9101799668955893
Epoch 41: loss 0.9020741439306458
Epoch 42: loss 0.8934222191750453
Epoch 43: loss 0.883892878574026
Epoch 44: loss 0.8738518052933462
Epoch 45: loss 0.8630690671556213
Epoch 46: loss 0.8515774009019023
Epoch 47: loss 0.8393763122414687
Epoch 48: loss 0.8264800044523107
Epoch 49: loss 0.8128964438783274
-----------Time: 0:11:19.944277, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9863314032554626-------------


Epoch 0: loss 12.289568202660226
Epoch 1: loss 2.1746150662615094
Epoch 2: loss 1.090726168730001
Epoch 3: loss 1.0138696217711949
Epoch 4: loss 1.002151820745852
Epoch 5: loss 0.9994345540388438
Epoch 6: loss 0.9985792489237212
Epoch 7: loss 0.9980493999486906
Epoch 8: loss 0.997695268367282
Epoch 9: loss 0.9973754421759715
Epoch 10: loss 0.9969225765538643
Epoch 11: loss 0.9965719240654286
Epoch 12: loss 0.9959462555677363
Epoch 13: loss 0.9953515721670372
Epoch 14: loss 0.9945668140269285
Epoch 15: loss 0.9936411156804748
Epoch 16: loss 0.9926104881699932
Epoch 17: loss 0.9913280768975283
Epoch 18: loss 0.9896149812767595
Epoch 19: loss 0.9875982094550017
Epoch 20: loss 0.9853258811954314
Epoch 21: loss 0.9824182313865394
Epoch 22: loss 0.979204900890929
Epoch 23: loss 0.9757782874093099
Epoch 24: loss 0.9718420293552841
Epoch 25: loss 0.9677605260773285
Epoch 26: loss 0.9633625138382862
Epoch 27: loss 0.9586204643512692
Epoch 28: loss 0.9536313919543961
Epoch 29: loss 0.9484059191093683
Epoch 30: loss 0.9425804528806312
Epoch 31: loss 0.9365072076964988
Epoch 32: loss 0.9296351281309205
Epoch 33: loss 0.9222479277068082
Epoch 34: loss 0.9138653229247493
Epoch 35: loss 0.9046502105887653
Epoch 36: loss 0.8947396189573992
Epoch 37: loss 0.8836121090473592
Epoch 38: loss 0.8714954639142334
Epoch 39: loss 0.8585450771290559
Epoch 40: loss 0.8442974922577929
Epoch 41: loss 0.8290150438786331
Epoch 42: loss 0.8125734918139563
Epoch 43: loss 0.7950910596564906
Epoch 44: loss 0.7763634321642934
Epoch 45: loss 0.7567657465322826
Epoch 46: loss 0.7360031440602884
Epoch 47: loss 0.7144027524518863
Epoch 48: loss 0.6919479539142865
Epoch 49: loss 0.6687966890718187
-----------Time: 0:10:41.181453, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9894066452980042-------------


Epoch 0: loss 10.84646452212217
Epoch 1: loss 1.551301340278301
Epoch 2: loss 1.0447304995083304
Epoch 3: loss 1.0078953186768174
Epoch 4: loss 1.0026360860883443
Epoch 5: loss 1.0013726929272564
Epoch 6: loss 1.0008264362423105
Epoch 7: loss 1.000354401260825
Epoch 8: loss 1.000078070708239
Epoch 9: loss 0.9996966675061646
Epoch 10: loss 0.999216403129504
Epoch 11: loss 0.9984104989857179
Epoch 12: loss 0.9977551979367794
Epoch 13: loss 0.9968235754914877
Epoch 14: loss 0.9956283131619392
Epoch 15: loss 0.9940283124442979
Epoch 16: loss 0.9922533359554814
Epoch 17: loss 0.9901130760960634
Epoch 18: loss 0.9874962086486972
Epoch 19: loss 0.9842625439912486
Epoch 20: loss 0.9810068475253689
Epoch 21: loss 0.9772696742841639
Epoch 22: loss 0.9732876750617782
Epoch 23: loss 0.9690165100958504
Epoch 24: loss 0.9642075523693578
Epoch 25: loss 0.9593351780720026
Epoch 26: loss 0.9537843515880734
Epoch 27: loss 0.9478022749006522
Epoch 28: loss 0.941089592156947
Epoch 29: loss 0.9337119431583826
Epoch 30: loss 0.9254796553915645
Epoch 31: loss 0.9160758988438513
Epoch 32: loss 0.9055911404014346
Epoch 33: loss 0.893947550837536
Epoch 34: loss 0.880957686395474
Epoch 35: loss 0.866565489409215
Epoch 36: loss 0.8506162876406074
Epoch 37: loss 0.8329362231891657
Epoch 38: loss 0.813814394512275
Epoch 39: loss 0.7927753536710278
Epoch 40: loss 0.7700926063986432
Epoch 41: loss 0.7457953633628117
Epoch 42: loss 0.7199224925206497
Epoch 43: loss 0.6927967243313854
Epoch 44: loss 0.6643938409967745
Epoch 45: loss 0.6351369439891138
Epoch 46: loss 0.6050651060491233
Epoch 47: loss 0.5748013923144198
Epoch 48: loss 0.544201927625206
Epoch 49: loss 0.5139812945898231
-----------Time: 0:13:46.786480, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.99834144115448-------------


Epoch 0: loss 5.265016593901991
Epoch 1: loss 1.0298544401186456
Epoch 2: loss 1.0229263091910592
Epoch 3: loss 1.0140999104744588
Epoch 4: loss 1.0007424477878766
Epoch 5: loss 0.9813264513579706
Epoch 6: loss 0.9580797114697664
Epoch 7: loss 0.9306054654945946
Epoch 8: loss 0.9006690856650706
Epoch 9: loss 0.871149085571482
Epoch 10: loss 0.8450376298389984
Epoch 11: loss 0.8232110845387922
Epoch 12: loss 0.8055204718802661
Epoch 13: loss 0.7913147723985924
Epoch 14: loss 0.7799312918000795
Epoch 15: loss 0.7705905252238082
Epoch 16: loss 0.7628933665154743
Epoch 17: loss 0.7564392029396407
Epoch 18: loss 0.7508313200958661
Epoch 19: loss 0.7459586274857233
Epoch 20: loss 0.7416369500304124
Epoch 21: loss 0.7378274287588898
Epoch 22: loss 0.7343100959707564
Epoch 23: loss 0.7309635881498626
Epoch 24: loss 0.7282759998076762
Epoch 25: loss 0.7254131534473607
Epoch 26: loss 0.722910939204816
Epoch 27: loss 0.7204908361702007
Epoch 28: loss 0.718471597602602
Epoch 29: loss 0.716433690942909
Epoch 30: loss 0.7145269661898714
Epoch 31: loss 0.7126807514790804
Epoch 32: loss 0.7109843943189058
Epoch 33: loss 0.7093065450572656
Epoch 34: loss 0.7076670957152904
Epoch 35: loss 0.7062159676293176
Epoch 36: loss 0.7046285760085047
Epoch 37: loss 0.703144328807475
Epoch 38: loss 0.701980604716719
Epoch 39: loss 0.7007440762344166
Epoch 40: loss 0.6995281508884201
Epoch 41: loss 0.6981507239051977
Epoch 42: loss 0.6972367326126856
Epoch 43: loss 0.6959300562050629
Epoch 44: loss 0.6948610007066063
Epoch 45: loss 0.6938093521709737
Epoch 46: loss 0.6928537451277744
Epoch 47: loss 0.6916463811268555
Epoch 48: loss 0.6908952173508011
Epoch 49: loss 0.6899983330045854
-----------Time: 0:07:16.079404, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1349462270736694-------------


Epoch 0: loss 3.7250267123644476
Epoch 1: loss 1.0439489155829504
Epoch 2: loss 1.0146758638204862
Epoch 3: loss 0.9849173712983217
Epoch 4: loss 0.9506211440449114
Epoch 5: loss 0.9035787086664431
Epoch 6: loss 0.8438282017477056
Epoch 7: loss 0.7762923333520407
Epoch 8: loss 0.7109663120474486
Epoch 9: loss 0.6533361103480896
Epoch 10: loss 0.6080492595863964
Epoch 11: loss 0.5740570185413304
Epoch 12: loss 0.5482616080188959
Epoch 13: loss 0.5284227741241067
Epoch 14: loss 0.5122537687218274
Epoch 15: loss 0.499406201844957
Epoch 16: loss 0.4883855335928941
Epoch 17: loss 0.47886030221192843
Epoch 18: loss 0.4706491887034639
Epoch 19: loss 0.4633238838297832
Epoch 20: loss 0.4569886554723333
Epoch 21: loss 0.4510313219969658
Epoch 22: loss 0.44595159948390745
Epoch 23: loss 0.44110726134455547
Epoch 24: loss 0.4365937439577939
Epoch 25: loss 0.43258828284495415
Epoch 26: loss 0.42904371935560237
Epoch 27: loss 0.4252566454801435
Epoch 28: loss 0.4221645339294385
Epoch 29: loss 0.41898093915880213
Epoch 30: loss 0.416472985627471
Epoch 31: loss 0.4137017028431584
Epoch 32: loss 0.4109266853408686
Epoch 33: loss 0.40866997870836913
Epoch 34: loss 0.4062073408763262
Epoch 35: loss 0.4041781600578898
Epoch 36: loss 0.40215372939175276
Epoch 37: loss 0.40003117790943
Epoch 38: loss 0.398117467396411
Epoch 39: loss 0.3964377489668186
Epoch 40: loss 0.3948247061557197
Epoch 41: loss 0.3931810810489457
Epoch 42: loss 0.3915206064156633
Epoch 43: loss 0.3897494566568607
Epoch 44: loss 0.3884288461482868
Epoch 45: loss 0.3870733831606526
Epoch 46: loss 0.38568625777408827
Epoch 47: loss 0.38439862412269377
Epoch 48: loss 0.3831243229834459
Epoch 49: loss 0.38188246921993196
-----------Time: 0:09:36.697232, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4209586381912231-------------


Epoch 0: loss 2.9619830599973356
Epoch 1: loss 1.0633136422787106
Epoch 2: loss 1.0187326322566432
Epoch 3: loss 0.958779724996462
Epoch 4: loss 0.8731094650304337
Epoch 5: loss 0.7606798981019114
Epoch 6: loss 0.6345139241140779
Epoch 7: loss 0.5191613214131994
Epoch 8: loss 0.43027313010662777
Epoch 9: loss 0.36690535850908007
Epoch 10: loss 0.3226047878302006
Epoch 11: loss 0.28955039150239076
Epoch 12: loss 0.26498087116351526
Epoch 13: loss 0.24584160077468153
Epoch 14: loss 0.2302322962978179
Epoch 15: loss 0.2174058052052325
Epoch 16: loss 0.20670464402140645
Epoch 17: loss 0.19743188415097437
Epoch 18: loss 0.18949955281694666
Epoch 19: loss 0.1825905697842601
Epoch 20: loss 0.1764594225730502
Epoch 21: loss 0.17086950733714806
Epoch 22: loss 0.16618045176166177
Epoch 23: loss 0.16183446788914246
Epoch 24: loss 0.15760535315335478
Epoch 25: loss 0.1541051952762893
Epoch 26: loss 0.15071620684179876
Epoch 27: loss 0.14766152488506312
Epoch 28: loss 0.14469159850491137
Epoch 29: loss 0.14209398803720985
Epoch 30: loss 0.13955574827050957
Epoch 31: loss 0.13734308514053276
Epoch 32: loss 0.1349558367851216
Epoch 33: loss 0.1331190752243302
Epoch 34: loss 0.13105276887988254
Epoch 35: loss 0.12933194155786076
Epoch 36: loss 0.12757670007901323
Epoch 37: loss 0.12588572064176598
Epoch 38: loss 0.12434177193638746
Epoch 39: loss 0.1230301626954408
Epoch 40: loss 0.12150284157921076
Epoch 41: loss 0.12021179486606969
Epoch 42: loss 0.11886623320172279
Epoch 43: loss 0.11777416777520026
Epoch 44: loss 0.11648530993649109
Epoch 45: loss 0.1153242555591255
Epoch 46: loss 0.11424810018708455
Epoch 47: loss 0.11333633624341354
Epoch 48: loss 0.11220523585677308
Epoch 49: loss 0.11128938581051064
-----------Time: 0:09:12.669963, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.6234368085861206-------------


Epoch 0: loss 2.676726917636466
Epoch 1: loss 1.0720457214094363
Epoch 2: loss 1.0045628272495302
Epoch 3: loss 0.9068561875184895
Epoch 4: loss 0.7614460873370459
Epoch 5: loss 0.5876734208613391
Epoch 6: loss 0.4296085028349543
Epoch 7: loss 0.31562830787709123
Epoch 8: loss 0.2417446827073341
Epoch 9: loss 0.19432227532093196
Epoch 10: loss 0.16253291688506988
Epoch 11: loss 0.14044923735797696
Epoch 12: loss 0.12386285348138516
Epoch 13: loss 0.11143536817701762
Epoch 14: loss 0.10168629056810036
Epoch 15: loss 0.09367708956124894
Epoch 16: loss 0.08735676832262138
Epoch 17: loss 0.08187039078290759
Epoch 18: loss 0.07747618297278039
Epoch 19: loss 0.0734818815868986
Epoch 20: loss 0.07033863377743697
Epoch 21: loss 0.06716320622872406
Epoch 22: loss 0.06460343568386787
Epoch 23: loss 0.06235706554386459
Epoch 24: loss 0.060318817008264435
Epoch 25: loss 0.05856782497555552
Epoch 26: loss 0.05674819728079018
Epoch 27: loss 0.055284809103400695
Epoch 28: loss 0.053911982933712625
Epoch 29: loss 0.05259768852114256
Epoch 30: loss 0.051359378134648456
Epoch 31: loss 0.0503144928706428
Epoch 32: loss 0.04933404026511508
Epoch 33: loss 0.04830278889106434
Epoch 34: loss 0.047536966004502586
Epoch 35: loss 0.04668043921395221
Epoch 36: loss 0.04572481645272195
Epoch 37: loss 0.04507737546245597
Epoch 38: loss 0.04454714911035249
Epoch 39: loss 0.04375683863370266
Epoch 40: loss 0.04309583612519675
Epoch 41: loss 0.04257155696204608
Epoch 42: loss 0.04201943612829167
Epoch 43: loss 0.041524629428164345
Epoch 44: loss 0.04091676997345968
Epoch 45: loss 0.04043707046293172
Epoch 46: loss 0.040026291515100836
Epoch 47: loss 0.039587789117406735
Epoch 48: loss 0.039153782223908876
Epoch 49: loss 0.03872909623108573
-----------Time: 0:12:45.987198, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4557958841323853-------------


Epoch 0: loss 2.5557743306228686
Epoch 1: loss 1.079286290453205
Epoch 2: loss 0.9873815495206845
Epoch 3: loss 0.8279384548667983
Epoch 4: loss 0.6195490999899849
Epoch 5: loss 0.42055503409831924
Epoch 6: loss 0.2766728201570104
Epoch 7: loss 0.1892970997967786
Epoch 8: loss 0.13953284631205226
Epoch 9: loss 0.11038990421175082
Epoch 10: loss 0.09205184899100374
Epoch 11: loss 0.07986656424263822
Epoch 12: loss 0.07162783532329978
Epoch 13: loss 0.06577961147641799
Epoch 14: loss 0.061409455376143944
Epoch 15: loss 0.05794293314518327
Epoch 16: loss 0.05531477152616547
Epoch 17: loss 0.05304795883944367
Epoch 18: loss 0.051250742112305125
Epoch 19: loss 0.049676530333585295
Epoch 20: loss 0.04840136013313309
Epoch 21: loss 0.04707809382378083
Epoch 22: loss 0.046156346116289944
Epoch 23: loss 0.04519298899961335
Epoch 24: loss 0.04417381847663182
Epoch 25: loss 0.04347673469223588
Epoch 26: loss 0.04291888403535586
Epoch 27: loss 0.04203849048224592
Epoch 28: loss 0.04144367246213508
Epoch 29: loss 0.04088227251314676
Epoch 30: loss 0.040304208047234245
Epoch 31: loss 0.039801369513354105
Epoch 32: loss 0.03950110372498251
Epoch 33: loss 0.03886732748846861
Epoch 34: loss 0.03855399135601478
Epoch 35: loss 0.03810719517730642
Epoch 36: loss 0.037780490438005666
Epoch 37: loss 0.03742419826216254
Epoch 38: loss 0.037054091771986576
Epoch 39: loss 0.036746396918139454
Epoch 40: loss 0.03650742979206716
Epoch 41: loss 0.036120041121605655
Epoch 42: loss 0.035933762715340406
Epoch 43: loss 0.03550433581051796
Epoch 44: loss 0.03546309067253398
Epoch 45: loss 0.03509240995843395
Epoch 46: loss 0.03482764022847179
Epoch 47: loss 0.034763295480970326
Epoch 48: loss 0.03448407238328049
Epoch 49: loss 0.03426651329322718
-----------Time: 0:11:48.239850, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2532044649124146-------------


Epoch 0: loss 16.12934397613957
Epoch 1: loss 16.129342836034628
Epoch 2: loss 16.129341772679986
Epoch 3: loss 16.129346055917082
Epoch 4: loss 16.12933991822683
Epoch 5: loss 16.129345725061064
Epoch 6: loss 16.12933579108325
Epoch 7: loss 16.129337439918206
Epoch 8: loss 16.129337893159835
Epoch 9: loss 16.129332032133664
Epoch 10: loss 16.129326559266946
Epoch 11: loss 16.129329514412728
Epoch 12: loss 16.12931986824853
Epoch 13: loss 16.129324712592535
Epoch 14: loss 16.129324466524857
Epoch 15: loss 16.12931874836833
Epoch 16: loss 16.129325787615297
Epoch 17: loss 16.12931047281926
Epoch 18: loss 16.129319069371267
Epoch 19: loss 16.129311324073413
Epoch 20: loss 16.129310324504495
Epoch 21: loss 16.12930456823211
Epoch 22: loss 16.12931014170395
Epoch 23: loss 16.129304918794283
Epoch 24: loss 16.12930194264589
Epoch 25: loss 16.12930168854017
Epoch 26: loss 16.12930042812395
Epoch 27: loss 16.129299082400813
Epoch 28: loss 16.12929998758761
Epoch 29: loss 16.129293847823025
Epoch 30: loss 16.129297087411636
Epoch 31: loss 16.129286634072848
Epoch 32: loss 16.129283184976664
Epoch 33: loss 16.12928574418425
Epoch 34: loss 16.129280075811703
Epoch 35: loss 16.12928261686888
Epoch 36: loss 16.12927831548138
Epoch 37: loss 16.129276038382997
Epoch 38: loss 16.12927831236988
Epoch 39: loss 16.12927864659669
Epoch 40: loss 16.129269478306814
Epoch 41: loss 16.12927193664999
Epoch 42: loss 16.129273315821738
Epoch 43: loss 16.12927147018449
Epoch 44: loss 16.12926675885705
Epoch 45: loss 16.129267993344115
Epoch 46: loss 16.129263125145286
Epoch 47: loss 16.129257260489037
Epoch 48: loss 16.129255298689177
Epoch 49: loss 16.12925628036698
-----------Time: 0:09:33.297441, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017179012298584-------------


Epoch 0: loss 16.12922734213368
Epoch 1: loss 16.12923068025308
Epoch 2: loss 16.129220237285953
Epoch 3: loss 16.129216246789017
Epoch 4: loss 16.129222853019098
Epoch 5: loss 16.129215481619656
Epoch 6: loss 16.129217497611446
Epoch 7: loss 16.12921137055215
Epoch 8: loss 16.129211052920002
Epoch 9: loss 16.129211483603264
Epoch 10: loss 16.1292077793642
Epoch 11: loss 16.129207127245955
Epoch 12: loss 16.129201834586862
Epoch 13: loss 16.129201333117006
Epoch 14: loss 16.12920126025608
Epoch 15: loss 16.12919756172143
Epoch 16: loss 16.129194751519623
Epoch 17: loss 16.129190205101608
Epoch 18: loss 16.129195569325166
Epoch 19: loss 16.129189451600364
Epoch 20: loss 16.129187263179695
Epoch 21: loss 16.129184525838813
Epoch 22: loss 16.129180311573272
Epoch 23: loss 16.129181661704365
Epoch 24: loss 16.129179019523484
Epoch 25: loss 16.129178528943875
Epoch 26: loss 16.129172273275966
Epoch 27: loss 16.129176040263587
Epoch 28: loss 16.12917630033301
Epoch 29: loss 16.129173477166628
Epoch 30: loss 16.129165513545292
Epoch 31: loss 16.129160260817095
Epoch 32: loss 16.1291611473349
Epoch 33: loss 16.129158350097672
Epoch 34: loss 16.129163316567997
Epoch 35: loss 16.129161256237353
Epoch 36: loss 16.1291609417167
Epoch 37: loss 16.129155532635703
Epoch 38: loss 16.1291527315091
Epoch 39: loss 16.129153874725542
Epoch 40: loss 16.129147985955175
Epoch 41: loss 16.129147474891536
Epoch 42: loss 16.1291451104119
Epoch 43: loss 16.12914584524415
Epoch 44: loss 16.12913803330837
Epoch 45: loss 16.129141274452728
Epoch 46: loss 16.129138836852878
Epoch 47: loss 16.12913176908384
Epoch 48: loss 16.129133463813403
Epoch 49: loss 16.12913255421865
-----------Time: 0:07:42.845599, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.129172162299184
Epoch 1: loss 16.12916910162174
Epoch 2: loss 16.129167596434304
Epoch 3: loss 16.129166225300594
Epoch 4: loss 16.129161905244096
Epoch 5: loss 16.129159817169256
Epoch 6: loss 16.129156923994156
Epoch 7: loss 16.129158184669663
Epoch 8: loss 16.12915272761973
Epoch 9: loss 16.129154012668646
Epoch 10: loss 16.129151520098986
Epoch 11: loss 16.129146662271822
Epoch 12: loss 16.1291458589866
Epoch 13: loss 16.129144005052027
Epoch 14: loss 16.129145262097456
Epoch 15: loss 16.12913667954719
Epoch 16: loss 16.129141861488794
Epoch 17: loss 16.129133360874658
Epoch 18: loss 16.129138662090373
Epoch 19: loss 16.12913857574629
Epoch 20: loss 16.129128208233002
Epoch 21: loss 16.129131409965048
Epoch 22: loss 16.129126916961088
Epoch 23: loss 16.129122033723352
Epoch 24: loss 16.12912385835798
Epoch 25: loss 16.129124341418134
Epoch 26: loss 16.129120696816127
Epoch 27: loss 16.129118418161994
Epoch 28: loss 16.129113747543325
Epoch 29: loss 16.12911522498657
Epoch 30: loss 16.129110220141097
Epoch 31: loss 16.129103805786762
Epoch 32: loss 16.129108390061347
Epoch 33: loss 16.129107310112044
Epoch 34: loss 16.129107236732537
Epoch 35: loss 16.12910161684751
Epoch 36: loss 16.129105387205918
Epoch 37: loss 16.129102665941115
Epoch 38: loss 16.129096865070586
Epoch 39: loss 16.129087077333203
Epoch 40: loss 16.129094737324138
Epoch 41: loss 16.129096706124866
Epoch 42: loss 16.12908834967683
Epoch 43: loss 16.12908648381485
Epoch 44: loss 16.12908254647268
Epoch 45: loss 16.12908449841946
Epoch 46: loss 16.129085529621946
Epoch 47: loss 16.12907930558761
Epoch 48: loss 16.129077008005197
Epoch 49: loss 16.12908005338444
-----------Time: 0:11:36.016223, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017179489135742-------------


Epoch 0: loss 16.129177345537244
Epoch 1: loss 16.1291722232327
Epoch 2: loss 16.129171161433806
Epoch 3: loss 16.1291697840771
Epoch 4: loss 16.12917069289398
Epoch 5: loss 16.129168020375985
Epoch 6: loss 16.129168589520933
Epoch 7: loss 16.12916283350784
Epoch 8: loss 16.129165360563277
Epoch 9: loss 16.129164056326786
Epoch 10: loss 16.1291518939974
Epoch 11: loss 16.129156344736835
Epoch 12: loss 16.12915731163502
Epoch 13: loss 16.129152708691446
Epoch 14: loss 16.129148902810094
Epoch 15: loss 16.12914249519734
Epoch 16: loss 16.129146771692852
Epoch 17: loss 16.129146110758697
Epoch 18: loss 16.12914500954749
Epoch 19: loss 16.12914088292249
Epoch 20: loss 16.129136088621753
Epoch 21: loss 16.129141024236382
Epoch 22: loss 16.12913100417377
Epoch 23: loss 16.129130711952197
Epoch 24: loss 16.12913266778835
Epoch 25: loss 16.12912880226994
Epoch 26: loss 16.129123990337376
Epoch 27: loss 16.129120309693842
Epoch 28: loss 16.129122891200502
Epoch 29: loss 16.129129779280497
Epoch 30: loss 16.12912307477892
Epoch 31: loss 16.129111850307066
Epoch 32: loss 16.12911832818783
Epoch 33: loss 16.12911548583387
Epoch 34: loss 16.12911415151956
Epoch 35: loss 16.129106307172336
Epoch 36: loss 16.129110176580117
Epoch 37: loss 16.129104408639613
Epoch 38: loss 16.12910319774808
Epoch 39: loss 16.129108174071487
Epoch 40: loss 16.129097903014657
Epoch 41: loss 16.129101673113773
Epoch 42: loss 16.129094986244024
Epoch 43: loss 16.129097849341306
Epoch 44: loss 16.129087372666277
Epoch 45: loss 16.129087064627917
Epoch 46: loss 16.129090292029826
Epoch 47: loss 16.12908685589822
Epoch 48: loss 16.129082686230824
Epoch 49: loss 16.1290858195099
-----------Time: 0:10:30.793339, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017174243927002-------------


Epoch 0: loss 16.129173372931426
Epoch 1: loss 16.129172356508555
Epoch 2: loss 16.129172137925778
Epoch 3: loss 16.12916341587666
Epoch 4: loss 16.129166163329913
Epoch 5: loss 16.12915460281621
Epoch 6: loss 16.129158287349117
Epoch 7: loss 16.129164236793706
Epoch 8: loss 16.12915012822195
Epoch 9: loss 16.129154378010437
Epoch 10: loss 16.129153871873335
Epoch 11: loss 16.129153283281518
Epoch 12: loss 16.129149868930405
Epoch 13: loss 16.129157804548253
Epoch 14: loss 16.12914574489832
Epoch 15: loss 16.12914094644892
Epoch 16: loss 16.129138119652453
Epoch 17: loss 16.129133725179287
Epoch 18: loss 16.129127943755623
Epoch 19: loss 16.129135407203563
Epoch 20: loss 16.12913322345014
Epoch 21: loss 16.129132747390855
Epoch 22: loss 16.12913157954172
Epoch 23: loss 16.129127336235523
Epoch 24: loss 16.129129067525195
Epoch 25: loss 16.129125307797736
Epoch 26: loss 16.129125161038722
Epoch 27: loss 16.129118849623133
Epoch 28: loss 16.129119970021915
Epoch 29: loss 16.129115963967486
Epoch 30: loss 16.129112872952934
Epoch 31: loss 16.12911321936644
Epoch 32: loss 16.12911251564918
Epoch 33: loss 16.12911890640798
Epoch 34: loss 16.129103788673522
Epoch 35: loss 16.129106015987926
Epoch 36: loss 16.129108719361614
Epoch 37: loss 16.129097111656847
Epoch 38: loss 16.1290972757884
Epoch 39: loss 16.129096748907973
Epoch 40: loss 16.12910309558721
Epoch 41: loss 16.12909133256681
Epoch 42: loss 16.129094500590952
Epoch 43: loss 16.129090920293248
Epoch 44: loss 16.129087206201103
Epoch 45: loss 16.129092614504227
Epoch 46: loss 16.129081438001307
Epoch 47: loss 16.129087347255705
Epoch 48: loss 16.129081276462674
Epoch 49: loss 16.129067681288188
-----------Time: 0:13:52.481751, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017175674438477-------------


Epoch 0: loss 16.12940349832963
Epoch 1: loss 16.12937853970302
Epoch 2: loss 16.129357329135747
Epoch 3: loss 16.129336976564208
Epoch 4: loss 16.129320202216043
Epoch 5: loss 16.129303343598124
Epoch 6: loss 16.129278958005838
Epoch 7: loss 16.129262262741594
Epoch 8: loss 16.129246630831993
Epoch 9: loss 16.1292202173205
Epoch 10: loss 16.129194967768775
Epoch 11: loss 16.129184450644264
Epoch 12: loss 16.12916609383978
Epoch 13: loss 16.12914540678214
Epoch 14: loss 16.12912999267744
Epoch 15: loss 16.1291093548852
Epoch 16: loss 16.12908379418361
Epoch 17: loss 16.129073520793156
Epoch 18: loss 16.129044595783725
Epoch 19: loss 16.12903100942515
Epoch 20: loss 16.129010724269417
Epoch 21: loss 16.12898512493339
Epoch 22: loss 16.128967031569115
Epoch 23: loss 16.128946097925215
Epoch 24: loss 16.12893252582768
Epoch 25: loss 16.12891714206009
Epoch 26: loss 16.12888975750175
Epoch 27: loss 16.12887505904172
Epoch 28: loss 16.12885435201864
Epoch 29: loss 16.12883391284372
Epoch 30: loss 16.12882304204554
Epoch 31: loss 16.128791985659575
Epoch 32: loss 16.128777947355832
Epoch 33: loss 16.128759154422962
Epoch 34: loss 16.128737889404462
Epoch 35: loss 16.128724097686984
Epoch 36: loss 16.128700991439196
Epoch 37: loss 16.128678470412435
Epoch 38: loss 16.128655967795375
Epoch 39: loss 16.128643823357105
Epoch 40: loss 16.12862928410209
Epoch 41: loss 16.128603410176314
Epoch 42: loss 16.12858359252324
Epoch 43: loss 16.12856349587246
Epoch 44: loss 16.128556955502432
Epoch 45: loss 16.128524746046953
Epoch 46: loss 16.12851036781199
Epoch 47: loss 16.12849072155063
Epoch 48: loss 16.128471495600873
Epoch 49: loss 16.128449527124403
-----------Time: 0:07:03.532461, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017086982727051-------------


Epoch 0: loss 16.129205826380254
Epoch 1: loss 16.129190385568524
Epoch 2: loss 16.12917354717535
Epoch 3: loss 16.129154197024935
Epoch 4: loss 16.129133408065723
Epoch 5: loss 16.12911517468401
Epoch 6: loss 16.12909077716431
Epoch 7: loss 16.129069077054595
Epoch 8: loss 16.12905292578402
Epoch 9: loss 16.12902674667209
Epoch 10: loss 16.129008817698658
Epoch 11: loss 16.128998105846197
Epoch 12: loss 16.128980551808343
Epoch 13: loss 16.128958601741573
Epoch 14: loss 16.128936444500855
Epoch 15: loss 16.128920879747763
Epoch 16: loss 16.128898784218435
Epoch 17: loss 16.12888304392497
Epoch 18: loss 16.128860861792262
Epoch 19: loss 16.128842071970887
Epoch 20: loss 16.128819705222597
Epoch 21: loss 16.12880209336273
Epoch 22: loss 16.128780201377268
Epoch 23: loss 16.128758739815776
Epoch 24: loss 16.128746462879523
Epoch 25: loss 16.12873091938834
Epoch 26: loss 16.12870888764473
Epoch 27: loss 16.128683618127553
Epoch 28: loss 16.128670209642983
Epoch 29: loss 16.128648001840414
Epoch 30: loss 16.128624906482873
Epoch 31: loss 16.128609661695553
Epoch 32: loss 16.1285934085234
Epoch 33: loss 16.128569898817965
Epoch 34: loss 16.12854249170126
Epoch 35: loss 16.128530452535358
Epoch 36: loss 16.128510861243807
Epoch 37: loss 16.128487807891496
Epoch 38: loss 16.12847847313645
Epoch 39: loss 16.128452388147352
Epoch 40: loss 16.128436306626206
Epoch 41: loss 16.12841853970999
Epoch 42: loss 16.12839741833901
Epoch 43: loss 16.128374674321517
Epoch 44: loss 16.128355064620266
Epoch 45: loss 16.12833717998569
Epoch 46: loss 16.128315997940486
Epoch 47: loss 16.128304458688692
Epoch 48: loss 16.128280901014318
Epoch 49: loss 16.128258344205324
-----------Time: 0:10:02.841995, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017066955566406-------------


Epoch 0: loss 16.129168561258155
Epoch 1: loss 16.129154829177732
Epoch 2: loss 16.129125712551843
Epoch 3: loss 16.129113472434994
Epoch 4: loss 16.12909054094971
Epoch 5: loss 16.129068793648933
Epoch 6: loss 16.129056160446094
Epoch 7: loss 16.129035222134945
Epoch 8: loss 16.129007654257478
Epoch 9: loss 16.128992035053162
Epoch 10: loss 16.12897720954028
Epoch 11: loss 16.12896618679725
Epoch 12: loss 16.128932987625927
Epoch 13: loss 16.12891253989439
Epoch 14: loss 16.12889817229038
Epoch 15: loss 16.12887697909564
Epoch 16: loss 16.128865945462366
Epoch 17: loss 16.128843663761707
Epoch 18: loss 16.128822415078574
Epoch 19: loss 16.128807605123182
Epoch 20: loss 16.128786279949043
Epoch 21: loss 16.128763899717594
Epoch 22: loss 16.1287409656394
Epoch 23: loss 16.128723770201756
Epoch 24: loss 16.128707810029056
Epoch 25: loss 16.128680109135026
Epoch 26: loss 16.128670353290502
Epoch 27: loss 16.128647777812517
Epoch 28: loss 16.128624168279835
Epoch 29: loss 16.12861688192802
Epoch 30: loss 16.128590753637233
Epoch 31: loss 16.12857178646044
Epoch 32: loss 16.128545769146438
Epoch 33: loss 16.12853484052624
Epoch 34: loss 16.128509471700397
Epoch 35: loss 16.128494726827185
Epoch 36: loss 16.128468070618805
Epoch 37: loss 16.12844782383822
Epoch 38: loss 16.12843545537205
Epoch 39: loss 16.128407852230932
Epoch 40: loss 16.128396312201264
Epoch 41: loss 16.128379082537137
Epoch 42: loss 16.128356815616094
Epoch 43: loss 16.1283414780024
Epoch 44: loss 16.128315550662563
Epoch 45: loss 16.128298024887492
Epoch 46: loss 16.128285149247056
Epoch 47: loss 16.128264486562824
Epoch 48: loss 16.1282478981267
Epoch 49: loss 16.128219617716056
-----------Time: 0:09:15.090924, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0170698165893555-------------


Epoch 0: loss 16.129171445617345
Epoch 1: loss 16.129142558983062
Epoch 2: loss 16.129131530794908
Epoch 3: loss 16.12911100086795
Epoch 4: loss 16.129089041985267
Epoch 5: loss 16.129070357436262
Epoch 6: loss 16.129046961559816
Epoch 7: loss 16.12902743120178
Epoch 8: loss 16.12901207803059
Epoch 9: loss 16.12898930004591
Epoch 10: loss 16.12896757530349
Epoch 11: loss 16.128953510552012
Epoch 12: loss 16.128932952102986
Epoch 13: loss 16.128917298931476
Epoch 14: loss 16.12888825775943
Epoch 15: loss 16.128879182555224
Epoch 16: loss 16.12885983058977
Epoch 17: loss 16.128839405157308
Epoch 18: loss 16.12881379544962
Epoch 19: loss 16.128799171406268
Epoch 20: loss 16.128781600514465
Epoch 21: loss 16.1287689864992
Epoch 22: loss 16.128744608685658
Epoch 23: loss 16.128721479101632
Epoch 24: loss 16.12870705756498
Epoch 25: loss 16.128677022009846
Epoch 26: loss 16.12866607057199
Epoch 27: loss 16.12863939128666
Epoch 28: loss 16.128625246414092
Epoch 29: loss 16.128606244492232
Epoch 30: loss 16.128582043256237
Epoch 31: loss 16.12856438965043
Epoch 32: loss 16.128543895764995
Epoch 33: loss 16.128528928678925
Epoch 34: loss 16.128508752944224
Epoch 35: loss 16.128485011691435
Epoch 36: loss 16.128471870795742
Epoch 37: loss 16.128451766106924
Epoch 38: loss 16.128427071439113
Epoch 39: loss 16.12841504886787
Epoch 40: loss 16.128390502774113
Epoch 41: loss 16.128365659791534
Epoch 42: loss 16.128350730043447
Epoch 43: loss 16.128332163731386
Epoch 44: loss 16.12832155170617
Epoch 45: loss 16.128296545629205
Epoch 46: loss 16.128279040338164
Epoch 47: loss 16.128261783448426
Epoch 48: loss 16.12824015386601
Epoch 49: loss 16.128225942096222
-----------Time: 0:12:47.888862, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.0170698165893555-------------


Epoch 0: loss 16.12917113809757
Epoch 1: loss 16.129150492526577
Epoch 2: loss 16.129128054732405
Epoch 3: loss 16.129115016516167
Epoch 4: loss 16.129097206557557
Epoch 5: loss 16.129076441712456
Epoch 6: loss 16.129054888621045
Epoch 7: loss 16.129036258263973
Epoch 8: loss 16.12901308356322
Epoch 9: loss 16.128994882852243
Epoch 10: loss 16.12897987142732
Epoch 11: loss 16.128952806056876
Epoch 12: loss 16.128938631106486
Epoch 13: loss 16.1289135949517
Epoch 14: loss 16.128895967015758
Epoch 15: loss 16.12887477226527
Epoch 16: loss 16.128856457725306
Epoch 17: loss 16.12883979227959
Epoch 18: loss 16.128819852500197
Epoch 19: loss 16.1288043769434
Epoch 20: loss 16.128791123255883
Epoch 21: loss 16.128764536278346
Epoch 22: loss 16.128745473682265
Epoch 23: loss 16.12872215170391
Epoch 24: loss 16.128709343219985
Epoch 25: loss 16.128684331179315
Epoch 26: loss 16.1286716940871
Epoch 27: loss 16.128654263990608
Epoch 28: loss 16.128637345476342
Epoch 29: loss 16.12860401536279
Epoch 30: loss 16.12858440929162
Epoch 31: loss 16.128573261829352
Epoch 32: loss 16.128546442267297
Epoch 33: loss 16.12853761287148
Epoch 34: loss 16.128512850269278
Epoch 35: loss 16.12849984731669
Epoch 36: loss 16.12846978401736
Epoch 37: loss 16.128453444760414
Epoch 38: loss 16.128437723395226
Epoch 39: loss 16.128412162175056
Epoch 40: loss 16.12839777797639
Epoch 41: loss 16.12838122739683
Epoch 42: loss 16.128361777937755
Epoch 43: loss 16.12833924083492
Epoch 44: loss 16.12832140287282
Epoch 45: loss 16.128302266119356
Epoch 46: loss 16.128274427800804
Epoch 47: loss 16.128258725882485
Epoch 48: loss 16.128242117999495
Epoch 49: loss 16.128220576835496
-----------Time: 0:11:52.078446, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017069339752197-------------


Epoch 0: loss 16.129415760745555
Epoch 1: loss 16.129221471254432
Epoch 2: loss 16.129035725419843
Epoch 3: loss 16.128840050533974
Epoch 4: loss 16.128648590691522
Epoch 5: loss 16.128445934380704
Epoch 6: loss 16.128257299260383
Epoch 7: loss 16.12806442420466
Epoch 8: loss 16.12786987594057
Epoch 9: loss 16.127681725436155
Epoch 10: loss 16.12748333447131
Epoch 11: loss 16.127292565381545
Epoch 12: loss 16.12709953267656
Epoch 13: loss 16.126902311894476
Epoch 14: loss 16.126707094139608
Epoch 15: loss 16.126517036805144
Epoch 16: loss 16.126323700729046
Epoch 17: loss 16.12613429966149
Epoch 18: loss 16.125936855110805
Epoch 19: loss 16.12574448411785
Epoch 20: loss 16.12555453335221
Epoch 21: loss 16.12535863340128
Epoch 22: loss 16.125168613664087
Epoch 23: loss 16.12497155442064
Epoch 24: loss 16.124777925604384
Epoch 25: loss 16.124578933861024
Epoch 26: loss 16.124397595204886
Epoch 27: loss 16.124193274501614
Epoch 28: loss 16.124003942146324
Epoch 29: loss 16.12380775256673
Epoch 30: loss 16.123615310268598
Epoch 31: loss 16.12342846296351
Epoch 32: loss 16.123233421786185
Epoch 33: loss 16.123036544824696
Epoch 34: loss 16.12284043147682
Epoch 35: loss 16.122650107331353
Epoch 36: loss 16.122459283012486
Epoch 37: loss 16.122266390584226
Epoch 38: loss 16.122072931863226
Epoch 39: loss 16.121884565887534
Epoch 40: loss 16.12168178615394
Epoch 41: loss 16.121488823716962
Epoch 42: loss 16.121297504151237
Epoch 43: loss 16.121104412327778
Epoch 44: loss 16.12091039664853
Epoch 45: loss 16.120714630751326
Epoch 46: loss 16.120520634518947
Epoch 47: loss 16.120329952032556
Epoch 48: loss 16.120139405155648
Epoch 49: loss 16.119944398464103
-----------Time: 0:09:33.669042, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016017436981201-------------


Epoch 0: loss 16.12910604839937
Epoch 1: loss 16.12890552009429
Epoch 2: loss 16.128716254117634
Epoch 3: loss 16.12851966263614
Epoch 4: loss 16.128330311352563
Epoch 5: loss 16.12813351217854
Epoch 6: loss 16.127944071698668
Epoch 7: loss 16.127750361724157
Epoch 8: loss 16.127560341468385
Epoch 9: loss 16.127363742208143
Epoch 10: loss 16.12716456921999
Epoch 11: loss 16.126975619579024
Epoch 12: loss 16.126779965955063
Epoch 13: loss 16.12658833575806
Epoch 14: loss 16.12639196752659
Epoch 15: loss 16.12620229031354
Epoch 16: loss 16.126010980860183
Epoch 17: loss 16.125818992322266
Epoch 18: loss 16.125615600919904
Epoch 19: loss 16.1254253607849
Epoch 20: loss 16.125231335252572
Epoch 21: loss 16.12503816278944
Epoch 22: loss 16.12484721582567
Epoch 23: loss 16.124664021938504
Epoch 24: loss 16.12445845300572
Epoch 25: loss 16.124268335515612
Epoch 26: loss 16.12407235181351
Epoch 27: loss 16.123881192749256
Epoch 28: loss 16.123689564108005
Epoch 29: loss 16.12349673053893
Epoch 30: loss 16.123301116327283
Epoch 31: loss 16.123107067718006
Epoch 32: loss 16.122919694051074
Epoch 33: loss 16.12271182520225
Epoch 34: loss 16.12253024358993
Epoch 35: loss 16.122333959368923
Epoch 36: loss 16.122135721386094
Epoch 37: loss 16.121943321611777
Epoch 38: loss 16.121755430398913
Epoch 39: loss 16.121552648072402
Epoch 40: loss 16.121360362386365
Epoch 41: loss 16.121174669706544
Epoch 42: loss 16.120978514353435
Epoch 43: loss 16.1207863453486
Epoch 44: loss 16.120596195965632
Epoch 45: loss 16.120395604134124
Epoch 46: loss 16.1202038090277
Epoch 47: loss 16.120016132508237
Epoch 48: loss 16.11982587551928
Epoch 49: loss 16.119627484295147
-----------Time: 0:07:41.885774, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015995025634766-------------


Epoch 0: loss 16.12910786292163
Epoch 1: loss 16.128917141800798
Epoch 2: loss 16.128722246086035
Epoch 3: loss 16.128524886842264
Epoch 4: loss 16.1283344060847
Epoch 5: loss 16.128139230853648
Epoch 6: loss 16.127947258651094
Epoch 7: loss 16.127749983677077
Epoch 8: loss 16.127551818555176
Epoch 9: loss 16.12736487972017
Epoch 10: loss 16.127159007156976
Epoch 11: loss 16.12698073332695
Epoch 12: loss 16.126787490077223
Epoch 13: loss 16.12658588778655
Epoch 14: loss 16.126390342546454
Epoch 15: loss 16.126209360416198
Epoch 16: loss 16.126007662187654
Epoch 17: loss 16.12581749180207
Epoch 18: loss 16.125624807066345
Epoch 19: loss 16.12542922474756
Epoch 20: loss 16.125244099656936
Epoch 21: loss 16.125041180165194
Epoch 22: loss 16.12484323803402
Epoch 23: loss 16.12465522158334
Epoch 24: loss 16.124460975912488
Epoch 25: loss 16.1242708057862
Epoch 26: loss 16.124081558219242
Epoch 27: loss 16.123883475811343
Epoch 28: loss 16.123688856760662
Epoch 29: loss 16.123495849466245
Epoch 30: loss 16.123293437408066
Epoch 31: loss 16.123107763656524
Epoch 32: loss 16.122915425852888
Epoch 33: loss 16.122721581824646
Epoch 34: loss 16.122532238579108
Epoch 35: loss 16.12233193663555
Epoch 36: loss 16.12213962968761
Epoch 37: loss 16.12193991348366
Epoch 38: loss 16.121762244062232
Epoch 39: loss 16.121554027244148
Epoch 40: loss 16.121364361699214
Epoch 41: loss 16.121175226146207
Epoch 42: loss 16.120985499408466
Epoch 43: loss 16.120786756325703
Epoch 44: loss 16.120597020253467
Epoch 45: loss 16.120400934909075
Epoch 46: loss 16.120201843338467
Epoch 47: loss 16.120014518159053
Epoch 48: loss 16.11982268597394
Epoch 49: loss 16.119625558277523
-----------Time: 0:11:37.337966, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015993118286133-------------


Epoch 0: loss 16.12908988623855
Epoch 1: loss 16.128903067196237
Epoch 2: loss 16.128695694112857
Epoch 3: loss 16.12850316209985
Epoch 4: loss 16.128309168719674
Epoch 5: loss 16.128119952526994
Epoch 6: loss 16.127922268909497
Epoch 7: loss 16.127733457470924
Epoch 8: loss 16.127539977488016
Epoch 9: loss 16.12734439465065
Epoch 10: loss 16.12714671388536
Epoch 11: loss 16.126955923533686
Epoch 12: loss 16.12675955452434
Epoch 13: loss 16.12657360229368
Epoch 14: loss 16.1263821862715
Epoch 15: loss 16.12618530905072
Epoch 16: loss 16.125984269681997
Epoch 17: loss 16.125797662221586
Epoch 18: loss 16.125598843684983
Epoch 19: loss 16.125410274424716
Epoch 20: loss 16.125211803079495
Epoch 21: loss 16.125022042893146
Epoch 22: loss 16.124832043380696
Epoch 23: loss 16.124633653712312
Epoch 24: loss 16.124439291360265
Epoch 25: loss 16.124247126763382
Epoch 26: loss 16.1240605185251
Epoch 27: loss 16.1238603866768
Epoch 28: loss 16.12367401387524
Epoch 29: loss 16.12347243699514
Epoch 30: loss 16.123285153302373
Epoch 31: loss 16.12308694954603
Epoch 32: loss 16.12289002746781
Epoch 33: loss 16.1227049327143
Epoch 34: loss 16.122510791797232
Epoch 35: loss 16.122310610683538
Epoch 36: loss 16.12211442006678
Epoch 37: loss 16.121925118048598
Epoch 38: loss 16.121733554489527
Epoch 39: loss 16.121540758258433
Epoch 40: loss 16.121345272914684
Epoch 41: loss 16.121151354988353
Epoch 42: loss 16.12096074873368
Epoch 43: loss 16.120763686897316
Epoch 44: loss 16.120577247717122
Epoch 45: loss 16.120384241459874
Epoch 46: loss 16.12018268921247
Epoch 47: loss 16.119985084938186
Epoch 48: loss 16.11979936503275
Epoch 49: loss 16.119598313477326
-----------Time: 0:10:41.014786, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015991687774658-------------


Epoch 0: loss 16.12908504059809
Epoch 1: loss 16.128890997952517
Epoch 2: loss 16.128694629461755
Epoch 3: loss 16.12850069260714
Epoch 4: loss 16.12831394590717
Epoch 5: loss 16.12811705157315
Epoch 6: loss 16.1279272330462
Epoch 7: loss 16.127726084256444
Epoch 8: loss 16.12753478102609
Epoch 9: loss 16.12733941106708
Epoch 10: loss 16.127156193584383
Epoch 11: loss 16.126956285504686
Epoch 12: loss 16.12675518935112
Epoch 13: loss 16.126564025360324
Epoch 14: loss 16.126374970706276
Epoch 15: loss 16.12618034258039
Epoch 16: loss 16.125990177899226
Epoch 17: loss 16.12578942582554
Epoch 18: loss 16.12560127272821
Epoch 19: loss 16.125402059809158
Epoch 20: loss 16.125221527031158
Epoch 21: loss 16.125020264930995
Epoch 22: loss 16.124827723842785
Epoch 23: loss 16.124631420952785
Epoch 24: loss 16.12444240960043
Epoch 25: loss 16.124245898499314
Epoch 26: loss 16.12404923467548
Epoch 27: loss 16.12385205434288
Epoch 28: loss 16.123661789575177
Epoch 29: loss 16.123470012100576
Epoch 30: loss 16.123278322785104
Epoch 31: loss 16.123086491896448
Epoch 32: loss 16.12288778874458
Epoch 33: loss 16.122691045058946
Epoch 34: loss 16.12250217242757
Epoch 35: loss 16.122305274204173
Epoch 36: loss 16.12211501280726
Epoch 37: loss 16.121923511996744
Epoch 38: loss 16.121730073759775
Epoch 39: loss 16.121537024460128
Epoch 40: loss 16.1213385655609
Epoch 41: loss 16.121148560084748
Epoch 42: loss 16.120955127292902
Epoch 43: loss 16.120756601496453
Epoch 44: loss 16.120574194040554
Epoch 45: loss 16.120373516642832
Epoch 46: loss 16.12017818635543
Epoch 47: loss 16.11998620404051
Epoch 48: loss 16.119792041342954
Epoch 49: loss 16.11960761426518
-----------Time: 0:13:48.675944, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015990257263184-------------


Epoch 0: loss 16.12825963807015
Epoch 1: loss 16.12632506719551
Epoch 2: loss 16.12438193581038
Epoch 3: loss 16.122454820086347
Epoch 4: loss 16.120522226309763
Epoch 5: loss 16.118585258803336
Epoch 6: loss 16.1166504452318
Epoch 7: loss 16.114715281098093
Epoch 8: loss 16.112778786280156
Epoch 9: loss 16.110848204087407
Epoch 10: loss 16.10891363736143
Epoch 11: loss 16.10697767460975
Epoch 12: loss 16.105049085331846
Epoch 13: loss 16.103112184722637
Epoch 14: loss 16.101172754983658
Epoch 15: loss 16.099247695441605
Epoch 16: loss 16.09731325810211
Epoch 17: loss 16.09538159284856
Epoch 18: loss 16.093448399071335
Epoch 19: loss 16.091518980579053
Epoch 20: loss 16.089586459663394
Epoch 21: loss 16.087650315926215
Epoch 22: loss 16.085722742552598
Epoch 23: loss 16.083792022157454
Epoch 24: loss 16.081867802460728
Epoch 25: loss 16.079927874363396
Epoch 26: loss 16.077997686293802
Epoch 27: loss 16.076065805568977
Epoch 28: loss 16.074143483886388
Epoch 29: loss 16.072205036343796
Epoch 30: loss 16.070277595986745
Epoch 31: loss 16.068338460284384
Epoch 32: loss 16.066410778786192
Epoch 33: loss 16.064478799271285
Epoch 34: loss 16.06255388674754
Epoch 35: loss 16.06061880584642
Epoch 36: loss 16.05868417326039
Epoch 37: loss 16.056752182595947
Epoch 38: loss 16.054827625560915
Epoch 39: loss 16.05290002573956
Epoch 40: loss 16.050969933089256
Epoch 41: loss 16.04903137431059
Epoch 42: loss 16.047114343212765
Epoch 43: loss 16.045184427917878
Epoch 44: loss 16.043255450739817
Epoch 45: loss 16.041327631039227
Epoch 46: loss 16.039390924120806
Epoch 47: loss 16.03746298281248
Epoch 48: loss 16.03553888498278
Epoch 49: loss 16.033606761301776
-----------Time: 0:07:10.794533, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.00526237487793-------------


Epoch 0: loss 16.1282020761242
Epoch 1: loss 16.12627737362661
Epoch 2: loss 16.124335298854543
Epoch 3: loss 16.122407427036354
Epoch 4: loss 16.12047016756764
Epoch 5: loss 16.118535290469676
Epoch 6: loss 16.116597505416994
Epoch 7: loss 16.11466500524466
Epoch 8: loss 16.11273385753704
Epoch 9: loss 16.110799115270684
Epoch 10: loss 16.108865003601373
Epoch 11: loss 16.1069300531239
Epoch 12: loss 16.104997768422844
Epoch 13: loss 16.10306618795763
Epoch 14: loss 16.101139866962193
Epoch 15: loss 16.099201223395195
Epoch 16: loss 16.09727083463394
Epoch 17: loss 16.09533269875979
Epoch 18: loss 16.093401529530976
Epoch 19: loss 16.091473211731323
Epoch 20: loss 16.08954464967903
Epoch 21: loss 16.087603151312077
Epoch 22: loss 16.0856693515705
Epoch 23: loss 16.083752983481162
Epoch 24: loss 16.08181505789246
Epoch 25: loss 16.079875171800357
Epoch 26: loss 16.07794373238975
Epoch 27: loss 16.07601026272631
Epoch 28: loss 16.074083724703847
Epoch 29: loss 16.072159277867726
Epoch 30: loss 16.070222406817752
Epoch 31: loss 16.068294886598903
Epoch 32: loss 16.06636805505841
Epoch 33: loss 16.0644318815027
Epoch 34: loss 16.062502873468944
Epoch 35: loss 16.060579292407304
Epoch 36: loss 16.058639151431606
Epoch 37: loss 16.056710007010498
Epoch 38: loss 16.05478254772516
Epoch 39: loss 16.05284664798133
Epoch 40: loss 16.05092051030502
Epoch 41: loss 16.048991794492835
Epoch 42: loss 16.04705893597958
Epoch 43: loss 16.04513485733746
Epoch 44: loss 16.043193378676662
Epoch 45: loss 16.041274522944207
Epoch 46: loss 16.039345005143264
Epoch 47: loss 16.03741600177676
Epoch 48: loss 16.035489300141325
Epoch 49: loss 16.033564059354482
-----------Time: 0:09:51.531933, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.0052032470703125-------------


Epoch 0: loss 16.128215152715587
Epoch 1: loss 16.126277404741597
Epoch 2: loss 16.124342464635784
Epoch 3: loss 16.122410028767753
Epoch 4: loss 16.120473255729987
Epoch 5: loss 16.11854433170669
Epoch 6: loss 16.11661238512181
Epoch 7: loss 16.114674441382704
Epoch 8: loss 16.112740722280797
Epoch 9: loss 16.11080737163218
Epoch 10: loss 16.108872724007238
Epoch 11: loss 16.106940133342153
Epoch 12: loss 16.105008716489905
Epoch 13: loss 16.103071372232858
Epoch 14: loss 16.101127448452758
Epoch 15: loss 16.099201456757587
Epoch 16: loss 16.097262690804975
Epoch 17: loss 16.095329608523112
Epoch 18: loss 16.093407708189293
Epoch 19: loss 16.091463984582266
Epoch 20: loss 16.08954105382383
Epoch 21: loss 16.08760701060749
Epoch 22: loss 16.085678525305493
Epoch 23: loss 16.083740121064594
Epoch 24: loss 16.08180835805813
Epoch 25: loss 16.079872469463837
Epoch 26: loss 16.077943604559014
Epoch 27: loss 16.07601644994125
Epoch 28: loss 16.074077772925637
Epoch 29: loss 16.07214904440817
Epoch 30: loss 16.0702174136404
Epoch 31: loss 16.068285975526248
Epoch 32: loss 16.06635819290435
Epoch 33: loss 16.064424891002545
Epoch 34: loss 16.062491617882102
Epoch 35: loss 16.06056568737974
Epoch 36: loss 16.05863257242714
Epoch 37: loss 16.05670798220279
Epoch 38: loss 16.054765041396955
Epoch 39: loss 16.052842419454752
Epoch 40: loss 16.050908548408
Epoch 41: loss 16.04897964720236
Epoch 42: loss 16.047050563974054
Epoch 43: loss 16.045121921022798
Epoch 44: loss 16.0431927791946
Epoch 45: loss 16.041257933470916
Epoch 46: loss 16.039323519986244
Epoch 47: loss 16.03740902475976
Epoch 48: loss 16.035471870563416
Epoch 49: loss 16.033547979648375
-----------Time: 0:09:15.851080, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.0052032470703125-------------


Epoch 0: loss 16.1282133757906
Epoch 1: loss 16.126281424538476
Epoch 2: loss 16.12433889963628
Epoch 3: loss 16.122407761003867
Epoch 4: loss 16.12047060291815
Epoch 5: loss 16.118538565062646
Epoch 6: loss 16.11660162322608
Epoch 7: loss 16.114671088224394
Epoch 8: loss 16.112732151917236
Epoch 9: loss 16.110801406111516
Epoch 10: loss 16.108868064278816
Epoch 11: loss 16.10693210723155
Epoch 12: loss 16.104998171880492
Epoch 13: loss 16.103067906023433
Epoch 14: loss 16.10113652624988
Epoch 15: loss 16.09920109219367
Epoch 16: loss 16.097271014841567
Epoch 17: loss 16.095335071796047
Epoch 18: loss 16.09340245105314
Epoch 19: loss 16.091468216220342
Epoch 20: loss 16.089542611128874
Epoch 21: loss 16.087599563494916
Epoch 22: loss 16.085681580797104
Epoch 23: loss 16.083744789090346
Epoch 24: loss 16.08181036523401
Epoch 25: loss 16.079882684772983
Epoch 26: loss 16.077944545528045
Epoch 27: loss 16.07600559158906
Epoch 28: loss 16.074082653829752
Epoch 29: loss 16.072156482186248
Epoch 30: loss 16.07021349496722
Epoch 31: loss 16.068287277429114
Epoch 32: loss 16.066360137590966
Epoch 33: loss 16.064417744149587
Epoch 34: loss 16.0624923389719
Epoch 35: loss 16.060556264465564
Epoch 36: loss 16.05863048798238
Epoch 37: loss 16.056699355832254
Epoch 38: loss 16.054763945630867
Epoch 39: loss 16.052834173205625
Epoch 40: loss 16.050905168024077
Epoch 41: loss 16.04896669896029
Epoch 42: loss 16.047041912192945
Epoch 43: loss 16.045114876590002
Epoch 44: loss 16.043179849362232
Epoch 45: loss 16.041248402172876
Epoch 46: loss 16.039322917651976
Epoch 47: loss 16.037394290258213
Epoch 48: loss 16.03545629492008
Epoch 49: loss 16.03352523874238
-----------Time: 0:12:41.858984, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005198955535889-------------


Epoch 0: loss 16.128210331189237
Epoch 1: loss 16.126287664130306
Epoch 2: loss 16.12434821468517
Epoch 3: loss 16.122417942605114
Epoch 4: loss 16.120481117190455
Epoch 5: loss 16.11854294553407
Epoch 6: loss 16.11661072228511
Epoch 7: loss 16.11467508053637
Epoch 8: loss 16.11274565996976
Epoch 9: loss 16.110804899287263
Epoch 10: loss 16.10887566100261
Epoch 11: loss 16.106933313196546
Epoch 12: loss 16.10500870482179
Epoch 13: loss 16.103077247520062
Epoch 14: loss 16.1011377988528
Epoch 15: loss 16.099208103955732
Epoch 16: loss 16.09726967949009
Epoch 17: loss 16.095338826856256
Epoch 18: loss 16.093407377333275
Epoch 19: loss 16.091472927807075
Epoch 20: loss 16.089541661862512
Epoch 21: loss 16.08761332228237
Epoch 22: loss 16.08567164189275
Epoch 23: loss 16.083745227811647
Epoch 24: loss 16.081805670241938
Epoch 25: loss 16.079883357634554
Epoch 26: loss 16.077944548898834
Epoch 27: loss 16.07601437457169
Epoch 28: loss 16.074080223230773
Epoch 29: loss 16.07215289670271
Epoch 30: loss 16.070216309317694
Epoch 31: loss 16.0682913031897
Epoch 32: loss 16.066358391780966
Epoch 33: loss 16.06442169393775
Epoch 34: loss 16.062494363779603
Epoch 35: loss 16.060562227911895
Epoch 36: loss 16.058628995759516
Epoch 37: loss 16.056695466199734
Epoch 38: loss 16.0547643926495
Epoch 39: loss 16.05282746896334
Epoch 40: loss 16.050896564471195
Epoch 41: loss 16.04896052470993
Epoch 42: loss 16.04703814753895
Epoch 43: loss 16.045099975882568
Epoch 44: loss 16.043173980816608
Epoch 45: loss 16.041238787123664
Epoch 46: loss 16.039303128002388
Epoch 47: loss 16.037374768197505
Epoch 48: loss 16.035436220827666
Epoch 49: loss 16.033510099746014
-----------Time: 0:11:48.417175, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.0051960945129395-------------


Epoch 0: loss 16.11982117793429
Epoch 1: loss 16.100504568700494
Epoch 2: loss 16.081172660228155
Epoch 3: loss 16.061856216163076
Epoch 4: loss 16.042560968663526
Epoch 5: loss 16.02326994268968
Epoch 6: loss 16.003997713538084
Epoch 7: loss 15.984742690714368
Epoch 8: loss 15.965521021599741
Epoch 9: loss 15.946289531558412
Epoch 10: loss 15.927062150510256
Epoch 11: loss 15.907867241061336
Epoch 12: loss 15.888679036113993
Epoch 13: loss 15.869501126078305
Epoch 14: loss 15.850320938166359
Epoch 15: loss 15.83117169358926
Epoch 16: loss 15.812022089374786
Epoch 17: loss 15.792901543186305
Epoch 18: loss 15.773777283683557
Epoch 19: loss 15.75468502309155
Epoch 20: loss 15.735579135950783
Epoch 21: loss 15.716502518158622
Epoch 22: loss 15.697432248860743
Epoch 23: loss 15.678370817515304
Epoch 24: loss 15.659323866047115
Epoch 25: loss 15.640309478485953
Epoch 26: loss 15.621293815210372
Epoch 27: loss 15.60229121011648
Epoch 28: loss 15.583285265088147
Epoch 29: loss 15.564318487085423
Epoch 30: loss 15.545339980029489
Epoch 31: loss 15.526383127707252
Epoch 32: loss 15.507444209284987
Epoch 33: loss 15.488514823975803
Epoch 34: loss 15.469585096661069
Epoch 35: loss 15.450668083448136
Epoch 36: loss 15.43176999955543
Epoch 37: loss 15.412884322504041
Epoch 38: loss 15.39400167890448
Epoch 39: loss 15.375140265578352
Epoch 40: loss 15.356301785552548
Epoch 41: loss 15.337460420130387
Epoch 42: loss 15.31863876164381
Epoch 43: loss 15.299825663927004
Epoch 44: loss 15.281016698280322
Epoch 45: loss 15.262217372834131
Epoch 46: loss 15.243445129093754
Epoch 47: loss 15.22467355095478
Epoch 48: loss 15.205897111099269
Epoch 49: loss 15.187137238677265
-----------Time: 0:09:24.754896, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.8974878787994385-------------


Epoch 0: loss 16.119587670596413
Epoch 1: loss 16.100261689528878
Epoch 2: loss 16.080961232602824
Epoch 3: loss 16.061642314896368
Epoch 4: loss 16.0423514890956
Epoch 5: loss 16.023070695544146
Epoch 6: loss 16.003800997337354
Epoch 7: loss 15.984536620311726
Epoch 8: loss 15.965305440642382
Epoch 9: loss 15.946070405566998
Epoch 10: loss 15.92686685937588
Epoch 11: loss 15.907651431668123
Epoch 12: loss 15.888457955064302
Epoch 13: loss 15.869267699120808
Epoch 14: loss 15.850092133599302
Epoch 15: loss 15.830924577593738
Epoch 16: loss 15.811768563692176
Epoch 17: loss 15.792625403391272
Epoch 18: loss 15.773495328756958
Epoch 19: loss 15.754377158716231
Epoch 20: loss 15.73525873145829
Epoch 21: loss 15.716163467492274
Epoch 22: loss 15.697073674059355
Epoch 23: loss 15.677995156178726
Epoch 24: loss 15.658926057063606
Epoch 25: loss 15.63986215700333
Epoch 26: loss 15.620788561513464
Epoch 27: loss 15.601726453676374
Epoch 28: loss 15.582667660363152
Epoch 29: loss 15.563624599564653
Epoch 30: loss 15.544576531327763
Epoch 31: loss 15.525522017621553
Epoch 32: loss 15.506477686035172
Epoch 33: loss 15.487419589697634
Epoch 34: loss 15.46835654037293
Epoch 35: loss 15.449295772035981
Epoch 36: loss 15.43022154128182
Epoch 37: loss 15.41113758553883
Epoch 38: loss 15.39204374715775
Epoch 39: loss 15.37293053062277
Epoch 40: loss 15.35378646357932
Epoch 41: loss 15.334632037579203
Epoch 42: loss 15.315429661830137
Epoch 43: loss 15.296197555193505
Epoch 44: loss 15.276949382333926
Epoch 45: loss 15.257657287041734
Epoch 46: loss 15.238322480727046
Epoch 47: loss 15.21892860701967
Epoch 48: loss 15.199476233249516
Epoch 49: loss 15.179951603999923
-----------Time: 0:07:51.526188, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.896529197692871-------------


Epoch 0: loss 16.119559966590884
Epoch 1: loss 16.100230827352288
Epoch 2: loss 16.08091170013826
Epoch 3: loss 16.06160694986274
Epoch 4: loss 16.04231340046354
Epoch 5: loss 16.02302802160034
Epoch 6: loss 16.0037342525812
Epoch 7: loss 15.984460189719767
Epoch 8: loss 15.965189195055233
Epoch 9: loss 15.945922278787469
Epoch 10: loss 15.926662069614197
Epoch 11: loss 15.907397313763617
Epoch 12: loss 15.888124487722873
Epoch 13: loss 15.868841255015818
Epoch 14: loss 15.849556607873366
Epoch 15: loss 15.830243396136733
Epoch 16: loss 15.810889068020629
Epoch 17: loss 15.791505225266109
Epoch 18: loss 15.772075022479125
Epoch 19: loss 15.752566291950137
Epoch 20: loss 15.732979996428664
Epoch 21: loss 15.71330261774981
Epoch 22: loss 15.693481455684163
Epoch 23: loss 15.673517706862217
Epoch 24: loss 15.65336482378372
Epoch 25: loss 15.633008204185813
Epoch 26: loss 15.612397409379968
Epoch 27: loss 15.591494822385455
Epoch 28: loss 15.570249371583076
Epoch 29: loss 15.548635253833648
Epoch 30: loss 15.526586503811412
Epoch 31: loss 15.504069480512245
Epoch 32: loss 15.48101429138059
Epoch 33: loss 15.457368649497766
Epoch 34: loss 15.433092818433918
Epoch 35: loss 15.408115744525936
Epoch 36: loss 15.382396672328184
Epoch 37: loss 15.355876921829548
Epoch 38: loss 15.328515387022216
Epoch 39: loss 15.300256762315295
Epoch 40: loss 15.27105348923078
Epoch 41: loss 15.240873288563764
Epoch 42: loss 15.20966535965472
Epoch 43: loss 15.177399468071892
Epoch 44: loss 15.144033736415114
Epoch 45: loss 15.109527770431875
Epoch 46: loss 15.073873214452016
Epoch 47: loss 15.037032215798789
Epoch 48: loss 14.998983000464385
Epoch 49: loss 14.959700700314944
-----------Time: 0:11:38.039417, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.8669304847717285-------------


Epoch 0: loss 16.119590311221547
Epoch 1: loss 16.10024479720306
Epoch 2: loss 16.080925983213223
Epoch 3: loss 16.061614913484078
Epoch 4: loss 16.042307946265815
Epoch 5: loss 16.023007780524168
Epoch 6: loss 16.003697139403947
Epoch 7: loss 15.984385522569632
Epoch 8: loss 15.965066953091728
Epoch 9: loss 15.945726051092018
Epoch 10: loss 15.92633325681536
Epoch 11: loss 15.906880518221998
Epoch 12: loss 15.887349902449126
Epoch 13: loss 15.867701108589193
Epoch 14: loss 15.84788758162249
Epoch 15: loss 15.827857837656259
Epoch 16: loss 15.807540083009306
Epoch 17: loss 15.786870667310822
Epoch 18: loss 15.765746293215729
Epoch 19: loss 15.744066384903045
Epoch 20: loss 15.721718117359735
Epoch 21: loss 15.698596532996936
Epoch 22: loss 15.674570168925083
Epoch 23: loss 15.649526761756636
Epoch 24: loss 15.623367241376636
Epoch 25: loss 15.595955329592167
Epoch 26: loss 15.56719729639772
Epoch 27: loss 15.536979675033677
Epoch 28: loss 15.505213004470585
Epoch 29: loss 15.471806198439564
Epoch 30: loss 15.436688045368951
Epoch 31: loss 15.399807184788765
Epoch 32: loss 15.361061005957946
Epoch 33: loss 15.320425461918974
Epoch 34: loss 15.277837769112683
Epoch 35: loss 15.23326366682815
Epoch 36: loss 15.186678519256741
Epoch 37: loss 15.138046722816604
Epoch 38: loss 15.087337959584106
Epoch 39: loss 15.03455196897124
Epoch 40: loss 14.979668220273174
Epoch 41: loss 14.922657832300747
Epoch 42: loss 14.863534002475209
Epoch 43: loss 14.802274112016887
Epoch 44: loss 14.73892979780055
Epoch 45: loss 14.673460555115494
Epoch 46: loss 14.605920528704345
Epoch 47: loss 14.5362655689951
Epoch 48: loss 14.464529799274157
Epoch 49: loss 14.39071209369761
-----------Time: 0:10:35.293292, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7906718254089355-------------


Epoch 0: loss 16.119574856148784
Epoch 1: loss 16.10024846047406
Epoch 2: loss 16.080929115714426
Epoch 3: loss 16.061596304388914
Epoch 4: loss 16.042287207090578
Epoch 5: loss 16.02296954539238
Epoch 6: loss 16.003633563709105
Epoch 7: loss 15.984256235914039
Epoch 8: loss 15.964825830879647
Epoch 9: loss 15.945309479512748
Epoch 10: loss 15.925653643035059
Epoch 11: loss 15.905770763098513
Epoch 12: loss 15.88557583719184
Epoch 13: loss 15.864947038995888
Epoch 14: loss 15.843721625466525
Epoch 15: loss 15.821720240490796
Epoch 16: loss 15.798735676672615
Epoch 17: loss 15.774566625239345
Epoch 18: loss 15.748946003190456
Epoch 19: loss 15.72169283824877
Epoch 20: loss 15.692579095165259
Epoch 21: loss 15.661417476763992
Epoch 22: loss 15.628049816755967
Epoch 23: loss 15.592320859918392
Epoch 24: loss 15.554084634184512
Epoch 25: loss 15.513247288459667
Epoch 26: loss 15.469704610099607
Epoch 27: loss 15.423417296987827
Epoch 28: loss 15.37430107496302
Epoch 29: loss 15.322302076205927
Epoch 30: loss 15.267423181704945
Epoch 31: loss 15.209623034458046
Epoch 32: loss 15.148887087172175
Epoch 33: loss 15.085180739723773
Epoch 34: loss 15.018591374142654
Epoch 35: loss 14.94906516176258
Epoch 36: loss 14.876594217527554
Epoch 37: loss 14.80122026041061
Epoch 38: loss 14.722967162347475
Epoch 39: loss 14.641856445573866
Epoch 40: loss 14.557924856888075
Epoch 41: loss 14.471186910393836
Epoch 42: loss 14.381646196501226
Epoch 43: loss 14.289352741625207
Epoch 44: loss 14.194355741929204
Epoch 45: loss 14.096647862573887
Epoch 46: loss 13.996303589723887
Epoch 47: loss 13.893349106639802
Epoch 48: loss 13.78780150322761
Epoch 49: loss 13.679778340719782
-----------Time: 0:14:01.865529, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.693854808807373-------------


Epoch 0: loss 16.033480374822155
Epoch 1: loss 15.841524686289585
Epoch 2: loss 15.650758829370927
Epoch 3: loss 15.461198396822757
Epoch 4: loss 15.272861352622865
Epoch 5: loss 15.08550720385976
Epoch 6: loss 14.898973069027626
Epoch 7: loss 14.713170231004458
Epoch 8: loss 14.527664686818042
Epoch 9: loss 14.341966103962934
Epoch 10: loss 14.155503392284366
Epoch 11: loss 13.967308967511507
Epoch 12: loss 13.776392252954729
Epoch 13: loss 13.581554769626967
Epoch 14: loss 13.381377975210798
Epoch 15: loss 13.174323956301317
Epoch 16: loss 12.95873265217153
Epoch 17: loss 12.733179694482203
Epoch 18: loss 12.496020531252457
Epoch 19: loss 12.24601733263191
Epoch 20: loss 11.982120809767673
Epoch 21: loss 11.703592696353233
Epoch 22: loss 11.409905708503826
Epoch 23: loss 11.100938042199372
Epoch 24: loss 10.776750956364726
Epoch 25: loss 10.43769822320321
Epoch 26: loss 10.084517971337522
Epoch 27: loss 9.7178627159882
Epoch 28: loss 9.339073017798668
Epoch 29: loss 8.949578088415
Epoch 30: loss 8.550689428639581
Epoch 31: loss 8.144220474558463
Epoch 32: loss 7.73167273141302
Epoch 33: loss 7.315261145789316
Epoch 34: loss 6.8973745440192165
Epoch 35: loss 6.479812406339744
Epoch 36: loss 6.065000818759738
Epoch 37: loss 5.655608755534858
Epoch 38: loss 5.253842422403415
Epoch 39: loss 4.8624115982024545
Epoch 40: loss 4.483965505394616
Epoch 41: loss 4.121133199653397
Epoch 42: loss 3.7763720497351225
Epoch 43: loss 3.45231679141878
Epoch 44: loss 3.151207872225319
Epoch 45: loss 2.8752525635164936
Epoch 46: loss 2.62584550942073
Epoch 47: loss 2.4040672384960096
Epoch 48: loss 2.20998971562077
Epoch 49: loss 2.042827708037928
-----------Time: 0:07:07.199115, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.4139379262924194-------------


Epoch 0: loss 16.033470681466895
Epoch 1: loss 15.841513279017189
Epoch 2: loss 15.650587901529766
Epoch 3: loss 15.459923397269568
Epoch 4: loss 15.267523121483757
Epoch 5: loss 15.068650449728953
Epoch 6: loss 14.854152706928264
Epoch 7: loss 14.61098617778776
Epoch 8: loss 14.325624033962663
Epoch 9: loss 13.987718541444547
Epoch 10: loss 13.591437443240043
Epoch 11: loss 13.134879616825007
Epoch 12: loss 12.619603045029509
Epoch 13: loss 12.049497579218837
Epoch 14: loss 11.429568923381828
Epoch 15: loss 10.766203611684015
Epoch 16: loss 10.066906106802612
Epoch 17: loss 9.34000612096076
Epoch 18: loss 8.594055387881737
Epoch 19: loss 7.838676312489636
Epoch 20: loss 7.084252241190651
Epoch 21: loss 6.341634339648397
Epoch 22: loss 5.621760342418013
Epoch 23: loss 4.935949495980894
Epoch 24: loss 4.29582532415447
Epoch 25: loss 3.713000407745295
Epoch 26: loss 3.1967966488485042
Epoch 27: loss 2.7551000842975495
Epoch 28: loss 2.3916834225597556
Epoch 29: loss 2.1042514602861826
Epoch 30: loss 1.8838631382709357
Epoch 31: loss 1.717819498698954
Epoch 32: loss 1.5922361087578674
Epoch 33: loss 1.4954824403228157
Epoch 34: loss 1.4190238166140627
Epoch 35: loss 1.357087940966195
Epoch 36: loss 1.305894217862585
Epoch 37: loss 1.262914426774678
Epoch 38: loss 1.2264453609589976
Epoch 39: loss 1.1952755054810438
Epoch 40: loss 1.1685243454428067
Epoch 41: loss 1.145433404544179
Epoch 42: loss 1.1254608326628521
Epoch 43: loss 1.1081473746819623
Epoch 44: loss 1.0930836855425272
Epoch 45: loss 1.0799903512454798
Epoch 46: loss 1.0685550775004184
Epoch 47: loss 1.058578606287768
Epoch 48: loss 1.0498328318798134
Epoch 49: loss 1.042173545116831
-----------Time: 0:10:05.254927, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.0293595790863037-------------


Epoch 0: loss 16.033412863600184
Epoch 1: loss 15.840960971682923
Epoch 2: loss 15.643899595562436
Epoch 3: loss 15.416874169655117
Epoch 4: loss 15.106564711850256
Epoch 5: loss 14.668422830694716
Epoch 6: loss 14.087836086652796
Epoch 7: loss 13.368863104477468
Epoch 8: loss 12.525716398642594
Epoch 9: loss 11.57679282588243
Epoch 10: loss 10.544332778084337
Epoch 11: loss 9.45402447382609
Epoch 12: loss 8.332899693369281
Epoch 13: loss 7.211441654819325
Epoch 14: loss 6.122247446743673
Epoch 15: loss 5.0981336936411354
Epoch 16: loss 4.173233895537774
Epoch 17: loss 3.3784626021600404
Epoch 18: loss 2.737588887340417
Epoch 19: loss 2.257431157229062
Epoch 20: loss 1.9212843826635173
Epoch 21: loss 1.694049143998632
Epoch 22: loss 1.5383395672557534
Epoch 23: loss 1.4265670172866107
Epoch 24: loss 1.3425031185571494
Epoch 25: loss 1.2770316407141185
Epoch 26: loss 1.225022897910268
Epoch 27: loss 1.183105195894262
Epoch 28: loss 1.149075927797253
Epoch 29: loss 1.1212940468873713
Epoch 30: loss 1.0985564519653765
Epoch 31: loss 1.079883088457513
Epoch 32: loss 1.064460970821946
Epoch 33: loss 1.0517026007078212
Epoch 34: loss 1.0410972716952225
Epoch 35: loss 1.0322595195092217
Epoch 36: loss 1.024857757320347
Epoch 37: loss 1.0186641310757434
Epoch 38: loss 1.0134463216572627
Epoch 39: loss 1.009029764238423
Epoch 40: loss 1.0052912349996521
Epoch 41: loss 1.0021097545652302
Epoch 42: loss 0.9994050093874327
Epoch 43: loss 0.9970890244366748
Epoch 44: loss 0.9951016491784422
Epoch 45: loss 0.9933934120168629
Epoch 46: loss 0.9919210273879842
Epoch 47: loss 0.9906514922907458
Epoch 48: loss 0.9895469626050205
Epoch 49: loss 0.9885895644244194
-----------Time: 0:09:12.880157, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0035169124603271-------------


Epoch 0: loss 16.033410514937334
Epoch 1: loss 15.838139073754084
Epoch 2: loss 15.602252199962257
Epoch 3: loss 15.212266526058876
Epoch 4: loss 14.590006992698948
Epoch 5: loss 13.73960699887299
Epoch 6: loss 12.691747110521359
Epoch 7: loss 11.484689841133022
Epoch 8: loss 10.163075140081325
Epoch 9: loss 8.776976555310364
Epoch 10: loss 7.380282540370615
Epoch 11: loss 6.0319833174680095
Epoch 12: loss 4.79370544940768
Epoch 13: loss 3.7260406972144593
Epoch 14: loss 2.8784968468597105
Epoch 15: loss 2.270942839187666
Epoch 16: loss 1.8761756383510566
Epoch 17: loss 1.6298152836462023
Epoch 18: loss 1.4701703041386773
Epoch 19: loss 1.359146355221361
Epoch 20: loss 1.2776161659470455
Epoch 21: loss 1.2158032645020167
Epoch 22: loss 1.1681016580023929
Epoch 23: loss 1.1309867832341747
Epoch 24: loss 1.1019498271550607
Epoch 25: loss 1.0790523498715623
Epoch 26: loss 1.0609576519641493
Epoch 27: loss 1.046551916750311
Epoch 28: loss 1.0350468109618587
Epoch 29: loss 1.0257986268315256
Epoch 30: loss 1.0183189781124402
Epoch 31: loss 1.012256819440853
Epoch 32: loss 1.0073188576865546
Epoch 33: loss 1.0032586328292294
Epoch 34: loss 0.9999247894720126
Epoch 35: loss 0.9971778455183777
Epoch 36: loss 0.9948932498029032
Epoch 37: loss 0.9929933875329213
Epoch 38: loss 0.9913976007729137
Epoch 39: loss 0.990058433221954
Epoch 40: loss 0.9889336360933471
Epoch 41: loss 0.9879866391987565
Epoch 42: loss 0.9871781646816415
Epoch 43: loss 0.9864874415063158
Epoch 44: loss 0.9858949215694768
Epoch 45: loss 0.9853994714753533
Epoch 46: loss 0.9849557916615047
Epoch 47: loss 0.9845724542092992
Epoch 48: loss 0.9842452500228197
Epoch 49: loss 0.9839556508896596
-----------Time: 0:12:46.902747, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0010507106781006-------------


Epoch 0: loss 16.033280847123162
Epoch 1: loss 15.830580744294773
Epoch 2: loss 15.497687263291708
Epoch 3: loss 14.84221294096075
Epoch 4: loss 13.857130031471605
Epoch 5: loss 12.60298231855043
Epoch 6: loss 11.14713702271852
Epoch 7: loss 9.56294141546937
Epoch 8: loss 7.9307922239599185
Epoch 9: loss 6.339478526444718
Epoch 10: loss 4.882606783984082
Epoch 11: loss 3.6524007587474348
Epoch 12: loss 2.721137677540657
Epoch 13: loss 2.104169598865146
Epoch 14: loss 1.7367328389822756
Epoch 15: loss 1.519013086533404
Epoch 16: loss 1.3790591921639093
Epoch 17: loss 1.281809665185722
Epoch 18: loss 1.2110691003951124
Epoch 19: loss 1.1584944442732947
Epoch 20: loss 1.118986555420231
Epoch 21: loss 1.0891142997682062
Epoch 22: loss 1.0663765954737952
Epoch 23: loss 1.0489284716396632
Epoch 24: loss 1.0354383030684504
Epoch 25: loss 1.0249812534932794
Epoch 26: loss 1.0167935930431504
Epoch 27: loss 1.0103341069572838
Epoch 28: loss 1.0052233642213302
Epoch 29: loss 1.001147156357441
Epoch 30: loss 0.9978894884817103
Epoch 31: loss 0.9952592552194393
Epoch 32: loss 0.9931306871562758
Epoch 33: loss 0.9913916210074215
Epoch 34: loss 0.9899662276238705
Epoch 35: loss 0.988800273277372
Epoch 36: loss 0.9878276580698533
Epoch 37: loss 0.9870241592117079
Epoch 38: loss 0.986362068305656
Epoch 39: loss 0.9857857524700434
Epoch 40: loss 0.9853108263067606
Epoch 41: loss 0.9849143811643869
Epoch 42: loss 0.9845714875541997
Epoch 43: loss 0.9842639024682898
Epoch 44: loss 0.9840215679223671
Epoch 45: loss 0.9837875147919865
Epoch 46: loss 0.983606951109672
Epoch 47: loss 0.9834419540927227
Epoch 48: loss 0.9832902974639008
Epoch 49: loss 0.9831630818023184
-----------Time: 0:11:42.358835, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0004836320877075-------------


Epoch 0: loss 15.196383442549424
Epoch 1: loss 13.32011020358585
Epoch 2: loss 10.645620603805135
Epoch 3: loss 6.705112418900241
Epoch 4: loss 3.1758416053262204
Epoch 5: loss 1.5892876850450215
Epoch 6: loss 1.2060747112736487
Epoch 7: loss 1.0837383436210264
Epoch 8: loss 1.0320172896036446
Epoch 9: loss 1.0082002637210004
Epoch 10: loss 0.996292195950856
Epoch 11: loss 0.9898776047705566
Epoch 12: loss 0.9860425667149532
Epoch 13: loss 0.9834485641149414
Epoch 14: loss 0.981455143182546
Epoch 15: loss 0.9797861504613089
Epoch 16: loss 0.9782319741284089
Epoch 17: loss 0.9767521595164056
Epoch 18: loss 0.9752397469005356
Epoch 19: loss 0.9737234669512157
Epoch 20: loss 0.972174765040785
Epoch 21: loss 0.9706116780727826
Epoch 22: loss 0.9690125903855076
Epoch 23: loss 0.9674049389952224
Epoch 24: loss 0.9657246879807888
Epoch 25: loss 0.9641024041363828
Epoch 26: loss 0.9624131945827332
Epoch 27: loss 0.9607440881297405
Epoch 28: loss 0.9590211270489467
Epoch 29: loss 0.9573347316350932
Epoch 30: loss 0.9555921171948338
Epoch 31: loss 0.9538468674611241
Epoch 32: loss 0.9521091930596578
Epoch 33: loss 0.950303446248279
Epoch 34: loss 0.9485076566051049
Epoch 35: loss 0.9466452910856037
Epoch 36: loss 0.944756121383276
Epoch 37: loss 0.9428734933474324
Epoch 38: loss 0.9409168474436973
Epoch 39: loss 0.9389292481089494
Epoch 40: loss 0.9368857491949838
Epoch 41: loss 0.9348360647190648
Epoch 42: loss 0.9327475124075725
Epoch 43: loss 0.930587689649028
Epoch 44: loss 0.9284275328257344
Epoch 45: loss 0.926231541310788
Epoch 46: loss 0.9239936909998416
Epoch 47: loss 0.9217551178650859
Epoch 48: loss 0.9194286896717037
Epoch 49: loss 0.9171369172685363
-----------Time: 0:09:30.749707, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9925373196601868-------------


Epoch 0: loss 15.114034080712804
Epoch 1: loss 9.996360971073278
Epoch 2: loss 3.1873546137376736
Epoch 3: loss 1.3282550834423177
Epoch 4: loss 1.08177823448972
Epoch 5: loss 1.020222912345521
Epoch 6: loss 1.0015715699179009
Epoch 7: loss 0.995008102735143
Epoch 8: loss 0.9923631181173703
Epoch 9: loss 0.9911209105667438
Epoch 10: loss 0.9903728093642006
Epoch 11: loss 0.9898559145949469
Epoch 12: loss 0.9894656590821628
Epoch 13: loss 0.9890689867517752
Epoch 14: loss 0.9886978371691743
Epoch 15: loss 0.9882990255700693
Epoch 16: loss 0.9879351848490281
Epoch 17: loss 0.9874425423475891
Epoch 18: loss 0.986968293956209
Epoch 19: loss 0.9864253457179856
Epoch 20: loss 0.9858452091889902
Epoch 21: loss 0.9852234043784606
Epoch 22: loss 0.9845677980623406
Epoch 23: loss 0.9837281309987878
Epoch 24: loss 0.9828796287863847
Epoch 25: loss 0.9818784693876902
Epoch 26: loss 0.9808795098995242
Epoch 27: loss 0.9796037006274457
Epoch 28: loss 0.9782584345392588
Epoch 29: loss 0.9768032720226383
Epoch 30: loss 0.9751313968058706
Epoch 31: loss 0.9733068080879282
Epoch 32: loss 0.9713105089383128
Epoch 33: loss 0.9690769006208987
Epoch 34: loss 0.9666354863652722
Epoch 35: loss 0.9640088769130697
Epoch 36: loss 0.9611122331715201
Epoch 37: loss 0.9580110127411179
Epoch 38: loss 0.9547752731516673
Epoch 39: loss 0.9513018263980965
Epoch 40: loss 0.9476200387716422
Epoch 41: loss 0.9437632807867238
Epoch 42: loss 0.9397135986913605
Epoch 43: loss 0.9355118782413985
Epoch 44: loss 0.9310265203705944
Epoch 45: loss 0.9264064645430133
Epoch 46: loss 0.9216232626512039
Epoch 47: loss 0.9165071143508929
Epoch 48: loss 0.9113007281350337
Epoch 49: loss 0.9058181442048902
-----------Time: 0:07:49.129332, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9901320934295654-------------


Epoch 0: loss 13.886333050968986
Epoch 1: loss 4.016556272077587
Epoch 2: loss 1.2495509022406226
Epoch 3: loss 1.0420916972923953
Epoch 4: loss 1.0068268670440692
Epoch 5: loss 0.9987712399735277
Epoch 6: loss 0.9963906334268456
Epoch 7: loss 0.9953946046948497
Epoch 8: loss 0.9948198092962621
Epoch 9: loss 0.9945080070277801
Epoch 10: loss 0.9941258776797232
Epoch 11: loss 0.9937849307656612
Epoch 12: loss 0.9933854386396808
Epoch 13: loss 0.9930409100504269
Epoch 14: loss 0.9925034094628203
Epoch 15: loss 0.9919398430412784
Epoch 16: loss 0.9912294367851934
Epoch 17: loss 0.9904478971824625
Epoch 18: loss 0.9896145653407038
Epoch 19: loss 0.9884490040477817
Epoch 20: loss 0.987258638422926
Epoch 21: loss 0.985812224079047
Epoch 22: loss 0.9840924457209859
Epoch 23: loss 0.9821339617156671
Epoch 24: loss 0.9798868712910885
Epoch 25: loss 0.9774233625270933
Epoch 26: loss 0.9744956962371274
Epoch 27: loss 0.9714058223724884
Epoch 28: loss 0.9680676122214756
Epoch 29: loss 0.9644302221307811
Epoch 30: loss 0.9606181632863108
Epoch 31: loss 0.956540425359198
Epoch 32: loss 0.9521273586690393
Epoch 33: loss 0.9476069718550703
Epoch 34: loss 0.9426709616099965
Epoch 35: loss 0.9374345973531081
Epoch 36: loss 0.9319325962618942
Epoch 37: loss 0.9259270059148406
Epoch 38: loss 0.9194331157298241
Epoch 39: loss 0.9125120165783662
Epoch 40: loss 0.9049861771928156
Epoch 41: loss 0.8968573296407177
Epoch 42: loss 0.8882440708136027
Epoch 43: loss 0.8789055243726007
Epoch 44: loss 0.8690082811674
Epoch 45: loss 0.8585106024701936
Epoch 46: loss 0.8471799051392137
Epoch 47: loss 0.8352221622455372
Epoch 48: loss 0.8226746886628292
Epoch 49: loss 0.8094665724632207
-----------Time: 0:11:32.087945, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9863319993019104-------------


Epoch 0: loss 12.320914028000741
Epoch 1: loss 2.1892375807486
Epoch 2: loss 1.0910012227902923
Epoch 3: loss 1.0139462158788866
Epoch 4: loss 1.0022974089962948
Epoch 5: loss 0.9993668633871198
Epoch 6: loss 0.9985337365639475
Epoch 7: loss 0.9980712218282014
Epoch 8: loss 0.997784485257534
Epoch 9: loss 0.9973459988265273
Epoch 10: loss 0.9969954651908552
Epoch 11: loss 0.9965551473350224
Epoch 12: loss 0.9960541955441998
Epoch 13: loss 0.9954323751113543
Epoch 14: loss 0.9946372308375849
Epoch 15: loss 0.9936988487750568
Epoch 16: loss 0.9926744646647496
Epoch 17: loss 0.9913649761754576
Epoch 18: loss 0.9896518701506154
Epoch 19: loss 0.9879077556601291
Epoch 20: loss 0.9855590535448322
Epoch 21: loss 0.9828855709870138
Epoch 22: loss 0.9797990735196885
Epoch 23: loss 0.9763358473907416
Epoch 24: loss 0.9723823404688107
Epoch 25: loss 0.9681017143107938
Epoch 26: loss 0.963345675542343
Epoch 27: loss 0.9584633165594675
Epoch 28: loss 0.9531018864110218
Epoch 29: loss 0.9473534516824076
Epoch 30: loss 0.9411903750060239
Epoch 31: loss 0.9345455129811658
Epoch 32: loss 0.9272716080546055
Epoch 33: loss 0.919341852397229
Epoch 34: loss 0.9106335109378282
Epoch 35: loss 0.9011412331304711
Epoch 36: loss 0.8907463620414288
Epoch 37: loss 0.8793487579781054
Epoch 38: loss 0.867199415224541
Epoch 39: loss 0.8537623920053033
Epoch 40: loss 0.8393943723029842
Epoch 41: loss 0.8237886956977482
Epoch 42: loss 0.8071844415407974
Epoch 43: loss 0.789452688356273
Epoch 44: loss 0.7706390248004867
Epoch 45: loss 0.750878561411687
Epoch 46: loss 0.7300685364763136
Epoch 47: loss 0.7084692872983727
Epoch 48: loss 0.6861937557746691
Epoch 49: loss 0.6630494701774824
-----------Time: 0:10:39.844175, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9897101521492004-------------


Epoch 0: loss 10.797811992794616
Epoch 1: loss 1.5434177730996173
Epoch 2: loss 1.0445598772746705
Epoch 3: loss 1.0078792712849207
Epoch 4: loss 1.0026673337828536
Epoch 5: loss 1.0015295448674009
Epoch 6: loss 1.0008686281145105
Epoch 7: loss 1.0005986857764289
Epoch 8: loss 1.000150191534337
Epoch 9: loss 0.9999391544993123
Epoch 10: loss 0.9993512233908634
Epoch 11: loss 0.9990452461394361
Epoch 12: loss 0.9984923103472796
Epoch 13: loss 0.9978610796245172
Epoch 14: loss 0.9970177391775928
Epoch 15: loss 0.995923403652548
Epoch 16: loss 0.9947220446980473
Epoch 17: loss 0.9931502571859199
Epoch 18: loss 0.9912154260149723
Epoch 19: loss 0.9887706689240817
Epoch 20: loss 0.9858621055662405
Epoch 21: loss 0.9825240781023297
Epoch 22: loss 0.9788028331247602
Epoch 23: loss 0.9742015867598878
Epoch 24: loss 0.9693792820819505
Epoch 25: loss 0.9639761173867738
Epoch 26: loss 0.9581285746380193
Epoch 27: loss 0.9516181556863458
Epoch 28: loss 0.9446504197249016
Epoch 29: loss 0.9367287704612718
Epoch 30: loss 0.9280433158079783
Epoch 31: loss 0.9183280649329087
Epoch 32: loss 0.9072601643510977
Epoch 33: loss 0.8951520562139524
Epoch 34: loss 0.8816443297641312
Epoch 35: loss 0.8666830847385203
Epoch 36: loss 0.8503178081446332
Epoch 37: loss 0.8325871521036803
Epoch 38: loss 0.8130766218968745
Epoch 39: loss 0.7922932069673688
Epoch 40: loss 0.7698096359566932
Epoch 41: loss 0.7459305195201669
Epoch 42: loss 0.7206087132197738
Epoch 43: loss 0.6941202159255921
Epoch 44: loss 0.6663898459801277
Epoch 45: loss 0.6377477963101028
Epoch 46: loss 0.6083574284186112
Epoch 47: loss 0.5785501635126474
Epoch 48: loss 0.5486469209972058
Epoch 49: loss 0.518811224043661
-----------Time: 0:13:45.709644, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9993464350700378-------------


Epoch 0: loss 5.436983726234006
Epoch 1: loss 1.026343545961017
Epoch 2: loss 1.0098803567782635
Epoch 3: loss 0.9901427394381809
Epoch 4: loss 0.9706788753413065
Epoch 5: loss 0.9520526319628763
Epoch 6: loss 0.9326253241984465
Epoch 7: loss 0.9114694547530034
Epoch 8: loss 0.8885871125278815
Epoch 9: loss 0.864787987258656
Epoch 10: loss 0.8418219992328818
Epoch 11: loss 0.8213957069100343
Epoch 12: loss 0.8042438972929757
Epoch 13: loss 0.789970746744969
Epoch 14: loss 0.7784718530634424
Epoch 15: loss 0.7694727186115622
Epoch 16: loss 0.761744393772117
Epoch 17: loss 0.7554409524680091
Epoch 18: loss 0.7498692852313782
Epoch 19: loss 0.7451002706387174
Epoch 20: loss 0.7410257264137139
Epoch 21: loss 0.7374082110769532
Epoch 22: loss 0.7338743859040082
Epoch 23: loss 0.7309392172075472
Epoch 24: loss 0.7279867454917617
Epoch 25: loss 0.7252086489631016
Epoch 26: loss 0.7230014823389287
Epoch 27: loss 0.7204936130935566
Epoch 28: loss 0.7184487668439835
Epoch 29: loss 0.7164858639434734
Epoch 30: loss 0.7144572712630924
Epoch 31: loss 0.7128142778837143
Epoch 32: loss 0.7110778967636704
Epoch 33: loss 0.709289708122603
Epoch 34: loss 0.7078642346503748
Epoch 35: loss 0.706503437760085
Epoch 36: loss 0.7048991925046132
Epoch 37: loss 0.703694771632933
Epoch 38: loss 0.7022953165118755
Epoch 39: loss 0.700877160538857
Epoch 40: loss 0.7001245850661709
Epoch 41: loss 0.6983763676353484
Epoch 42: loss 0.6975435231968785
Epoch 43: loss 0.6963637146922542
Epoch 44: loss 0.6954165814022061
Epoch 45: loss 0.6943337820579721
Epoch 46: loss 0.693319186641186
Epoch 47: loss 0.6924020272029878
Epoch 48: loss 0.6914277715550745
Epoch 49: loss 0.690505266489381
-----------Time: 0:07:09.732444, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1305818557739258-------------


Epoch 0: loss 3.6637993788122807
Epoch 1: loss 1.0456963830372767
Epoch 2: loss 1.0218937454533485
Epoch 3: loss 0.9911675747319886
Epoch 4: loss 0.9545690497994228
Epoch 5: loss 0.9088017665510141
Epoch 6: loss 0.852166441940237
Epoch 7: loss 0.7859039540813305
Epoch 8: loss 0.7201110029136312
Epoch 9: loss 0.6614155345649548
Epoch 10: loss 0.6145994691760596
Epoch 11: loss 0.5793496251203496
Epoch 12: loss 0.5527179687810113
Epoch 13: loss 0.5321152040075516
Epoch 14: loss 0.5157717040224915
Epoch 15: loss 0.5020847801056292
Epoch 16: loss 0.4907010423790313
Epoch 17: loss 0.48128721495566645
Epoch 18: loss 0.47272918560927557
Epoch 19: loss 0.4653970275465207
Epoch 20: loss 0.4584951676173078
Epoch 21: loss 0.4527293512230013
Epoch 22: loss 0.4475834483990791
Epoch 23: loss 0.44257261427023153
Epoch 24: loss 0.4380748479294349
Epoch 25: loss 0.4339556545181207
Epoch 26: loss 0.4299699620360457
Epoch 27: loss 0.42640787797365715
Epoch 28: loss 0.4230924796286713
Epoch 29: loss 0.42003246028504987
Epoch 30: loss 0.41716934475579814
Epoch 31: loss 0.41458781732042693
Epoch 32: loss 0.41161939351049176
Epoch 33: loss 0.40934538835170414
Epoch 34: loss 0.40691719271846283
Epoch 35: loss 0.4048129370625477
Epoch 36: loss 0.4027993169700795
Epoch 37: loss 0.4005773362134902
Epoch 38: loss 0.3988145373320048
Epoch 39: loss 0.39692753181064044
Epoch 40: loss 0.3951122397926594
Epoch 41: loss 0.39341144645389103
Epoch 42: loss 0.3919219263039768
Epoch 43: loss 0.3903413674651831
Epoch 44: loss 0.38893126578856707
Epoch 45: loss 0.38733905595142337
Epoch 46: loss 0.38596056701535436
Epoch 47: loss 0.3845986139480999
Epoch 48: loss 0.3832767578353371
Epoch 49: loss 0.3818543453970767
-----------Time: 0:09:46.657015, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4227488040924072-------------


Epoch 0: loss 2.9633346717600073
Epoch 1: loss 1.0707576666402066
Epoch 2: loss 1.0372463172547775
Epoch 3: loss 0.9842772895937189
Epoch 4: loss 0.8962706342509675
Epoch 5: loss 0.7712845124259728
Epoch 6: loss 0.6354407176189023
Epoch 7: loss 0.5173720443391359
Epoch 8: loss 0.4286673950834596
Epoch 9: loss 0.3659848183020747
Epoch 10: loss 0.32169911784766614
Epoch 11: loss 0.28985127277546696
Epoch 12: loss 0.26534377796906633
Epoch 13: loss 0.24618041048625555
Epoch 14: loss 0.23075484185843703
Epoch 15: loss 0.21805991410658113
Epoch 16: loss 0.2072302505734532
Epoch 17: loss 0.19812954182758352
Epoch 18: loss 0.19008204232356418
Epoch 19: loss 0.18321192040512782
Epoch 20: loss 0.17703661993510572
Epoch 21: loss 0.17150588447654488
Epoch 22: loss 0.16662715993266575
Epoch 23: loss 0.1620240796472859
Epoch 24: loss 0.1580411635517523
Epoch 25: loss 0.15438178496241828
Epoch 26: loss 0.15091911242497946
Epoch 27: loss 0.14771340081854123
Epoch 28: loss 0.144884653282091
Epoch 29: loss 0.14226630566689294
Epoch 30: loss 0.1397427858064653
Epoch 31: loss 0.13731257961642618
Epoch 32: loss 0.13517033907140708
Epoch 33: loss 0.1331928050294397
Epoch 34: loss 0.13114407698078412
Epoch 35: loss 0.12937470635094353
Epoch 36: loss 0.12758681320298165
Epoch 37: loss 0.12595134212667364
Epoch 38: loss 0.12430231531655549
Epoch 39: loss 0.12295824012001486
Epoch 40: loss 0.12154873973644902
Epoch 41: loss 0.12019961258642631
Epoch 42: loss 0.11883510734993294
Epoch 43: loss 0.11767407030053538
Epoch 44: loss 0.11647053939949209
Epoch 45: loss 0.11542579806559172
Epoch 46: loss 0.11428113815604085
Epoch 47: loss 0.11326504110593132
Epoch 48: loss 0.11223047163161232
Epoch 49: loss 0.11129423790186432
-----------Time: 0:09:08.050198, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.626402497291565-------------


Epoch 0: loss 2.6997332320127754
Epoch 1: loss 1.0719399377501095
Epoch 2: loss 1.011887379837399
Epoch 3: loss 0.9117810774296765
Epoch 4: loss 0.7579049052687816
Epoch 5: loss 0.5817667619568813
Epoch 6: loss 0.4272600856405552
Epoch 7: loss 0.3152102257908525
Epoch 8: loss 0.24199793806634823
Epoch 9: loss 0.19464615098990262
Epoch 10: loss 0.1629099701716078
Epoch 11: loss 0.1409614695121634
Epoch 12: loss 0.12423677552671715
Epoch 13: loss 0.11196683435039945
Epoch 14: loss 0.1019595214532908
Epoch 15: loss 0.09423517488964132
Epoch 16: loss 0.0876269474465556
Epoch 17: loss 0.08242943873016195
Epoch 18: loss 0.07775433049772731
Epoch 19: loss 0.07377268818509423
Epoch 20: loss 0.0706810136388961
Epoch 21: loss 0.06762963084806853
Epoch 22: loss 0.06503773769014325
Epoch 23: loss 0.06272975879844406
Epoch 24: loss 0.06074875790793945
Epoch 25: loss 0.058796502407711115
Epoch 26: loss 0.05719254424470381
Epoch 27: loss 0.055710930510942155
Epoch 28: loss 0.05424816040864063
Epoch 29: loss 0.05289781152241868
Epoch 30: loss 0.05187510457264625
Epoch 31: loss 0.05067422480108201
Epoch 32: loss 0.04981767100876054
Epoch 33: loss 0.04862318116849437
Epoch 34: loss 0.047876243938374384
Epoch 35: loss 0.04694466278981974
Epoch 36: loss 0.046149338266896836
Epoch 37: loss 0.045477334321833554
Epoch 38: loss 0.044788368502069645
Epoch 39: loss 0.04413030064011994
Epoch 40: loss 0.04352091940534121
Epoch 41: loss 0.042970696168426525
Epoch 42: loss 0.04235278801408117
Epoch 43: loss 0.041849685632312277
Epoch 44: loss 0.04124578046842867
Epoch 45: loss 0.040876194594155486
Epoch 46: loss 0.04033950699595588
Epoch 47: loss 0.03988935165775736
Epoch 48: loss 0.0395318090438624
Epoch 49: loss 0.03906543345290351
-----------Time: 0:12:46.569797, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4585459232330322-------------


Epoch 0: loss 2.565683323235665
Epoch 1: loss 1.076574973288018
Epoch 2: loss 0.9827317075278202
Epoch 3: loss 0.8235250743852484
Epoch 4: loss 0.6138614174576154
Epoch 5: loss 0.4162710624878143
Epoch 6: loss 0.2746044549485339
Epoch 7: loss 0.18824808258622025
Epoch 8: loss 0.13910194212502924
Epoch 9: loss 0.10998430162619612
Epoch 10: loss 0.09178046010487297
Epoch 11: loss 0.07995454117234406
Epoch 12: loss 0.07160240568042353
Epoch 13: loss 0.06587846137789878
Epoch 14: loss 0.06148911579479142
Epoch 15: loss 0.05805728555762813
Epoch 16: loss 0.05528636116082737
Epoch 17: loss 0.05329435754338479
Epoch 18: loss 0.05136955975919006
Epoch 19: loss 0.04977297442881649
Epoch 20: loss 0.048427823015355496
Epoch 21: loss 0.04732906678716828
Epoch 22: loss 0.04622891242653861
Epoch 23: loss 0.0452969491161718
Epoch 24: loss 0.04436032122478108
Epoch 25: loss 0.04362726284119099
Epoch 26: loss 0.04287298643113787
Epoch 27: loss 0.04232179023712355
Epoch 28: loss 0.04141492219798797
Epoch 29: loss 0.04109316833961457
Epoch 30: loss 0.040381909835761565
Epoch 31: loss 0.03998508012401322
Epoch 32: loss 0.0395829069117705
Epoch 33: loss 0.03901305821202108
Epoch 34: loss 0.03876934435052464
Epoch 35: loss 0.0381561538159985
Epoch 36: loss 0.03782673450536284
Epoch 37: loss 0.03766040391754106
Epoch 38: loss 0.03719637644957789
Epoch 39: loss 0.036862274746310784
Epoch 40: loss 0.036536404923264945
Epoch 41: loss 0.03630543016039074
Epoch 42: loss 0.035990739763370506
Epoch 43: loss 0.035690430192202895
Epoch 44: loss 0.0355528918945227
Epoch 45: loss 0.035212535886149685
Epoch 46: loss 0.035051623513276904
Epoch 47: loss 0.034871917688948685
Epoch 48: loss 0.03459928680794712
Epoch 49: loss 0.03438379598731837
-----------Time: 0:11:43.634675, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2517681121826172-------------


Epoch 0: loss 16.129357566646807
Epoch 1: loss 16.129356447803772
Epoch 2: loss 16.12935573241839
Epoch 3: loss 16.12934308754743
Epoch 4: loss 16.129348418063092
Epoch 5: loss 16.129351547971375
Epoch 6: loss 16.129345238111537
Epoch 7: loss 16.12934290630264
Epoch 8: loss 16.129342131798783
Epoch 9: loss 16.129340600682188
Epoch 10: loss 16.129335139742878
Epoch 11: loss 16.129343004833427
Epoch 12: loss 16.129337887714712
Epoch 13: loss 16.129335184600315
Epoch 14: loss 16.12933288183207
Epoch 15: loss 16.129337319088346
Epoch 16: loss 16.129335988404115
Epoch 17: loss 16.12932169184599
Epoch 18: loss 16.129324429446164
Epoch 19: loss 16.129321399883707
Epoch 20: loss 16.129320426762526
Epoch 21: loss 16.129314916039238
Epoch 22: loss 16.129317268850752
Epoch 23: loss 16.129321984326857
Epoch 24: loss 16.12931760333685
Epoch 25: loss 16.129309608341234
Epoch 26: loss 16.12930966590396
Epoch 27: loss 16.129303462612945
Epoch 28: loss 16.129311194946222
Epoch 29: loss 16.129303169613497
Epoch 30: loss 16.1293029608838
Epoch 31: loss 16.12929023926254
Epoch 32: loss 16.129297471681713
Epoch 33: loss 16.129297470385254
Epoch 34: loss 16.129292476170733
Epoch 35: loss 16.1292904951833
Epoch 36: loss 16.129287192586844
Epoch 37: loss 16.129289982304616
Epoch 38: loss 16.12928123147414
Epoch 39: loss 16.12927799032978
Epoch 40: loss 16.129281319633265
Epoch 41: loss 16.129274641838716
Epoch 42: loss 16.12928284089678
Epoch 43: loss 16.129274340801228
Epoch 44: loss 16.12927139447136
Epoch 45: loss 16.12927334797389
Epoch 46: loss 16.129271404065147
Epoch 47: loss 16.12926598046382
Epoch 48: loss 16.129264972856863
Epoch 49: loss 16.129268800000123
-----------Time: 0:09:32.277867, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017167091369629-------------


Epoch 0: loss 16.12917413187779
Epoch 1: loss 16.12916844976279
Epoch 2: loss 16.129165288739518
Epoch 3: loss 16.12916527240415
Epoch 4: loss 16.129164829274895
Epoch 5: loss 16.129171378460832
Epoch 6: loss 16.129154470318227
Epoch 7: loss 16.129156076888666
Epoch 8: loss 16.129150529086687
Epoch 9: loss 16.129161682512656
Epoch 10: loss 16.12915347126789
Epoch 11: loss 16.12915004550795
Epoch 12: loss 16.12914859217882
Epoch 13: loss 16.12914925700235
Epoch 14: loss 16.12914071567944
Epoch 15: loss 16.129143932450393
Epoch 16: loss 16.129142436856743
Epoch 17: loss 16.129138136247114
Epoch 18: loss 16.129141375835722
Epoch 19: loss 16.129134118524565
Epoch 20: loss 16.12913340469493
Epoch 21: loss 16.129129037965956
Epoch 22: loss 16.12912833010003
Epoch 23: loss 16.12912298065609
Epoch 24: loss 16.129129014111136
Epoch 25: loss 16.129120701742664
Epoch 26: loss 16.12911343457843
Epoch 27: loss 16.12911987512121
Epoch 28: loss 16.129118766649835
Epoch 29: loss 16.129116723691723
Epoch 30: loss 16.12910890138428
Epoch 31: loss 16.129111048059013
Epoch 32: loss 16.129104164127686
Epoch 33: loss 16.129107390233134
Epoch 34: loss 16.12911087744517
Epoch 35: loss 16.129109196717355
Epoch 36: loss 16.129105715987603
Epoch 37: loss 16.129099434649834
Epoch 38: loss 16.129099432834792
Epoch 39: loss 16.129096735943396
Epoch 40: loss 16.12909096748431
Epoch 41: loss 16.129092569387495
Epoch 42: loss 16.129094590046538
Epoch 43: loss 16.129091845186203
Epoch 44: loss 16.129087695743547
Epoch 45: loss 16.12908548606097
Epoch 46: loss 16.129081146557606
Epoch 47: loss 16.12908298545327
Epoch 48: loss 16.129081449669428
Epoch 49: loss 16.12907200912343
-----------Time: 0:07:40.688031, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017167091369629-------------


Epoch 0: loss 16.12917033999818
Epoch 1: loss 16.129157526847006
Epoch 2: loss 16.12916287317945
Epoch 3: loss 16.12916979444876
Epoch 4: loss 16.129162472055423
Epoch 5: loss 16.129157399534854
Epoch 6: loss 16.129154525288037
Epoch 7: loss 16.12915674249007
Epoch 8: loss 16.129148059075394
Epoch 9: loss 16.129150195637756
Epoch 10: loss 16.12914570626388
Epoch 11: loss 16.12914832329348
Epoch 12: loss 16.12914752104543
Epoch 13: loss 16.12914229787647
Epoch 14: loss 16.129136998475797
Epoch 15: loss 16.12914317687482
Epoch 16: loss 16.129142181973148
Epoch 17: loss 16.129139579982457
Epoch 18: loss 16.129134856468312
Epoch 19: loss 16.129130840042222
Epoch 20: loss 16.129136361655753
Epoch 21: loss 16.129133379025067
Epoch 22: loss 16.129131649550438
Epoch 23: loss 16.129125102698122
Epoch 24: loss 16.12912449491873
Epoch 25: loss 16.129117388774546
Epoch 26: loss 16.129113227923064
Epoch 27: loss 16.12911462706026
Epoch 28: loss 16.129128565536757
Epoch 29: loss 16.12912247218536
Epoch 30: loss 16.12911049732376
Epoch 31: loss 16.129117884280696
Epoch 32: loss 16.129112520835008
Epoch 33: loss 16.129109231462422
Epoch 34: loss 16.129105576748042
Epoch 35: loss 16.129107511062994
Epoch 36: loss 16.12909821883176
Epoch 37: loss 16.12909725582295
Epoch 38: loss 16.12910075703673
Epoch 39: loss 16.129095119260587
Epoch 40: loss 16.12909078390589
Epoch 41: loss 16.129094401541582
Epoch 42: loss 16.129082524432896
Epoch 43: loss 16.129091284857164
Epoch 44: loss 16.129087139822467
Epoch 45: loss 16.12908422434829
Epoch 46: loss 16.12908614258717
Epoch 47: loss 16.129074504545294
Epoch 48: loss 16.129080424171352
Epoch 49: loss 16.12907221344517
-----------Time: 0:11:35.934347, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017179489135742-------------


Epoch 0: loss 16.12917519186164
Epoch 1: loss 16.12916522884317
Epoch 2: loss 16.129167206719103
Epoch 3: loss 16.12917258675945
Epoch 4: loss 16.129172143630193
Epoch 5: loss 16.129171289783123
Epoch 6: loss 16.129162024258918
Epoch 7: loss 16.129160388388538
Epoch 8: loss 16.12916013272707
Epoch 9: loss 16.12916235278131
Epoch 10: loss 16.129155068763122
Epoch 11: loss 16.129155155625792
Epoch 12: loss 16.12915177861266
Epoch 13: loss 16.12915052623448
Epoch 14: loss 16.129151598145743
Epoch 15: loss 16.129151042743246
Epoch 16: loss 16.129140262697106
Epoch 17: loss 16.12913858922945
Epoch 18: loss 16.129146399609482
Epoch 19: loss 16.129139485859625
Epoch 20: loss 16.129137498908488
Epoch 21: loss 16.12913886718999
Epoch 22: loss 16.129129653264805
Epoch 23: loss 16.12913736226184
Epoch 24: loss 16.129128461042264
Epoch 25: loss 16.129132536846118
Epoch 26: loss 16.12912340330131
Epoch 27: loss 16.129126105897125
Epoch 28: loss 16.129118413494748
Epoch 29: loss 16.129119493444048
Epoch 30: loss 16.129114768892737
Epoch 31: loss 16.129118153943907
Epoch 32: loss 16.129119011421057
Epoch 33: loss 16.12911162394554
Epoch 34: loss 16.12910950423713
Epoch 35: loss 16.12911296344568
Epoch 36: loss 16.129111470185652
Epoch 37: loss 16.129109109336103
Epoch 38: loss 16.12910757510801
Epoch 39: loss 16.129100213302355
Epoch 40: loss 16.129099003966573
Epoch 41: loss 16.12909811822664
Epoch 42: loss 16.12910254796346
Epoch 43: loss 16.129093085896262
Epoch 44: loss 16.12909174302533
Epoch 45: loss 16.129086421844168
Epoch 46: loss 16.129085206285385
Epoch 47: loss 16.129084978627407
Epoch 48: loss 16.12908866730898
Epoch 49: loss 16.129081251829977
-----------Time: 0:10:28.701258, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017178058624268-------------


Epoch 0: loss 16.129176877256707
Epoch 1: loss 16.12917609082544
Epoch 2: loss 16.1291792951504
Epoch 3: loss 16.12917459445391
Epoch 4: loss 16.12916257836496
Epoch 5: loss 16.12917245607651
Epoch 6: loss 16.12916579954387
Epoch 7: loss 16.129167219942975
Epoch 8: loss 16.129161233678985
Epoch 9: loss 16.129157014227612
Epoch 10: loss 16.12915075026238
Epoch 11: loss 16.12915395977317
Epoch 12: loss 16.129153900136114
Epoch 13: loss 16.12914995553378
Epoch 14: loss 16.129154793395497
Epoch 15: loss 16.12914600444916
Epoch 16: loss 16.129143656304894
Epoch 17: loss 16.129144718103788
Epoch 18: loss 16.129141137028206
Epoch 19: loss 16.12913860452765
Epoch 20: loss 16.12913313684676
Epoch 21: loss 16.12912988688649
Epoch 22: loss 16.12913278809963
Epoch 23: loss 16.129134149898842
Epoch 24: loss 16.12913127798565
Epoch 25: loss 16.12912825153469
Epoch 26: loss 16.129124719724505
Epoch 27: loss 16.129125854643615
Epoch 28: loss 16.12912059258092
Epoch 29: loss 16.129120803384954
Epoch 30: loss 16.12912268298939
Epoch 31: loss 16.129115912887052
Epoch 32: loss 16.129116240112985
Epoch 33: loss 16.129106649955762
Epoch 34: loss 16.129109798014454
Epoch 35: loss 16.129106918581808
Epoch 36: loss 16.1291076674158
Epoch 37: loss 16.12909958996547
Epoch 38: loss 16.129100466111616
Epoch 39: loss 16.12909424544807
Epoch 40: loss 16.12910060068393
Epoch 41: loss 16.129092376733876
Epoch 42: loss 16.129095910618396
Epoch 43: loss 16.129088741725653
Epoch 44: loss 16.12909204924865
Epoch 45: loss 16.129084955291166
Epoch 46: loss 16.12908445874785
Epoch 47: loss 16.12908367879887
Epoch 48: loss 16.129091514848767
Epoch 49: loss 16.12908630334793
-----------Time: 0:14:06.314948, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129354350394436
Epoch 1: loss 16.12934191554963
Epoch 2: loss 16.129321220713248
Epoch 3: loss 16.129303161316166
Epoch 4: loss 16.129281387049065
Epoch 5: loss 16.129265097057516
Epoch 6: loss 16.129241540938892
Epoch 7: loss 16.129226207992446
Epoch 8: loss 16.12920211928898
Epoch 9: loss 16.12918912048506
Epoch 10: loss 16.12917427759964
Epoch 11: loss 16.129144023461727
Epoch 12: loss 16.12913150097638
Epoch 13: loss 16.129113082460503
Epoch 14: loss 16.129093654004045
Epoch 15: loss 16.12907020626929
Epoch 16: loss 16.129046680228488
Epoch 17: loss 16.129030400090013
Epoch 18: loss 16.12900705321967
Epoch 19: loss 16.128993975331824
Epoch 20: loss 16.128972259664614
Epoch 21: loss 16.128949978223243
Epoch 22: loss 16.128941135344263
Epoch 23: loss 16.128918730480116
Epoch 24: loss 16.128896793637217
Epoch 25: loss 16.1288768852321
Epoch 26: loss 16.128860230158047
Epoch 27: loss 16.128830757524863
Epoch 28: loss 16.128813753185096
Epoch 29: loss 16.128796610902224
Epoch 30: loss 16.128774579158616
Epoch 31: loss 16.128757751655684
Epoch 32: loss 16.128742483791417
Epoch 33: loss 16.128718353860595
Epoch 34: loss 16.12870326335175
Epoch 35: loss 16.12868035286908
Epoch 36: loss 16.128663941269792
Epoch 37: loss 16.128645209529726
Epoch 38: loss 16.128626217979527
Epoch 39: loss 16.12860524647906
Epoch 40: loss 16.12859089702546
Epoch 41: loss 16.128567004087117
Epoch 42: loss 16.128550947717248
Epoch 43: loss 16.128532979850085
Epoch 44: loss 16.128513261764965
Epoch 45: loss 16.128483608924157
Epoch 46: loss 16.128463866206342
Epoch 47: loss 16.12845156878606
Epoch 48: loss 16.128429585529968
Epoch 49: loss 16.12841321412087
-----------Time: 0:06:57.348186, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017117500305176-------------


Epoch 0: loss 16.12916594863651
Epoch 1: loss 16.12914791957654
Epoch 2: loss 16.129133554306154
Epoch 3: loss 16.12911175903644
Epoch 4: loss 16.12908654760057
Epoch 5: loss 16.129066565815943
Epoch 6: loss 16.12905016847769
Epoch 7: loss 16.12902966292414
Epoch 8: loss 16.12900849669572
Epoch 9: loss 16.128996381038814
Epoch 10: loss 16.128970527078483
Epoch 11: loss 16.128944352633802
Epoch 12: loss 16.12893778451958
Epoch 13: loss 16.12891443816782
Epoch 14: loss 16.128890316015745
Epoch 15: loss 16.1288738600776
Epoch 16: loss 16.128853732830418
Epoch 17: loss 16.128844048550363
Epoch 18: loss 16.128816088624077
Epoch 19: loss 16.128797349883136
Epoch 20: loss 16.128781249174416
Epoch 21: loss 16.12875762408424
Epoch 22: loss 16.128744202635094
Epoch 23: loss 16.128720930959297
Epoch 24: loss 16.128707602077235
Epoch 25: loss 16.128680391762813
Epoch 26: loss 16.128664090881017
Epoch 27: loss 16.128643939001137
Epoch 28: loss 16.12861706109848
Epoch 29: loss 16.12860371302884
Epoch 30: loss 16.128588992529036
Epoch 31: loss 16.128568006767534
Epoch 32: loss 16.12854400855682
Epoch 33: loss 16.1285289325683
Epoch 34: loss 16.128506519406823
Epoch 35: loss 16.128486982047914
Epoch 36: loss 16.128466505016434
Epoch 37: loss 16.128443039649852
Epoch 38: loss 16.12842243400976
Epoch 39: loss 16.128417137979877
Epoch 40: loss 16.128392774945954
Epoch 41: loss 16.128380529902564
Epoch 42: loss 16.12834395216236
Epoch 43: loss 16.12833156295287
Epoch 44: loss 16.128313262155356
Epoch 45: loss 16.12828940759216
Epoch 46: loss 16.128276732643382
Epoch 47: loss 16.128256298135714
Epoch 48: loss 16.12823613225409
Epoch 49: loss 16.128220501381655
-----------Time: 0:10:08.266593, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.0170745849609375-------------


Epoch 0: loss 16.129161653990586
Epoch 1: loss 16.129145575062356
Epoch 2: loss 16.12912579215435
Epoch 3: loss 16.1291039489157
Epoch 4: loss 16.129088474655358
Epoch 5: loss 16.12907032528411
Epoch 6: loss 16.12905075914384
Epoch 7: loss 16.12903183112007
Epoch 8: loss 16.1290116421615
Epoch 9: loss 16.12899336003298
Epoch 10: loss 16.128971552317267
Epoch 11: loss 16.128944758165783
Epoch 12: loss 16.128922844918414
Epoch 13: loss 16.12890891733216
Epoch 14: loss 16.128893356468446
Epoch 15: loss 16.128880249280655
Epoch 16: loss 16.12886276032498
Epoch 17: loss 16.128834328488075
Epoch 18: loss 16.12882053391839
Epoch 19: loss 16.128803172534155
Epoch 20: loss 16.12878043603612
Epoch 21: loss 16.12875627447173
Epoch 22: loss 16.12874235181202
Epoch 23: loss 16.128716842190865
Epoch 24: loss 16.128710246332446
Epoch 25: loss 16.12867772235632
Epoch 26: loss 16.12866065449012
Epoch 27: loss 16.128638215140196
Epoch 28: loss 16.128621952114965
Epoch 29: loss 16.128600756845895
Epoch 30: loss 16.12858206451814
Epoch 31: loss 16.128569924228536
Epoch 32: loss 16.1285445738124
Epoch 33: loss 16.1285273555571
Epoch 34: loss 16.12851038829602
Epoch 35: loss 16.12848049223974
Epoch 36: loss 16.128469657483084
Epoch 37: loss 16.12845183533777
Epoch 38: loss 16.12843275640632
Epoch 39: loss 16.128407805558453
Epoch 40: loss 16.128393846857218
Epoch 41: loss 16.128369755301545
Epoch 42: loss 16.128348026410467
Epoch 43: loss 16.128327191815938
Epoch 44: loss 16.128321462250586
Epoch 45: loss 16.12830025453552
Epoch 46: loss 16.12827469876047
Epoch 47: loss 16.12825961084454
Epoch 48: loss 16.12823008064863
Epoch 49: loss 16.12821764476666
-----------Time: 0:09:27.598393, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0170674324035645-------------


Epoch 0: loss 16.129165576293847
Epoch 1: loss 16.12915291534681
Epoch 2: loss 16.129128216789624
Epoch 3: loss 16.129114022133077
Epoch 4: loss 16.129088984422545
Epoch 5: loss 16.129067334356094
Epoch 6: loss 16.12905582388566
Epoch 7: loss 16.129034272609292
Epoch 8: loss 16.12901057413961
Epoch 9: loss 16.128990798491767
Epoch 10: loss 16.12897833849568
Epoch 11: loss 16.128962345392953
Epoch 12: loss 16.128932897651758
Epoch 13: loss 16.12892251276594
Epoch 14: loss 16.128896989920918
Epoch 15: loss 16.128882611685956
Epoch 16: loss 16.12886084156752
Epoch 17: loss 16.12883975364515
Epoch 18: loss 16.128818486293028
Epoch 19: loss 16.12880375256935
Epoch 20: loss 16.12878217251162
Epoch 21: loss 16.128758620800955
Epoch 22: loss 16.12874505440783
Epoch 23: loss 16.128720434934564
Epoch 24: loss 16.128706176233006
Epoch 25: loss 16.128683651835455
Epoch 26: loss 16.128667114220473
Epoch 27: loss 16.12865118723709
Epoch 28: loss 16.128624990493336
Epoch 29: loss 16.128606951580288
Epoch 30: loss 16.128591558737494
Epoch 31: loss 16.128568253094507
Epoch 32: loss 16.128547833366458
Epoch 33: loss 16.12852941951783
Epoch 34: loss 16.12850990238366
Epoch 35: loss 16.128483869512163
Epoch 36: loss 16.128479417994853
Epoch 37: loss 16.1284574274786
Epoch 38: loss 16.12843660066282
Epoch 39: loss 16.128415703060448
Epoch 40: loss 16.12839699621237
Epoch 41: loss 16.128373596187256
Epoch 42: loss 16.128358773785873
Epoch 43: loss 16.128334651633796
Epoch 44: loss 16.12831628964348
Epoch 45: loss 16.128297680029732
Epoch 46: loss 16.128276901960763
Epoch 47: loss 16.128260421908504
Epoch 48: loss 16.128243032520786
Epoch 49: loss 16.128222033535415
-----------Time: 0:12:49.244265, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017068386077881-------------


Epoch 0: loss 16.12916460395054
Epoch 1: loss 16.129141005826682
Epoch 2: loss 16.129131729671528
Epoch 3: loss 16.129105684354034
Epoch 4: loss 16.129090938443657
Epoch 5: loss 16.12906908587051
Epoch 6: loss 16.129049403308333
Epoch 7: loss 16.129034356360464
Epoch 8: loss 16.129007035587843
Epoch 9: loss 16.128992716989938
Epoch 10: loss 16.128973276087486
Epoch 11: loss 16.128956032162325
Epoch 12: loss 16.128930751236318
Epoch 13: loss 16.128917491325804
Epoch 14: loss 16.12889327868098
Epoch 15: loss 16.128878789728528
Epoch 16: loss 16.128862645199533
Epoch 17: loss 16.128823635563894
Epoch 18: loss 16.12881853711417
Epoch 19: loss 16.128802763631384
Epoch 20: loss 16.128781831802527
Epoch 21: loss 16.128758384845643
Epoch 22: loss 16.128743727353683
Epoch 23: loss 16.128723488092554
Epoch 24: loss 16.12870688669185
Epoch 25: loss 16.128679613110293
Epoch 26: loss 16.12866214775015
Epoch 27: loss 16.128637744266744
Epoch 28: loss 16.12862641996764
Epoch 29: loss 16.12860564967742
Epoch 30: loss 16.12858455216126
Epoch 31: loss 16.128566746351314
Epoch 32: loss 16.128546600694435
Epoch 33: loss 16.128529899725777
Epoch 34: loss 16.12850736236365
Epoch 35: loss 16.128494460275473
Epoch 36: loss 16.12847050744078
Epoch 37: loss 16.12844539816578
Epoch 38: loss 16.128424687512613
Epoch 39: loss 16.12841156165583
Epoch 40: loss 16.12839252602607
Epoch 41: loss 16.12837678936268
Epoch 42: loss 16.12835210402936
Epoch 43: loss 16.128333717147054
Epoch 44: loss 16.128306527316667
Epoch 45: loss 16.128299005268836
Epoch 46: loss 16.12828362357558
Epoch 47: loss 16.128261813526247
Epoch 48: loss 16.128236882643833
Epoch 49: loss 16.128217616763173
-----------Time: 0:11:52.008225, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017069339752197-------------


Epoch 0: loss 16.129260853232736
Epoch 1: loss 16.129064571345353
Epoch 2: loss 16.12887099542457
Epoch 3: loss 16.12868037931682
Epoch 4: loss 16.128488721894207
Epoch 5: loss 16.128289075180394
Epoch 6: loss 16.128104088551456
Epoch 7: loss 16.127902814264594
Epoch 8: loss 16.12771291069002
Epoch 9: loss 16.1275189523142
Epoch 10: loss 16.127326269134223
Epoch 11: loss 16.12713062691909
Epoch 12: loss 16.12694155022526
Epoch 13: loss 16.126748346906435
Epoch 14: loss 16.1265501522253
Epoch 15: loss 16.126353320121247
Epoch 16: loss 16.126168260631385
Epoch 17: loss 16.125971438121123
Epoch 18: loss 16.1257820144952
Epoch 19: loss 16.125589925352163
Epoch 20: loss 16.1253913938513
Epoch 21: loss 16.1252033348768
Epoch 22: loss 16.12500308686589
Epoch 23: loss 16.12480914456615
Epoch 24: loss 16.124617455509966
Epoch 25: loss 16.124427143810493
Epoch 26: loss 16.124230136425357
Epoch 27: loss 16.12403600873216
Epoch 28: loss 16.123845241976017
Epoch 29: loss 16.12365030529319
Epoch 30: loss 16.123461184260506
Epoch 31: loss 16.123265246193718
Epoch 32: loss 16.123075413665024
Epoch 33: loss 16.122876533935614
Epoch 34: loss 16.122682620676528
Epoch 35: loss 16.12249026627823
Epoch 36: loss 16.122299048354794
Epoch 37: loss 16.122105738726436
Epoch 38: loss 16.121912720541776
Epoch 39: loss 16.12170952931249
Epoch 40: loss 16.12151383394259
Epoch 41: loss 16.121321528550396
Epoch 42: loss 16.12113213526159
Epoch 43: loss 16.120944565570245
Epoch 44: loss 16.120753528113724
Epoch 45: loss 16.12055160585728
Epoch 46: loss 16.12035532137698
Epoch 47: loss 16.120165666463
Epoch 48: loss 16.119973533240398
Epoch 49: loss 16.119779093360176
-----------Time: 0:09:25.615175, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.015994548797607-------------


Epoch 0: loss 16.12908216038756
Epoch 1: loss 16.128894202018184
Epoch 2: loss 16.1286966407863
Epoch 3: loss 16.128505724159645
Epoch 4: loss 16.128309808132634
Epoch 5: loss 16.128117538004094
Epoch 6: loss 16.127921503480845
Epoch 7: loss 16.127733720392555
Epoch 8: loss 16.127538891834302
Epoch 9: loss 16.127349994570228
Epoch 10: loss 16.127151494702936
Epoch 11: loss 16.126951026553495
Epoch 12: loss 16.12675978010799
Epoch 13: loss 16.12656696287428
Epoch 14: loss 16.12637989491208
Epoch 15: loss 16.126181990637477
Epoch 16: loss 16.125990000284517
Epoch 17: loss 16.125798962050123
Epoch 18: loss 16.125600277567248
Epoch 19: loss 16.12540949317928
Epoch 20: loss 16.125213633418536
Epoch 21: loss 16.12502963987623
Epoch 22: loss 16.124837563179184
Epoch 23: loss 16.12463907757293
Epoch 24: loss 16.1244446532502
Epoch 25: loss 16.124252967824102
Epoch 26: loss 16.124058376517613
Epoch 27: loss 16.12385773179063
Epoch 28: loss 16.12367063867715
Epoch 29: loss 16.12347487926224
Epoch 30: loss 16.12328700775553
Epoch 31: loss 16.123084626553045
Epoch 32: loss 16.122898677952463
Epoch 33: loss 16.122702370654505
Epoch 34: loss 16.122502045115418
Epoch 35: loss 16.122321862640298
Epoch 36: loss 16.122126660702214
Epoch 37: loss 16.12192849661748
Epoch 38: loss 16.12173509727424
Epoch 39: loss 16.121541605882506
Epoch 40: loss 16.121344921056053
Epoch 41: loss 16.12115158757287
Epoch 42: loss 16.12096169670358
Epoch 43: loss 16.120767689840246
Epoch 44: loss 16.120579679353266
Epoch 45: loss 16.120379490460827
Epoch 46: loss 16.12018961411186
Epoch 47: loss 16.119995238276655
Epoch 48: loss 16.1197972035784
Epoch 49: loss 16.11960414546284
-----------Time: 0:07:43.414016, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015995025634766-------------


Epoch 0: loss 16.129077300226772
Epoch 1: loss 16.128883418082673
Epoch 2: loss 16.128692218568936
Epoch 3: loss 16.128494557250512
Epoch 4: loss 16.128301612704654
Epoch 5: loss 16.128105086046045
Epoch 6: loss 16.127917637703145
Epoch 7: loss 16.12772298001802
Epoch 8: loss 16.127523650158484
Epoch 9: loss 16.12733026377982
Epoch 10: loss 16.127138819235572
Epoch 11: loss 16.12694014849515
Epoch 12: loss 16.12675432409522
Epoch 13: loss 16.126560712651496
Epoch 14: loss 16.12637063664804
Epoch 15: loss 16.126173068415284
Epoch 16: loss 16.125978746512715
Epoch 17: loss 16.125785009831173
Epoch 18: loss 16.125593029072
Epoch 19: loss 16.125397739752664
Epoch 20: loss 16.12520811725013
Epoch 21: loss 16.125005395338547
Epoch 22: loss 16.1248174820859
Epoch 23: loss 16.124626358803877
Epoch 24: loss 16.124431112267647
Epoch 25: loss 16.124237010762897
Epoch 26: loss 16.124042799837113
Epoch 27: loss 16.123854879324302
Epoch 28: loss 16.123658060703413
Epoch 29: loss 16.123465190055644
Epoch 30: loss 16.123266941441862
Epoch 31: loss 16.123073792833555
Epoch 32: loss 16.122883799803393
Epoch 33: loss 16.122685268302533
Epoch 34: loss 16.122503209853054
Epoch 35: loss 16.12229706192224
Epoch 36: loss 16.12210469507795
Epoch 37: loss 16.12191253203682
Epoch 38: loss 16.121722003051026
Epoch 39: loss 16.121528996275195
Epoch 40: loss 16.121336991142616
Epoch 41: loss 16.121137262233383
Epoch 42: loss 16.12095125866299
Epoch 43: loss 16.120755657415923
Epoch 44: loss 16.1205609629114
Epoch 45: loss 16.12037558190002
Epoch 46: loss 16.12017354140663
Epoch 47: loss 16.119984568948006
Epoch 48: loss 16.11978634185542
Epoch 49: loss 16.119592292986855
-----------Time: 0:11:33.767457, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015993595123291-------------


Epoch 0: loss 16.12908603861126
Epoch 1: loss 16.128887903826467
Epoch 2: loss 16.12869999913044
Epoch 3: loss 16.12850067290098
Epoch 4: loss 16.128310177882383
Epoch 5: loss 16.12812302176106
Epoch 6: loss 16.127929880672205
Epoch 7: loss 16.12773023888493
Epoch 8: loss 16.12754541742471
Epoch 9: loss 16.12734647728037
Epoch 10: loss 16.127148978537747
Epoch 11: loss 16.126954103825597
Epoch 12: loss 16.126758059967855
Epoch 13: loss 16.12657526850117
Epoch 14: loss 16.126372341489972
Epoch 15: loss 16.12618236090581
Epoch 16: loss 16.12598914903036
Epoch 17: loss 16.12579951926766
Epoch 18: loss 16.125607477834265
Epoch 19: loss 16.125407441923837
Epoch 20: loss 16.125216829705458
Epoch 21: loss 16.12502044825012
Epoch 22: loss 16.124830471036745
Epoch 23: loss 16.124628861226615
Epoch 24: loss 16.124448232770032
Epoch 25: loss 16.124252504210812
Epoch 26: loss 16.124053815838565
Epoch 27: loss 16.12385931554341
Epoch 28: loss 16.123666922510676
Epoch 29: loss 16.12347589231432
Epoch 30: loss 16.123283564623055
Epoch 31: loss 16.123088899677768
Epoch 32: loss 16.122891286587574
Epoch 33: loss 16.122705372213474
Epoch 34: loss 16.122503447882696
Epoch 35: loss 16.122314341629632
Epoch 36: loss 16.122120292761068
Epoch 37: loss 16.121930351070635
Epoch 38: loss 16.121737463828207
Epoch 39: loss 16.121539153503036
Epoch 40: loss 16.121345106968093
Epoch 41: loss 16.12115119785767
Epoch 42: loss 16.12095221415235
Epoch 43: loss 16.120764287935124
Epoch 44: loss 16.12057063085609
Epoch 45: loss 16.120379669371996
Epoch 46: loss 16.120183696560137
Epoch 47: loss 16.1199945311886
Epoch 48: loss 16.119804707475822
Epoch 49: loss 16.119599413392077
-----------Time: 0:10:33.614702, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.0159912109375-------------


Epoch 0: loss 16.129074753205888
Epoch 1: loss 16.1288881605251
Epoch 2: loss 16.128693771466022
Epoch 3: loss 16.128492228553114
Epoch 4: loss 16.12830808643675
Epoch 5: loss 16.128116579403237
Epoch 6: loss 16.12791485731987
Epoch 7: loss 16.127719426427348
Epoch 8: loss 16.127526208847485
Epoch 9: loss 16.127343724122703
Epoch 10: loss 16.12714444067635
Epoch 11: loss 16.126943705715906
Epoch 12: loss 16.12675545745858
Epoch 13: loss 16.126558817748858
Epoch 14: loss 16.12636911823673
Epoch 15: loss 16.12617573496957
Epoch 16: loss 16.12597562153097
Epoch 17: loss 16.125784303261703
Epoch 18: loss 16.125589832266495
Epoch 19: loss 16.12540384632793
Epoch 20: loss 16.1252082930498
Epoch 21: loss 16.125013819461675
Epoch 22: loss 16.12482237621388
Epoch 23: loss 16.12462839346466
Epoch 24: loss 16.12443301961628
Epoch 25: loss 16.124239540151954
Epoch 26: loss 16.124041342359316
Epoch 27: loss 16.123849025039714
Epoch 28: loss 16.123661978080133
Epoch 29: loss 16.123468774242724
Epoch 30: loss 16.123271720703688
Epoch 31: loss 16.123084149456595
Epoch 32: loss 16.122889926344108
Epoch 33: loss 16.122697026396395
Epoch 34: loss 16.122502568365764
Epoch 35: loss 16.122311517685375
Epoch 36: loss 16.12210844521162
Epoch 37: loss 16.121919365406296
Epoch 38: loss 16.121729692082614
Epoch 39: loss 16.12153140742731
Epoch 40: loss 16.12134296496061
Epoch 41: loss 16.121141827579685
Epoch 42: loss 16.12095992185293
Epoch 43: loss 16.120758819735652
Epoch 44: loss 16.120563739923888
Epoch 45: loss 16.120377494693773
Epoch 46: loss 16.12018061358362
Epoch 47: loss 16.11998887759567
Epoch 48: loss 16.11979872043396
Epoch 49: loss 16.119603148746126
-----------Time: 0:13:51.786821, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015988826751709-------------


Epoch 0: loss 16.128262727528956
Epoch 1: loss 16.1263293750653
Epoch 2: loss 16.12438834601605
Epoch 3: loss 16.122456162179404
Epoch 4: loss 16.12051709778222
Epoch 5: loss 16.11858543538088
Epoch 6: loss 16.116649905386797
Epoch 7: loss 16.114719045233507
Epoch 8: loss 16.11278218351803
Epoch 9: loss 16.110849539179593
Epoch 10: loss 16.108913768822244
Epoch 11: loss 16.106977918084514
Epoch 12: loss 16.105048655167167
Epoch 13: loss 16.10311329863913
Epoch 14: loss 16.101182442634503
Epoch 15: loss 16.099245425344098
Epoch 16: loss 16.097313972191035
Epoch 17: loss 16.095374315831243
Epoch 18: loss 16.093447712726604
Epoch 19: loss 16.091517543326
Epoch 20: loss 16.089578548678244
Epoch 21: loss 16.087642281518285
Epoch 22: loss 16.085720100112425
Epoch 23: loss 16.083781824739425
Epoch 24: loss 16.08185761904444
Epoch 25: loss 16.07992131013855
Epoch 26: loss 16.07798393943376
Epoch 27: loss 16.07605465784742
Epoch 28: loss 16.074115392758575
Epoch 29: loss 16.072194438838906
Epoch 30: loss 16.070266091220724
Epoch 31: loss 16.068326113598705
Epoch 32: loss 16.06639860919664
Epoch 33: loss 16.064458679024973
Epoch 34: loss 16.062532884909967
Epoch 35: loss 16.06059787998127
Epoch 36: loss 16.058667755956687
Epoch 37: loss 16.0567384266607
Epoch 38: loss 16.054805619746464
Epoch 39: loss 16.052877502638466
Epoch 40: loss 16.050950765739387
Epoch 41: loss 16.049019807833183
Epoch 42: loss 16.047086056319834
Epoch 43: loss 16.04516292357328
Epoch 44: loss 16.043227598938106
Epoch 45: loss 16.041298917093116
Epoch 46: loss 16.03936707137265
Epoch 47: loss 16.037437377512745
Epoch 48: loss 16.03551333828294
Epoch 49: loss 16.0335810284306
-----------Time: 0:07:08.910984, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005191326141357-------------


Epoch 0: loss 16.128256323805576
Epoch 1: loss 16.12632679355864
Epoch 2: loss 16.12439507618749
Epoch 3: loss 16.122455197614844
Epoch 4: loss 16.12052853279881
Epoch 5: loss 16.11859384679872
Epoch 6: loss 16.1166518259593
Epoch 7: loss 16.11472163088883
Epoch 8: loss 16.11278781584905
Epoch 9: loss 16.11085296442095
Epoch 10: loss 16.10892231325665
Epoch 11: loss 16.106987057593027
Epoch 12: loss 16.105061057081944
Epoch 13: loss 16.103119870902017
Epoch 14: loss 16.10118720322734
Epoch 15: loss 16.099257662868034
Epoch 16: loss 16.09732353408548
Epoch 17: loss 16.095387037452504
Epoch 18: loss 16.09345894782941
Epoch 19: loss 16.09152391204502
Epoch 20: loss 16.089596085862144
Epoch 21: loss 16.087660131926377
Epoch 22: loss 16.085726267880496
Epoch 23: loss 16.083800968753053
Epoch 24: loss 16.081862278772867
Epoch 25: loss 16.07993698794275
Epoch 26: loss 16.07799801118611
Epoch 27: loss 16.07607960273158
Epoch 28: loss 16.074135653800198
Epoch 29: loss 16.072208769364224
Epoch 30: loss 16.070268738068854
Epoch 31: loss 16.068346249143218
Epoch 32: loss 16.066416094522232
Epoch 33: loss 16.06448628035105
Epoch 34: loss 16.062551830046978
Epoch 35: loss 16.060624265489274
Epoch 36: loss 16.058691830399116
Epoch 37: loss 16.05676711130687
Epoch 38: loss 16.05483012409428
Epoch 39: loss 16.052904436029515
Epoch 40: loss 16.05097132341054
Epoch 41: loss 16.049036968525755
Epoch 42: loss 16.047107661788136
Epoch 43: loss 16.04518308219474
Epoch 44: loss 16.043257393870682
Epoch 45: loss 16.041331663800687
Epoch 46: loss 16.0393964529945
Epoch 47: loss 16.03746656051727
Epoch 48: loss 16.035543451366248
Epoch 49: loss 16.03360825585826
-----------Time: 0:09:32.065846, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.005195140838623-------------


Epoch 0: loss 16.128215992301623
Epoch 1: loss 16.126284518145944
Epoch 2: loss 16.124346790655984
Epoch 3: loss 16.12240914562074
Epoch 4: loss 16.120470522019186
Epoch 5: loss 16.11854178676014
Epoch 6: loss 16.116606718305015
Epoch 7: loss 16.114669492803493
Epoch 8: loss 16.112736592025716
Epoch 9: loss 16.110806660914044
Epoch 10: loss 16.10887230914076
Epoch 11: loss 16.106940430749557
Epoch 12: loss 16.10500941968859
Epoch 13: loss 16.10306879565274
Epoch 14: loss 16.10113997508677
Epoch 15: loss 16.099199636012333
Epoch 16: loss 16.09727114915459
Epoch 17: loss 16.09533761700189
Epoch 18: loss 16.093399393487196
Epoch 19: loss 16.09147057110619
Epoch 20: loss 16.089540898508194
Epoch 21: loss 16.08760836696158
Epoch 22: loss 16.08567493152463
Epoch 23: loss 16.08374137629498
Epoch 24: loss 16.081806722965624
Epoch 25: loss 16.07987945166666
Epoch 26: loss 16.077945755900995
Epoch 27: loss 16.07600919963096
Epoch 28: loss 16.07407902219232
Epoch 29: loss 16.072142214409485
Epoch 30: loss 16.070213825563947
Epoch 31: loss 16.06828732539805
Epoch 32: loss 16.066355359884888
Epoch 33: loss 16.064420812346484
Epoch 34: loss 16.062492699905736
Epoch 35: loss 16.060563162139346
Epoch 36: loss 16.058630520134532
Epoch 37: loss 16.056699944683363
Epoch 38: loss 16.054769030078848
Epoch 39: loss 16.05283906085132
Epoch 40: loss 16.05091088877352
Epoch 41: loss 16.048980023175105
Epoch 42: loss 16.047044683241726
Epoch 43: loss 16.045112878230036
Epoch 44: loss 16.043188670979305
Epoch 45: loss 16.041255886623432
Epoch 46: loss 16.039318993274385
Epoch 47: loss 16.03739837695231
Epoch 48: loss 16.035459180575728
Epoch 49: loss 16.033534273237816
-----------Time: 0:09:17.366194, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.005197048187256-------------


Epoch 0: loss 16.128208851412367
Epoch 1: loss 16.126278005520113
Epoch 2: loss 16.1243360886566
Epoch 3: loss 16.122416320217894
Epoch 4: loss 16.120458832637585
Epoch 5: loss 16.118533520286274
Epoch 6: loss 16.116608042247663
Epoch 7: loss 16.11467107940848
Epoch 8: loss 16.112740105944784
Epoch 9: loss 16.11079818389544
Epoch 10: loss 16.108875655038904
Epoch 11: loss 16.10693529470256
Epoch 12: loss 16.104999309392518
Epoch 13: loss 16.103068763241293
Epoch 14: loss 16.101137854341193
Epoch 15: loss 16.099199623307047
Epoch 16: loss 16.09726947853914
Epoch 17: loss 16.095331604290163
Epoch 18: loss 16.09340184327375
Epoch 19: loss 16.091473057712143
Epoch 20: loss 16.08954323861442
Epoch 21: loss 16.08761198122648
Epoch 22: loss 16.085679043888593
Epoch 23: loss 16.083742260738454
Epoch 24: loss 16.081808067392306
Epoch 25: loss 16.079882026950322
Epoch 26: loss 16.077944784854143
Epoch 27: loss 16.076017277859165
Epoch 28: loss 16.07408289678593
Epoch 29: loss 16.072161565078478
Epoch 30: loss 16.070221887456782
Epoch 31: loss 16.068297828002233
Epoch 32: loss 16.0663542703418
Epoch 33: loss 16.06442208106003
Epoch 34: loss 16.06249141200461
Epoch 35: loss 16.060566694208823
Epoch 36: loss 16.058637274679377
Epoch 37: loss 16.056704238810703
Epoch 38: loss 16.05477785999325
Epoch 39: loss 16.052848966307067
Epoch 40: loss 16.05091752352567
Epoch 41: loss 16.048981295518736
Epoch 42: loss 16.04704998393886
Epoch 43: loss 16.045122788871613
Epoch 44: loss 16.04318327175138
Epoch 45: loss 16.041262416881086
Epoch 46: loss 16.039329191729582
Epoch 47: loss 16.03740006105092
Epoch 48: loss 16.03546814713678
Epoch 49: loss 16.033535699341336
-----------Time: 0:12:50.429229, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005200386047363-------------


Epoch 0: loss 16.128219149954102
Epoch 1: loss 16.126284024454836
Epoch 2: loss 16.124347157034943
Epoch 3: loss 16.12242292722585
Epoch 4: loss 16.12048121235045
Epoch 5: loss 16.118552663522028
Epoch 6: loss 16.116614615029132
Epoch 7: loss 16.114676625136127
Epoch 8: loss 16.112741990994344
Epoch 9: loss 16.110814816151837
Epoch 10: loss 16.10887896437694
Epoch 11: loss 16.10694161130398
Epoch 12: loss 16.1050121959232
Epoch 13: loss 16.103072093322655
Epoch 14: loss 16.10114660646813
Epoch 15: loss 16.099209959705348
Epoch 16: loss 16.097276857458034
Epoch 17: loss 16.095342068519198
Epoch 18: loss 16.093407288396275
Epoch 19: loss 16.091477062729403
Epoch 20: loss 16.08955076921887
Epoch 21: loss 16.087614595144583
Epoch 22: loss 16.08567984198798
Epoch 23: loss 16.083752960922798
Epoch 24: loss 16.081817449338413
Epoch 25: loss 16.079881458583248
Epoch 26: loss 16.077949575524798
Epoch 27: loss 16.076018839831452
Epoch 28: loss 16.074081972930145
Epoch 29: loss 16.072161076054616
Epoch 30: loss 16.070224794892916
Epoch 31: loss 16.06828533300179
Epoch 32: loss 16.06635925262891
Epoch 33: loss 16.064428786339484
Epoch 34: loss 16.062497117974434
Epoch 35: loss 16.060557491951755
Epoch 36: loss 16.058623254266752
Epoch 37: loss 16.056696211403644
Epoch 38: loss 16.0547591829637
Epoch 39: loss 16.052829421687996
Epoch 40: loss 16.05090404866246
Epoch 41: loss 16.048971449440753
Epoch 42: loss 16.047042429479585
Epoch 43: loss 16.045110377103754
Epoch 44: loss 16.043174058604073
Epoch 45: loss 16.041244153680847
Epoch 46: loss 16.039309147455693
Epoch 47: loss 16.03737551106779
Epoch 48: loss 16.035440239846675
Epoch 49: loss 16.03350997373032
-----------Time: 0:11:33.666039, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005195140838623-------------


Epoch 0: loss 16.119870197779463
Epoch 1: loss 16.100537868217646
Epoch 2: loss 16.081201387138844
Epoch 3: loss 16.0619066579631
Epoch 4: loss 16.04261028047098
Epoch 5: loss 16.023326232292007
Epoch 6: loss 16.004053546268697
Epoch 7: loss 15.98480150114913
Epoch 8: loss 15.965560273672978
Epoch 9: loss 15.94633684163511
Epoch 10: loss 15.927122794654851
Epoch 11: loss 15.907918259785058
Epoch 12: loss 15.888724150250566
Epoch 13: loss 15.869548593804264
Epoch 14: loss 15.85039123988398
Epoch 15: loss 15.831251548644705
Epoch 16: loss 15.812111652305815
Epoch 17: loss 15.792979707142269
Epoch 18: loss 15.773862677465774
Epoch 19: loss 15.754771022319535
Epoch 20: loss 15.73568408420515
Epoch 21: loss 15.716596819124124
Epoch 22: loss 15.697532869020579
Epoch 23: loss 15.678472777434573
Epoch 24: loss 15.65944026176407
Epoch 25: loss 15.640419504965305
Epoch 26: loss 15.621403796832285
Epoch 27: loss 15.60240101567943
Epoch 28: loss 15.583415191934574
Epoch 29: loss 15.564432448054735
Epoch 30: loss 15.545468453806079
Epoch 31: loss 15.526526677472361
Epoch 32: loss 15.507601555175532
Epoch 33: loss 15.48866167789312
Epoch 34: loss 15.469746940222818
Epoch 35: loss 15.450846283639365
Epoch 36: loss 15.431954291023724
Epoch 37: loss 15.413081124530272
Epoch 38: loss 15.394218405930486
Epoch 39: loss 15.375368410766999
Epoch 40: loss 15.356526041367962
Epoch 41: loss 15.337696543201407
Epoch 42: loss 15.318882786624778
Epoch 43: loss 15.300072348720681
Epoch 44: loss 15.28127064920107
Epoch 45: loss 15.262492340890658
Epoch 46: loss 15.243707872331628
Epoch 47: loss 15.224949250991349
Epoch 48: loss 15.206200978754655
Epoch 49: loss 15.187457282754504
-----------Time: 0:09:41.632161, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.897508144378662-------------


Epoch 0: loss 16.119550130106692
Epoch 1: loss 16.10023714991741
Epoch 2: loss 16.08091279979372
Epoch 3: loss 16.061604888754218
Epoch 4: loss 16.042316563301853
Epoch 5: loss 16.023036941229616
Epoch 6: loss 16.00376536393697
Epoch 7: loss 15.98450489884294
Epoch 8: loss 15.965249518715472
Epoch 9: loss 15.946012064449938
Epoch 10: loss 15.926793896549094
Epoch 11: loss 15.907580824764349
Epoch 12: loss 15.888385170889393
Epoch 13: loss 15.869191183999805
Epoch 14: loss 15.85001042201637
Epoch 15: loss 15.830837786230074
Epoch 16: loss 15.811672527547632
Epoch 17: loss 15.792526449179636
Epoch 18: loss 15.773382374098146
Epoch 19: loss 15.754252461002467
Epoch 20: loss 15.73510859231705
Epoch 21: loss 15.715986905245758
Epoch 22: loss 15.696877637461776
Epoch 23: loss 15.677768201397578
Epoch 24: loss 15.658667218497163
Epoch 25: loss 15.639565672674797
Epoch 26: loss 15.62045299883703
Epoch 27: loss 15.601339289129525
Epoch 28: loss 15.58223499455745
Epoch 29: loss 15.563141305009719
Epoch 30: loss 15.5440307068008
Epoch 31: loss 15.524901643662819
Epoch 32: loss 15.5057826296281
Epoch 33: loss 15.486648402698926
Epoch 34: loss 15.467497419312702
Epoch 35: loss 15.448332396585569
Epoch 36: loss 15.429155619653947
Epoch 37: loss 15.409948062481861
Epoch 38: loss 15.390727736238166
Epoch 39: loss 15.371470752133177
Epoch 40: loss 15.35219114847064
Epoch 41: loss 15.332867882963495
Epoch 42: loss 15.313504935218434
Epoch 43: loss 15.29409336408498
Epoch 44: loss 15.274624347168185
Epoch 45: loss 15.255093729320981
Epoch 46: loss 15.235512323778826
Epoch 47: loss 15.215856822565108
Epoch 48: loss 15.19611030638769
Epoch 49: loss 15.1762922044809
-----------Time: 0:07:34.385796, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.8960366249084473-------------


Epoch 0: loss 16.119563438245432
Epoch 1: loss 16.10023283530604
Epoch 2: loss 16.08091916177116
Epoch 3: loss 16.06161009766214
Epoch 4: loss 16.042312114118168
Epoch 5: loss 16.02301311985574
Epoch 6: loss 16.003719488001828
Epoch 7: loss 15.984441423234633
Epoch 8: loss 15.965169964438227
Epoch 9: loss 15.945899433905563
Epoch 10: loss 15.926636152386731
Epoch 11: loss 15.907376924891262
Epoch 12: loss 15.888098868680688
Epoch 13: loss 15.868813964839601
Epoch 14: loss 15.849510244470112
Epoch 15: loss 15.830198617004843
Epoch 16: loss 15.810844035560894
Epoch 17: loss 15.791446923302074
Epoch 18: loss 15.772029165472267
Epoch 19: loss 15.752526995538044
Epoch 20: loss 15.732941450665605
Epoch 21: loss 15.71325060645804
Epoch 22: loss 15.69344251242716
Epoch 23: loss 15.673493528703167
Epoch 24: loss 15.65336297866506
Epoch 25: loss 15.633022942058155
Epoch 26: loss 15.61243851020195
Epoch 27: loss 15.591549293576147
Epoch 28: loss 15.570351766852855
Epoch 29: loss 15.548776292100817
Epoch 30: loss 15.526790130131914
Epoch 31: loss 15.504344614774654
Epoch 32: loss 15.481373673875154
Epoch 33: loss 15.457826209366484
Epoch 34: loss 15.43365642750892
Epoch 35: loss 15.408838849796297
Epoch 36: loss 15.383300523773492
Epoch 37: loss 15.356981335806418
Epoch 38: loss 15.329844413858966
Epoch 39: loss 15.301858037499516
Epoch 40: loss 15.272949576831628
Epoch 41: loss 15.243093217048003
Epoch 42: loss 15.2122540258726
Epoch 43: loss 15.180383144480304
Epoch 44: loss 15.147444615874878
Epoch 45: loss 15.113393618919202
Epoch 46: loss 15.078224581437892
Epoch 47: loss 15.04190060572497
Epoch 48: loss 15.004390372991432
Epoch 49: loss 14.965687166808285
-----------Time: 0:11:37.293366, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.867753744125366-------------


Epoch 0: loss 16.119588953311705
Epoch 1: loss 16.100269706045694
Epoch 2: loss 16.080940435346278
Epoch 3: loss 16.061634199589474
Epoch 4: loss 16.04232138430962
Epoch 5: loss 16.023027584434786
Epoch 6: loss 16.00371752257189
Epoch 7: loss 15.984403217662088
Epoch 8: loss 15.96508043054785
Epoch 9: loss 15.945739677122198
Epoch 10: loss 15.926357939555762
Epoch 11: loss 15.906914099848354
Epoch 12: loss 15.887407578224071
Epoch 13: loss 15.867772880489895
Epoch 14: loss 15.847987323043267
Epoch 15: loss 15.828008685182008
Epoch 16: loss 15.807732599983401
Epoch 17: loss 15.787115078636198
Epoch 18: loss 15.76608940594348
Epoch 19: loss 15.744514568270777
Epoch 20: loss 15.7222975896324
Epoch 21: loss 15.699320803004934
Epoch 22: loss 15.67545171170341
Epoch 23: loss 15.650589717607772
Epoch 24: loss 15.62458036981762
Epoch 25: loss 15.597317107803734
Epoch 26: loss 15.56868079021095
Epoch 27: loss 15.538564633260025
Epoch 28: loss 15.506862700795791
Epoch 29: loss 15.47345935682553
Epoch 30: loss 15.4383062126197
Epoch 31: loss 15.401308552605617
Epoch 32: loss 15.362414413719737
Epoch 33: loss 15.321555823201132
Epoch 34: loss 15.278700264333835
Epoch 35: loss 15.233826279964312
Epoch 36: loss 15.186894207560803
Epoch 37: loss 15.137884727050714
Epoch 38: loss 15.086759212549904
Epoch 39: loss 15.033506636907381
Epoch 40: loss 14.978177958172648
Epoch 41: loss 14.920734412777742
Epoch 42: loss 14.861165783079892
Epoch 43: loss 14.79946838039753
Epoch 44: loss 14.735694463127265
Epoch 45: loss 14.669812969438015
Epoch 46: loss 14.601859040854093
Epoch 47: loss 14.531820283498758
Epoch 48: loss 14.459707257020337
Epoch 49: loss 14.385556874163711
-----------Time: 0:10:21.565339, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7899773120880127-------------


Epoch 0: loss 16.119583811819584
Epoch 1: loss 16.100253780618058
Epoch 2: loss 16.08093563352609
Epoch 3: loss 16.061605212868653
Epoch 4: loss 16.04227699670797
Epoch 5: loss 16.022946244416644
Epoch 6: loss 16.003606430404272
Epoch 7: loss 15.984230598462153
Epoch 8: loss 15.964796048393064
Epoch 9: loss 15.945259508586176
Epoch 10: loss 15.925554475167187
Epoch 11: loss 15.90561755109313
Epoch 12: loss 15.885327100624139
Epoch 13: loss 15.864551233558178
Epoch 14: loss 15.843078544050407
Epoch 15: loss 15.820762349575222
Epoch 16: loss 15.79732981630484
Epoch 17: loss 15.77255494365101
Epoch 18: loss 15.746220870533996
Epoch 19: loss 15.71808274198058
Epoch 20: loss 15.687969308109501
Epoch 21: loss 15.655682124541855
Epoch 22: loss 15.621073837705511
Epoch 23: loss 15.584035884303331
Epoch 24: loss 15.544416761839111
Epoch 25: loss 15.502174966033222
Epoch 26: loss 15.457213089348118
Epoch 27: loss 15.409481865352364
Epoch 28: loss 15.358928903929236
Epoch 29: loss 15.305547856065875
Epoch 30: loss 15.249298299246991
Epoch 31: loss 15.190173327501473
Epoch 32: loss 15.128139528779636
Epoch 33: loss 15.06318852537154
Epoch 34: loss 14.995327799912703
Epoch 35: loss 14.92458047757919
Epoch 36: loss 14.850947260791806
Epoch 37: loss 14.774459398849428
Epoch 38: loss 14.695097535637943
Epoch 39: loss 14.612924561285338
Epoch 40: loss 14.52790055163468
Epoch 41: loss 14.44011690773479
Epoch 42: loss 14.349568499761668
Epoch 43: loss 14.256330100397108
Epoch 44: loss 14.160334843017926
Epoch 45: loss 14.061668440334948
Epoch 46: loss 13.96036277302197
Epoch 47: loss 13.856460854435435
Epoch 48: loss 13.75004838847025
Epoch 49: loss 13.641070151730942
-----------Time: 0:14:05.599529, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.688570261001587-------------


Epoch 0: loss 16.03372580723765
Epoch 1: loss 15.841888745625814
Epoch 2: loss 15.651264544866084
Epoch 3: loss 15.461916612632383
Epoch 4: loss 15.273910366626717
Epoch 5: loss 15.087118183548778
Epoch 6: loss 14.90146551967121
Epoch 7: loss 14.716983558951414
Epoch 8: loss 14.533444374524231
Epoch 9: loss 14.350688977109277
Epoch 10: loss 14.16855373273666
Epoch 11: loss 13.986510441185276
Epoch 12: loss 13.804113986247126
Epoch 13: loss 13.620611410675131
Epoch 14: loss 13.4348065374726
Epoch 15: loss 13.245604168921465
Epoch 16: loss 13.051590621309348
Epoch 17: loss 12.851006077708856
Epoch 18: loss 12.64215482662008
Epoch 19: loss 12.423196639101942
Epoch 20: loss 12.192699028396813
Epoch 21: loss 11.948965206685566
Epoch 22: loss 11.690785531702089
Epoch 23: loss 11.41747425067937
Epoch 24: loss 11.12841198533822
Epoch 25: loss 10.823422568851738
Epoch 26: loss 10.502500706486497
Epoch 27: loss 10.16604904664347
Epoch 28: loss 9.814806575941093
Epoch 29: loss 9.449965694535356
Epoch 30: loss 9.07248066689023
Epoch 31: loss 8.683858789523832
Epoch 32: loss 8.28577334677285
Epoch 33: loss 7.880008991416736
Epoch 34: loss 7.468531332436044
Epoch 35: loss 7.0534090295701395
Epoch 36: loss 6.637143656635751
Epoch 37: loss 6.221989441981064
Epoch 38: loss 5.810312331273804
Epoch 39: loss 5.405107963467629
Epoch 40: loss 5.008724405300105
Epoch 41: loss 4.624006933981858
Epoch 42: loss 4.253698370973982
Epoch 43: loss 3.9005015962729317
Epoch 44: loss 3.567049711861644
Epoch 45: loss 3.2558508730245843
Epoch 46: loss 2.96911579445045
Epoch 47: loss 2.708637135258312
Epoch 48: loss 2.475611792659552
Epoch 49: loss 2.270563080914971
-----------Time: 0:06:51.507439, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.486880898475647-------------


Epoch 0: loss 16.033483538438343
Epoch 1: loss 15.841619407307395
Epoch 2: loss 15.650604106732981
Epoch 3: loss 15.459925032880658
Epoch 4: loss 15.267584894843047
Epoch 5: loss 15.068804575996337
Epoch 6: loss 14.854810364418798
Epoch 7: loss 14.61334788352008
Epoch 8: loss 14.331438947203628
Epoch 9: loss 13.999214236667585
Epoch 10: loss 13.610500962401034
Epoch 11: loss 13.162761282959732
Epoch 12: loss 12.656809349174146
Epoch 13: loss 12.095612842275631
Epoch 14: loss 11.484313053952539
Epoch 15: loss 10.828692411844855
Epoch 16: loss 10.13552322460297
Epoch 17: loss 9.41334450186563
Epoch 18: loss 8.67092524254173
Epoch 19: loss 7.917390775654613
Epoch 20: loss 7.163024196033571
Epoch 21: loss 6.418612808167383
Epoch 22: loss 5.694949717285712
Epoch 23: loss 5.003987215601147
Epoch 24: loss 4.357432555737478
Epoch 25: loss 3.7669188404549976
Epoch 26: loss 3.24262182786453
Epoch 27: loss 2.7925772193724074
Epoch 28: loss 2.4211003771938016
Epoch 29: loss 2.1262662437961697
Epoch 30: loss 1.8998578605670007
Epoch 31: loss 1.729247190402603
Epoch 32: loss 1.6005049683984172
Epoch 33: loss 1.5016151279972714
Epoch 34: loss 1.4237112882119147
Epoch 35: loss 1.3607939095066708
Epoch 36: loss 1.3088696558492348
Epoch 37: loss 1.265373090854347
Epoch 38: loss 1.2284803001079434
Epoch 39: loss 1.197001143147467
Epoch 40: loss 1.1699825871762146
Epoch 41: loss 1.1466774922178258
Epoch 42: loss 1.1265434783849462
Epoch 43: loss 1.1090695746020172
Epoch 44: loss 1.0938946027156773
Epoch 45: loss 1.0806922229202367
Epoch 46: loss 1.0691782316965794
Epoch 47: loss 1.0591203220160257
Epoch 48: loss 1.0503312493550383
Epoch 49: loss 1.0426254776620942
-----------Time: 0:09:17.927357, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.0294880867004395-------------


Epoch 0: loss 16.03339276487507
Epoch 1: loss 15.84103778706354
Epoch 2: loss 15.645173482495588
Epoch 3: loss 15.422761368971925
Epoch 4: loss 15.119368913836166
Epoch 5: loss 14.683901717314065
Epoch 6: loss 14.099619133178146
Epoch 7: loss 13.37410145728986
Epoch 8: loss 12.52358433965628
Epoch 9: loss 11.568037795138917
Epoch 10: loss 10.53019630189432
Epoch 11: loss 9.435495520234433
Epoch 12: loss 8.311062871016128
Epoch 13: loss 7.187720111688237
Epoch 14: loss 6.097674599803615
Epoch 15: loss 5.0743094993195115
Epoch 16: loss 4.151040092961434
Epoch 17: loss 3.3592546217593244
Epoch 18: loss 2.7223565354173505
Epoch 19: loss 2.2461817612979904
Epoch 20: loss 1.913531387156932
Epoch 21: loss 1.6888202968792398
Epoch 22: loss 1.5346782361119256
Epoch 23: loss 1.4238391123497336
Epoch 24: loss 1.3403724363441374
Epoch 25: loss 1.2753568451128425
Epoch 26: loss 1.2236305255485398
Epoch 27: loss 1.181955309909864
Epoch 28: loss 1.1481591734534828
Epoch 29: loss 1.120566988214972
Epoch 30: loss 1.0979612126111855
Epoch 31: loss 1.0793789721916005
Epoch 32: loss 1.0640609796407625
Epoch 33: loss 1.0513771464800044
Epoch 34: loss 1.0408357993586677
Epoch 35: loss 1.0320575047667224
Epoch 36: loss 1.024697618892233
Epoch 37: loss 1.018523023820818
Epoch 38: loss 1.013332670638586
Epoch 39: loss 1.0089398727039722
Epoch 40: loss 1.005215613251474
Epoch 41: loss 1.002056059952726
Epoch 42: loss 0.9993510980071939
Epoch 43: loss 0.9970488447965391
Epoch 44: loss 0.9950717314064924
Epoch 45: loss 0.9933781192444018
Epoch 46: loss 0.9919058853449414
Epoch 47: loss 0.9906374656713287
Epoch 48: loss 0.9895438133957206
Epoch 49: loss 0.9885907304909256
-----------Time: 0:09:31.300328, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0035182237625122-------------


Epoch 0: loss 16.03343882490721
Epoch 1: loss 15.838528912633965
Epoch 2: loss 15.604482531482724
Epoch 3: loss 15.213677463095884
Epoch 4: loss 14.58217300339326
Epoch 5: loss 13.718899296184413
Epoch 6: loss 12.660302125091718
Epoch 7: loss 11.445753165468565
Epoch 8: loss 10.119107948301147
Epoch 9: loss 8.72940999078258
Epoch 10: loss 7.331818591906623
Epoch 11: loss 5.98515694461613
Epoch 12: loss 4.751426310238467
Epoch 13: loss 3.690403465151203
Epoch 14: loss 2.851713808092363
Epoch 15: loss 2.2527866698659715
Epoch 16: loss 1.8648702929433498
Epoch 17: loss 1.622757161389621
Epoch 18: loss 1.4654547404892877
Epoch 19: loss 1.3557630185850162
Epoch 20: loss 1.2750697252768548
Epoch 21: loss 1.2138795539728644
Epoch 22: loss 1.1666319969881482
Epoch 23: loss 1.129853967563039
Epoch 24: loss 1.1010425883735762
Epoch 25: loss 1.0783660373265098
Epoch 26: loss 1.0603959651956616
Epoch 27: loss 1.0461168991540553
Epoch 28: loss 1.0346932737415027
Epoch 29: loss 1.0255165672645807
Epoch 30: loss 1.0180954859430469
Epoch 31: loss 1.0120854332856732
Epoch 32: loss 1.0071885515614136
Epoch 33: loss 1.0031652971899594
Epoch 34: loss 0.9998522846675425
Epoch 35: loss 0.9971180708917864
Epoch 36: loss 0.9948516098813819
Epoch 37: loss 0.992957776091292
Epoch 38: loss 0.9913674924702927
Epoch 39: loss 0.9900499175202918
Epoch 40: loss 0.9889349741673845
Epoch 41: loss 0.9879833200238463
Epoch 42: loss 0.9871756190058328
Epoch 43: loss 0.9864919855582966
Epoch 44: loss 0.985905485982283
Epoch 45: loss 0.9853989763095238
Epoch 46: loss 0.9849597625981342
Epoch 47: loss 0.9845821582279236
Epoch 48: loss 0.9842590341074042
Epoch 49: loss 0.9839671132290992
-----------Time: 0:12:31.922786, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0010520219802856-------------


Epoch 0: loss 16.033267714006218
Epoch 1: loss 15.827648023342944
Epoch 2: loss 15.478147816307976
Epoch 3: loss 14.801064536388703
Epoch 4: loss 13.79413119524053
Epoch 5: loss 12.52215907313112
Epoch 6: loss 11.053932899363083
Epoch 7: loss 9.46228426380479
Epoch 8: loss 7.828057552434103
Epoch 9: loss 6.240958671715028
Epoch 10: loss 4.795317014317723
Epoch 11: loss 3.5824821520396197
Epoch 12: loss 2.6719869139273573
Epoch 13: loss 2.0738363707435075
Epoch 14: loss 1.7191042056415573
Epoch 15: loss 1.508155822883552
Epoch 16: loss 1.3717893785338742
Epoch 17: loss 1.2765940020206248
Epoch 18: loss 1.2072389415420224
Epoch 19: loss 1.1556724060191093
Epoch 20: loss 1.1169032616061707
Epoch 21: loss 1.0875507753915667
Epoch 22: loss 1.065194241828151
Epoch 23: loss 1.0480307889023575
Epoch 24: loss 1.0347734281490393
Epoch 25: loss 1.024467740226142
Epoch 26: loss 1.0164020246238148
Epoch 27: loss 1.0100381562993732
Epoch 28: loss 1.0049873587552332
Epoch 29: loss 1.0009814808069202
Epoch 30: loss 0.9977540060869697
Epoch 31: loss 0.9951557860937114
Epoch 32: loss 0.9930443437673787
Epoch 33: loss 0.9913292267259014
Epoch 34: loss 0.9899212235079439
Epoch 35: loss 0.9887650669139646
Epoch 36: loss 0.9878167012841109
Epoch 37: loss 0.9870120453212234
Epoch 38: loss 0.9863568961847211
Epoch 39: loss 0.9857916579809184
Epoch 40: loss 0.9853175051222682
Epoch 41: loss 0.9849199248339056
Epoch 42: loss 0.9845741614496792
Epoch 43: loss 0.9842836656863434
Epoch 44: loss 0.9840310397454096
Epoch 45: loss 0.9838101638601551
Epoch 46: loss 0.9836243666858372
Epoch 47: loss 0.9834494882948441
Epoch 48: loss 0.9833101673318096
Epoch 49: loss 0.9831856028290791
-----------Time: 0:11:52.693722, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0004780292510986-------------


Epoch 0: loss 15.195248533034208
Epoch 1: loss 13.302535602098706
Epoch 2: loss 10.424180650270264
Epoch 3: loss 6.152154991327258
Epoch 4: loss 2.729607464600802
Epoch 5: loss 1.45646422120979
Epoch 6: loss 1.165837521492106
Epoch 7: loss 1.0662680610552504
Epoch 8: loss 1.0242641881764616
Epoch 9: loss 1.00515517623221
Epoch 10: loss 0.9958289131125139
Epoch 11: loss 0.9909493794513825
Epoch 12: loss 0.9882006048021789
Epoch 13: loss 0.9865522710702678
Epoch 14: loss 0.9854721767932193
Epoch 15: loss 0.9846743191630377
Epoch 16: loss 0.9840054111904655
Epoch 17: loss 0.983422696801098
Epoch 18: loss 0.9829257085246843
Epoch 19: loss 0.9824577354263391
Epoch 20: loss 0.9819540405416047
Epoch 21: loss 0.9814632951467824
Epoch 22: loss 0.9809708547468999
Epoch 23: loss 0.9804022132929023
Epoch 24: loss 0.9798476740580139
Epoch 25: loss 0.9792704448355093
Epoch 26: loss 0.9786235354658441
Epoch 27: loss 0.9779669195332561
Epoch 28: loss 0.9772630601132805
Epoch 29: loss 0.9764894682016627
Epoch 30: loss 0.9756952491449623
Epoch 31: loss 0.9748121516383038
Epoch 32: loss 0.9738813537725487
Epoch 33: loss 0.972871633766137
Epoch 34: loss 0.9718202118999353
Epoch 35: loss 0.9706923325402766
Epoch 36: loss 0.9694732861424219
Epoch 37: loss 0.9681946565075501
Epoch 38: loss 0.9668277198202653
Epoch 39: loss 0.9653677522098972
Epoch 40: loss 0.9638479002926128
Epoch 41: loss 0.9621792460602868
Epoch 42: loss 0.9605061492237985
Epoch 43: loss 0.9586876979368157
Epoch 44: loss 0.9567616192979486
Epoch 45: loss 0.9548019134784406
Epoch 46: loss 0.9527346894525847
Epoch 47: loss 0.9505757671649719
Epoch 48: loss 0.9483092895923197
Epoch 49: loss 0.946017277328264
-----------Time: 0:09:27.795753, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9948476552963257-------------


Epoch 0: loss 15.092917465658017
Epoch 1: loss 9.660968349963962
Epoch 2: loss 2.9897057058825447
Epoch 3: loss 1.3050180252736647
Epoch 4: loss 1.0768644849102804
Epoch 5: loss 1.0188456270246158
Epoch 6: loss 1.0012450642790596
Epoch 7: loss 0.9949449854029333
Epoch 8: loss 0.9924617299021036
Epoch 9: loss 0.9912353394114498
Epoch 10: loss 0.9905061166593729
Epoch 11: loss 0.9899980488388742
Epoch 12: loss 0.9896590140379272
Epoch 13: loss 0.989281713897732
Epoch 14: loss 0.9889537888645413
Epoch 15: loss 0.9886168324798265
Epoch 16: loss 0.9882406711643192
Epoch 17: loss 0.9878637956950638
Epoch 18: loss 0.9874913818878995
Epoch 19: loss 0.9869720970501777
Epoch 20: loss 0.9864557220956565
Epoch 21: loss 0.9859629579540696
Epoch 22: loss 0.9853996894099001
Epoch 23: loss 0.9846468626350862
Epoch 24: loss 0.9839942383662458
Epoch 25: loss 0.9831321725805147
Epoch 26: loss 0.9822469142839402
Epoch 27: loss 0.9812178951913472
Epoch 28: loss 0.9801060057580957
Epoch 29: loss 0.9789087356854678
Epoch 30: loss 0.9774958109356515
Epoch 31: loss 0.9759602432603613
Epoch 32: loss 0.9742550911334237
Epoch 33: loss 0.9723657500393822
Epoch 34: loss 0.9702974614110441
Epoch 35: loss 0.9680159045763156
Epoch 36: loss 0.9655130296183903
Epoch 37: loss 0.9627954310181998
Epoch 38: loss 0.9598332507542905
Epoch 39: loss 0.9566446159798662
Epoch 40: loss 0.9532720148725442
Epoch 41: loss 0.9495830852548476
Epoch 42: loss 0.9456758773346321
Epoch 43: loss 0.9416071701108145
Epoch 44: loss 0.9372553944166359
Epoch 45: loss 0.9326982695068465
Epoch 46: loss 0.9279313596323043
Epoch 47: loss 0.9229015567952229
Epoch 48: loss 0.917595236326703
Epoch 49: loss 0.9121540937979886
-----------Time: 0:08:06.711722, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9905994534492493-------------


Epoch 0: loss 13.848770260227447
Epoch 1: loss 3.9613153259024276
Epoch 2: loss 1.2457045277964491
Epoch 3: loss 1.0414917860230783
Epoch 4: loss 1.0068169207012867
Epoch 5: loss 0.9988042658894785
Epoch 6: loss 0.9963956537974017
Epoch 7: loss 0.9954838442569067
Epoch 8: loss 0.9949762677614815
Epoch 9: loss 0.9946337429611108
Epoch 10: loss 0.9942591744249706
Epoch 11: loss 0.9939827630382443
Epoch 12: loss 0.9935959314055388
Epoch 13: loss 0.9932393662108877
Epoch 14: loss 0.9927695005111166
Epoch 15: loss 0.9922903757208129
Epoch 16: loss 0.9917716990352131
Epoch 17: loss 0.9910058899052834
Epoch 18: loss 0.9903788700448618
Epoch 19: loss 0.9893232361008902
Epoch 20: loss 0.9883911793584601
Epoch 21: loss 0.9872981076366296
Epoch 22: loss 0.9858169874592667
Epoch 23: loss 0.9840780322709896
Epoch 24: loss 0.9823304670313379
Epoch 25: loss 0.9801208827403527
Epoch 26: loss 0.9776368986950937
Epoch 27: loss 0.9748315094378152
Epoch 28: loss 0.9716510783724969
Epoch 29: loss 0.9683184186487888
Epoch 30: loss 0.9646586596868556
Epoch 31: loss 0.9606323983599272
Epoch 32: loss 0.9563437985006138
Epoch 33: loss 0.9519264974498178
Epoch 34: loss 0.9471937163716234
Epoch 35: loss 0.9421411405736302
Epoch 36: loss 0.9367104829686566
Epoch 37: loss 0.9309274717930415
Epoch 38: loss 0.9247362461668308
Epoch 39: loss 0.9179555674556029
Epoch 40: loss 0.9106552452254126
Epoch 41: loss 0.902885536725525
Epoch 42: loss 0.8941988993786547
Epoch 43: loss 0.8850499486683373
Epoch 44: loss 0.8750778049019902
Epoch 45: loss 0.8644797360689631
Epoch 46: loss 0.8530453480314211
Epoch 47: loss 0.8409254413833431
Epoch 48: loss 0.8281614460849192
Epoch 49: loss 0.8147099959001132
-----------Time: 0:11:21.251389, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9865216016769409-------------


Epoch 0: loss 12.142300154865923
Epoch 1: loss 2.1156442390958405
Epoch 2: loss 1.0875893925408295
Epoch 3: loss 1.0134847902000566
Epoch 4: loss 1.0021495765774973
Epoch 5: loss 0.999532372910021
Epoch 6: loss 0.9987754408855553
Epoch 7: loss 0.9983195346486899
Epoch 8: loss 0.9980298500789644
Epoch 9: loss 0.9975921681648107
Epoch 10: loss 0.997348579458078
Epoch 11: loss 0.9970093125101663
Epoch 12: loss 0.9965459139467647
Epoch 13: loss 0.9961457504663472
Epoch 14: loss 0.9956624592984092
Epoch 15: loss 0.9951179915306814
Epoch 16: loss 0.9943403187266117
Epoch 17: loss 0.9934552982003875
Epoch 18: loss 0.9923581644333596
Epoch 19: loss 0.9913940318354191
Epoch 20: loss 0.9897743182346962
Epoch 21: loss 0.9880066115354441
Epoch 22: loss 0.9858835075068824
Epoch 23: loss 0.9833603595961301
Epoch 24: loss 0.9803915750144162
Epoch 25: loss 0.9770153014636546
Epoch 26: loss 0.9730686350579233
Epoch 27: loss 0.968766932272924
Epoch 28: loss 0.9640486426590706
Epoch 29: loss 0.9586095977503687
Epoch 30: loss 0.952780410720318
Epoch 31: loss 0.9466122255304574
Epoch 32: loss 0.9398602551905729
Epoch 33: loss 0.932411341238696
Epoch 34: loss 0.9241677843370536
Epoch 35: loss 0.9152766464520175
Epoch 36: loss 0.9053543517776259
Epoch 37: loss 0.8945486547507431
Epoch 38: loss 0.882673445215946
Epoch 39: loss 0.869553235917068
Epoch 40: loss 0.8554404924070141
Epoch 41: loss 0.8402267960085047
Epoch 42: loss 0.8237263872093192
Epoch 43: loss 0.806097338872477
Epoch 44: loss 0.7873026365135726
Epoch 45: loss 0.7674372382401253
Epoch 46: loss 0.7465812547042229
Epoch 47: loss 0.7247867702935428
Epoch 48: loss 0.7020384962627458
Epoch 49: loss 0.6786483005053845
-----------Time: 0:10:53.906605, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9882640242576599-------------


Epoch 0: loss 10.822105010609837
Epoch 1: loss 1.5456219387644332
Epoch 2: loss 1.0447249051311083
Epoch 3: loss 1.0081090740109995
Epoch 4: loss 1.0027479502317242
Epoch 5: loss 1.001451076114223
Epoch 6: loss 1.0009596315883567
Epoch 7: loss 1.0005965878160963
Epoch 8: loss 1.0003900952219897
Epoch 9: loss 0.9998907782481765
Epoch 10: loss 0.9995638701078229
Epoch 11: loss 0.9993107332004808
Epoch 12: loss 0.9988434105025743
Epoch 13: loss 0.9981170991732414
Epoch 14: loss 0.9975694726672492
Epoch 15: loss 0.9967657094775755
Epoch 16: loss 0.9956601517008594
Epoch 17: loss 0.9945323821997889
Epoch 18: loss 0.9929365509388692
Epoch 19: loss 0.9911941863746342
Epoch 20: loss 0.9888301676968766
Epoch 21: loss 0.9860016647695912
Epoch 22: loss 0.9824288311440248
Epoch 23: loss 0.9783917025841729
Epoch 24: loss 0.9736824297366938
Epoch 25: loss 0.9683053829606426
Epoch 26: loss 0.9622708206561805
Epoch 27: loss 0.9556225378627424
Epoch 28: loss 0.9484800100812709
Epoch 29: loss 0.9403626472000974
Epoch 30: loss 0.9317354202238096
Epoch 31: loss 0.9219718600596469
Epoch 32: loss 0.9113786267709187
Epoch 33: loss 0.8994889265820668
Epoch 34: loss 0.8863880824887149
Epoch 35: loss 0.8719797446197501
Epoch 36: loss 0.8559802556374981
Epoch 37: loss 0.8384943273710518
Epoch 38: loss 0.8194149858832295
Epoch 39: loss 0.7985047791203835
Epoch 40: loss 0.776097084935947
Epoch 41: loss 0.7519081208354562
Epoch 42: loss 0.7263500533912932
Epoch 43: loss 0.699395311930505
Epoch 44: loss 0.67123830465923
Epoch 45: loss 0.6421576409057277
Epoch 46: loss 0.6125250517949259
Epoch 47: loss 0.5823568559944013
Epoch 48: loss 0.5521859871654033
Epoch 49: loss 0.5220678771576069
-----------Time: 0:13:54.381148, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9973713159561157-------------


Epoch 0: loss 5.382822921316283
Epoch 1: loss 1.02765703034375
Epoch 2: loss 1.0148025095592446
Epoch 3: loss 0.9982640157256715
Epoch 4: loss 0.9789095046631728
Epoch 5: loss 0.957800246357853
Epoch 6: loss 0.9342235705494686
Epoch 7: loss 0.9100434348562478
Epoch 8: loss 0.8849765116621321
Epoch 9: loss 0.8607200651210309
Epoch 10: loss 0.837993139786977
Epoch 11: loss 0.8182188118683896
Epoch 12: loss 0.8016722153521803
Epoch 13: loss 0.7881163200358452
Epoch 14: loss 0.776816399347659
Epoch 15: loss 0.7679467609779157
Epoch 16: loss 0.7602972457303891
Epoch 17: loss 0.7541029043434884
Epoch 18: loss 0.7483702553815982
Epoch 19: loss 0.7432857116987162
Epoch 20: loss 0.7395298200142391
Epoch 21: loss 0.7355947192918353
Epoch 22: loss 0.7321968033723432
Epoch 23: loss 0.7292214830068222
Epoch 24: loss 0.7261813651808842
Epoch 25: loss 0.7236023535946907
Epoch 26: loss 0.7210768781209393
Epoch 27: loss 0.7190266189090968
Epoch 28: loss 0.7168660536773053
Epoch 29: loss 0.7147299420289075
Epoch 30: loss 0.7128511938049198
Epoch 31: loss 0.7111616800860778
Epoch 32: loss 0.7094565158858773
Epoch 33: loss 0.7077406412836259
Epoch 34: loss 0.7061752114868346
Epoch 35: loss 0.7047387411359084
Epoch 36: loss 0.7033444461192432
Epoch 37: loss 0.702048986234569
Epoch 38: loss 0.7006592887730752
Epoch 39: loss 0.6995218734805385
Epoch 40: loss 0.6982296442551221
Epoch 41: loss 0.6971453422758487
Epoch 42: loss 0.6959214818467907
Epoch 43: loss 0.6947714686474766
Epoch 44: loss 0.6938446105304394
Epoch 45: loss 0.6927682134186592
Epoch 46: loss 0.69168522058621
Epoch 47: loss 0.6909291808974036
Epoch 48: loss 0.6899733587799673
Epoch 49: loss 0.6891703259983809
-----------Time: 0:07:04.360740, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1275084018707275-------------


Epoch 0: loss 3.63177213189458
Epoch 1: loss 1.052372444463593
Epoch 2: loss 1.038988040380338
Epoch 3: loss 1.0160794303493696
Epoch 4: loss 0.9840760832736517
Epoch 5: loss 0.9331676587746025
Epoch 6: loss 0.8651356460641039
Epoch 7: loss 0.7905117675394646
Epoch 8: loss 0.7200972402368051
Epoch 9: loss 0.6606056428574686
Epoch 10: loss 0.6145415201888518
Epoch 11: loss 0.5795689181117275
Epoch 12: loss 0.5536017679468065
Epoch 13: loss 0.5329224780615158
Epoch 14: loss 0.5167981068795631
Epoch 15: loss 0.5032563990568454
Epoch 16: loss 0.49199645772227907
Epoch 17: loss 0.4823538302694604
Epoch 18: loss 0.4741522447449672
Epoch 19: loss 0.46662186887550766
Epoch 20: loss 0.45995585002155004
Epoch 21: loss 0.45407372337926527
Epoch 22: loss 0.4486316734840845
Epoch 23: loss 0.4437870013772177
Epoch 24: loss 0.43938175327528944
Epoch 25: loss 0.4351792320722533
Epoch 26: loss 0.43129898572888564
Epoch 27: loss 0.4279147194026928
Epoch 28: loss 0.4245080717835756
Epoch 29: loss 0.42156225591168706
Epoch 30: loss 0.41867101055536016
Epoch 31: loss 0.4157671722836181
Epoch 32: loss 0.4131423504967415
Epoch 33: loss 0.4108682807500496
Epoch 34: loss 0.4084892755474585
Epoch 35: loss 0.40635662260928707
Epoch 36: loss 0.40422887370889765
Epoch 37: loss 0.402315706841125
Epoch 38: loss 0.40021181464713834
Epoch 39: loss 0.39871689160681084
Epoch 40: loss 0.3970054896703358
Epoch 41: loss 0.39513299513310435
Epoch 42: loss 0.3936811204171168
Epoch 43: loss 0.3921386978795115
Epoch 44: loss 0.3906358086330337
Epoch 45: loss 0.38934803358266507
Epoch 46: loss 0.38788867676190025
Epoch 47: loss 0.38662802888249626
Epoch 48: loss 0.38531944626874415
Epoch 49: loss 0.3838137637027714
-----------Time: 0:09:09.469280, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4159904718399048-------------


Epoch 0: loss 2.9338238302976816
Epoch 1: loss 1.0655499078483281
Epoch 2: loss 1.029383322607894
Epoch 3: loss 0.9714235958682252
Epoch 4: loss 0.8810079914678238
Epoch 5: loss 0.7663783232695127
Epoch 6: loss 0.6399262515648738
Epoch 7: loss 0.5237697918350895
Epoch 8: loss 0.43398130881585395
Epoch 9: loss 0.3698741578055893
Epoch 10: loss 0.32457897911793254
Epoch 11: loss 0.29186105514639027
Epoch 12: loss 0.26722977957027383
Epoch 13: loss 0.24769565898594875
Epoch 14: loss 0.23222248773776752
Epoch 15: loss 0.2192758485581675
Epoch 16: loss 0.2085266335097067
Epoch 17: loss 0.19943831565872427
Epoch 18: loss 0.19128596518936203
Epoch 19: loss 0.18449571005931167
Epoch 20: loss 0.17819319843492346
Epoch 21: loss 0.17285751915208147
Epoch 22: loss 0.16767410957064557
Epoch 23: loss 0.1633815800182647
Epoch 24: loss 0.15928239115048454
Epoch 25: loss 0.15543570588867
Epoch 26: loss 0.1521921522789769
Epoch 27: loss 0.1489670206025413
Epoch 28: loss 0.1460555386387699
Epoch 29: loss 0.14346226395116415
Epoch 30: loss 0.14082698241005845
Epoch 31: loss 0.138595758722067
Epoch 32: loss 0.13635528805529443
Epoch 33: loss 0.1343332187370349
Epoch 34: loss 0.13237016110429303
Epoch 35: loss 0.13061615684925051
Epoch 36: loss 0.12893281320779462
Epoch 37: loss 0.1272673291732041
Epoch 38: loss 0.1255799545164566
Epoch 39: loss 0.1241967540848119
Epoch 40: loss 0.12292274048773733
Epoch 41: loss 0.1214130124226091
Epoch 42: loss 0.12015732353864123
Epoch 43: loss 0.11895627940560793
Epoch 44: loss 0.11786995825416988
Epoch 45: loss 0.11670569989419036
Epoch 46: loss 0.11560540070403783
Epoch 47: loss 0.11469309151683119
Epoch 48: loss 0.11367416904673038
Epoch 49: loss 0.11275006877385027
-----------Time: 0:09:43.161850, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.621016025543213-------------


Epoch 0: loss 2.682518271971423
Epoch 1: loss 1.0729050170377001
Epoch 2: loss 1.0105127092124717
Epoch 3: loss 0.9145525226682473
Epoch 4: loss 0.765382277861829
Epoch 5: loss 0.5863075609875478
Epoch 6: loss 0.4289240770977823
Epoch 7: loss 0.3152160809140869
Epoch 8: loss 0.2417389366769674
Epoch 9: loss 0.19444248579592405
Epoch 10: loss 0.1630168155082838
Epoch 11: loss 0.1405342212772324
Epoch 12: loss 0.12413112490016069
Epoch 13: loss 0.11191140340014182
Epoch 14: loss 0.1021975003610314
Epoch 15: loss 0.09426464388362735
Epoch 16: loss 0.08797602892598586
Epoch 17: loss 0.08250434782969815
Epoch 18: loss 0.07810992466983684
Epoch 19: loss 0.07413331963134046
Epoch 20: loss 0.07095406154528171
Epoch 21: loss 0.0681312728314668
Epoch 22: loss 0.06544730658863245
Epoch 23: loss 0.06320153799489375
Epoch 24: loss 0.061155073218100285
Epoch 25: loss 0.05940856079937097
Epoch 26: loss 0.05770512624092242
Epoch 27: loss 0.05617580775023641
Epoch 28: loss 0.054888910611421926
Epoch 29: loss 0.05358577208707941
Epoch 30: loss 0.05247054249048233
Epoch 31: loss 0.051277510733700696
Epoch 32: loss 0.050319022762874276
Epoch 33: loss 0.04942735683430207
Epoch 34: loss 0.04850062840963071
Epoch 35: loss 0.04779956223824998
Epoch 36: loss 0.04695136364561553
Epoch 37: loss 0.04622816588362966
Epoch 38: loss 0.045598619913276184
Epoch 39: loss 0.04498012381027969
Epoch 40: loss 0.0444014390734039
Epoch 41: loss 0.04377389305269689
Epoch 42: loss 0.04331520273410444
Epoch 43: loss 0.04272662725627763
Epoch 44: loss 0.04226748128241822
Epoch 45: loss 0.0417773526631875
Epoch 46: loss 0.041329568250954574
Epoch 47: loss 0.04094488981072024
Epoch 48: loss 0.0406166282935699
Epoch 49: loss 0.040029874075530276
-----------Time: 0:12:18.883925, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4480301141738892-------------


Epoch 0: loss 2.5361044224252383
Epoch 1: loss 1.0797508353445437
Epoch 2: loss 0.9944510594893565
Epoch 3: loss 0.8462082615943627
Epoch 4: loss 0.6341795888184982
Epoch 5: loss 0.4281470108657767
Epoch 6: loss 0.28054747923746776
Epoch 7: loss 0.1912883644545772
Epoch 8: loss 0.14096034591882822
Epoch 9: loss 0.11130598868188196
Epoch 10: loss 0.0926549059367734
Epoch 11: loss 0.08082370648830076
Epoch 12: loss 0.07239981449010127
Epoch 13: loss 0.06650682037079413
Epoch 14: loss 0.06197216528983885
Epoch 15: loss 0.05890188820542623
Epoch 16: loss 0.05578275893213731
Epoch 17: loss 0.05397366682526338
Epoch 18: loss 0.052095428610533526
Epoch 19: loss 0.05041355940818689
Epoch 20: loss 0.04933391091003082
Epoch 21: loss 0.04796097694190706
Epoch 22: loss 0.04711715441426063
Epoch 23: loss 0.04608744652987402
Epoch 24: loss 0.04526029154860889
Epoch 25: loss 0.04449174746257203
Epoch 26: loss 0.043747325260708715
Epoch 27: loss 0.04321502266242055
Epoch 28: loss 0.04258520073901058
Epoch 29: loss 0.04195878096481926
Epoch 30: loss 0.041596361515459125
Epoch 31: loss 0.04111070818885099
Epoch 32: loss 0.04054868597103776
Epoch 33: loss 0.04017411221209721
Epoch 34: loss 0.039823210610318856
Epoch 35: loss 0.03942010244795199
Epoch 36: loss 0.0390382271033143
Epoch 37: loss 0.03874975788017584
Epoch 38: loss 0.038389339274815436
Epoch 39: loss 0.03809158420448494
Epoch 40: loss 0.037896424582579524
Epoch 41: loss 0.03753064231528754
Epoch 42: loss 0.03736877072979294
Epoch 43: loss 0.03705245870266306
Epoch 44: loss 0.03680121310889851
Epoch 45: loss 0.03660928712709077
Epoch 46: loss 0.03628987052375091
Epoch 47: loss 0.036272454374814696
Epoch 48: loss 0.03597525488271248
Epoch 49: loss 0.0357478536612771
-----------Time: 0:11:59.561260, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2483562231063843-------------


Epoch 0: loss 16.129271860418275
Epoch 1: loss 16.12926730622151
Epoch 2: loss 16.129269659810898
Epoch 3: loss 16.12926386697841
Epoch 4: loss 16.12926495548433
Epoch 5: loss 16.129260227302936
Epoch 6: loss 16.12926314588861
Epoch 7: loss 16.12924907958138
Epoch 8: loss 16.12924879825005
Epoch 9: loss 16.12924863256275
Epoch 10: loss 16.129253463682886
Epoch 11: loss 16.129246324349385
Epoch 12: loss 16.129241554940634
Epoch 13: loss 16.129248010781616
Epoch 14: loss 16.129236526240337
Epoch 15: loss 16.129246010088025
Epoch 16: loss 16.129231758905924
Epoch 17: loss 16.129238600313435
Epoch 18: loss 16.129237009300493
Epoch 19: loss 16.12923242113654
Epoch 20: loss 16.129228465643962
Epoch 21: loss 16.12923172701306
Epoch 22: loss 16.129232325976538
Epoch 23: loss 16.12921633442956
Epoch 24: loss 16.12922290072874
Epoch 25: loss 16.129223075231952
Epoch 26: loss 16.12921713667761
Epoch 27: loss 16.12921449501531
Epoch 28: loss 16.12920816467144
Epoch 29: loss 16.129214197607908
Epoch 30: loss 16.129209733903895
Epoch 31: loss 16.129213062688798
Epoch 32: loss 16.12920748299396
Epoch 33: loss 16.129204564408287
Epoch 34: loss 16.129201918856616
Epoch 35: loss 16.12919617554881
Epoch 36: loss 16.129201659824357
Epoch 37: loss 16.129196819110433
Epoch 38: loss 16.129201271146325
Epoch 39: loss 16.12919273993579
Epoch 40: loss 16.129194937950246
Epoch 41: loss 16.129193010636165
Epoch 42: loss 16.12918052911888
Epoch 43: loss 16.129187414346667
Epoch 44: loss 16.129189091703697
Epoch 45: loss 16.129186819531853
Epoch 46: loss 16.129176477947723
Epoch 47: loss 16.129177285900187
Epoch 48: loss 16.12918142626764
Epoch 49: loss 16.129178193679902
-----------Time: 0:09:22.241272, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017190933227539-------------


Epoch 0: loss 16.12920089335854
Epoch 1: loss 16.12920102222644
Epoch 2: loss 16.129200811940994
Epoch 3: loss 16.129194890759184
Epoch 4: loss 16.129184548656468
Epoch 5: loss 16.12919715178149
Epoch 6: loss 16.129197705368945
Epoch 7: loss 16.129187158944493
Epoch 8: loss 16.12919165376349
Epoch 9: loss 16.129190725240452
Epoch 10: loss 16.1291879961969
Epoch 11: loss 16.12918411693604
Epoch 12: loss 16.129175663512967
Epoch 13: loss 16.129177960058215
Epoch 14: loss 16.129178829462777
Epoch 15: loss 16.129178552280113
Epoch 16: loss 16.1291749139011
Epoch 17: loss 16.129170834726455
Epoch 18: loss 16.12917049401736
Epoch 19: loss 16.129166690728923
Epoch 20: loss 16.129168060566176
Epoch 21: loss 16.129161973437775
Epoch 22: loss 16.129155111286938
Epoch 23: loss 16.1291575577027
Epoch 24: loss 16.12916038890712
Epoch 25: loss 16.129162516394278
Epoch 26: loss 16.129147349394426
Epoch 27: loss 16.129156714486584
Epoch 28: loss 16.12914812208324
Epoch 29: loss 16.12915070229344
Epoch 30: loss 16.12914266088464
Epoch 31: loss 16.129143818621404
Epoch 32: loss 16.129137617923305
Epoch 33: loss 16.12913596105031
Epoch 34: loss 16.129137402711322
Epoch 35: loss 16.12914005241166
Epoch 36: loss 16.12913518939866
Epoch 37: loss 16.129132023967433
Epoch 38: loss 16.129129072451732
Epoch 39: loss 16.129129348597232
Epoch 40: loss 16.129121578407393
Epoch 41: loss 16.12912518774575
Epoch 42: loss 16.12911651392486
Epoch 43: loss 16.129118635707606
Epoch 44: loss 16.129113601043603
Epoch 45: loss 16.12910871028641
Epoch 46: loss 16.129117551091056
Epoch 47: loss 16.129107856957923
Epoch 48: loss 16.129114972436604
Epoch 49: loss 16.12910868876521
-----------Time: 0:07:53.971430, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017169952392578-------------


Epoch 0: loss 16.129187017630596
Epoch 1: loss 16.129180445627004
Epoch 2: loss 16.12918326153322
Epoch 3: loss 16.129175080106982
Epoch 4: loss 16.129171382090913
Epoch 5: loss 16.129169631613667
Epoch 6: loss 16.12917006514914
Epoch 7: loss 16.129167481308855
Epoch 8: loss 16.129167019510607
Epoch 9: loss 16.129165837400436
Epoch 10: loss 16.12916642106571
Epoch 11: loss 16.129167587099808
Epoch 12: loss 16.129161343618602
Epoch 13: loss 16.12915515407004
Epoch 14: loss 16.12915053012385
Epoch 15: loss 16.129152945165337
Epoch 16: loss 16.129151282587927
Epoch 17: loss 16.129152553116516
Epoch 18: loss 16.12914260435908
Epoch 19: loss 16.1291482398016
Epoch 20: loss 16.129144127956224
Epoch 21: loss 16.12913899968797
Epoch 22: loss 16.12913722820811
Epoch 23: loss 16.129133313424305
Epoch 24: loss 16.129137198648873
Epoch 25: loss 16.129142584912216
Epoch 26: loss 16.129134530538835
Epoch 27: loss 16.12912641678769
Epoch 28: loss 16.12912392188441
Epoch 29: loss 16.129131543759485
Epoch 30: loss 16.129131433560577
Epoch 31: loss 16.12911868212079
Epoch 32: loss 16.12912309863374
Epoch 33: loss 16.12911533855627
Epoch 34: loss 16.129114900872136
Epoch 35: loss 16.129107666897216
Epoch 36: loss 16.129111492225434
Epoch 37: loss 16.129110482803437
Epoch 38: loss 16.129103137851732
Epoch 39: loss 16.129102258853383
Epoch 40: loss 16.129102835517788
Epoch 41: loss 16.129106047102912
Epoch 42: loss 16.12909839774293
Epoch 43: loss 16.129094791775362
Epoch 44: loss 16.1290989907427
Epoch 45: loss 16.12909316394302
Epoch 46: loss 16.129092401107282
Epoch 47: loss 16.12909092107112
Epoch 48: loss 16.1290844756018
Epoch 49: loss 16.12908789384229
-----------Time: 0:11:14.048954, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017175674438477-------------


Epoch 0: loss 16.129178604916298
Epoch 1: loss 16.12918033983605
Epoch 2: loss 16.129172276387465
Epoch 3: loss 16.129175271982728
Epoch 4: loss 16.1291748770817
Epoch 5: loss 16.129165596000004
Epoch 6: loss 16.12916586021809
Epoch 7: loss 16.12916603264697
Epoch 8: loss 16.129161227455988
Epoch 9: loss 16.129162624518855
Epoch 10: loss 16.129159184497876
Epoch 11: loss 16.12915162459348
Epoch 12: loss 16.12916032719573
Epoch 13: loss 16.129149715170517
Epoch 14: loss 16.129148690191023
Epoch 15: loss 16.129150162448436
Epoch 16: loss 16.129149398834826
Epoch 17: loss 16.129147163482383
Epoch 18: loss 16.12914486097343
Epoch 19: loss 16.12913894264383
Epoch 20: loss 16.129141889751573
Epoch 21: loss 16.12913025015395
Epoch 22: loss 16.12913149604984
Epoch 23: loss 16.12913697721389
Epoch 24: loss 16.129132086715988
Epoch 25: loss 16.129130676429256
Epoch 26: loss 16.129126341593143
Epoch 27: loss 16.129123289731613
Epoch 28: loss 16.12912185610864
Epoch 29: loss 16.12911798203361
Epoch 30: loss 16.129118415050495
Epoch 31: loss 16.129113316600773
Epoch 32: loss 16.129118505543246
Epoch 33: loss 16.129119592234126
Epoch 34: loss 16.129110847367354
Epoch 35: loss 16.129109126190052
Epoch 36: loss 16.12910756188414
Epoch 37: loss 16.12910501071459
Epoch 38: loss 16.129106220309666
Epoch 39: loss 16.12910996136813
Epoch 40: loss 16.1290972910866
Epoch 41: loss 16.12910429403275
Epoch 42: loss 16.12909313256874
Epoch 43: loss 16.12909070508126
Epoch 44: loss 16.129091914417046
Epoch 45: loss 16.12908871916729
Epoch 46: loss 16.129088249331
Epoch 47: loss 16.12909101960191
Epoch 48: loss 16.129084530053028
Epoch 49: loss 16.129077454764534
-----------Time: 0:10:48.389170, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129178448563493
Epoch 1: loss 16.129176248733994
Epoch 2: loss 16.129176496098133
Epoch 3: loss 16.129171489437617
Epoch 4: loss 16.129167588136973
Epoch 5: loss 16.12916492910214
Epoch 6: loss 16.12916250498545
Epoch 7: loss 16.129161360731846
Epoch 8: loss 16.12916670006342
Epoch 9: loss 16.129162125901207
Epoch 10: loss 16.129162331519403
Epoch 11: loss 16.129149814219886
Epoch 12: loss 16.129154226584173
Epoch 13: loss 16.129152281638266
Epoch 14: loss 16.129146851813942
Epoch 15: loss 16.129145708856797
Epoch 16: loss 16.129144405138888
Epoch 17: loss 16.129146859592687
Epoch 18: loss 16.129141245671367
Epoch 19: loss 16.129140068747027
Epoch 20: loss 16.129138104354254
Epoch 21: loss 16.129136836937164
Epoch 22: loss 16.12913423831726
Epoch 23: loss 16.129133132438806
Epoch 24: loss 16.129142532794614
Epoch 25: loss 16.129129249807153
Epoch 26: loss 16.12912698074681
Epoch 27: loss 16.129128750670922
Epoch 28: loss 16.129117294392422
Epoch 29: loss 16.129116752213793
Epoch 30: loss 16.129127846780584
Epoch 31: loss 16.129120937697973
Epoch 32: loss 16.129118294739218
Epoch 33: loss 16.12911063241466
Epoch 34: loss 16.129115091451425
Epoch 35: loss 16.129110626710247
Epoch 36: loss 16.129103355915927
Epoch 37: loss 16.129108497148756
Epoch 38: loss 16.12910730259259
Epoch 39: loss 16.129100861531228
Epoch 40: loss 16.12909613101621
Epoch 41: loss 16.129094699726863
Epoch 42: loss 16.129098292989145
Epoch 43: loss 16.12910184294974
Epoch 44: loss 16.12909501372893
Epoch 45: loss 16.12908677422138
Epoch 46: loss 16.129091619343264
Epoch 47: loss 16.129084339214447
Epoch 48: loss 16.129079490462484
Epoch 49: loss 16.12908333057032
-----------Time: 0:13:37.306659, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017175674438477-------------


Epoch 0: loss 16.12940447093223
Epoch 1: loss 16.12938387851601
Epoch 2: loss 16.12937253943729
Epoch 3: loss 16.129347757388224
Epoch 4: loss 16.129331310525284
Epoch 5: loss 16.129315537042498
Epoch 6: loss 16.12929418049408
Epoch 7: loss 16.129271260417628
Epoch 8: loss 16.129250111043163
Epoch 9: loss 16.12923211335747
Epoch 10: loss 16.129212496655345
Epoch 11: loss 16.129189739673276
Epoch 12: loss 16.129175897393946
Epoch 13: loss 16.12915813047773
Epoch 14: loss 16.12913490962308
Epoch 15: loss 16.12911407450997
Epoch 16: loss 16.12909987259326
Epoch 17: loss 16.129078695474597
Epoch 18: loss 16.12906196416883
Epoch 19: loss 16.129038937004964
Epoch 20: loss 16.129023452632254
Epoch 21: loss 16.129005370158225
Epoch 22: loss 16.128981436511108
Epoch 23: loss 16.128960778753417
Epoch 24: loss 16.128938966111168
Epoch 25: loss 16.128916350442992
Epoch 26: loss 16.12890389952211
Epoch 27: loss 16.128880178753352
Epoch 28: loss 16.12886367899494
Epoch 29: loss 16.12884057559936
Epoch 30: loss 16.128822794940692
Epoch 31: loss 16.128810987062852
Epoch 32: loss 16.12879157131168
Epoch 33: loss 16.128769469559355
Epoch 34: loss 16.128751262884673
Epoch 35: loss 16.12872476847422
Epoch 36: loss 16.128706619621553
Epoch 37: loss 16.128694252192552
Epoch 38: loss 16.128671901520338
Epoch 39: loss 16.128654948001714
Epoch 40: loss 16.128641619897525
Epoch 41: loss 16.128607348037065
Epoch 42: loss 16.128591827622827
Epoch 43: loss 16.12857774264661
Epoch 44: loss 16.12855697650505
Epoch 45: loss 16.128541500948252
Epoch 46: loss 16.12851334992409
Epoch 47: loss 16.128494448607352
Epoch 48: loss 16.128475167428494
Epoch 49: loss 16.128453699384714
-----------Time: 0:07:19.473898, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017076015472412-------------


Epoch 0: loss 16.12915345752544
Epoch 1: loss 16.12913807375785
Epoch 2: loss 16.129113045381814
Epoch 3: loss 16.129096361526397
Epoch 4: loss 16.129081841199667
Epoch 5: loss 16.129056254828217
Epoch 6: loss 16.129034755410157
Epoch 7: loss 16.129014359796223
Epoch 8: loss 16.128999970671014
Epoch 9: loss 16.128982330289077
Epoch 10: loss 16.12896434712371
Epoch 11: loss 16.128946953068745
Epoch 12: loss 16.128924395222583
Epoch 13: loss 16.128903444724735
Epoch 14: loss 16.128883565619564
Epoch 15: loss 16.12886470527089
Epoch 16: loss 16.12884676333288
Epoch 17: loss 16.12883056072258
Epoch 18: loss 16.128804439691955
Epoch 19: loss 16.128783897319003
Epoch 20: loss 16.128767858840252
Epoch 21: loss 16.128746818108944
Epoch 22: loss 16.128728823016168
Epoch 23: loss 16.128707788767148
Epoch 24: loss 16.12869628944625
Epoch 25: loss 16.128676311032418
Epoch 26: loss 16.128648857761817
Epoch 27: loss 16.128632430605034
Epoch 28: loss 16.128621183574815
Epoch 29: loss 16.128597152952658
Epoch 30: loss 16.12856981091813
Epoch 31: loss 16.128555259217123
Epoch 32: loss 16.12853750915486
Epoch 33: loss 16.128518930915387
Epoch 34: loss 16.12849765785885
Epoch 35: loss 16.12847092956742
Epoch 36: loss 16.128449634989686
Epoch 37: loss 16.128442260738037
Epoch 38: loss 16.128420992608042
Epoch 39: loss 16.12840838766798
Epoch 40: loss 16.128376947011944
Epoch 41: loss 16.128363820895867
Epoch 42: loss 16.128346488811584
Epoch 43: loss 16.128318206067316
Epoch 44: loss 16.12830452506733
Epoch 45: loss 16.128287763165158
Epoch 46: loss 16.128271624081286
Epoch 47: loss 16.128248572284726
Epoch 48: loss 16.128227578485188
Epoch 49: loss 16.12820461277342
-----------Time: 0:09:35.221383, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017063617706299-------------


Epoch 0: loss 16.129166896865705
Epoch 1: loss 16.12914461490575
Epoch 2: loss 16.129124388609196
Epoch 3: loss 16.12910732100229
Epoch 4: loss 16.129089038614477
Epoch 5: loss 16.129066835219863
Epoch 6: loss 16.129045778153188
Epoch 7: loss 16.129027136128
Epoch 8: loss 16.129007176642446
Epoch 9: loss 16.1289890547561
Epoch 10: loss 16.128967764586324
Epoch 11: loss 16.128950901301156
Epoch 12: loss 16.128940181410655
Epoch 13: loss 16.128915006794184
Epoch 14: loss 16.128891060701072
Epoch 15: loss 16.1288760967265
Epoch 16: loss 16.12884984708727
Epoch 17: loss 16.128833034882536
Epoch 18: loss 16.128813587238504
Epoch 19: loss 16.12879613147215
Epoch 20: loss 16.12877869515266
Epoch 21: loss 16.128761176378458
Epoch 22: loss 16.128743461061262
Epoch 23: loss 16.128722222749794
Epoch 24: loss 16.1287025959353
Epoch 25: loss 16.12868032953284
Epoch 26: loss 16.1286659445563
Epoch 27: loss 16.12864560883871
Epoch 28: loss 16.12862634736601
Epoch 29: loss 16.128601805161626
Epoch 30: loss 16.128585130381417
Epoch 31: loss 16.128567634165577
Epoch 32: loss 16.128548705363936
Epoch 33: loss 16.12852492158733
Epoch 34: loss 16.128509019495937
Epoch 35: loss 16.128490747220493
Epoch 36: loss 16.128472115048382
Epoch 37: loss 16.12845117933015
Epoch 38: loss 16.128423861669027
Epoch 39: loss 16.128408617918875
Epoch 40: loss 16.1283855788276
Epoch 41: loss 16.128365899117632
Epoch 42: loss 16.128348822694814
Epoch 43: loss 16.128343548964
Epoch 44: loss 16.128316233636504
Epoch 45: loss 16.128286913726043
Epoch 46: loss 16.128272315352554
Epoch 47: loss 16.128258700990497
Epoch 48: loss 16.128242618432182
Epoch 49: loss 16.128218065596844
-----------Time: 0:09:54.879863, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0170745849609375-------------


Epoch 0: loss 16.129165244919246
Epoch 1: loss 16.129152594862457
Epoch 2: loss 16.129133659319233
Epoch 3: loss 16.129108744512894
Epoch 4: loss 16.129089381657195
Epoch 5: loss 16.12907157247646
Epoch 6: loss 16.12905611143999
Epoch 7: loss 16.129036136137653
Epoch 8: loss 16.129015665847753
Epoch 9: loss 16.12899621353647
Epoch 10: loss 16.12898142302795
Epoch 11: loss 16.128954082030585
Epoch 12: loss 16.12893792687064
Epoch 13: loss 16.128914422351034
Epoch 14: loss 16.128893310055258
Epoch 15: loss 16.128877175120053
Epoch 16: loss 16.128855584431367
Epoch 17: loss 16.128837695648127
Epoch 18: loss 16.12882205103324
Epoch 19: loss 16.128796436917593
Epoch 20: loss 16.128788084877517
Epoch 21: loss 16.128761781824224
Epoch 22: loss 16.128741899348263
Epoch 23: loss 16.128721858185877
Epoch 24: loss 16.12870599913688
Epoch 25: loss 16.128676695821078
Epoch 26: loss 16.12866538370868
Epoch 27: loss 16.128641845740464
Epoch 28: loss 16.128627549700923
Epoch 29: loss 16.128610841990685
Epoch 30: loss 16.128586671869673
Epoch 31: loss 16.128568093630204
Epoch 32: loss 16.12854801668558
Epoch 33: loss 16.128526911390676
Epoch 34: loss 16.128508953376592
Epoch 35: loss 16.128492906600513
Epoch 36: loss 16.128474384368015
Epoch 37: loss 16.128456924452998
Epoch 38: loss 16.128434681386775
Epoch 39: loss 16.128410143071765
Epoch 40: loss 16.12839062308539
Epoch 41: loss 16.128378852804826
Epoch 42: loss 16.128351036007473
Epoch 43: loss 16.128339053885714
Epoch 44: loss 16.128320283251913
Epoch 45: loss 16.128296622120214
Epoch 46: loss 16.128280923831976
Epoch 47: loss 16.128259150861332
Epoch 48: loss 16.128241636495087
Epoch 49: loss 16.128225438811327
-----------Time: 0:12:23.894187, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.0170698165893555-------------


Epoch 0: loss 16.129170537059757
Epoch 1: loss 16.129152103764262
Epoch 2: loss 16.12913786450957
Epoch 3: loss 16.129112809685793
Epoch 4: loss 16.129094958499827
Epoch 5: loss 16.129079014662494
Epoch 6: loss 16.129055318526433
Epoch 7: loss 16.129039009088014
Epoch 8: loss 16.12901362237106
Epoch 9: loss 16.128994132721793
Epoch 10: loss 16.128971985852736
Epoch 11: loss 16.128963427157295
Epoch 12: loss 16.12893895522092
Epoch 13: loss 16.128917032120473
Epoch 14: loss 16.128907573683357
Epoch 15: loss 16.12888198964553
Epoch 16: loss 16.12885863344069
Epoch 17: loss 16.128841339731554
Epoch 18: loss 16.128824690361913
Epoch 19: loss 16.128803050407836
Epoch 20: loss 16.128786052031774
Epoch 21: loss 16.12876136929137
Epoch 22: loss 16.128742622253103
Epoch 23: loss 16.128730821116843
Epoch 24: loss 16.128708886088983
Epoch 25: loss 16.128686348208273
Epoch 26: loss 16.128663550776718
Epoch 27: loss 16.128641627157688
Epoch 28: loss 16.128625934573865
Epoch 29: loss 16.12861136705607
Epoch 30: loss 16.12859257256745
Epoch 31: loss 16.128568611435426
Epoch 32: loss 16.128557169677254
Epoch 33: loss 16.128522732388785
Epoch 34: loss 16.128514931602542
Epoch 35: loss 16.1284982148171
Epoch 36: loss 16.128470553853965
Epoch 37: loss 16.128455464382284
Epoch 38: loss 16.12843881216044
Epoch 39: loss 16.128416814124733
Epoch 40: loss 16.128398831737243
Epoch 41: loss 16.128375860580352
Epoch 42: loss 16.128356580957245
Epoch 43: loss 16.128330487152233
Epoch 44: loss 16.12832062603534
Epoch 45: loss 16.12830760259872
Epoch 46: loss 16.12828127154194
Epoch 47: loss 16.128265690712777
Epoch 48: loss 16.128246993199195
Epoch 49: loss 16.128215984782166
-----------Time: 0:12:13.029679, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0170698165893555-------------


Epoch 0: loss 16.1291852718206
Epoch 1: loss 16.12899896747201
Epoch 2: loss 16.12880516700475
Epoch 3: loss 16.12861316991021
Epoch 4: loss 16.12841287444894
Epoch 5: loss 16.128223642439476
Epoch 6: loss 16.128031079570775
Epoch 7: loss 16.127836845568044
Epoch 8: loss 16.127637292717772
Epoch 9: loss 16.12744627470812
Epoch 10: loss 16.127257673814285
Epoch 11: loss 16.127056167720777
Epoch 12: loss 16.126863313926957
Epoch 13: loss 16.126668636276385
Epoch 14: loss 16.126476153269483
Epoch 15: loss 16.12628789567766
Epoch 16: loss 16.126091868155285
Epoch 17: loss 16.12589762715168
Epoch 18: loss 16.12571529826112
Epoch 19: loss 16.125511224921986
Epoch 20: loss 16.12531898616772
Epoch 21: loss 16.12512345000283
Epoch 22: loss 16.12493269672985
Epoch 23: loss 16.12474642219979
Epoch 24: loss 16.124548166844427
Epoch 25: loss 16.12435299939212
Epoch 26: loss 16.124154839974633
Epoch 27: loss 16.123959813058345
Epoch 28: loss 16.123770449847356
Epoch 29: loss 16.123584407642525
Epoch 30: loss 16.123377258846332
Epoch 31: loss 16.12318979339019
Epoch 32: loss 16.122996716346346
Epoch 33: loss 16.122805919253093
Epoch 34: loss 16.122613967534573
Epoch 35: loss 16.122409173624227
Epoch 36: loss 16.122229273776895
Epoch 37: loss 16.122036887226447
Epoch 38: loss 16.121837112422607
Epoch 39: loss 16.12165371239866
Epoch 40: loss 16.121453793947303
Epoch 41: loss 16.12126109832133
Epoch 42: loss 16.12106259197175
Epoch 43: loss 16.120875991512214
Epoch 44: loss 16.120679082657865
Epoch 45: loss 16.1204884178033
Epoch 46: loss 16.120290426406736
Epoch 47: loss 16.12010113553809
Epoch 48: loss 16.11990547387609
Epoch 49: loss 16.119709919042208
-----------Time: 0:09:26.696415, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016015529632568-------------


Epoch 0: loss 16.1290886973868
Epoch 1: loss 16.12889010910109
Epoch 2: loss 16.128693388492405
Epoch 3: loss 16.128511773690768
Epoch 4: loss 16.128315320930252
Epoch 5: loss 16.128122723316483
Epoch 6: loss 16.12792092707573
Epoch 7: loss 16.12772846922011
Epoch 8: loss 16.127537705056884
Epoch 9: loss 16.127344414875388
Epoch 10: loss 16.127155022105164
Epoch 11: loss 16.12695746139186
Epoch 12: loss 16.126767065422634
Epoch 13: loss 16.12656741974599
Epoch 14: loss 16.12637796396792
Epoch 15: loss 16.126184523138033
Epoch 16: loss 16.12599331065972
Epoch 17: loss 16.125800594809004
Epoch 18: loss 16.125596477649598
Epoch 19: loss 16.12541270761661
Epoch 20: loss 16.12521402754169
Epoch 21: loss 16.12502271705117
Epoch 22: loss 16.12483575384276
Epoch 23: loss 16.124637850864612
Epoch 24: loss 16.12445138990393
Epoch 25: loss 16.12425330438453
Epoch 26: loss 16.1240584229308
Epoch 27: loss 16.123860133089664
Epoch 28: loss 16.123664533139053
Epoch 29: loss 16.12347126992388
Epoch 30: loss 16.12328784967519
Epoch 31: loss 16.12308830071429
Epoch 32: loss 16.122896455305305
Epoch 33: loss 16.12270647679547
Epoch 34: loss 16.122505188247573
Epoch 35: loss 16.122313302389106
Epoch 36: loss 16.122119700020587
Epoch 37: loss 16.121925618221994
Epoch 38: loss 16.121739507304902
Epoch 39: loss 16.121549559650763
Epoch 40: loss 16.121349477067856
Epoch 41: loss 16.121151408661703
Epoch 42: loss 16.120962301630765
Epoch 43: loss 16.120766460798304
Epoch 44: loss 16.12057508133623
Epoch 45: loss 16.120379727712592
Epoch 46: loss 16.120190572453428
Epoch 47: loss 16.11999405098065
Epoch 48: loss 16.119805964261957
Epoch 49: loss 16.1196060673318
-----------Time: 0:07:58.181460, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.0159807205200195-------------


Epoch 0: loss 16.129082404899492
Epoch 1: loss 16.128879151440238
Epoch 2: loss 16.128692878206635
Epoch 3: loss 16.128493457595052
Epoch 4: loss 16.128307528700628
Epoch 5: loss 16.128108260552477
Epoch 6: loss 16.127913818597925
Epoch 7: loss 16.127722873449194
Epoch 8: loss 16.127530331064527
Epoch 9: loss 16.127330646234856
Epoch 10: loss 16.127142367121834
Epoch 11: loss 16.12694497339229
Epoch 12: loss 16.12675652548047
Epoch 13: loss 16.126564914471043
Epoch 14: loss 16.126367141397964
Epoch 15: loss 16.126174475331226
Epoch 16: loss 16.12597947045472
Epoch 17: loss 16.125788476040597
Epoch 18: loss 16.125595802195114
Epoch 19: loss 16.125398234999523
Epoch 20: loss 16.125203770227312
Epoch 21: loss 16.125016330700326
Epoch 22: loss 16.124828590913722
Epoch 23: loss 16.124629970994445
Epoch 24: loss 16.124432340013133
Epoch 25: loss 16.124236928048894
Epoch 26: loss 16.12404513657255
Epoch 27: loss 16.12385886333895
Epoch 28: loss 16.12365549345779
Epoch 29: loss 16.123469928349405
Epoch 30: loss 16.123272227618667
Epoch 31: loss 16.123084961298435
Epoch 32: loss 16.122885973703738
Epoch 33: loss 16.12268891757179
Epoch 34: loss 16.122498928171712
Epoch 35: loss 16.122306842918043
Epoch 36: loss 16.12211183985658
Epoch 37: loss 16.121919512683895
Epoch 38: loss 16.12172641360027
Epoch 39: loss 16.12153707320694
Epoch 40: loss 16.121340489504192
Epoch 41: loss 16.12114072688706
Epoch 42: loss 16.120955247344888
Epoch 43: loss 16.120755986197608
Epoch 44: loss 16.120562867667118
Epoch 45: loss 16.120370136777495
Epoch 46: loss 16.120176312973992
Epoch 47: loss 16.119990854175146
Epoch 48: loss 16.119789748168497
Epoch 49: loss 16.11960316119212
-----------Time: 0:11:24.226205, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.0159912109375-------------


Epoch 0: loss 16.129080249149556
Epoch 1: loss 16.128884555335404
Epoch 2: loss 16.128692332916508
Epoch 3: loss 16.12850415311215
Epoch 4: loss 16.12831114633632
Epoch 5: loss 16.12811673057021
Epoch 6: loss 16.12792006441275
Epoch 7: loss 16.127722354347515
Epoch 8: loss 16.127537097536784
Epoch 9: loss 16.12734623976931
Epoch 10: loss 16.127145769286248
Epoch 11: loss 16.12695302439488
Epoch 12: loss 16.126760283652178
Epoch 13: loss 16.126574837558614
Epoch 14: loss 16.126372783841354
Epoch 15: loss 16.126183704813904
Epoch 16: loss 16.125987647732295
Epoch 17: loss 16.125790571634894
Epoch 18: loss 16.1255960480035
Epoch 19: loss 16.1254086274048
Epoch 20: loss 16.12522215529458
Epoch 21: loss 16.125019928370566
Epoch 22: loss 16.12482303714804
Epoch 23: loss 16.124635739972113
Epoch 24: loss 16.12443186110164
Epoch 25: loss 16.12425201026041
Epoch 26: loss 16.124049784373565
Epoch 27: loss 16.123856048210605
Epoch 28: loss 16.12366342440839
Epoch 29: loss 16.123469372169033
Epoch 30: loss 16.12327779305247
Epoch 31: loss 16.123088922236132
Epoch 32: loss 16.12289316593272
Epoch 33: loss 16.12269587203042
Epoch 34: loss 16.122507866729272
Epoch 35: loss 16.122306393047207
Epoch 36: loss 16.122113582555077
Epoch 37: loss 16.121920652788834
Epoch 38: loss 16.121733167885825
Epoch 39: loss 16.121536883405525
Epoch 40: loss 16.12134661785995
Epoch 41: loss 16.121153454731314
Epoch 42: loss 16.12096099843144
Epoch 43: loss 16.120762662177114
Epoch 44: loss 16.1205760702742
Epoch 45: loss 16.120377629266088
Epoch 46: loss 16.120180879098168
Epoch 47: loss 16.11998646022056
Epoch 48: loss 16.119799704186097
Epoch 49: loss 16.119599997057353
-----------Time: 0:10:52.348262, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015988349914551-------------


Epoch 0: loss 16.129080049754357
Epoch 1: loss 16.12888247166852
Epoch 2: loss 16.128692256944085
Epoch 3: loss 16.12850461568827
Epoch 4: loss 16.128305396805516
Epoch 5: loss 16.12811891199001
Epoch 6: loss 16.127916993363648
Epoch 7: loss 16.127730631452334
Epoch 8: loss 16.127531566588758
Epoch 9: loss 16.127339164999402
Epoch 10: loss 16.12714044265996
Epoch 11: loss 16.126950441851054
Epoch 12: loss 16.126755743716448
Epoch 13: loss 16.126564117408822
Epoch 14: loss 16.126368032323725
Epoch 15: loss 16.126173880775706
Epoch 16: loss 16.125971903549452
Epoch 17: loss 16.12578266376124
Epoch 18: loss 16.125595149817578
Epoch 19: loss 16.125408227836523
Epoch 20: loss 16.125206292096916
Epoch 21: loss 16.12502004479247
Epoch 22: loss 16.124824162732654
Epoch 23: loss 16.124623654393023
Epoch 24: loss 16.124438622128775
Epoch 25: loss 16.124241637561294
Epoch 26: loss 16.12405098100406
Epoch 27: loss 16.123858294712583
Epoch 28: loss 16.123663745152037
Epoch 29: loss 16.123468271217117
Epoch 30: loss 16.12327306150029
Epoch 31: loss 16.123082753430896
Epoch 32: loss 16.122887440256736
Epoch 33: loss 16.122693087757767
Epoch 34: loss 16.12249709861054
Epoch 35: loss 16.122308686480956
Epoch 36: loss 16.12210754910003
Epoch 37: loss 16.121924133518586
Epoch 38: loss 16.121723664850563
Epoch 39: loss 16.12153772143581
Epoch 40: loss 16.121347502562713
Epoch 41: loss 16.121147114793654
Epoch 42: loss 16.120957468954877
Epoch 43: loss 16.120761419652013
Epoch 44: loss 16.12056738115511
Epoch 45: loss 16.120365930031408
Epoch 46: loss 16.120179642018186
Epoch 47: loss 16.119984148636405
Epoch 48: loss 16.119796328210132
Epoch 49: loss 16.119603590578926
-----------Time: 0:13:54.755151, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015988826751709-------------


Epoch 0: loss 16.128448131617287
Epoch 1: loss 16.12651338935093
Epoch 2: loss 16.124575602483205
Epoch 3: loss 16.122645488830283
Epoch 4: loss 16.120709766960456
Epoch 5: loss 16.118782210181497
Epoch 6: loss 16.116838832469398
Epoch 7: loss 16.114902390028355
Epoch 8: loss 16.112973692366584
Epoch 9: loss 16.11104452953577
Epoch 10: loss 16.109108044311622
Epoch 11: loss 16.10717855606992
Epoch 12: loss 16.10524359418362
Epoch 13: loss 16.103307082771025
Epoch 14: loss 16.10137808536822
Epoch 15: loss 16.099448945095773
Epoch 16: loss 16.097511438262924
Epoch 17: loss 16.095577617259437
Epoch 18: loss 16.093643539038737
Epoch 19: loss 16.091715872838744
Epoch 20: loss 16.08978551778539
Epoch 21: loss 16.087849459355134
Epoch 22: loss 16.08591645434215
Epoch 23: loss 16.083989845273805
Epoch 24: loss 16.082061064379445
Epoch 25: loss 16.080124026086423
Epoch 26: loss 16.07818647698905
Epoch 27: loss 16.076261535424653
Epoch 28: loss 16.074329535166424
Epoch 29: loss 16.072404101466667
Epoch 30: loss 16.070468183053844
Epoch 31: loss 16.068527685033686
Epoch 32: loss 16.066603092735004
Epoch 33: loss 16.06467800752309
Epoch 34: loss 16.06274696145776
Epoch 35: loss 16.060815491710038
Epoch 36: loss 16.058887609520188
Epoch 37: loss 16.056954972960497
Epoch 38: loss 16.05502720174743
Epoch 39: loss 16.053098188787132
Epoch 40: loss 16.051166848166602
Epoch 41: loss 16.049239301500016
Epoch 42: loss 16.047303450243703
Epoch 43: loss 16.0453799692686
Epoch 44: loss 16.043453301859653
Epoch 45: loss 16.04151482009058
Epoch 46: loss 16.039591799617266
Epoch 47: loss 16.037662698238552
Epoch 48: loss 16.03574115935715
Epoch 49: loss 16.03380525287174
-----------Time: 0:06:59.586730, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005237102508545-------------


Epoch 0: loss 16.128257270997604
Epoch 1: loss 16.126316374705628
Epoch 2: loss 16.124381119819876
Epoch 3: loss 16.122447504953175
Epoch 4: loss 16.120507912119812
Epoch 5: loss 16.118574933035987
Epoch 6: loss 16.11664570097433
Epoch 7: loss 16.114709509008925
Epoch 8: loss 16.112778708751982
Epoch 9: loss 16.110838307447573
Epoch 10: loss 16.10891047685674
Epoch 11: loss 16.106980255338534
Epoch 12: loss 16.105046621024965
Epoch 13: loss 16.10310623735238
Epoch 14: loss 16.101172571923826
Epoch 15: loss 16.099247467783627
Epoch 16: loss 16.09731168316524
Epoch 17: loss 16.095378788091878
Epoch 18: loss 16.093454086372162
Epoch 19: loss 16.09151624090455
Epoch 20: loss 16.08958045187821
Epoch 21: loss 16.08764402525395
Epoch 22: loss 16.085710759912256
Epoch 23: loss 16.08378067607786
Epoch 24: loss 16.081857538145478
Epoch 25: loss 16.07992253762474
Epoch 26: loss 16.0779868528336
Epoch 27: loss 16.07605821273455
Epoch 28: loss 16.074126430540517
Epoch 29: loss 16.072197488626102
Epoch 30: loss 16.070259536330372
Epoch 31: loss 16.068336214300988
Epoch 32: loss 16.066404420957415
Epoch 33: loss 16.064474197105586
Epoch 34: loss 16.06254322208614
Epoch 35: loss 16.060610285007545
Epoch 36: loss 16.058689300491473
Epoch 37: loss 16.056754715355794
Epoch 38: loss 16.05482074214817
Epoch 39: loss 16.052895012078174
Epoch 40: loss 16.050960716052572
Epoch 41: loss 16.049032655470135
Epoch 42: loss 16.047102943719114
Epoch 43: loss 16.04517433292001
Epoch 44: loss 16.04324420733968
Epoch 45: loss 16.041320644428446
Epoch 46: loss 16.039393880303752
Epoch 47: loss 16.037453080986815
Epoch 48: loss 16.035529692838086
Epoch 49: loss 16.033601477717887
-----------Time: 0:09:47.415514, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.0052008628845215-------------


Epoch 0: loss 16.128227665088563
Epoch 1: loss 16.126291072777008
Epoch 2: loss 16.124354865513396
Epoch 3: loss 16.12241724848164
Epoch 4: loss 16.120483041652328
Epoch 5: loss 16.118550168359455
Epoch 6: loss 16.116616825748878
Epoch 7: loss 16.11468258884175
Epoch 8: loss 16.11274859826159
Epoch 9: loss 16.110810238100253
Epoch 10: loss 16.108877896873636
Epoch 11: loss 16.10693769729805
Epoch 12: loss 16.10500694734367
Epoch 13: loss 16.10307620127866
Epoch 14: loss 16.10113826246609
Epoch 15: loss 16.099211789525807
Epoch 16: loss 16.09728134372041
Epoch 17: loss 16.09534644873133
Epoch 18: loss 16.093410526947718
Epoch 19: loss 16.091476639306304
Epoch 20: loss 16.089546233172516
Epoch 21: loss 16.087614356596358
Epoch 22: loss 16.08567815451858
Epoch 23: loss 16.083748374833174
Epoch 24: loss 16.08181822176794
Epoch 25: loss 16.079882991514886
Epoch 26: loss 16.07795557760557
Epoch 27: loss 16.076021316065745
Epoch 28: loss 16.07409278720277
Epoch 29: loss 16.07215683793425
Epoch 30: loss 16.070235846417308
Epoch 31: loss 16.068295785821995
Epoch 32: loss 16.066362166547336
Epoch 33: loss 16.064424942860853
Epoch 34: loss 16.0625048015609
Epoch 35: loss 16.060575777451067
Epoch 36: loss 16.058639502771655
Epoch 37: loss 16.05671151116077
Epoch 38: loss 16.05478758057412
Epoch 39: loss 16.05285817141634
Epoch 40: loss 16.050925020681508
Epoch 41: loss 16.04899627349505
Epoch 42: loss 16.047061846267923
Epoch 43: loss 16.045128916968075
Epoch 44: loss 16.043206530981184
Epoch 45: loss 16.041277226317895
Epoch 46: loss 16.03934921837164
Epoch 47: loss 16.03741749192529
Epoch 48: loss 16.03549406358637
Epoch 49: loss 16.03356358407307
-----------Time: 0:09:38.190166, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.0052008628845215-------------


Epoch 0: loss 16.128213360751694
Epoch 1: loss 16.126284206477504
Epoch 2: loss 16.12434722133925
Epoch 3: loss 16.122411287368934
Epoch 4: loss 16.120481943293328
Epoch 5: loss 16.1185442609201
Epoch 6: loss 16.11661371036092
Epoch 7: loss 16.114677263252627
Epoch 8: loss 16.11274488650307
Epoch 9: loss 16.110799367561363
Epoch 10: loss 16.10887683144466
Epoch 11: loss 16.10694254397568
Epoch 12: loss 16.10500143947259
Epoch 13: loss 16.10306830066517
Epoch 14: loss 16.101142646049013
Epoch 15: loss 16.09920857068052
Epoch 16: loss 16.09727599920301
Epoch 17: loss 16.095334171276498
Epoch 18: loss 16.093406175776238
Epoch 19: loss 16.091470588997307
Epoch 20: loss 16.089542405769965
Epoch 21: loss 16.087608608880597
Epoch 22: loss 16.085675234636447
Epoch 23: loss 16.08375000992568
Epoch 24: loss 16.08181730983956
Epoch 25: loss 16.07987935909958
Epoch 26: loss 16.07794693567754
Epoch 27: loss 16.076014371978776
Epoch 28: loss 16.07409248720245
Epoch 29: loss 16.072160631628904
Epoch 30: loss 16.070230476489336
Epoch 31: loss 16.068287816496245
Epoch 32: loss 16.066357126178918
Epoch 33: loss 16.064426298955656
Epoch 34: loss 16.062499198789116
Epoch 35: loss 16.060570088075906
Epoch 36: loss 16.058636798620093
Epoch 37: loss 16.056696856002436
Epoch 38: loss 16.054776847719044
Epoch 39: loss 16.05284533544751
Epoch 40: loss 16.050913227064704
Epoch 41: loss 16.048986700710362
Epoch 42: loss 16.04705111445001
Epoch 43: loss 16.045126098728232
Epoch 44: loss 16.043194927165793
Epoch 45: loss 16.041264680237017
Epoch 46: loss 16.03932429786089
Epoch 47: loss 16.037402606516057
Epoch 48: loss 16.035470793207033
Epoch 49: loss 16.033537493898145
-----------Time: 0:12:43.344147, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005198001861572-------------


Epoch 0: loss 16.128216048049303
Epoch 1: loss 16.126283483831955
Epoch 2: loss 16.124348296102717
Epoch 3: loss 16.12241306507179
Epoch 4: loss 16.120473348037777
Epoch 5: loss 16.11853977595418
Epoch 6: loss 16.116602471628028
Epoch 7: loss 16.114679332139897
Epoch 8: loss 16.112740452876878
Epoch 9: loss 16.11081000007061
Epoch 10: loss 16.108877102404332
Epoch 11: loss 16.1069453077643
Epoch 12: loss 16.105010882092923
Epoch 13: loss 16.103074430576676
Epoch 14: loss 16.101139100237084
Epoch 15: loss 16.099214462562383
Epoch 16: loss 16.09727521147528
Epoch 17: loss 16.095344315021176
Epoch 18: loss 16.093409761259775
Epoch 19: loss 16.09147533455123
Epoch 20: loss 16.089547449768464
Epoch 21: loss 16.087615345793616
Epoch 22: loss 16.085677697128286
Epoch 23: loss 16.083750133089165
Epoch 24: loss 16.081812446048687
Epoch 25: loss 16.079885324620243
Epoch 26: loss 16.07795426870183
Epoch 27: loss 16.076021710966774
Epoch 28: loss 16.074092473719286
Epoch 29: loss 16.072149839396058
Epoch 30: loss 16.070218548300215
Epoch 31: loss 16.06829133145248
Epoch 32: loss 16.06636698055423
Epoch 33: loss 16.064431162487235
Epoch 34: loss 16.06249465781622
Epoch 35: loss 16.060574461287167
Epoch 36: loss 16.058628044159533
Epoch 37: loss 16.056699261709422
Epoch 38: loss 16.05475959653372
Epoch 39: loss 16.052844914617324
Epoch 40: loss 16.05090462273395
Epoch 41: loss 16.048976547371893
Epoch 42: loss 16.047039979952324
Epoch 43: loss 16.04510903371424
Epoch 44: loss 16.043178586093806
Epoch 45: loss 16.04124847840459
Epoch 46: loss 16.039316877973928
Epoch 47: loss 16.037384569158757
Epoch 48: loss 16.035451953860974
Epoch 49: loss 16.03351730597674
-----------Time: 0:11:51.708360, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005195617675781-------------


Epoch 0: loss 16.11976680760801
Epoch 1: loss 16.100440778052956
Epoch 2: loss 16.081109858518584
Epoch 3: loss 16.061814642393312
Epoch 4: loss 16.042517961270786
Epoch 5: loss 16.023238001860246
Epoch 6: loss 16.003961563888225
Epoch 7: loss 15.984704432246346
Epoch 8: loss 15.965463544701414
Epoch 9: loss 15.946244144387839
Epoch 10: loss 15.927014642075523
Epoch 11: loss 15.907801028112148
Epoch 12: loss 15.888616559296732
Epoch 13: loss 15.869440001725762
Epoch 14: loss 15.85027976059408
Epoch 15: loss 15.831128207544324
Epoch 16: loss 15.81198964137108
Epoch 17: loss 15.792863972359472
Epoch 18: loss 15.773741086583348
Epoch 19: loss 15.754641390546889
Epoch 20: loss 15.735558620025568
Epoch 21: loss 15.716482489182937
Epoch 22: loss 15.697416692664275
Epoch 23: loss 15.67836919798029
Epoch 24: loss 15.659337842120886
Epoch 25: loss 15.6403170184249
Epoch 26: loss 15.621298189718091
Epoch 27: loss 15.602309402789416
Epoch 28: loss 15.583325309556356
Epoch 29: loss 15.564352307260004
Epoch 30: loss 15.545395812241521
Epoch 31: loss 15.526457887943055
Epoch 32: loss 15.507521565547775
Epoch 33: loss 15.488591988622138
Epoch 34: loss 15.469693511384152
Epoch 35: loss 15.450808587574713
Epoch 36: loss 15.431912638429328
Epoch 37: loss 15.413043113685678
Epoch 38: loss 15.39419387669068
Epoch 39: loss 15.37534619440781
Epoch 40: loss 15.356513230550412
Epoch 41: loss 15.33769073610788
Epoch 42: loss 15.318880614539474
Epoch 43: loss 15.30007834535633
Epoch 44: loss 15.28128281204904
Epoch 45: loss 15.262506333299795
Epoch 46: loss 15.243742396742057
Epoch 47: loss 15.2249839802421
Epoch 48: loss 15.206236364013025
Epoch 49: loss 15.187515436144482
-----------Time: 0:09:41.256733, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.897535800933838-------------


Epoch 0: loss 16.11959636905
Epoch 1: loss 16.10025864259389
Epoch 2: loss 16.08093929498205
Epoch 3: loss 16.06164044696005
Epoch 4: loss 16.04234270637226
Epoch 5: loss 16.023052388523634
Epoch 6: loss 16.003776379160545
Epoch 7: loss 15.984529713044159
Epoch 8: loss 15.965275841993504
Epoch 9: loss 15.946038359465192
Epoch 10: loss 15.926808748509716
Epoch 11: loss 15.907599995485008
Epoch 12: loss 15.888392978417219
Epoch 13: loss 15.869206772607717
Epoch 14: loss 15.850024527217332
Epoch 15: loss 15.8308534541812
Epoch 16: loss 15.811695962836394
Epoch 17: loss 15.792554786375128
Epoch 18: loss 15.773406090199655
Epoch 19: loss 15.754279036571177
Epoch 20: loss 15.73515791475546
Epoch 21: loss 15.716056358042847
Epoch 22: loss 15.696946674873804
Epoch 23: loss 15.677845693529138
Epoch 24: loss 15.658739560579981
Epoch 25: loss 15.639651323414938
Epoch 26: loss 15.620561675703868
Epoch 27: loss 15.601486290324441
Epoch 28: loss 15.58240840667094
Epoch 29: loss 15.563326545485081
Epoch 30: loss 15.54424201022548
Epoch 31: loss 15.525142296557195
Epoch 32: loss 15.50605301937375
Epoch 33: loss 15.486948817368758
Epoch 34: loss 15.467830489938061
Epoch 35: loss 15.448707820411089
Epoch 36: loss 15.429570166944096
Epoch 37: loss 15.410417663590813
Epoch 38: loss 15.391239472223793
Epoch 39: loss 15.37204013070443
Epoch 40: loss 15.352810322168795
Epoch 41: loss 15.333559876100209
Epoch 42: loss 15.314265529898083
Epoch 43: loss 15.294926319516438
Epoch 44: loss 15.275541138558236
Epoch 45: loss 15.256107441817631
Epoch 46: loss 15.236604170672203
Epoch 47: loss 15.217039345009551
Epoch 48: loss 15.197416990590265
Epoch 49: loss 15.17769282534435
-----------Time: 0:07:35.928210, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.8962275981903076-------------


Epoch 0: loss 16.119556813864943
Epoch 1: loss 16.100229733401243
Epoch 2: loss 16.080912539465004
Epoch 3: loss 16.061586485536544
Epoch 4: loss 16.042285058860095
Epoch 5: loss 16.022990025794655
Epoch 6: loss 16.003704428348676
Epoch 7: loss 15.984413861320872
Epoch 8: loss 15.965149159402936
Epoch 9: loss 15.94585850317884
Epoch 10: loss 15.926589456312419
Epoch 11: loss 15.907320285245346
Epoch 12: loss 15.88804369506919
Epoch 13: loss 15.868737984637063
Epoch 14: loss 15.849425115683857
Epoch 15: loss 15.830085924489786
Epoch 16: loss 15.810709820550832
Epoch 17: loss 15.791303307676912
Epoch 18: loss 15.77183470484872
Epoch 19: loss 15.752270150664321
Epoch 20: loss 15.732629623796837
Epoch 21: loss 15.712874254628067
Epoch 22: loss 15.692978203199427
Epoch 23: loss 15.672912530499739
Epoch 24: loss 15.652646703134094
Epoch 25: loss 15.632135026930726
Epoch 26: loss 15.611359340331683
Epoch 27: loss 15.590269525392085
Epoch 28: loss 15.568816092949058
Epoch 29: loss 15.546941673969561
Epoch 30: loss 15.524608622950792
Epoch 31: loss 15.50177389334699
Epoch 32: loss 15.478355292330104
Epoch 33: loss 15.454317760571247
Epoch 34: loss 15.429607506223578
Epoch 35: loss 15.404149342516702
Epoch 36: loss 15.377907143914875
Epoch 37: loss 15.350827028856386
Epoch 38: loss 15.322854978095714
Epoch 39: loss 15.293923353617316
Epoch 40: loss 15.264019169841141
Epoch 41: loss 15.233059281820575
Epoch 42: loss 15.201043944439206
Epoch 43: loss 15.1679073329073
Epoch 44: loss 15.133613580721367
Epoch 45: loss 15.098173114513171
Epoch 46: loss 15.061503618291178
Epoch 47: loss 15.023595589279415
Epoch 48: loss 14.984444907075883
Epoch 49: loss 14.94402064649614
-----------Time: 0:11:33.819324, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.864841938018799-------------


Epoch 0: loss 16.11957942305083
Epoch 1: loss 16.100242824253666
Epoch 2: loss 16.080913756320243
Epoch 3: loss 16.061596900759476
Epoch 4: loss 16.042296363453037
Epoch 5: loss 16.022986378340438
Epoch 6: loss 16.003693414162267
Epoch 7: loss 15.984394751533728
Epoch 8: loss 15.965090037299733
Epoch 9: loss 15.945754035391712
Epoch 10: loss 15.92638785817041
Epoch 11: loss 15.906965691087816
Epoch 12: loss 15.887465695053939
Epoch 13: loss 15.867870896066123
Epoch 14: loss 15.848135558465955
Epoch 15: loss 15.828208791101154
Epoch 16: loss 15.808008540123945
Epoch 17: loss 15.787493358039544
Epoch 18: loss 15.76655653418893
Epoch 19: loss 15.74508563612647
Epoch 20: loss 15.722984045598091
Epoch 21: loss 15.700150225848333
Epoch 22: loss 15.676412018697635
Epoch 23: loss 15.651688620602068
Epoch 24: loss 15.625809624896483
Epoch 25: loss 15.598667664914238
Epoch 26: loss 15.570176899854484
Epoch 27: loss 15.540183558679262
Epoch 28: loss 15.508625729567594
Epoch 29: loss 15.475416412685409
Epoch 30: loss 15.440443097540319
Epoch 31: loss 15.40363467783303
Epoch 32: loss 15.364962377387457
Epoch 33: loss 15.324368386634216
Epoch 34: loss 15.281801175785946
Epoch 35: loss 15.23720806794947
Epoch 36: loss 15.19056776802847
Epoch 37: loss 15.141844057142249
Epoch 38: loss 15.09101965065687
Epoch 39: loss 15.038092775796017
Epoch 40: loss 14.983038829681082
Epoch 41: loss 14.925833136831827
Epoch 42: loss 14.866490481274488
Epoch 43: loss 14.805021333201804
Epoch 44: loss 14.74139749828793
Epoch 45: loss 14.675649693246896
Epoch 46: loss 14.607771803983987
Epoch 47: loss 14.537771733187022
Epoch 48: loss 14.465692739761543
Epoch 49: loss 14.391518965436429
-----------Time: 0:10:30.962232, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7907466888427734-------------


Epoch 0: loss 16.119591189960605
Epoch 1: loss 16.10026438019728
Epoch 2: loss 16.080940280808516
Epoch 3: loss 16.061623688687963
Epoch 4: loss 16.042293883847957
Epoch 5: loss 16.022971938134795
Epoch 6: loss 16.00363402032152
Epoch 7: loss 15.984258928656773
Epoch 8: loss 15.96481532568255
Epoch 9: loss 15.945280358738197
Epoch 10: loss 15.925594398805536
Epoch 11: loss 15.905673261438134
Epoch 12: loss 15.885427169789432
Epoch 13: loss 15.864700673130818
Epoch 14: loss 15.843327289434022
Epoch 15: loss 15.821118031460802
Epoch 16: loss 15.79784049438095
Epoch 17: loss 15.773261726777667
Epoch 18: loss 15.747149884409072
Epoch 19: loss 15.719293683297353
Epoch 20: loss 15.68946985137923
Epoch 21: loss 15.657503601518126
Epoch 22: loss 15.62320133886498
Epoch 23: loss 15.586467140326363
Epoch 24: loss 15.54715629036236
Epoch 25: loss 15.505173464583729
Epoch 26: loss 15.46045323024955
Epoch 27: loss 15.412922345794628
Epoch 28: loss 15.362547894156842
Epoch 29: loss 15.30930558957115
Epoch 30: loss 15.253163801839392
Epoch 31: loss 15.194109158777815
Epoch 32: loss 15.132126801424924
Epoch 33: loss 15.067188023613353
Epoch 34: loss 14.99933056030149
Epoch 35: loss 14.92855199515139
Epoch 36: loss 14.854834539725639
Epoch 37: loss 14.778246544832248
Epoch 38: loss 14.69879356657052
Epoch 39: loss 14.61642436893042
Epoch 40: loss 14.531240864899445
Epoch 41: loss 14.44322250821527
Epoch 42: loss 14.352415864787845
Epoch 43: loss 14.258846119605309
Epoch 44: loss 14.162593490731268
Epoch 45: loss 14.063698676307931
Epoch 46: loss 13.962091035205038
Epoch 47: loss 13.85784610916312
Epoch 48: loss 13.751071446612194
Epoch 49: loss 13.641816967139107
-----------Time: 0:14:05.147676, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.6886394023895264-------------


Epoch 0: loss 16.033455505132544
Epoch 1: loss 15.84158239732818
Epoch 2: loss 15.651074413114944
Epoch 3: loss 15.46173673949208
Epoch 4: loss 15.2736079756367
Epoch 5: loss 15.086616106951217
Epoch 6: loss 14.900738375934976
Epoch 7: loss 14.71577433466328
Epoch 8: loss 14.531542507314759
Epoch 9: loss 14.347760082522834
Epoch 10: loss 14.164108285442392
Epoch 11: loss 13.980079333506051
Epoch 12: loss 13.794881880315767
Epoch 13: loss 13.607657205676047
Epoch 14: loss 13.417363216852351
Epoch 15: loss 13.2226144993934
Epoch 16: loss 13.021991884792675
Epoch 17: loss 12.814040322741974
Epoch 18: loss 12.596958718395804
Epoch 19: loss 12.36915251170247
Epoch 20: loss 12.129183319355237
Epoch 21: loss 11.875719290573095
Epoch 22: loss 11.607719861145185
Epoch 23: loss 11.324310783201614
Epoch 24: loss 11.025227627590859
Epoch 25: loss 10.710291838114387
Epoch 26: loss 10.379659496357156
Epoch 27: loss 10.033774805820915
Epoch 28: loss 9.673385747429338
Epoch 29: loss 9.299654708600938
Epoch 30: loss 8.914061318278767
Epoch 31: loss 8.518031562910966
Epoch 32: loss 8.113086813490568
Epoch 33: loss 7.701374831829725
Epoch 34: loss 7.285168828362677
Epoch 35: loss 6.8667445304667325
Epoch 36: loss 6.4480150159511185
Epoch 37: loss 6.031899898450228
Epoch 38: loss 5.620775482718098
Epoch 39: loss 5.217422891920711
Epoch 40: loss 4.82442490117196
Epoch 41: loss 4.444778422093249
Epoch 42: loss 4.081084852765733
Epoch 43: loss 3.7360213582706296
Epoch 44: loss 3.412294630838387
Epoch 45: loss 3.112150626120326
Epoch 46: loss 2.8375861310258776
Epoch 47: loss 2.590202913879376
Epoch 48: loss 2.370673382748982
Epoch 49: loss 2.179274495883504
-----------Time: 0:06:53.934236, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.4580190181732178-------------


Epoch 0: loss 16.033377019655063
Epoch 1: loss 15.841421709688154
Epoch 2: loss 15.650290598617811
Epoch 3: loss 15.459278625791134
Epoch 4: loss 15.265901204877213
Epoch 5: loss 15.06495503409522
Epoch 6: loss 14.846907706672955
Epoch 7: loss 14.598779661490775
Epoch 8: loss 14.307326348207773
Epoch 9: loss 13.961918472789954
Epoch 10: loss 13.556800679450063
Epoch 11: loss 13.090218954724161
Epoch 12: loss 12.564061130888764
Epoch 13: loss 11.981838301253617
Epoch 14: loss 11.349445941721765
Epoch 15: loss 10.673817646509931
Epoch 16: loss 9.962537587369635
Epoch 17: loss 9.2243304559885
Epoch 18: loss 8.468510859423581
Epoch 19: loss 7.7050933018529335
Epoch 20: loss 6.944330851625916
Epoch 21: loss 6.197771377573848
Epoch 22: loss 5.476887599225275
Epoch 23: loss 4.79377496125842
Epoch 24: loss 4.1599150820877835
Epoch 25: loss 3.586575400122746
Epoch 26: loss 3.083917961748091
Epoch 27: loss 2.6585172495808274
Epoch 28: loss 2.312589874988409
Epoch 29: loss 2.041852314521464
Epoch 30: loss 1.836063512548796
Epoch 31: loss 1.6813704939949052
Epoch 32: loss 1.5641482301935545
Epoch 33: loss 1.4733009898448652
Epoch 34: loss 1.4010070801883239
Epoch 35: loss 1.3421486964557663
Epoch 36: loss 1.2932801693434817
Epoch 37: loss 1.252129957104196
Epoch 38: loss 1.2171866601092975
Epoch 39: loss 1.1872765468863964
Epoch 40: loss 1.1615554380183508
Epoch 41: loss 1.139379222345067
Epoch 42: loss 1.1202001276549078
Epoch 43: loss 1.1035550385308435
Epoch 44: loss 1.089100173726686
Epoch 45: loss 1.0765171942826262
Epoch 46: loss 1.065539968765709
Epoch 47: loss 1.0559457560121008
Epoch 48: loss 1.0475513354113986
Epoch 49: loss 1.0401916366805843
-----------Time: 0:09:33.927562, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.028394103050232-------------


Epoch 0: loss 16.03333027483394
Epoch 1: loss 15.840729319576079
Epoch 2: loss 15.64328795220944
Epoch 3: loss 15.414823726832445
Epoch 4: loss 15.10139903586607
Epoch 5: loss 14.655903582082876
Epoch 6: loss 14.063212383046237
Epoch 7: loss 13.329787201956604
Epoch 8: loss 12.471647377439398
Epoch 9: loss 11.509295063475152
Epoch 10: loss 10.465609012964693
Epoch 11: loss 9.366990057523644
Epoch 12: loss 8.24087915918372
Epoch 13: loss 7.1178721065687185
Epoch 14: loss 6.030203277096274
Epoch 15: loss 5.011676714107872
Epoch 16: loss 4.0961294651939015
Epoch 17: loss 3.314027473678921
Epoch 18: loss 2.68750379009698
Epoch 19: loss 2.221477479471857
Epoch 20: loss 1.8970360137834181
Epoch 21: loss 1.6779490809780284
Epoch 22: loss 1.5272070659833994
Epoch 23: loss 1.4184909565997164
Epoch 24: loss 1.3363863207711806
Epoch 25: loss 1.272342156025299
Epoch 26: loss 1.2212654191589407
Epoch 27: loss 1.1801322271896484
Epoch 28: loss 1.1467296421009279
Epoch 29: loss 1.1194349807393882
Epoch 30: loss 1.097086653704874
Epoch 31: loss 1.0787127693591396
Epoch 32: loss 1.0635511080910163
Epoch 33: loss 1.0509918240699643
Epoch 34: loss 1.0405410307136937
Epoch 35: loss 1.0318414187256313
Epoch 36: loss 1.0245702462944406
Epoch 37: loss 1.0184531193864677
Epoch 38: loss 1.01330149194804
Epoch 39: loss 1.0089573529409674
Epoch 40: loss 1.0052588915552634
Epoch 41: loss 1.00212485658014
Epoch 42: loss 0.9994417216837439
Epoch 43: loss 0.9971486005269684
Epoch 44: loss 0.9951799805881278
Epoch 45: loss 0.9934849604567216
Epoch 46: loss 0.9920292259105332
Epoch 47: loss 0.9907701467125879
Epoch 48: loss 0.9896676592395123
Epoch 49: loss 0.9887144421675466
-----------Time: 0:09:32.371485, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.003576636314392-------------


Epoch 0: loss 16.0333845751515
Epoch 1: loss 15.837703933788422
Epoch 2: loss 15.595034599822782
Epoch 3: loss 15.177219655346521
Epoch 4: loss 14.512745020504942
Epoch 5: loss 13.619786228285722
Epoch 6: loss 12.534221614424855
Epoch 7: loss 11.296621759538096
Epoch 8: loss 9.953683884005109
Epoch 9: loss 8.555893989410524
Epoch 10: loss 7.157951362240632
Epoch 11: loss 5.81989553235289
Epoch 12: loss 4.603595776231215
Epoch 13: loss 3.5687431445777773
Epoch 14: loss 2.76083326122565
Epoch 15: loss 2.1927937851968573
Epoch 16: loss 1.8278996198640174
Epoch 17: loss 1.5998395951044437
Epoch 18: loss 1.4501661442426834
Epoch 19: loss 1.3449393208257656
Epoch 20: loss 1.2670663699564433
Epoch 21: loss 1.2078207270842392
Epoch 22: loss 1.1620263841535943
Epoch 23: loss 1.126366409093806
Epoch 24: loss 1.0983923836659322
Epoch 25: loss 1.0763408647988917
Epoch 26: loss 1.058876956413465
Epoch 27: loss 1.044960949053383
Epoch 28: loss 1.033822891276579
Epoch 29: loss 1.0248719732874174
Epoch 30: loss 1.017628179629125
Epoch 31: loss 1.0117529720278653
Epoch 32: loss 1.006950065355835
Epoch 33: loss 1.0030061587034975
Epoch 34: loss 0.9997625074320478
Epoch 35: loss 0.9970841149294616
Epoch 36: loss 0.9948537345325383
Epoch 37: loss 0.992990407154831
Epoch 38: loss 0.9914286798144761
Epoch 39: loss 0.9901149896190762
Epoch 40: loss 0.9890116105882159
Epoch 41: loss 0.9880754725481907
Epoch 42: loss 0.987274164330901
Epoch 43: loss 0.9865935295901265
Epoch 44: loss 0.9860052414858321
Epoch 45: loss 0.98551222857021
Epoch 46: loss 0.9850739644979121
Epoch 47: loss 0.9846916801907434
Epoch 48: loss 0.9843606492175299
Epoch 49: loss 0.9840788623118543
-----------Time: 0:12:34.991319, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0010857582092285-------------


Epoch 0: loss 16.03322953021416
Epoch 1: loss 15.826487725036438
Epoch 2: loss 15.468295466063656
Epoch 3: loss 14.777073739597887
Epoch 4: loss 13.761440038032802
Epoch 5: loss 12.484647147483058
Epoch 6: loss 11.013311094664697
Epoch 7: loss 9.41995945982858
Epoch 8: loss 7.7870446050082816
Epoch 9: loss 6.20293508488695
Epoch 10: loss 4.762624949135814
Epoch 11: loss 3.557113259949718
Epoch 12: loss 2.655131392954743
Epoch 13: loss 2.064141591616854
Epoch 14: loss 1.7140394664770109
Epoch 15: loss 1.5054177819807417
Epoch 16: loss 1.3701844424770993
Epoch 17: loss 1.275588810978667
Epoch 18: loss 1.2066491702090405
Epoch 19: loss 1.1553581686464582
Epoch 20: loss 1.1167777070472784
Epoch 21: loss 1.0875788332004142
Epoch 22: loss 1.0652807567870248
Epoch 23: loss 1.0481844506813431
Epoch 24: loss 1.0349700976448775
Epoch 25: loss 1.0246832904412475
Epoch 26: loss 1.0166213569626852
Epoch 27: loss 1.0102742342860755
Epoch 28: loss 1.0052337018512656
Epoch 29: loss 1.0012157315660781
Epoch 30: loss 0.9979655978924687
Epoch 31: loss 0.995363548746441
Epoch 32: loss 0.9932347184098759
Epoch 33: loss 0.9915100997106221
Epoch 34: loss 0.9900935377648111
Epoch 35: loss 0.9889221967174152
Epoch 36: loss 0.987949247477559
Epoch 37: loss 0.9871440934156062
Epoch 38: loss 0.9864746539277703
Epoch 39: loss 0.9859071473442645
Epoch 40: loss 0.9854263742862025
Epoch 41: loss 0.9850142897471842
Epoch 42: loss 0.9846618706730152
Epoch 43: loss 0.9843630820042806
Epoch 44: loss 0.9841094713550731
Epoch 45: loss 0.9838794704006054
Epoch 46: loss 0.9836849821300984
Epoch 47: loss 0.9835109154350931
Epoch 48: loss 0.9833558756997884
Epoch 49: loss 0.9832224632294817
-----------Time: 0:11:51.252459, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0004702806472778-------------


Epoch 0: loss 15.196246005063992
Epoch 1: loss 13.328841262566389
Epoch 2: loss 10.628909722302776
Epoch 3: loss 6.506286565929473
Epoch 4: loss 2.953124556481806
Epoch 5: loss 1.5135064937291811
Epoch 6: loss 1.1825987822566102
Epoch 7: loss 1.0733140456799646
Epoch 8: loss 1.0275694539587157
Epoch 9: loss 1.0067484052431461
Epoch 10: loss 0.9966396163539306
Epoch 11: loss 0.9913799724156213
Epoch 12: loss 0.9884376629271411
Epoch 13: loss 0.9866351337618254
Epoch 14: loss 0.9854537833638007
Epoch 15: loss 0.9845962169022583
Epoch 16: loss 0.983948706786486
Epoch 17: loss 0.9833231222344326
Epoch 18: loss 0.9828107439350213
Epoch 19: loss 0.9822962840430564
Epoch 20: loss 0.9818116429384925
Epoch 21: loss 0.9812583919288413
Epoch 22: loss 0.980760244943319
Epoch 23: loss 0.9802082600380948
Epoch 24: loss 0.9796463758352983
Epoch 25: loss 0.9790261804150006
Epoch 26: loss 0.9783883523428939
Epoch 27: loss 0.977685127069018
Epoch 28: loss 0.9769724547830595
Epoch 29: loss 0.9762000281924849
Epoch 30: loss 0.9753533975302752
Epoch 31: loss 0.9744636756593342
Epoch 32: loss 0.9735147644800619
Epoch 33: loss 0.9725237851592754
Epoch 34: loss 0.9714479580432452
Epoch 35: loss 0.9702505602695295
Epoch 36: loss 0.9690475594647362
Epoch 37: loss 0.9677233913464673
Epoch 38: loss 0.9663385793915127
Epoch 39: loss 0.9648345720191052
Epoch 40: loss 0.9632528744001891
Epoch 41: loss 0.9615906605972033
Epoch 42: loss 0.9598070053771632
Epoch 43: loss 0.9579538490831885
Epoch 44: loss 0.9559829226163757
Epoch 45: loss 0.9539449821839584
Epoch 46: loss 0.9517571490507142
Epoch 47: loss 0.949550600545182
Epoch 48: loss 0.9472154189284047
Epoch 49: loss 0.9447914186283711
-----------Time: 0:09:33.149796, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9952397346496582-------------


Epoch 0: loss 15.100381538231389
Epoch 1: loss 9.883326431796108
Epoch 2: loss 3.1530405281353673
Epoch 3: loss 1.326894656091102
Epoch 4: loss 1.0823239025018214
Epoch 5: loss 1.0206183265284912
Epoch 6: loss 1.001885230755793
Epoch 7: loss 0.995232839568793
Epoch 8: loss 0.9924154096708925
Epoch 9: loss 0.9910827062420381
Epoch 10: loss 0.9902309263737841
Epoch 11: loss 0.9897122485537837
Epoch 12: loss 0.9892480919191279
Epoch 13: loss 0.9887959972356182
Epoch 14: loss 0.9883559593174236
Epoch 15: loss 0.9879185752482308
Epoch 16: loss 0.9874194327290127
Epoch 17: loss 0.9868087693036024
Epoch 18: loss 0.9862300331272335
Epoch 19: loss 0.9855725567518465
Epoch 20: loss 0.9847997323432132
Epoch 21: loss 0.9839014563954609
Epoch 22: loss 0.9829440679058813
Epoch 23: loss 0.9818185486178997
Epoch 24: loss 0.9806501546620415
Epoch 25: loss 0.9792961184747716
Epoch 26: loss 0.9777644434786802
Epoch 27: loss 0.9760572868336018
Epoch 28: loss 0.9742549343592709
Epoch 29: loss 0.9722319656153487
Epoch 30: loss 0.9700619972160551
Epoch 31: loss 0.967774937890547
Epoch 32: loss 0.9652815750755
Epoch 33: loss 0.9627143689281854
Epoch 34: loss 0.9600360526573146
Epoch 35: loss 0.9572675141904456
Epoch 36: loss 0.954294508034409
Epoch 37: loss 0.951370607030463
Epoch 38: loss 0.9482247414732835
Epoch 39: loss 0.945024160958944
Epoch 40: loss 0.9417819055997528
Epoch 41: loss 0.9382801079730614
Epoch 42: loss 0.9347346559888063
Epoch 43: loss 0.9309991777850727
Epoch 44: loss 0.9270552346698612
Epoch 45: loss 0.9229331231752513
Epoch 46: loss 0.9185669396220504
Epoch 47: loss 0.9139165915731116
Epoch 48: loss 0.9090526605702536
Epoch 49: loss 0.9039755677839801
-----------Time: 0:07:53.127766, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9903994798660278-------------


Epoch 0: loss 13.840570261064855
Epoch 1: loss 3.95629320101222
Epoch 2: loss 1.2462370392581312
Epoch 3: loss 1.0421147455723148
Epoch 4: loss 1.0069285767956102
Epoch 5: loss 0.9989417485726664
Epoch 6: loss 0.9964956048229844
Epoch 7: loss 0.9955465426712078
Epoch 8: loss 0.9950588956645418
Epoch 9: loss 0.9946210714317393
Epoch 10: loss 0.994340879849729
Epoch 11: loss 0.9940009620638242
Epoch 12: loss 0.9936478904986783
Epoch 13: loss 0.9932574546162106
Epoch 14: loss 0.9927806517655205
Epoch 15: loss 0.9923170167120207
Epoch 16: loss 0.9917047736014667
Epoch 17: loss 0.9910155883491396
Epoch 18: loss 0.9901919433155288
Epoch 19: loss 0.9892769874017351
Epoch 20: loss 0.9882337895666147
Epoch 21: loss 0.9870195660725957
Epoch 22: loss 0.985544786302858
Epoch 23: loss 0.983840487698876
Epoch 24: loss 0.9819255368647334
Epoch 25: loss 0.9797095789930106
Epoch 26: loss 0.9772743373347599
Epoch 27: loss 0.9745550320289782
Epoch 28: loss 0.9714451701567185
Epoch 29: loss 0.9681448488212138
Epoch 30: loss 0.96446734232511
Epoch 31: loss 0.960606587830933
Epoch 32: loss 0.9563561365162309
Epoch 33: loss 0.951901706827925
Epoch 34: loss 0.9472456654658326
Epoch 35: loss 0.9421922975159521
Epoch 36: loss 0.9367277494521813
Epoch 37: loss 0.9311161765979644
Epoch 38: loss 0.924922535136188
Epoch 39: loss 0.9184313112623734
Epoch 40: loss 0.9113828717300981
Epoch 41: loss 0.9038363568415909
Epoch 42: loss 0.8956728834474521
Epoch 43: loss 0.886961808828247
Epoch 44: loss 0.8775684784585331
Epoch 45: loss 0.8675111498989574
Epoch 46: loss 0.8568458370763106
Epoch 47: loss 0.8453869522266119
Epoch 48: loss 0.8332915521627409
Epoch 49: loss 0.8204627330533961
-----------Time: 0:11:29.067568, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.985887348651886-------------


Epoch 0: loss 12.21921567310128
Epoch 1: loss 2.1538191828441207
Epoch 2: loss 1.0901210849021945
Epoch 3: loss 1.0138657687636028
Epoch 4: loss 1.0022047405790024
Epoch 5: loss 0.9995251230210828
Epoch 6: loss 0.9984443538243386
Epoch 7: loss 0.9980791767956836
Epoch 8: loss 0.9976187866300652
Epoch 9: loss 0.9970798813142875
Epoch 10: loss 0.996740938642869
Epoch 11: loss 0.99578383182494
Epoch 12: loss 0.9950363990273924
Epoch 13: loss 0.9940504603505199
Epoch 14: loss 0.9929804798213602
Epoch 15: loss 0.991335698286433
Epoch 16: loss 0.989893716434346
Epoch 17: loss 0.9879969467508721
Epoch 18: loss 0.9858446526845037
Epoch 19: loss 0.9836214278916291
Epoch 20: loss 0.981149127701821
Epoch 21: loss 0.9786000703909657
Epoch 22: loss 0.9755740714922361
Epoch 23: loss 0.9726025635875911
Epoch 24: loss 0.9693124095112685
Epoch 25: loss 0.9659571883923467
Epoch 26: loss 0.9624444943624323
Epoch 27: loss 0.9586329459657872
Epoch 28: loss 0.9543583792405909
Epoch 29: loss 0.9497813828423206
Epoch 30: loss 0.9448103517893541
Epoch 31: loss 0.9393221895418588
Epoch 32: loss 0.9331320686597031
Epoch 33: loss 0.9261852655642833
Epoch 34: loss 0.9184556431326936
Epoch 35: loss 0.9098676972865021
Epoch 36: loss 0.9005207232867589
Epoch 37: loss 0.8900123241156453
Epoch 38: loss 0.8785665283681517
Epoch 39: loss 0.866098841112421
Epoch 40: loss 0.8527009275343316
Epoch 41: loss 0.8382276415889713
Epoch 42: loss 0.8224758860888592
Epoch 43: loss 0.8060027857783568
Epoch 44: loss 0.788280574603856
Epoch 45: loss 0.7696300448769782
Epoch 46: loss 0.7499374154795118
Epoch 47: loss 0.7293498006088699
Epoch 48: loss 0.7079826093047388
Epoch 49: loss 0.6858272419198173
-----------Time: 0:10:36.115003, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9872286319732666-------------


Epoch 0: loss 10.808033442873226
Epoch 1: loss 1.5460482197750387
Epoch 2: loss 1.0452449765945662
Epoch 3: loss 1.008286922927004
Epoch 4: loss 1.0029087934304737
Epoch 5: loss 1.0014952757457083
Epoch 6: loss 1.0010340628480057
Epoch 7: loss 1.0006521792759255
Epoch 8: loss 1.0002726092651009
Epoch 9: loss 0.9999634596463971
Epoch 10: loss 0.9996074986496718
Epoch 11: loss 0.9990176041856157
Epoch 12: loss 0.9986145890491562
Epoch 13: loss 0.997989533646331
Epoch 14: loss 0.997280045493003
Epoch 15: loss 0.9961443022096073
Epoch 16: loss 0.9950044020451302
Epoch 17: loss 0.9934494240475842
Epoch 18: loss 0.99174235337386
Epoch 19: loss 0.98931500210329
Epoch 20: loss 0.9867014274641247
Epoch 21: loss 0.9834227653350954
Epoch 22: loss 0.9797123371096265
Epoch 23: loss 0.9752946021830406
Epoch 24: loss 0.9705392786055042
Epoch 25: loss 0.9652144522236508
Epoch 26: loss 0.9596576903454955
Epoch 27: loss 0.9535506673323324
Epoch 28: loss 0.9470199353672876
Epoch 29: loss 0.9396838879475067
Epoch 30: loss 0.9316818151641242
Epoch 31: loss 0.9228227799550679
Epoch 32: loss 0.9126998665962872
Epoch 33: loss 0.901377566276263
Epoch 34: loss 0.888882749199932
Epoch 35: loss 0.8750156268534159
Epoch 36: loss 0.8597211006577083
Epoch 37: loss 0.8429182262735694
Epoch 38: loss 0.8246627536203759
Epoch 39: loss 0.8047836787224854
Epoch 40: loss 0.7835222112218474
Epoch 41: loss 0.760571516163003
Epoch 42: loss 0.7362959846781284
Epoch 43: loss 0.7107181433849583
Epoch 44: loss 0.6840985553656148
Epoch 45: loss 0.6562923763905484
Epoch 46: loss 0.6278373642483763
Epoch 47: loss 0.5989334511329066
Epoch 48: loss 0.569599249072539
Epoch 49: loss 0.5402188612155255
-----------Time: 0:13:52.419280, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9954748153686523-------------


Epoch 0: loss 5.1839356150330245
Epoch 1: loss 1.028844787268745
Epoch 2: loss 1.015810181340684
Epoch 3: loss 1.0003529969053855
Epoch 4: loss 0.9852034518613706
Epoch 5: loss 0.9679629240041197
Epoch 6: loss 0.9486011856758705
Epoch 7: loss 0.9266343112708824
Epoch 8: loss 0.9021265254421297
Epoch 9: loss 0.8768155769753417
Epoch 10: loss 0.8522257810866722
Epoch 11: loss 0.8304511900503004
Epoch 12: loss 0.8117795509932935
Epoch 13: loss 0.7967076988049082
Epoch 14: loss 0.784533332423583
Epoch 15: loss 0.7748587012550247
Epoch 16: loss 0.7668355194170103
Epoch 17: loss 0.7603930939695899
Epoch 18: loss 0.7545348577618664
Epoch 19: loss 0.749588522430086
Epoch 20: loss 0.7452737393425883
Epoch 21: loss 0.7412132702650357
Epoch 22: loss 0.737882756397671
Epoch 23: loss 0.7346493060365846
Epoch 24: loss 0.7316235341151944
Epoch 25: loss 0.7289695706038193
Epoch 26: loss 0.7266064351865168
Epoch 27: loss 0.7239711459011018
Epoch 28: loss 0.7215308260502797
Epoch 29: loss 0.7197114110738444
Epoch 30: loss 0.7179970207617554
Epoch 31: loss 0.7159605930038222
Epoch 32: loss 0.7143026245327473
Epoch 33: loss 0.7127059140141857
Epoch 34: loss 0.7112112923233065
Epoch 35: loss 0.709508341111652
Epoch 36: loss 0.708270341457719
Epoch 37: loss 0.7067436669153516
Epoch 38: loss 0.7054876118266369
Epoch 39: loss 0.7042946822135069
Epoch 40: loss 0.702994009012953
Epoch 41: loss 0.7017781502886788
Epoch 42: loss 0.7009122387474163
Epoch 43: loss 0.6995260615900458
Epoch 44: loss 0.6986290197890371
Epoch 45: loss 0.6977193821231719
Epoch 46: loss 0.6968259289982268
Epoch 47: loss 0.695696158625303
Epoch 48: loss 0.6948518189173668
Epoch 49: loss 0.6940046338412086
-----------Time: 0:06:59.523521, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1206327676773071-------------


Epoch 0: loss 3.54096027555642
Epoch 1: loss 1.0503507038216802
Epoch 2: loss 1.0318398571584846
Epoch 3: loss 1.010073317140908
Epoch 4: loss 0.9812426498041262
Epoch 5: loss 0.9384817147365143
Epoch 6: loss 0.8813900218961549
Epoch 7: loss 0.8129448581156749
Epoch 8: loss 0.741891402583073
Epoch 9: loss 0.6790380087347379
Epoch 10: loss 0.6281890563824827
Epoch 11: loss 0.5904020582320186
Epoch 12: loss 0.5621660765485312
Epoch 13: loss 0.5407924219325809
Epoch 14: loss 0.5237924893178519
Epoch 15: loss 0.5097347280783391
Epoch 16: loss 0.4983537365814601
Epoch 17: loss 0.48856415861712127
Epoch 18: loss 0.48012026510788347
Epoch 19: loss 0.47262313473380213
Epoch 20: loss 0.4660968077432338
Epoch 21: loss 0.46038426219801204
Epoch 22: loss 0.4549478235664155
Epoch 23: loss 0.4500934102731013
Epoch 24: loss 0.44575881169599446
Epoch 25: loss 0.4419463018219778
Epoch 26: loss 0.4382244784459917
Epoch 27: loss 0.4348782290091263
Epoch 28: loss 0.4316480883101914
Epoch 29: loss 0.42879254330008365
Epoch 30: loss 0.4259024361850387
Epoch 31: loss 0.42357976910470296
Epoch 32: loss 0.4211103409010149
Epoch 33: loss 0.4187303282656768
Epoch 34: loss 0.4166529664061781
Epoch 35: loss 0.41461986074018503
Epoch 36: loss 0.4127283693413296
Epoch 37: loss 0.4107139489292904
Epoch 38: loss 0.40927844880682546
Epoch 39: loss 0.40742520738824545
Epoch 40: loss 0.4058862409801053
Epoch 41: loss 0.4045562464024724
Epoch 42: loss 0.40295756821352996
Epoch 43: loss 0.40139548237830025
Epoch 44: loss 0.40029279149100466
Epoch 45: loss 0.3990143953927384
Epoch 46: loss 0.39787763227921846
Epoch 47: loss 0.3965872331983762
Epoch 48: loss 0.3956810292778228
Epoch 49: loss 0.3944102106833665
-----------Time: 0:09:13.836821, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3678861856460571-------------


Epoch 0: loss 2.8970291299655297
Epoch 1: loss 1.0702186864215826
Epoch 2: loss 1.036572072900074
Epoch 3: loss 0.9900009421436212
Epoch 4: loss 0.9122451564875421
Epoch 5: loss 0.7974520637296736
Epoch 6: loss 0.6647456879360382
Epoch 7: loss 0.5423100062642426
Epoch 8: loss 0.4478145958030269
Epoch 9: loss 0.3803922183297133
Epoch 10: loss 0.33323165545034433
Epoch 11: loss 0.29937898405726804
Epoch 12: loss 0.2740847267749324
Epoch 13: loss 0.2541010391945551
Epoch 14: loss 0.23837043177042724
Epoch 15: loss 0.22541078380417928
Epoch 16: loss 0.21459507899457117
Epoch 17: loss 0.20571940049504767
Epoch 18: loss 0.19774181223583326
Epoch 19: loss 0.19073299173308236
Epoch 20: loss 0.18489172863670184
Epoch 21: loss 0.17976917386208946
Epoch 22: loss 0.17486611938852537
Epoch 23: loss 0.17044945875544337
Epoch 24: loss 0.16664160785628376
Epoch 25: loss 0.1631830979671376
Epoch 26: loss 0.15999084689602247
Epoch 27: loss 0.15717060821026546
Epoch 28: loss 0.15434952341161906
Epoch 29: loss 0.15170271757254788
Epoch 30: loss 0.14956552793371605
Epoch 31: loss 0.14735539495750508
Epoch 32: loss 0.14535623081656432
Epoch 33: loss 0.14350192612031998
Epoch 34: loss 0.14179889474503693
Epoch 35: loss 0.1399848640649749
Epoch 36: loss 0.13852285277331375
Epoch 37: loss 0.13705867488198528
Epoch 38: loss 0.13568480319914253
Epoch 39: loss 0.1342636583177469
Epoch 40: loss 0.13299990829856193
Epoch 41: loss 0.1319249025594125
Epoch 42: loss 0.13075751642726438
Epoch 43: loss 0.1295889141245696
Epoch 44: loss 0.12865379680193204
Epoch 45: loss 0.1275601930178949
Epoch 46: loss 0.12661704205709867
Epoch 47: loss 0.12579656905854703
Epoch 48: loss 0.12476409450497493
Epoch 49: loss 0.12421483718073517
-----------Time: 0:09:36.793236, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5431874990463257-------------


Epoch 0: loss 2.647519836628677
Epoch 1: loss 1.0866075073849448
Epoch 2: loss 1.040148409927196
Epoch 3: loss 0.9506301351414225
Epoch 4: loss 0.8011150370154969
Epoch 5: loss 0.6197044301561326
Epoch 6: loss 0.4562062943448445
Epoch 7: loss 0.33619662562649555
Epoch 8: loss 0.2574249590469029
Epoch 9: loss 0.20626803477815858
Epoch 10: loss 0.17230425661611065
Epoch 11: loss 0.14897279170518532
Epoch 12: loss 0.1321723499740998
Epoch 13: loss 0.11909763078859087
Epoch 14: loss 0.10942580181216013
Epoch 15: loss 0.10157162219978759
Epoch 16: loss 0.09516623307398281
Epoch 17: loss 0.0899510429630207
Epoch 18: loss 0.0856019676819468
Epoch 19: loss 0.08197725026312731
Epoch 20: loss 0.07865764940985037
Epoch 21: loss 0.07593775407298224
Epoch 22: loss 0.07349919061854587
Epoch 23: loss 0.07141461351692936
Epoch 24: loss 0.06949525554079508
Epoch 25: loss 0.06784383932866922
Epoch 26: loss 0.06648545045468424
Epoch 27: loss 0.06496530974232159
Epoch 28: loss 0.06385559477864108
Epoch 29: loss 0.06266477290295822
Epoch 30: loss 0.06174405107842865
Epoch 31: loss 0.06069107524556366
Epoch 32: loss 0.059851662583483047
Epoch 33: loss 0.058992596791855596
Epoch 34: loss 0.058469658436951524
Epoch 35: loss 0.05754580091160397
Epoch 36: loss 0.05701665993012152
Epoch 37: loss 0.05654173372733716
Epoch 38: loss 0.05585332301532918
Epoch 39: loss 0.055267449813758696
Epoch 40: loss 0.054984510618466084
Epoch 41: loss 0.05434947269459728
Epoch 42: loss 0.053872664428157545
Epoch 43: loss 0.053472359156790085
Epoch 44: loss 0.05315437587813685
Epoch 45: loss 0.052787266670644706
Epoch 46: loss 0.05237674735924062
Epoch 47: loss 0.05206992994546242
Epoch 48: loss 0.05171192736811065
Epoch 49: loss 0.05139907158591165
-----------Time: 0:12:28.900261, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4036262035369873-------------


Epoch 0: loss 2.5280007740088686
Epoch 1: loss 1.0860921718654197
Epoch 2: loss 1.0024368221536806
Epoch 3: loss 0.8546425047259152
Epoch 4: loss 0.6507411891753547
Epoch 5: loss 0.4490890508439633
Epoch 6: loss 0.29929141468931114
Epoch 7: loss 0.20508766989869356
Epoch 8: loss 0.1509966094830783
Epoch 9: loss 0.11918990787213882
Epoch 10: loss 0.09961736470849829
Epoch 11: loss 0.08701624418039113
Epoch 12: loss 0.07865356136459803
Epoch 13: loss 0.07282379915904356
Epoch 14: loss 0.0685025451916547
Epoch 15: loss 0.06531033962721926
Epoch 16: loss 0.06291769031190139
Epoch 17: loss 0.06106325599419124
Epoch 18: loss 0.05939700997567747
Epoch 19: loss 0.0582748525089758
Epoch 20: loss 0.05711174495173836
Epoch 21: loss 0.056314396018151226
Epoch 22: loss 0.05562355940885003
Epoch 23: loss 0.054789429069651154
Epoch 24: loss 0.05434340590336616
Epoch 25: loss 0.053745239835319605
Epoch 26: loss 0.05324988445148577
Epoch 27: loss 0.05281903307283294
Epoch 28: loss 0.05244892389554573
Epoch 29: loss 0.05202021109382862
Epoch 30: loss 0.051701767782337836
Epoch 31: loss 0.05135869042965107
Epoch 32: loss 0.051021293887403625
Epoch 33: loss 0.05076389217495011
Epoch 34: loss 0.05043981479538596
Epoch 35: loss 0.0504150981718574
Epoch 36: loss 0.049802626356028744
Epoch 37: loss 0.04979522552497203
Epoch 38: loss 0.04960625417846548
Epoch 39: loss 0.04911414562618687
Epoch 40: loss 0.049199779887135225
Epoch 41: loss 0.04883416679795667
Epoch 42: loss 0.048639224380327674
Epoch 43: loss 0.04852713393113288
Epoch 44: loss 0.04823086848517939
Epoch 45: loss 0.04797041253503281
Epoch 46: loss 0.04799517566290577
Epoch 47: loss 0.0477862441694529
Epoch 48: loss 0.047468565576600924
Epoch 49: loss 0.04746622023896363
-----------Time: 0:11:59.994277, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.231353998184204-------------


Epoch 0: loss 16.129346278648523
Epoch 1: loss 16.129334292637388
Epoch 2: loss 16.129336350893702
Epoch 3: loss 16.129340988582342
Epoch 4: loss 16.129336016407603
Epoch 5: loss 16.12934271261185
Epoch 6: loss 16.129337303790145
Epoch 7: loss 16.129326274046242
Epoch 8: loss 16.129329091767502
Epoch 9: loss 16.129325250103914
Epoch 10: loss 16.12932117404077
Epoch 11: loss 16.129315496593016
Epoch 12: loss 16.12931692191866
Epoch 13: loss 16.1293183933982
Epoch 14: loss 16.129313730817568
Epoch 15: loss 16.12931079926732
Epoch 16: loss 16.12930396200847
Epoch 17: loss 16.129305922252577
Epoch 18: loss 16.129308000474342
Epoch 19: loss 16.12930141187609
Epoch 20: loss 16.12929569890539
Epoch 21: loss 16.129301747140058
Epoch 22: loss 16.129300725272067
Epoch 23: loss 16.129301558894394
Epoch 24: loss 16.129287801921983
Epoch 25: loss 16.12928942508708
Epoch 26: loss 16.12928774098847
Epoch 27: loss 16.12928809103206
Epoch 28: loss 16.129283615141347
Epoch 29: loss 16.129287321973326
Epoch 30: loss 16.12928547763254
Epoch 31: loss 16.129281595519473
Epoch 32: loss 16.129277057398784
Epoch 33: loss 16.129277328358455
Epoch 34: loss 16.12927252964976
Epoch 35: loss 16.129276577190836
Epoch 36: loss 16.129269679776346
Epoch 37: loss 16.129262721687635
Epoch 38: loss 16.12926523681566
Epoch 39: loss 16.129268174070322
Epoch 40: loss 16.129260487890942
Epoch 41: loss 16.129263917021675
Epoch 42: loss 16.129263473633127
Epoch 43: loss 16.129254649163848
Epoch 44: loss 16.12925069730135
Epoch 45: loss 16.12925984951515
Epoch 46: loss 16.129245461427107
Epoch 47: loss 16.129248944231193
Epoch 48: loss 16.12924910291762
Epoch 49: loss 16.12925131960107
-----------Time: 0:09:25.703169, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.0171966552734375-------------


Epoch 0: loss 16.12918956153998
Epoch 1: loss 16.12919011383098
Epoch 2: loss 16.129182975534643
Epoch 3: loss 16.12918915574871
Epoch 4: loss 16.129177191777355
Epoch 5: loss 16.129181960926815
Epoch 6: loss 16.1291765767378
Epoch 7: loss 16.12917966464086
Epoch 8: loss 16.12917948598898
Epoch 9: loss 16.129174753918214
Epoch 10: loss 16.129161407922908
Epoch 11: loss 16.12916925693738
Epoch 12: loss 16.1291712812265
Epoch 13: loss 16.129162279401804
Epoch 14: loss 16.129160833332836
Epoch 15: loss 16.129161995218265
Epoch 16: loss 16.129157608264553
Epoch 17: loss 16.129157052862055
Epoch 18: loss 16.129151794688738
Epoch 19: loss 16.129151282328635
Epoch 20: loss 16.12915460437196
Epoch 21: loss 16.129148680078654
Epoch 22: loss 16.129153191751602
Epoch 23: loss 16.129149649569754
Epoch 24: loss 16.12914580505396
Epoch 25: loss 16.12915149546629
Epoch 26: loss 16.129138366498008
Epoch 27: loss 16.129146058641094
Epoch 28: loss 16.129134932440735
Epoch 29: loss 16.12913424661459
Epoch 30: loss 16.129127169511058
Epoch 31: loss 16.129131747821933
Epoch 32: loss 16.12912698904414
Epoch 33: loss 16.12912820123213
Epoch 34: loss 16.129130051536624
Epoch 35: loss 16.129125642283835
Epoch 36: loss 16.129124031305444
Epoch 37: loss 16.129122745737945
Epoch 38: loss 16.129119741326768
Epoch 39: loss 16.12911752308757
Epoch 40: loss 16.129112918328953
Epoch 41: loss 16.129113719021255
Epoch 42: loss 16.129109292914517
Epoch 43: loss 16.12911174762761
Epoch 44: loss 16.12910840406309
Epoch 45: loss 16.129099179766243
Epoch 46: loss 16.129104281586756
Epoch 47: loss 16.129103580980992
Epoch 48: loss 16.129098491087888
Epoch 49: loss 16.12909660655691
-----------Time: 0:08:06.256041, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129187650561267
Epoch 1: loss 16.1291805807179
Epoch 2: loss 16.12917857665352
Epoch 3: loss 16.129173704306027
Epoch 4: loss 16.129171612860393
Epoch 5: loss 16.129179611745382
Epoch 6: loss 16.129171118391408
Epoch 7: loss 16.129165682344087
Epoch 8: loss 16.129169415105224
Epoch 9: loss 16.129165437313574
Epoch 10: loss 16.12916499937015
Epoch 11: loss 16.129159601438687
Epoch 12: loss 16.12916081544172
Epoch 13: loss 16.12916123912411
Epoch 14: loss 16.129158269717294
Epoch 15: loss 16.129165231176795
Epoch 16: loss 16.12915638959427
Epoch 17: loss 16.129146786991054
Epoch 18: loss 16.129149750434166
Epoch 19: loss 16.129145589841976
Epoch 20: loss 16.129142321731294
Epoch 21: loss 16.129150395292246
Epoch 22: loss 16.129142528127367
Epoch 23: loss 16.12913738378304
Epoch 24: loss 16.12913188939512
Epoch 25: loss 16.129129578070255
Epoch 26: loss 16.129137137456066
Epoch 27: loss 16.12913882025822
Epoch 28: loss 16.129128571500463
Epoch 29: loss 16.129131887061497
Epoch 30: loss 16.129122805375
Epoch 31: loss 16.129120272096568
Epoch 32: loss 16.129127649719006
Epoch 33: loss 16.12911772429781
Epoch 34: loss 16.129120480566975
Epoch 35: loss 16.129121176764784
Epoch 36: loss 16.129114582980698
Epoch 37: loss 16.129111370099118
Epoch 38: loss 16.12910854330265
Epoch 39: loss 16.12911195998739
Epoch 40: loss 16.12910945030449
Epoch 41: loss 16.12910579559011
Epoch 42: loss 16.129101940443363
Epoch 43: loss 16.12909764450098
Epoch 44: loss 16.12909876775197
Epoch 45: loss 16.129097184777066
Epoch 46: loss 16.129093278809176
Epoch 47: loss 16.129099718055496
Epoch 48: loss 16.129090484424154
Epoch 49: loss 16.129087450713033
-----------Time: 0:11:26.682701, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017175674438477-------------


Epoch 0: loss 16.129164636621272
Epoch 1: loss 16.129168111128028
Epoch 2: loss 16.129169527378465
Epoch 3: loss 16.129166870417965
Epoch 4: loss 16.129166942241724
Epoch 5: loss 16.129164968773747
Epoch 6: loss 16.129163080094106
Epoch 7: loss 16.129155408694345
Epoch 8: loss 16.129153943437803
Epoch 9: loss 16.129150540754807
Epoch 10: loss 16.129147957173814
Epoch 11: loss 16.12914934905085
Epoch 12: loss 16.12914014471945
Epoch 13: loss 16.129154991753534
Epoch 14: loss 16.129142138152876
Epoch 15: loss 16.129143797359497
Epoch 16: loss 16.12914290902665
Epoch 17: loss 16.12913701040321
Epoch 18: loss 16.129138461139423
Epoch 19: loss 16.12913660098185
Epoch 20: loss 16.12912808118014
Epoch 21: loss 16.12913317677766
Epoch 22: loss 16.12913322189439
Epoch 23: loss 16.129131080924072
Epoch 24: loss 16.129112394819316
Epoch 25: loss 16.129120525942994
Epoch 26: loss 16.12912362369913
Epoch 27: loss 16.129114637431922
Epoch 28: loss 16.12911605134874
Epoch 29: loss 16.129110811325827
Epoch 30: loss 16.129109909769113
Epoch 31: loss 16.129111205967565
Epoch 32: loss 16.12910553655785
Epoch 33: loss 16.129102575966947
Epoch 34: loss 16.12910090094354
Epoch 35: loss 16.129105238891153
Epoch 36: loss 16.12909829195198
Epoch 37: loss 16.129099799473043
Epoch 38: loss 16.129097998174654
Epoch 39: loss 16.129100464815156
Epoch 40: loss 16.129098997484284
Epoch 41: loss 16.129097434474826
Epoch 42: loss 16.129085043709587
Epoch 43: loss 16.129088463246532
Epoch 44: loss 16.129084392109924
Epoch 45: loss 16.12907415865037
Epoch 46: loss 16.1290829263348
Epoch 47: loss 16.129080478622576
Epoch 48: loss 16.129076339551585
Epoch 49: loss 16.129073502124164
-----------Time: 0:10:43.983901, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017178058624268-------------


Epoch 0: loss 16.129173657114965
Epoch 1: loss 16.12917270266277
Epoch 2: loss 16.129172480449917
Epoch 3: loss 16.12916910110316
Epoch 4: loss 16.129162417085617
Epoch 5: loss 16.129167511386676
Epoch 6: loss 16.129164677070754
Epoch 7: loss 16.129161288908087
Epoch 8: loss 16.129157550961118
Epoch 9: loss 16.129149265299677
Epoch 10: loss 16.129155728660116
Epoch 11: loss 16.129154755798222
Epoch 12: loss 16.12915656617182
Epoch 13: loss 16.12915289304774
Epoch 14: loss 16.12914959874861
Epoch 15: loss 16.129145603325135
Epoch 16: loss 16.12914292043548
Epoch 17: loss 16.129135604524432
Epoch 18: loss 16.129151337298445
Epoch 19: loss 16.129135091645747
Epoch 20: loss 16.129131875393377
Epoch 21: loss 16.129130707544242
Epoch 22: loss 16.12913542353893
Epoch 23: loss 16.12912549267261
Epoch 24: loss 16.12913156813289
Epoch 25: loss 16.129126002180506
Epoch 26: loss 16.129119547117398
Epoch 27: loss 16.129118135534206
Epoch 28: loss 16.129120670109096
Epoch 29: loss 16.129119337609826
Epoch 30: loss 16.12911965472339
Epoch 31: loss 16.129114334579395
Epoch 32: loss 16.129110488507852
Epoch 33: loss 16.129111066209422
Epoch 34: loss 16.12910917752978
Epoch 35: loss 16.129109375628524
Epoch 36: loss 16.129104276141632
Epoch 37: loss 16.129105649090384
Epoch 38: loss 16.129100892386923
Epoch 39: loss 16.12909750707646
Epoch 40: loss 16.129099076049624
Epoch 41: loss 16.129096713644323
Epoch 42: loss 16.12909578797349
Epoch 43: loss 16.12909904519393
Epoch 44: loss 16.129088707499168
Epoch 45: loss 16.129095016581136
Epoch 46: loss 16.129085994531696
Epoch 47: loss 16.1290901271204
Epoch 48: loss 16.129080566781703
Epoch 49: loss 16.129078148110136
-----------Time: 0:13:47.305078, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017176628112793-------------


Epoch 0: loss 16.12915612304256
Epoch 1: loss 16.129128936582962
Epoch 2: loss 16.129119640203065
Epoch 3: loss 16.1291024486548
Epoch 4: loss 16.12906988345131
Epoch 5: loss 16.129054200720567
Epoch 6: loss 16.1290418444411
Epoch 7: loss 16.1290212668045
Epoch 8: loss 16.129001889169185
Epoch 9: loss 16.128980307814995
Epoch 10: loss 16.128961018598098
Epoch 11: loss 16.12894310207066
Epoch 12: loss 16.1289233393874
Epoch 13: loss 16.128912031164372
Epoch 14: loss 16.128889237622193
Epoch 15: loss 16.128866353587263
Epoch 16: loss 16.128851768696936
Epoch 17: loss 16.128827508083177
Epoch 18: loss 16.12880900244534
Epoch 19: loss 16.12878982342735
Epoch 20: loss 16.128765150799317
Epoch 21: loss 16.128750092701914
Epoch 22: loss 16.12873229181851
Epoch 23: loss 16.128710089201768
Epoch 24: loss 16.128697764296582
Epoch 25: loss 16.128674193139048
Epoch 26: loss 16.128650680581405
Epoch 27: loss 16.128631350396443
Epoch 28: loss 16.12861921218117
Epoch 29: loss 16.12859326280155
Epoch 30: loss 16.128583919489884
Epoch 31: loss 16.128562392068336
Epoch 32: loss 16.12853791105676
Epoch 33: loss 16.128521304729517
Epoch 34: loss 16.128505299440086
Epoch 35: loss 16.128475884110337
Epoch 36: loss 16.12845632056298
Epoch 37: loss 16.12843717317856
Epoch 38: loss 16.128421038502648
Epoch 39: loss 16.128401329492732
Epoch 40: loss 16.128377852976612
Epoch 41: loss 16.128364788053347
Epoch 42: loss 16.12834260773568
Epoch 43: loss 16.128325407112207
Epoch 44: loss 16.128304117201722
Epoch 45: loss 16.1282828651478
Epoch 46: loss 16.128271351565868
Epoch 47: loss 16.12824914505976
Epoch 48: loss 16.128236132513383
Epoch 49: loss 16.128213453318775
-----------Time: 0:07:05.167432, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.01708984375-------------


Epoch 0: loss 16.129202094656286
Epoch 1: loss 16.129184044075117
Epoch 2: loss 16.129156125116893
Epoch 3: loss 16.129138914381052
Epoch 4: loss 16.129119362761106
Epoch 5: loss 16.129103586944698
Epoch 6: loss 16.129089731182205
Epoch 7: loss 16.1290579612259
Epoch 8: loss 16.129038051783617
Epoch 9: loss 16.129023875018188
Epoch 10: loss 16.129000846557865
Epoch 11: loss 16.128984688545714
Epoch 12: loss 16.128966554731957
Epoch 13: loss 16.128944701121643
Epoch 14: loss 16.128918100920238
Epoch 15: loss 16.128911209469454
Epoch 16: loss 16.128885491118606
Epoch 17: loss 16.12887166984189
Epoch 18: loss 16.1288451028298
Epoch 19: loss 16.128832327794484
Epoch 20: loss 16.12880466475702
Epoch 21: loss 16.128788187297676
Epoch 22: loss 16.12877079946571
Epoch 23: loss 16.128754003337054
Epoch 24: loss 16.12873401973739
Epoch 25: loss 16.128715471057156
Epoch 26: loss 16.128696571555455
Epoch 27: loss 16.128681537831458
Epoch 28: loss 16.12865524877991
Epoch 29: loss 16.128631989809403
Epoch 30: loss 16.128615798089346
Epoch 31: loss 16.128598909912192
Epoch 32: loss 16.128571589917446
Epoch 33: loss 16.128559351097053
Epoch 34: loss 16.12854663362446
Epoch 35: loss 16.12851835528815
Epoch 36: loss 16.12850101075787
Epoch 37: loss 16.12847575239023
Epoch 38: loss 16.12846011607267
Epoch 39: loss 16.128446356507343
Epoch 40: loss 16.128423333232853
Epoch 41: loss 16.128405650845682
Epoch 42: loss 16.12838857001491
Epoch 43: loss 16.128374772593013
Epoch 44: loss 16.128352502819766
Epoch 45: loss 16.128330477558446
Epoch 46: loss 16.128310699317687
Epoch 47: loss 16.1282886024919
Epoch 48: loss 16.128263353458756
Epoch 49: loss 16.128246443241817
-----------Time: 0:09:04.328127, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017065525054932-------------


Epoch 0: loss 16.12916579591379
Epoch 1: loss 16.129152420359244
Epoch 2: loss 16.129130254821195
Epoch 3: loss 16.129106788158158
Epoch 4: loss 16.12908812331531
Epoch 5: loss 16.12907845822283
Epoch 6: loss 16.12905471048775
Epoch 7: loss 16.129036695429527
Epoch 8: loss 16.129011318565645
Epoch 9: loss 16.12899545329365
Epoch 10: loss 16.12897546347099
Epoch 11: loss 16.128952539505164
Epoch 12: loss 16.128938300509763
Epoch 13: loss 16.128915471185348
Epoch 14: loss 16.12890047039138
Epoch 15: loss 16.12887776734195
Epoch 16: loss 16.12885606801011
Epoch 17: loss 16.128839803169836
Epoch 18: loss 16.12882357307463
Epoch 19: loss 16.128797980998765
Epoch 20: loss 16.128786092221965
Epoch 21: loss 16.128762111643073
Epoch 22: loss 16.128746027788303
Epoch 23: loss 16.1287247783273
Epoch 24: loss 16.128710309340295
Epoch 25: loss 16.12868944855732
Epoch 26: loss 16.128658571341816
Epoch 27: loss 16.12864260027887
Epoch 28: loss 16.128625205186736
Epoch 29: loss 16.128602255551048
Epoch 30: loss 16.128587187859853
Epoch 31: loss 16.128574562954345
Epoch 32: loss 16.128549212538204
Epoch 33: loss 16.128527446568434
Epoch 34: loss 16.128507609468496
Epoch 35: loss 16.128491111784413
Epoch 36: loss 16.128480307105573
Epoch 37: loss 16.128451333867915
Epoch 38: loss 16.12843724163153
Epoch 39: loss 16.12840775784881
Epoch 40: loss 16.128398027933443
Epoch 41: loss 16.128372835685145
Epoch 42: loss 16.128356723049013
Epoch 43: loss 16.128338934870893
Epoch 44: loss 16.128317160603793
Epoch 45: loss 16.12829555695053
Epoch 46: loss 16.128281017695517
Epoch 47: loss 16.12825737056556
Epoch 48: loss 16.128240818948832
Epoch 49: loss 16.128218420566977
-----------Time: 0:09:44.169498, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017066955566406-------------


Epoch 0: loss 16.12915553030208
Epoch 1: loss 16.129139959325993
Epoch 2: loss 16.12912626224993
Epoch 3: loss 16.12910226170559
Epoch 4: loss 16.129089152184175
Epoch 5: loss 16.12906960004565
Epoch 6: loss 16.12904407745992
Epoch 7: loss 16.129027090751975
Epoch 8: loss 16.129006787705123
Epoch 9: loss 16.12898909624275
Epoch 10: loss 16.128967130618488
Epoch 11: loss 16.128948726622937
Epoch 12: loss 16.128928391942516
Epoch 13: loss 16.1289075754984
Epoch 14: loss 16.128893895794867
Epoch 15: loss 16.12887943666094
Epoch 16: loss 16.12884784976447
Epoch 17: loss 16.12883504387346
Epoch 18: loss 16.12881624679192
Epoch 19: loss 16.128794989033587
Epoch 20: loss 16.12877626585014
Epoch 21: loss 16.128755480521008
Epoch 22: loss 16.12874547809022
Epoch 23: loss 16.128723557064106
Epoch 24: loss 16.128696243033065
Epoch 25: loss 16.12867582771297
Epoch 26: loss 16.128656667104682
Epoch 27: loss 16.12864438161181
Epoch 28: loss 16.12862106352283
Epoch 29: loss 16.128599635150653
Epoch 30: loss 16.12858130012666
Epoch 31: loss 16.12856691774303
Epoch 32: loss 16.128543263871492
Epoch 33: loss 16.128520944054973
Epoch 34: loss 16.128507857091922
Epoch 35: loss 16.12848045853184
Epoch 36: loss 16.128459274930886
Epoch 37: loss 16.12844462962563
Epoch 38: loss 16.128427043176334
Epoch 39: loss 16.12840997997738
Epoch 40: loss 16.12839082040626
Epoch 41: loss 16.128379556262797
Epoch 42: loss 16.1283458869959
Epoch 43: loss 16.128330807636587
Epoch 44: loss 16.128314368811687
Epoch 45: loss 16.12829497950825
Epoch 46: loss 16.128270667814053
Epoch 47: loss 16.128254549473507
Epoch 48: loss 16.128231148929814
Epoch 49: loss 16.128218335000764
-----------Time: 0:12:19.595556, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017068862915039-------------


Epoch 0: loss 16.12915947671945
Epoch 1: loss 16.12913590426546
Epoch 2: loss 16.12912459189377
Epoch 3: loss 16.129105590749784
Epoch 4: loss 16.129085260995904
Epoch 5: loss 16.129065799609418
Epoch 6: loss 16.1290438632851
Epoch 7: loss 16.12903098531104
Epoch 8: loss 16.129001919506294
Epoch 9: loss 16.12898646391495
Epoch 10: loss 16.128972398385592
Epoch 11: loss 16.128947229992118
Epoch 12: loss 16.1289277481216
Epoch 13: loss 16.128906864780262
Epoch 14: loss 16.12888517919087
Epoch 15: loss 16.12887360208251
Epoch 16: loss 16.128857323240496
Epoch 17: loss 16.128831383195372
Epoch 18: loss 16.128812566148387
Epoch 19: loss 16.128797647290543
Epoch 20: loss 16.12877740569579
Epoch 21: loss 16.128758455632237
Epoch 22: loss 16.128734079374446
Epoch 23: loss 16.128720753085297
Epoch 24: loss 16.12869798210148
Epoch 25: loss 16.128677570930055
Epoch 26: loss 16.128655165806617
Epoch 27: loss 16.1286400037333
Epoch 28: loss 16.128618419526905
Epoch 29: loss 16.12861147751427
Epoch 30: loss 16.12858077506127
Epoch 31: loss 16.128563031999878
Epoch 32: loss 16.128538446234515
Epoch 33: loss 16.128525837664373
Epoch 34: loss 16.128513315697607
Epoch 35: loss 16.128483016183676
Epoch 36: loss 16.12846480899041
Epoch 37: loss 16.128447863509823
Epoch 38: loss 16.128426945423417
Epoch 39: loss 16.128407831228316
Epoch 40: loss 16.128381855919542
Epoch 41: loss 16.12837285616918
Epoch 42: loss 16.128352627538998
Epoch 43: loss 16.128336612137197
Epoch 44: loss 16.128311197416753
Epoch 45: loss 16.128295651591944
Epoch 46: loss 16.128275065658013
Epoch 47: loss 16.12825947860585
Epoch 48: loss 16.128243560438378
Epoch 49: loss 16.12821058451708
-----------Time: 0:12:17.590031, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017068862915039-------------


Epoch 0: loss 16.12922125863536
Epoch 1: loss 16.129029816684028
Epoch 2: loss 16.128836699449998
Epoch 3: loss 16.12864354254436
Epoch 4: loss 16.128458785907025
Epoch 5: loss 16.128249464247652
Epoch 6: loss 16.12806803769168
Epoch 7: loss 16.127876675861437
Epoch 8: loss 16.127675610822852
Epoch 9: loss 16.12748112401086
Epoch 10: loss 16.12729931318481
Epoch 11: loss 16.127100608477196
Epoch 12: loss 16.126905716392514
Epoch 13: loss 16.126712723618425
Epoch 14: loss 16.126518657118034
Epoch 15: loss 16.126321981366786
Epoch 16: loss 16.126131132155933
Epoch 17: loss 16.125936285965853
Epoch 18: loss 16.125745685415595
Epoch 19: loss 16.125551902839447
Epoch 20: loss 16.125359347490203
Epoch 21: loss 16.125163135352246
Epoch 22: loss 16.12497859263044
Epoch 23: loss 16.12477785974433
Epoch 24: loss 16.124585687368704
Epoch 25: loss 16.124391717843352
Epoch 26: loss 16.124200880041325
Epoch 27: loss 16.124007962202494
Epoch 28: loss 16.123810586882648
Epoch 29: loss 16.123619080108426
Epoch 30: loss 16.123422697875213
Epoch 31: loss 16.12322809671565
Epoch 32: loss 16.123033969800325
Epoch 33: loss 16.12284852292889
Epoch 34: loss 16.122649156768535
Epoch 35: loss 16.122460567283525
Epoch 36: loss 16.12226061642073
Epoch 37: loss 16.122064376019992
Epoch 38: loss 16.121874730181215
Epoch 39: loss 16.121685974749617
Epoch 40: loss 16.121487495366356
Epoch 41: loss 16.121294438028674
Epoch 42: loss 16.12110165190995
Epoch 43: loss 16.120918490174933
Epoch 44: loss 16.120715343284502
Epoch 45: loss 16.12052747592646
Epoch 46: loss 16.120326437594905
Epoch 47: loss 16.12013832883643
Epoch 48: loss 16.11994689103376
Epoch 49: loss 16.119749782265625
-----------Time: 0:09:16.673181, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.015963554382324-------------


Epoch 0: loss 16.129111896201668
Epoch 1: loss 16.128914372567056
Epoch 2: loss 16.12872430019368
Epoch 3: loss 16.128524245614265
Epoch 4: loss 16.128337967454122
Epoch 5: loss 16.128140273464965
Epoch 6: loss 16.12795173817189
Epoch 7: loss 16.127759043583083
Epoch 8: loss 16.12756481554406
Epoch 9: loss 16.12737295302183
Epoch 10: loss 16.127176326017395
Epoch 11: loss 16.12698313125519
Epoch 12: loss 16.126788496906308
Epoch 13: loss 16.126594309576053
Epoch 14: loss 16.12639942215862
Epoch 15: loss 16.126202593425358
Epoch 16: loss 16.126023167303686
Epoch 17: loss 16.12582141384604
Epoch 18: loss 16.125633818744124
Epoch 19: loss 16.125434489921748
Epoch 20: loss 16.125240381416127
Epoch 21: loss 16.125055374043864
Epoch 22: loss 16.124853045218273
Epoch 23: loss 16.12466082331796
Epoch 24: loss 16.124466998218
Epoch 25: loss 16.12427029575972
Epoch 26: loss 16.124076500478292
Epoch 27: loss 16.123881376327674
Epoch 28: loss 16.123690419251535
Epoch 29: loss 16.12350018793244
Epoch 30: loss 16.12330808012041
Epoch 31: loss 16.123111237644697
Epoch 32: loss 16.12292268549767
Epoch 33: loss 16.122727657544218
Epoch 34: loss 16.122531673842115
Epoch 35: loss 16.122342295073633
Epoch 36: loss 16.12214680687768
Epoch 37: loss 16.121956582300168
Epoch 38: loss 16.121765412086376
Epoch 39: loss 16.121572842735386
Epoch 40: loss 16.121366852972415
Epoch 41: loss 16.121174485350252
Epoch 42: loss 16.120989789905725
Epoch 43: loss 16.120791984161908
Epoch 44: loss 16.120601629679328
Epoch 45: loss 16.12040693387835
Epoch 46: loss 16.120204546971447
Epoch 47: loss 16.12001869742024
Epoch 48: loss 16.11982244379563
Epoch 49: loss 16.11963544350853
-----------Time: 0:07:52.748761, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015997409820557-------------


Epoch 0: loss 16.12908805123226
Epoch 1: loss 16.12887699906109
Epoch 2: loss 16.12869099574999
Epoch 3: loss 16.12850269174498
Epoch 4: loss 16.12830207268786
Epoch 5: loss 16.128114123393686
Epoch 6: loss 16.127920999677368
Epoch 7: loss 16.12772390231806
Epoch 8: loss 16.127536173680998
Epoch 9: loss 16.127342932764897
Epoch 10: loss 16.127152174306083
Epoch 11: loss 16.12695094565453
Epoch 12: loss 16.126757330840018
Epoch 13: loss 16.126570326922835
Epoch 14: loss 16.12637211642491
Epoch 15: loss 16.126183210604214
Epoch 16: loss 16.12599351835225
Epoch 17: loss 16.125794156340557
Epoch 18: loss 16.125602032193157
Epoch 19: loss 16.12540280993961
Epoch 20: loss 16.125210280000935
Epoch 21: loss 16.12502086830243
Epoch 22: loss 16.124823951928626
Epoch 23: loss 16.12463495613376
Epoch 24: loss 16.124437414608032
Epoch 25: loss 16.124243486310036
Epoch 26: loss 16.12404797581501
Epoch 27: loss 16.123865810278126
Epoch 28: loss 16.123667504879492
Epoch 29: loss 16.123474312969496
Epoch 30: loss 16.123279195041874
Epoch 31: loss 16.123084744271406
Epoch 32: loss 16.122887246825243
Epoch 33: loss 16.12269678966321
Epoch 34: loss 16.122502136386043
Epoch 35: loss 16.122314127714105
Epoch 36: loss 16.122120126814476
Epoch 37: loss 16.12192067871799
Epoch 38: loss 16.121728081363514
Epoch 39: loss 16.12152546913226
Epoch 40: loss 16.12134480852352
Epoch 41: loss 16.121150719724056
Epoch 42: loss 16.120957682611113
Epoch 43: loss 16.120760687931263
Epoch 44: loss 16.12056920941982
Epoch 45: loss 16.120382070152445
Epoch 46: loss 16.12017836630377
Epoch 47: loss 16.119985942674628
Epoch 48: loss 16.119797837805528
Epoch 49: loss 16.119600506824536
-----------Time: 0:11:13.548899, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.0159912109375-------------


Epoch 0: loss 16.129066292781943
Epoch 1: loss 16.12887748289912
Epoch 2: loss 16.128687680448248
Epoch 3: loss 16.12849249225262
Epoch 4: loss 16.128302530337443
Epoch 5: loss 16.128106153549354
Epoch 6: loss 16.127918715578115
Epoch 7: loss 16.127720519859807
Epoch 8: loss 16.127531666416004
Epoch 9: loss 16.12733984952909
Epoch 10: loss 16.127141757268113
Epoch 11: loss 16.126940694822444
Epoch 12: loss 16.126764910969158
Epoch 13: loss 16.126561519566796
Epoch 14: loss 16.126371090148957
Epoch 15: loss 16.126177261678208
Epoch 16: loss 16.125978315829453
Epoch 17: loss 16.125791693070845
Epoch 18: loss 16.125592735294678
Epoch 19: loss 16.125397711489885
Epoch 20: loss 16.125204747756452
Epoch 21: loss 16.125017216440256
Epoch 22: loss 16.124816344833167
Epoch 23: loss 16.124628351718723
Epoch 24: loss 16.12443101736694
Epoch 25: loss 16.124242589161277
Epoch 26: loss 16.124045084973535
Epoch 27: loss 16.123853782002467
Epoch 28: loss 16.123660597352632
Epoch 29: loss 16.123462787460152
Epoch 30: loss 16.12327144637323
Epoch 31: loss 16.123078969848617
Epoch 32: loss 16.122879930914195
Epoch 33: loss 16.12268950201494
Epoch 34: loss 16.122498727998636
Epoch 35: loss 16.122301606265925
Epoch 36: loss 16.12211105161027
Epoch 37: loss 16.121918879753224
Epoch 38: loss 16.121726357074714
Epoch 39: loss 16.12153043171321
Epoch 40: loss 16.121338469104444
Epoch 41: loss 16.12114792430187
Epoch 42: loss 16.12094841682762
Epoch 43: loss 16.12075920452431
Epoch 44: loss 16.120563929725304
Epoch 45: loss 16.120378539120132
Epoch 46: loss 16.1201766103814
Epoch 47: loss 16.119984278022887
Epoch 48: loss 16.119793960100417
Epoch 49: loss 16.119596564815122
-----------Time: 0:10:46.758886, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015991687774658-------------


Epoch 0: loss 16.12908335546231
Epoch 1: loss 16.12888973235047
Epoch 2: loss 16.128698839578632
Epoch 3: loss 16.12850068949564
Epoch 4: loss 16.12830849508023
Epoch 5: loss 16.12811497775934
Epoch 6: loss 16.12792429968091
Epoch 7: loss 16.127727427386667
Epoch 8: loss 16.127537214736567
Epoch 9: loss 16.127340471050932
Epoch 10: loss 16.127143524340017
Epoch 11: loss 16.126955408062088
Epoch 12: loss 16.126765796968378
Epoch 13: loss 16.12656419908566
Epoch 14: loss 16.12637523881374
Epoch 15: loss 16.126180940765995
Epoch 16: loss 16.125986629753672
Epoch 17: loss 16.125788110439515
Epoch 18: loss 16.125595935470972
Epoch 19: loss 16.12540221927346
Epoch 20: loss 16.125209716560402
Epoch 21: loss 16.125021307542312
Epoch 22: loss 16.124822783560905
Epoch 23: loss 16.12463661378463
Epoch 24: loss 16.124436766897745
Epoch 25: loss 16.124240344733632
Epoch 26: loss 16.124048095089123
Epoch 27: loss 16.123860614075486
Epoch 28: loss 16.12367085933426
Epoch 29: loss 16.12347631573742
Epoch 30: loss 16.123277544391875
Epoch 31: loss 16.12307670182544
Epoch 32: loss 16.122889773880676
Epoch 33: loss 16.122698605222634
Epoch 34: loss 16.122504579690307
Epoch 35: loss 16.122307750438463
Epoch 36: loss 16.122120626987876
Epoch 37: loss 16.121927955994597
Epoch 38: loss 16.121731788454788
Epoch 39: loss 16.12154083008219
Epoch 40: loss 16.121341383541452
Epoch 41: loss 16.12114602939923
Epoch 42: loss 16.120963346316415
Epoch 43: loss 16.120759057246712
Epoch 44: loss 16.12056426447069
Epoch 45: loss 16.120374787690004
Epoch 46: loss 16.120179982986574
Epoch 47: loss 16.11998846765573
Epoch 48: loss 16.119794403748255
Epoch 49: loss 16.119598676744786
-----------Time: 0:13:44.297243, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015988826751709-------------


Epoch 0: loss 16.12834634983131
Epoch 1: loss 16.126415916731204
Epoch 2: loss 16.124478827098454
Epoch 3: loss 16.122548539720196
Epoch 4: loss 16.12061411923465
Epoch 5: loss 16.118674172209033
Epoch 6: loss 16.11674370825323
Epoch 7: loss 16.11480704878516
Epoch 8: loss 16.11286901792409
Epoch 9: loss 16.110942353626644
Epoch 10: loss 16.109004055695276
Epoch 11: loss 16.10707299692466
Epoch 12: loss 16.105140563130963
Epoch 13: loss 16.103200470902078
Epoch 14: loss 16.10127328931799
Epoch 15: loss 16.099335976175926
Epoch 16: loss 16.097398658366615
Epoch 17: loss 16.095473974019434
Epoch 18: loss 16.093537775053157
Epoch 19: loss 16.091592117131178
Epoch 20: loss 16.089658678842017
Epoch 21: loss 16.087731911346534
Epoch 22: loss 16.08580053961102
Epoch 23: loss 16.08386647020623
Epoch 24: loss 16.081939032960676
Epoch 25: loss 16.080000608495034
Epoch 26: loss 16.078070012819126
Epoch 27: loss 16.076143323111108
Epoch 28: loss 16.074202096222404
Epoch 29: loss 16.07227146243064
Epoch 30: loss 16.070339066493506
Epoch 31: loss 16.068417976964355
Epoch 32: loss 16.066478810924885
Epoch 33: loss 16.06455053513046
Epoch 34: loss 16.062614900641886
Epoch 35: loss 16.060684226141344
Epoch 36: loss 16.058764034020246
Epoch 37: loss 16.05682461750514
Epoch 38: loss 16.05490065709996
Epoch 39: loss 16.052961458389756
Epoch 40: loss 16.05102989944574
Epoch 41: loss 16.049106632126875
Epoch 42: loss 16.04717945261712
Epoch 43: loss 16.04523770662674
Epoch 44: loss 16.043322998003315
Epoch 45: loss 16.04138566982234
Epoch 46: loss 16.039455325659233
Epoch 47: loss 16.037523070258118
Epoch 48: loss 16.03559608625419
Epoch 49: loss 16.033669824895814
-----------Time: 0:07:12.497557, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.0052103996276855-------------


Epoch 0: loss 16.12822921798565
Epoch 1: loss 16.126297479093303
Epoch 2: loss 16.12436632697773
Epoch 3: loss 16.122417555482834
Epoch 4: loss 16.12048524018537
Epoch 5: loss 16.118554343990557
Epoch 6: loss 16.116618809329225
Epoch 7: loss 16.114689164216134
Epoch 8: loss 16.112749413214928
Epoch 9: loss 16.11082098236416
Epoch 10: loss 16.108885623243207
Epoch 11: loss 16.106953935172
Epoch 12: loss 16.105017234735868
Epoch 13: loss 16.10308529178107
Epoch 14: loss 16.101156364127693
Epoch 15: loss 16.09922275055745
Epoch 16: loss 16.097283686160264
Epoch 17: loss 16.095354063605537
Epoch 18: loss 16.093418544242407
Epoch 19: loss 16.09148824856682
Epoch 20: loss 16.08955668184406
Epoch 21: loss 16.087617840696897
Epoch 22: loss 16.085690639665945
Epoch 23: loss 16.083757408550735
Epoch 24: loss 16.081820465936293
Epoch 25: loss 16.07989532938465
Epoch 26: loss 16.077962134310965
Epoch 27: loss 16.076028327568515
Epoch 28: loss 16.0741061754626
Epoch 29: loss 16.072169951863625
Epoch 30: loss 16.070245328709248
Epoch 31: loss 16.068304543653348
Epoch 32: loss 16.066379227671952
Epoch 33: loss 16.064453954473663
Epoch 34: loss 16.0625211177409
Epoch 35: loss 16.06058908403406
Epoch 36: loss 16.058668268057495
Epoch 37: loss 16.05673911300543
Epoch 38: loss 16.0548027499076
Epoch 39: loss 16.052864025700927
Epoch 40: loss 16.050939301682142
Epoch 41: loss 16.049009307303333
Epoch 42: loss 16.04708206141494
Epoch 43: loss 16.045147481983676
Epoch 44: loss 16.043229819510927
Epoch 45: loss 16.0412975584054
Epoch 46: loss 16.03936099539379
Epoch 47: loss 16.037429125299916
Epoch 48: loss 16.035503744236344
Epoch 49: loss 16.03358061304554
-----------Time: 0:08:57.127187, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.0052103996276855-------------


Epoch 0: loss 16.12822428392677
Epoch 1: loss 16.126284647532426
Epoch 2: loss 16.124349365939647
Epoch 3: loss 16.122424465602606
Epoch 4: loss 16.1204810513304
Epoch 5: loss 16.11855184649436
Epoch 6: loss 16.116612520731294
Epoch 7: loss 16.114682388927964
Epoch 8: loss 16.1127460237558
Epoch 9: loss 16.110809238790623
Epoch 10: loss 16.10888492367461
Epoch 11: loss 16.106944994540108
Epoch 12: loss 16.10501214873214
Epoch 13: loss 16.10308079151695
Epoch 14: loss 16.10115583283931
Epoch 15: loss 16.09920881519245
Epoch 16: loss 16.097274426859055
Epoch 17: loss 16.095340268517266
Epoch 18: loss 16.093416246919283
Epoch 19: loss 16.09148737656934
Epoch 20: loss 16.089555954531264
Epoch 21: loss 16.0876213903982
Epoch 22: loss 16.085681973105217
Epoch 23: loss 16.083762289454846
Epoch 24: loss 16.081820351070135
Epoch 25: loss 16.079891140011096
Epoch 26: loss 16.07796483561032
Epoch 27: loss 16.076035019105515
Epoch 28: loss 16.07410102774748
Epoch 29: loss 16.072164586602895
Epoch 30: loss 16.070235401473013
Epoch 31: loss 16.06830126257809
Epoch 32: loss 16.066377684109362
Epoch 33: loss 16.064449747727576
Epoch 34: loss 16.06250508315152
Epoch 35: loss 16.060593436761287
Epoch 36: loss 16.058659961134147
Epoch 37: loss 16.05672437150301
Epoch 38: loss 16.054798913429845
Epoch 39: loss 16.0528598692574
Epoch 40: loss 16.05093391930817
Epoch 41: loss 16.04900163045845
Epoch 42: loss 16.04708002701345
Epoch 43: loss 16.045141568321323
Epoch 44: loss 16.04321031248913
Epoch 45: loss 16.041280556139963
Epoch 46: loss 16.039351796766802
Epoch 47: loss 16.037427149498313
Epoch 48: loss 16.0354916879572
Epoch 49: loss 16.03357194596623
-----------Time: 0:10:01.862374, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.0052008628845215-------------


Epoch 0: loss 16.12820309877007
Epoch 1: loss 16.12627683818956
Epoch 2: loss 16.124339779671796
Epoch 3: loss 16.122398115358255
Epoch 4: loss 16.120467946994818
Epoch 5: loss 16.118538403005427
Epoch 6: loss 16.116595395561664
Epoch 7: loss 16.11466550075081
Epoch 8: loss 16.11273225200377
Epoch 9: loss 16.1107950679889
Epoch 10: loss 16.10886145571511
Epoch 11: loss 16.106929227280318
Epoch 12: loss 16.104989862364228
Epoch 13: loss 16.10306217801383
Epoch 14: loss 16.1011282877795
Epoch 15: loss 16.099192179824556
Epoch 16: loss 16.097265031689076
Epoch 17: loss 16.09533296608938
Epoch 18: loss 16.09339882460154
Epoch 19: loss 16.09145802321027
Epoch 20: loss 16.08953129305277
Epoch 21: loss 16.087597850874236
Epoch 22: loss 16.085668463237653
Epoch 23: loss 16.08374275546673
Epoch 24: loss 16.081806600839304
Epoch 25: loss 16.079867198844525
Epoch 26: loss 16.077938163844447
Epoch 27: loss 16.076007943363408
Epoch 28: loss 16.074071366609342
Epoch 29: loss 16.072142838264952
Epoch 30: loss 16.0702077490665
Epoch 31: loss 16.068275536448493
Epoch 32: loss 16.066345940600794
Epoch 33: loss 16.064413862036517
Epoch 34: loss 16.062488528423298
Epoch 35: loss 16.06055506835365
Epoch 36: loss 16.05862051018429
Epoch 37: loss 16.0566937245384
Epoch 38: loss 16.05475770292754
Epoch 39: loss 16.052826399904287
Epoch 40: loss 16.05089668530106
Epoch 41: loss 16.048968389281896
Epoch 42: loss 16.04703601279163
Epoch 43: loss 16.04509971010873
Epoch 44: loss 16.043172076320182
Epoch 45: loss 16.041237675540792
Epoch 46: loss 16.039309773644785
Epoch 47: loss 16.037378037604643
Epoch 48: loss 16.035444672694993
Epoch 49: loss 16.033518884284398
-----------Time: 0:12:14.222446, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005197048187256-------------


Epoch 0: loss 16.128209951067827
Epoch 1: loss 16.126280428081053
Epoch 2: loss 16.124348197053347
Epoch 3: loss 16.122413559022192
Epoch 4: loss 16.12047043515652
Epoch 5: loss 16.118541052965057
Epoch 6: loss 16.116601874998175
Epoch 7: loss 16.11466931648524
Epoch 8: loss 16.11273960136343
Epoch 9: loss 16.11080825555707
Epoch 10: loss 16.108875148901802
Epoch 11: loss 16.106946445017034
Epoch 12: loss 16.105001816482503
Epoch 13: loss 16.10306807300719
Epoch 14: loss 16.101140577421038
Epoch 15: loss 16.099209748123442
Epoch 16: loss 16.097273434031717
Epoch 17: loss 16.095339727635096
Epoch 18: loss 16.09340719712565
Epoch 19: loss 16.091471428064757
Epoch 20: loss 16.089535638779125
Epoch 21: loss 16.087608836279284
Epoch 22: loss 16.085672851487825
Epoch 23: loss 16.083741363848986
Epoch 24: loss 16.081803392624973
Epoch 25: loss 16.079870532555965
Epoch 26: loss 16.077940633077866
Epoch 27: loss 16.076015299205356
Epoch 28: loss 16.074074975169825
Epoch 29: loss 16.072143991075176
Epoch 30: loss 16.070212544663697
Epoch 31: loss 16.06828289903202
Epoch 32: loss 16.066349722886617
Epoch 33: loss 16.064412917178114
Epoch 34: loss 16.062481203177754
Epoch 35: loss 16.060556662737383
Epoch 36: loss 16.058625457985624
Epoch 37: loss 16.056690119607996
Epoch 38: loss 16.054755437237986
Epoch 39: loss 16.05282049765076
Epoch 40: loss 16.05089662073746
Epoch 41: loss 16.048964161792483
Epoch 42: loss 16.047028681323088
Epoch 43: loss 16.045102045029125
Epoch 44: loss 16.043163873372745
Epoch 45: loss 16.04123453785376
Epoch 46: loss 16.039304761020563
Epoch 47: loss 16.03736608193062
Epoch 48: loss 16.035433201377582
Epoch 49: loss 16.03350128435194
-----------Time: 0:12:09.940228, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005194187164307-------------


Epoch 0: loss 16.119646932457762
Epoch 1: loss 16.100328070239698
Epoch 2: loss 16.081002205853363
Epoch 3: loss 16.061685083740883
Epoch 4: loss 16.042380916352762
Epoch 5: loss 16.02309946368219
Epoch 6: loss 16.003821098396088
Epoch 7: loss 15.9845598129036
Epoch 8: loss 15.965309049462157
Epoch 9: loss 15.946085765220472
Epoch 10: loss 15.926868459723318
Epoch 11: loss 15.907657522167312
Epoch 12: loss 15.888469862769387
Epoch 13: loss 15.869297137527507
Epoch 14: loss 15.850123124384504
Epoch 15: loss 15.830972227083075
Epoch 16: loss 15.811824304115001
Epoch 17: loss 15.792695915394338
Epoch 18: loss 15.773581285720418
Epoch 19: loss 15.754479465049009
Epoch 20: loss 15.735386973687525
Epoch 21: loss 15.7163062650526
Epoch 22: loss 15.69723044406337
Epoch 23: loss 15.678173245393173
Epoch 24: loss 15.659144900167941
Epoch 25: loss 15.64010852079407
Epoch 26: loss 15.621092798400015
Epoch 27: loss 15.602083621302528
Epoch 28: loss 15.583091019417294
Epoch 29: loss 15.564100624881016
Epoch 30: loss 15.545133617405272
Epoch 31: loss 15.526162900245339
Epoch 32: loss 15.507220111378125
Epoch 33: loss 15.488277362701101
Epoch 34: loss 15.469352000300297
Epoch 35: loss 15.450441319505568
Epoch 36: loss 15.431540822127126
Epoch 37: loss 15.412664659519828
Epoch 38: loss 15.393794270557446
Epoch 39: loss 15.374935237268446
Epoch 40: loss 15.35607673941649
Epoch 41: loss 15.337237206148414
Epoch 42: loss 15.318398742976559
Epoch 43: loss 15.299586137395115
Epoch 44: loss 15.280779748587845
Epoch 45: loss 15.26198884337541
Epoch 46: loss 15.243199091491782
Epoch 47: loss 15.22442556633257
Epoch 48: loss 15.205668787777327
Epoch 49: loss 15.186919843456938
-----------Time: 0:09:08.966982, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.8974900245666504-------------


Epoch 0: loss 16.119577569375547
Epoch 1: loss 16.10023503876562
Epoch 2: loss 16.080903154148103
Epoch 3: loss 16.061594576470032
Epoch 4: loss 16.04229606734209
Epoch 5: loss 16.023013482863906
Epoch 6: loss 16.0037482369523
Epoch 7: loss 15.98449491222894
Epoch 8: loss 15.965245662790851
Epoch 9: loss 15.946003028139463
Epoch 10: loss 15.926786165771567
Epoch 11: loss 15.907568272979056
Epoch 12: loss 15.888364945370714
Epoch 13: loss 15.869168747242798
Epoch 14: loss 15.849992680510727
Epoch 15: loss 15.830817268230527
Epoch 16: loss 15.81165548094334
Epoch 17: loss 15.792506164023902
Epoch 18: loss 15.773351764212231
Epoch 19: loss 15.75422365397069
Epoch 20: loss 15.735088889530134
Epoch 21: loss 15.71595802301943
Epoch 22: loss 15.696851583846955
Epoch 23: loss 15.677747230674989
Epoch 24: loss 15.658648958149133
Epoch 25: loss 15.639550298760286
Epoch 26: loss 15.620450005056808
Epoch 27: loss 15.601350254309832
Epoch 28: loss 15.582250293277411
Epoch 29: loss 15.563149793177859
Epoch 30: loss 15.544059369407185
Epoch 31: loss 15.524950699290223
Epoch 32: loss 15.505830937977262
Epoch 33: loss 15.486706415812174
Epoch 34: loss 15.46758520220725
Epoch 35: loss 15.448429187527813
Epoch 36: loss 15.429269945965052
Epoch 37: loss 15.410063874014957
Epoch 38: loss 15.39084020602178
Epoch 39: loss 15.37159687257967
Epoch 40: loss 15.352317567880288
Epoch 41: loss 15.333007044737725
Epoch 42: loss 15.313646291636331
Epoch 43: loss 15.29424106614495
Epoch 44: loss 15.274776978619789
Epoch 45: loss 15.255269661748326
Epoch 46: loss 15.23569809191249
Epoch 47: loss 15.216038457332195
Epoch 48: loss 15.196308784993075
Epoch 49: loss 15.176497330543722
-----------Time: 0:08:04.111012, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.896059274673462-------------


Epoch 0: loss 16.11958249980435
Epoch 1: loss 16.10025367404923
Epoch 2: loss 16.080932261439497
Epoch 3: loss 16.061626071577294
Epoch 4: loss 16.04233370299181
Epoch 5: loss 16.02304129447543
Epoch 6: loss 16.00377061692446
Epoch 7: loss 15.984489905309132
Epoch 8: loss 15.965220696644785
Epoch 9: loss 15.945963668719525
Epoch 10: loss 15.926706744251593
Epoch 11: loss 15.90745680924665
Epoch 12: loss 15.88821315946885
Epoch 13: loss 15.868944361263024
Epoch 14: loss 15.84966129113177
Epoch 15: loss 15.830356575601318
Epoch 16: loss 15.811037241472635
Epoch 17: loss 15.79168027999156
Epoch 18: loss 15.772283010602063
Epoch 19: loss 15.752819533708498
Epoch 20: loss 15.73328453538987
Epoch 21: loss 15.71364586219767
Epoch 22: loss 15.693898376269857
Epoch 23: loss 15.674015138510974
Epoch 24: loss 15.653946965722172
Epoch 25: loss 15.633687465070317
Epoch 26: loss 15.613177946431925
Epoch 27: loss 15.592398328195129
Epoch 28: loss 15.571288353078323
Epoch 29: loss 15.549798624226423
Epoch 30: loss 15.527884064840842
Epoch 31: loss 15.505499087837224
Epoch 32: loss 15.4825744820782
Epoch 33: loss 15.459076216844451
Epoch 34: loss 15.43492769378239
Epoch 35: loss 15.41008711172356
Epoch 36: loss 15.384489725827523
Epoch 37: loss 15.358087324720158
Epoch 38: loss 15.33082527126418
Epoch 39: loss 15.302653553564955
Epoch 40: loss 15.273523273125752
Epoch 41: loss 15.243408194309087
Epoch 42: loss 15.212249371848591
Epoch 43: loss 15.180022935364285
Epoch 44: loss 15.14665929204164
Epoch 45: loss 15.112147378428855
Epoch 46: loss 15.076453582821753
Epoch 47: loss 15.039534115544994
Epoch 48: loss 15.001381308051021
Epoch 49: loss 14.961952335488348
-----------Time: 0:11:08.214188, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.8672051429748535-------------


Epoch 0: loss 16.119575965138736
Epoch 1: loss 16.10025197283738
Epoch 2: loss 16.080929483649133
Epoch 3: loss 16.06162529059115
Epoch 4: loss 16.04231418689481
Epoch 5: loss 16.02301037110603
Epoch 6: loss 16.003716120063903
Epoch 7: loss 15.984406640829114
Epoch 8: loss 15.965093491581746
Epoch 9: loss 15.945763791236235
Epoch 10: loss 15.926406235977513
Epoch 11: loss 15.907000845576707
Epoch 12: loss 15.88753594517254
Epoch 13: loss 15.86797166039206
Epoch 14: loss 15.84827652620582
Epoch 15: loss 15.828391022238801
Epoch 16: loss 15.808273809380088
Epoch 17: loss 15.787835046738111
Epoch 18: loss 15.767005633374412
Epoch 19: loss 15.74567727228427
Epoch 20: loss 15.723743364044736
Epoch 21: loss 15.701075959944609
Epoch 22: loss 15.677580644670812
Epoch 23: loss 15.653115575633793
Epoch 24: loss 15.627558034810766
Epoch 25: loss 15.600789806258101
Epoch 26: loss 15.572690621220982
Epoch 27: loss 15.543169142436307
Epoch 28: loss 15.51210987444738
Epoch 29: loss 15.479418862702213
Epoch 30: loss 15.445028983619695
Epoch 31: loss 15.408856831665204
Epoch 32: loss 15.370843549472214
Epoch 33: loss 15.330927250889607
Epoch 34: loss 15.28904725339764
Epoch 35: loss 15.24518067529501
Epoch 36: loss 15.199265626022646
Epoch 37: loss 15.151315684678957
Epoch 38: loss 15.101297926941665
Epoch 39: loss 15.049180649233097
Epoch 40: loss 14.994952330970452
Epoch 41: loss 14.938622543641962
Epoch 42: loss 14.88014979359895
Epoch 43: loss 14.819559301611779
Epoch 44: loss 14.756830470856277
Epoch 45: loss 14.691996752016566
Epoch 46: loss 14.62503133045713
Epoch 47: loss 14.55596947268081
Epoch 48: loss 14.4848345208907
Epoch 49: loss 14.411632862215525
-----------Time: 0:10:52.600252, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.793469190597534-------------


Epoch 0: loss 16.11959480318834
Epoch 1: loss 16.10027320622231
Epoch 2: loss 16.080952379352183
Epoch 3: loss 16.061636326298757
Epoch 4: loss 16.042321666418825
Epoch 5: loss 16.022996545162066
Epoch 6: loss 16.003663332712527
Epoch 7: loss 15.98429451590326
Epoch 8: loss 15.9648556265901
Epoch 9: loss 15.945316088076453
Epoch 10: loss 15.925622953028054
Epoch 11: loss 15.905703458791196
Epoch 12: loss 15.885433617592376
Epoch 13: loss 15.864685618663495
Epoch 14: loss 15.84330068015741
Epoch 15: loss 15.821104242336238
Epoch 16: loss 15.797880767025722
Epoch 17: loss 15.773413701962362
Epoch 18: loss 15.74749115979691
Epoch 19: loss 15.71989117685124
Epoch 20: loss 15.690386201195771
Epoch 21: loss 15.658820449430829
Epoch 22: loss 15.625016851098463
Epoch 23: loss 15.588831108896029
Epoch 24: loss 15.550122452404526
Epoch 25: loss 15.508782750565828
Epoch 26: loss 15.46473830390586
Epoch 27: loss 15.417912259706018
Epoch 28: loss 15.368238087384968
Epoch 29: loss 15.31568114429534
Epoch 30: loss 15.260213015194884
Epoch 31: loss 15.201792615856277
Epoch 32: loss 15.140421411795355
Epoch 33: loss 15.07611262947402
Epoch 34: loss 15.008871007186036
Epoch 35: loss 14.938675648884775
Epoch 36: loss 14.86554082986646
Epoch 37: loss 14.789520909307832
Epoch 38: loss 14.71059231501419
Epoch 39: loss 14.62880336570636
Epoch 40: loss 14.544185412325504
Epoch 41: loss 14.456763961121982
Epoch 42: loss 14.366555967170171
Epoch 43: loss 14.27357124906315
Epoch 44: loss 14.177910627911698
Epoch 45: loss 14.07956303352244
Epoch 46: loss 13.978574322383647
Epoch 47: loss 13.874995170954195
Epoch 48: loss 13.768875982918255
Epoch 49: loss 13.660222683087712
-----------Time: 0:13:48.390014, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.6911630630493164-------------


Epoch 0: loss 16.033715148280663
Epoch 1: loss 15.84182882334889
Epoch 2: loss 15.651147661940309
Epoch 3: loss 15.461882721152625
Epoch 4: loss 15.27375960492647
Epoch 5: loss 15.086763316616537
Epoch 6: loss 14.900773758081847
Epoch 7: loss 14.715625789905774
Epoch 8: loss 14.530975798150001
Epoch 9: loss 14.34645272053215
Epoch 10: loss 14.161747033988865
Epoch 11: loss 13.976034474282113
Epoch 12: loss 13.788437629407227
Epoch 13: loss 13.597952620240253
Epoch 14: loss 13.403118396466553
Epoch 15: loss 13.20264917278238
Epoch 16: loss 12.994952869000416
Epoch 17: loss 12.77840171552599
Epoch 18: loss 12.5515402845224
Epoch 19: loss 12.312979269312931
Epoch 20: loss 12.061435858149318
Epoch 21: loss 11.79607350519003
Epoch 22: loss 11.516133562257071
Epoch 23: loss 11.22103535214995
Epoch 24: loss 10.910773466306255
Epoch 25: loss 10.58542581898418
Epoch 26: loss 10.245265285498167
Epoch 27: loss 9.890973241514587
Epoch 28: loss 9.523314339107246
Epoch 29: loss 9.143342387099315
Epoch 30: loss 8.752513659655108
Epoch 31: loss 8.352447241269227
Epoch 32: loss 7.94470666192549
Epoch 33: loss 7.531338878012404
Epoch 34: loss 7.114474772888399
Epoch 35: loss 6.69642441242917
Epoch 36: loss 6.279679520095153
Epoch 37: loss 5.86644212340064
Epoch 38: loss 5.459536306880103
Epoch 39: loss 5.061488897143142
Epoch 40: loss 4.674945090632519
Epoch 41: loss 4.302832640598104
Epoch 42: loss 3.947782045476394
Epoch 43: loss 3.6122683289908015
Epoch 44: loss 3.299014588488775
Epoch 45: loss 3.0099238102172885
Epoch 46: loss 2.746992261465507
Epoch 47: loss 2.5115998563006756
Epoch 48: loss 2.304013456417984
Epoch 49: loss 2.1240129209913072
-----------Time: 0:07:29.278596, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.4404765367507935-------------


Epoch 0: loss 16.033487182521768
Epoch 1: loss 15.841611242994398
Epoch 2: loss 15.650649742304857
Epoch 3: loss 15.459854221395853
Epoch 4: loss 15.2673730627229
Epoch 5: loss 15.068125874144412
Epoch 6: loss 14.852670628069017
Epoch 7: loss 14.607053627845968
Epoch 8: loss 14.316712131832137
Epoch 9: loss 13.971210216036823
Epoch 10: loss 13.565200801256106
Epoch 11: loss 13.09769018893011
Epoch 12: loss 12.570968385751382
Epoch 13: loss 11.989244674456515
Epoch 14: loss 11.357694332077168
Epoch 15: loss 10.683185056735148
Epoch 16: loss 9.973280602009677
Epoch 17: loss 9.236133952967949
Epoch 18: loss 8.481764950526674
Epoch 19: loss 7.719984044064122
Epoch 20: loss 6.961565323664223
Epoch 21: loss 6.216987215480835
Epoch 22: loss 5.497902189797198
Epoch 23: loss 4.816081770092589
Epoch 24: loss 4.183765174514123
Epoch 25: loss 3.611333832331103
Epoch 26: loss 3.108827305786502
Epoch 27: loss 2.682715498020365
Epoch 28: loss 2.3355424844615804
Epoch 29: loss 2.0628241147924986
Epoch 30: loss 1.8549386945154565
Epoch 31: loss 1.6981850557187252
Epoch 32: loss 1.5791300062319065
Epoch 33: loss 1.486792464888958
Epoch 34: loss 1.4133359565237023
Epoch 35: loss 1.3534812392457274
Epoch 36: loss 1.3037758337504972
Epoch 37: loss 1.261970126658046
Epoch 38: loss 1.2264297569394436
Epoch 39: loss 1.1960158447748686
Epoch 40: loss 1.1698366301969836
Epoch 41: loss 1.1472410312030807
Epoch 42: loss 1.127665913286515
Epoch 43: loss 1.1106709485763178
Epoch 44: loss 1.095879054834429
Epoch 45: loss 1.0829803686287172
Epoch 46: loss 1.0717121734607473
Epoch 47: loss 1.0618501710399069
Epoch 48: loss 1.053197477209367
Epoch 49: loss 1.0455850714279815
-----------Time: 0:09:14.205180, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.030997395515442-------------


Epoch 0: loss 16.033300357515753
Epoch 1: loss 15.840654643869323
Epoch 2: loss 15.642964787039826
Epoch 3: loss 15.412231630187375
Epoch 4: loss 15.088228254748662
Epoch 5: loss 14.62333131964405
Epoch 6: loss 14.007855716122435
Epoch 7: loss 13.252195411949717
Epoch 8: loss 12.375004292053006
Epoch 9: loss 11.397504804703514
Epoch 10: loss 10.343292888705662
Epoch 11: loss 9.238257907796905
Epoch 12: loss 8.11105545815597
Epoch 13: loss 6.991720851443914
Epoch 14: loss 5.912688182838071
Epoch 15: loss 4.907488320754105
Epoch 16: loss 4.008730848306155
Epoch 17: loss 3.2463401366083957
Epoch 18: loss 2.6406456068069017
Epoch 19: loss 2.1930014596742544
Epoch 20: loss 1.8822847384677885
Epoch 21: loss 1.6716522083715488
Epoch 22: loss 1.525576777209271
Epoch 23: loss 1.4193941186374137
Epoch 24: loss 1.3387553613104206
Epoch 25: loss 1.2756070040081298
Epoch 26: loss 1.2251465891676794
Epoch 27: loss 1.184368835933316
Epoch 28: loss 1.1511954000101199
Epoch 29: loss 1.1240568362771979
Epoch 30: loss 1.1017364490738766
Epoch 31: loss 1.0833320443464272
Epoch 32: loss 1.0680917559141954
Epoch 33: loss 1.0554043668777544
Epoch 34: loss 1.0448395982911367
Epoch 35: loss 1.0359797252948806
Epoch 36: loss 1.0285408907509679
Epoch 37: loss 1.0222611142572597
Epoch 38: loss 1.016946644709122
Epoch 39: loss 1.0124229420100042
Epoch 40: loss 1.0085805075981489
Epoch 41: loss 1.0052794224545643
Epoch 42: loss 1.0024450764732558
Epoch 43: loss 1.000013864205673
Epoch 44: loss 0.9979153761175289
Epoch 45: loss 0.996086060417418
Epoch 46: loss 0.9945011627842643
Epoch 47: loss 0.9931193564559144
Epoch 48: loss 0.9919173445080596
Epoch 49: loss 0.9908592560454644
-----------Time: 0:09:57.803733, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0046324729919434-------------


Epoch 0: loss 16.033373336159322
Epoch 1: loss 15.837337245754473
Epoch 2: loss 15.592001340387956
Epoch 3: loss 15.171045824261448
Epoch 4: loss 14.506128918158742
Epoch 5: loss 13.616302510459116
Epoch 6: loss 12.537831764625686
Epoch 7: loss 11.309562592872009
Epoch 8: loss 9.975160880086213
Epoch 9: loss 8.585041504051453
Epoch 10: loss 7.1939338683563445
Epoch 11: loss 5.860752273993106
Epoch 12: loss 4.647121719460957
Epoch 13: loss 3.611314261718053
Epoch 14: loss 2.7992106921456315
Epoch 15: loss 2.22455718722792
Epoch 16: loss 1.8531652886365795
Epoch 17: loss 1.6198654396823076
Epoch 18: loss 1.4667217804725692
Epoch 19: loss 1.3591331772631832
Epoch 20: loss 1.2795096958960575
Epoch 21: loss 1.2188525686547962
Epoch 22: loss 1.1718837621582792
Epoch 23: loss 1.1352467140858429
Epoch 24: loss 1.10641825105134
Epoch 25: loss 1.083620799906945
Epoch 26: loss 1.0654988548008886
Epoch 27: loss 1.0510196766333453
Epoch 28: loss 1.0393710947996122
Epoch 29: loss 1.0299547930461288
Epoch 30: loss 1.0223140502151036
Epoch 31: loss 1.0160638378196725
Epoch 32: loss 1.010914925617521
Epoch 33: loss 1.0066783372606514
Epoch 34: loss 1.0031529964637083
Epoch 35: loss 1.0002129145003065
Epoch 36: loss 0.9977595800449823
Epoch 37: loss 0.9957033393822525
Epoch 38: loss 0.9939444022635003
Epoch 39: loss 0.9924772876057046
Epoch 40: loss 0.991212634546981
Epoch 41: loss 0.9901282457212445
Epoch 42: loss 0.9891999752188325
Epoch 43: loss 0.9884108569778911
Epoch 44: loss 0.9877125586972021
Epoch 45: loss 0.9871110452654052
Epoch 46: loss 0.9865861056841476
Epoch 47: loss 0.9861268235199344
Epoch 48: loss 0.9857178447588817
Epoch 49: loss 0.98536101046822
-----------Time: 0:12:11.163637, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0017215013504028-------------


Epoch 0: loss 16.033244664024696
Epoch 1: loss 15.824556829619498
Epoch 2: loss 15.452883817891571
Epoch 3: loss 14.74170162264195
Epoch 4: loss 13.71188415192339
Epoch 5: loss 12.427534091466143
Epoch 6: loss 10.95353999612382
Epoch 7: loss 9.361984871599322
Epoch 8: loss 7.734535230509025
Epoch 9: loss 6.159927032251861
Epoch 10: loss 4.732237219291902
Epoch 11: loss 3.54041004693009
Epoch 12: loss 2.6504931731545582
Epoch 13: loss 2.067939987255219
Epoch 14: loss 1.7215133311309005
Epoch 15: loss 1.5137930328979512
Epoch 16: loss 1.3785033106414957
Epoch 17: loss 1.2835767080337863
Epoch 18: loss 1.21418900650892
Epoch 19: loss 1.162438462664084
Epoch 20: loss 1.1233921431859852
Epoch 21: loss 1.093717626283323
Epoch 22: loss 1.0709879810781566
Epoch 23: loss 1.0534709253312453
Epoch 24: loss 1.0398542001041269
Epoch 25: loss 1.0291682359250491
Epoch 26: loss 1.0207716130801943
Epoch 27: loss 1.0140762191352928
Epoch 28: loss 1.0087416185478337
Epoch 29: loss 1.004439605423262
Epoch 30: loss 1.0009669527824707
Epoch 31: loss 0.9981243770170497
Epoch 32: loss 0.9958052593057994
Epoch 33: loss 0.9938878348626929
Epoch 34: loss 0.9922973766088357
Epoch 35: loss 0.9909799876519038
Epoch 36: loss 0.9898691108007674
Epoch 37: loss 0.988926291708845
Epoch 38: loss 0.9881354416596494
Epoch 39: loss 0.9874647234269236
Epoch 40: loss 0.9868922197442783
Epoch 41: loss 0.986399064591036
Epoch 42: loss 0.9859696602516682
Epoch 43: loss 0.9855936479315672
Epoch 44: loss 0.9852806361976559
Epoch 45: loss 0.9849961183685657
Epoch 46: loss 0.9847510058702238
Epoch 47: loss 0.9845287671972061
Epoch 48: loss 0.9843397904440655
Epoch 49: loss 0.9841672871150421
-----------Time: 0:12:25.792147, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0009419918060303-------------


Epoch 0: loss 15.19538444718179
Epoch 1: loss 13.302598069581473
Epoch 2: loss 10.505935933293047
Epoch 3: loss 6.418455331140137
Epoch 4: loss 2.9473298424090166
Epoch 5: loss 1.5258558661863815
Epoch 6: loss 1.1917363568581336
Epoch 7: loss 1.0801434750593246
Epoch 8: loss 1.032509296435646
Epoch 9: loss 1.0104083706821807
Epoch 10: loss 0.9993315485101993
Epoch 11: loss 0.9933335729077045
Epoch 12: loss 0.9898810767978349
Epoch 13: loss 0.9876969142929894
Epoch 14: loss 0.9862068164698646
Epoch 15: loss 0.9851518119075106
Epoch 16: loss 0.9842390144338032
Epoch 17: loss 0.9834587957270448
Epoch 18: loss 0.9827967116922188
Epoch 19: loss 0.9821118709184088
Epoch 20: loss 0.9814456369223447
Epoch 21: loss 0.980748873728653
Epoch 22: loss 0.9800495116071509
Epoch 23: loss 0.9793231320193179
Epoch 24: loss 0.9785381876592599
Epoch 25: loss 0.9777229144529133
Epoch 26: loss 0.9768172334626423
Epoch 27: loss 0.9759075707589367
Epoch 28: loss 0.9748849738966583
Epoch 29: loss 0.9738345763455143
Epoch 30: loss 0.9727195836623899
Epoch 31: loss 0.9715545407905081
Epoch 32: loss 0.9703014888084084
Epoch 33: loss 0.968983534903809
Epoch 34: loss 0.9676196647928486
Epoch 35: loss 0.9661908013591175
Epoch 36: loss 0.9646963733995395
Epoch 37: loss 0.9631916767491667
Epoch 38: loss 0.961612374190081
Epoch 39: loss 0.9599306585349228
Epoch 40: loss 0.9582177073557523
Epoch 41: loss 0.9565096993754907
Epoch 42: loss 0.9547094548928342
Epoch 43: loss 0.9528532948197068
Epoch 44: loss 0.9510145234212466
Epoch 45: loss 0.9490546005585065
Epoch 46: loss 0.9470658798850445
Epoch 47: loss 0.9450752673394139
Epoch 48: loss 0.9430111028403935
Epoch 49: loss 0.9408937922763462
-----------Time: 0:09:10.652964, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9929097294807434-------------


Epoch 0: loss 15.072660425423669
Epoch 1: loss 9.39803637752979
Epoch 2: loss 2.8720155271451846
Epoch 3: loss 1.3000456688681525
Epoch 4: loss 1.0795677084356239
Epoch 5: loss 1.0219072843939796
Epoch 6: loss 1.003655380013846
Epoch 7: loss 0.9967924531320309
Epoch 8: loss 0.993888292949831
Epoch 9: loss 0.9924514442115066
Epoch 10: loss 0.9916577222004994
Epoch 11: loss 0.9909685351007201
Epoch 12: loss 0.9905891943684475
Epoch 13: loss 0.9902662991375428
Epoch 14: loss 0.9899682965759611
Epoch 15: loss 0.9896457848372571
Epoch 16: loss 0.9893569810971286
Epoch 17: loss 0.9890882877664374
Epoch 18: loss 0.9887499759189456
Epoch 19: loss 0.9884070117976628
Epoch 20: loss 0.9880077000737774
Epoch 21: loss 0.9876392130750622
Epoch 22: loss 0.987199633292623
Epoch 23: loss 0.9867329618216468
Epoch 24: loss 0.9861689259527558
Epoch 25: loss 0.9856089769400223
Epoch 26: loss 0.9849313544053756
Epoch 27: loss 0.9843007023079101
Epoch 28: loss 0.9835071270559196
Epoch 29: loss 0.9825871388603644
Epoch 30: loss 0.9816190349044458
Epoch 31: loss 0.9805790430472686
Epoch 32: loss 0.9794095819770414
Epoch 33: loss 0.9780781966891867
Epoch 34: loss 0.976654006304982
Epoch 35: loss 0.9750435557976049
Epoch 36: loss 0.9732932621464255
Epoch 37: loss 0.9713971208560201
Epoch 38: loss 0.969366807549333
Epoch 39: loss 0.9671485967905772
Epoch 40: loss 0.9648286689228828
Epoch 41: loss 0.9622213945011525
Epoch 42: loss 0.959513053387128
Epoch 43: loss 0.9566900368583403
Epoch 44: loss 0.9536620752846436
Epoch 45: loss 0.9505116409455517
Epoch 46: loss 0.9472231171162249
Epoch 47: loss 0.9438278396989159
Epoch 48: loss 0.9402632753832175
Epoch 49: loss 0.936531619363793
-----------Time: 0:08:04.530985, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.991515576839447-------------


Epoch 0: loss 13.750562862704278
Epoch 1: loss 3.8815505795338803
Epoch 2: loss 1.2512438195499795
Epoch 3: loss 1.0461631275798524
Epoch 4: loss 1.0095313819223541
Epoch 5: loss 1.0006187729144498
Epoch 6: loss 0.9976817456161671
Epoch 7: loss 0.9964923021941162
Epoch 8: loss 0.9959194137228126
Epoch 9: loss 0.9954372286277987
Epoch 10: loss 0.9950828134624643
Epoch 11: loss 0.9948123862329807
Epoch 12: loss 0.9945103568574408
Epoch 13: loss 0.9941818435565488
Epoch 14: loss 0.9938292903962377
Epoch 15: loss 0.9935164506050347
Epoch 16: loss 0.9930318321722755
Epoch 17: loss 0.9926208765724549
Epoch 18: loss 0.9919548698292276
Epoch 19: loss 0.9912133183150009
Epoch 20: loss 0.9905285632532533
Epoch 21: loss 0.9896944420307858
Epoch 22: loss 0.9887085027543017
Epoch 23: loss 0.9874920681191891
Epoch 24: loss 0.9861547073927698
Epoch 25: loss 0.9846096991552744
Epoch 26: loss 0.9827426766623227
Epoch 27: loss 0.9808311896067459
Epoch 28: loss 0.9785560035653708
Epoch 29: loss 0.9761181142811155
Epoch 30: loss 0.9733124513091959
Epoch 31: loss 0.9704107782677375
Epoch 32: loss 0.9672910694099756
Epoch 33: loss 0.9639901137825196
Epoch 34: loss 0.9604701376953094
Epoch 35: loss 0.9567505949991691
Epoch 36: loss 0.9529252065433763
Epoch 37: loss 0.9489088125887483
Epoch 38: loss 0.9446363532718204
Epoch 39: loss 0.9401215640438063
Epoch 40: loss 0.9352404287873953
Epoch 41: loss 0.9302939057447392
Epoch 42: loss 0.9248535126334237
Epoch 43: loss 0.919058098586893
Epoch 44: loss 0.9127639585406058
Epoch 45: loss 0.9061312533513951
Epoch 46: loss 0.8992183950687634
Epoch 47: loss 0.8917477118411486
Epoch 48: loss 0.8840070142248132
Epoch 49: loss 0.875708102985463
-----------Time: 0:11:10.560017, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9860872626304626-------------


Epoch 0: loss 11.958297518558513
Epoch 1: loss 2.0737383096292006
Epoch 2: loss 1.0921757342764837
Epoch 3: loss 1.0165817132943866
Epoch 4: loss 1.0037596490377962
Epoch 5: loss 1.0008726067002407
Epoch 6: loss 0.9996004057617665
Epoch 7: loss 0.9991843349717116
Epoch 8: loss 0.9987333346768006
Epoch 9: loss 0.9985720687355926
Epoch 10: loss 0.9982391370256807
Epoch 11: loss 0.9979726689350612
Epoch 12: loss 0.9977295926208403
Epoch 13: loss 0.99720010804767
Epoch 14: loss 0.9970574092450124
Epoch 15: loss 0.9965278776752489
Epoch 16: loss 0.9961051920498759
Epoch 17: loss 0.9954406304680957
Epoch 18: loss 0.9948456177185125
Epoch 19: loss 0.9940612567968509
Epoch 20: loss 0.9929054213516605
Epoch 21: loss 0.9917283024458603
Epoch 22: loss 0.9903643967471348
Epoch 23: loss 0.9885371769330241
Epoch 24: loss 0.9866345216393017
Epoch 25: loss 0.9844458252596686
Epoch 26: loss 0.9816692834998552
Epoch 27: loss 0.9786897240416261
Epoch 28: loss 0.9754682374979375
Epoch 29: loss 0.9718624581909491
Epoch 30: loss 0.9680624374752138
Epoch 31: loss 0.963984000238794
Epoch 32: loss 0.9596031761124058
Epoch 33: loss 0.9551020115694966
Epoch 34: loss 0.950227190256767
Epoch 35: loss 0.9448484490914731
Epoch 36: loss 0.9392821308450248
Epoch 37: loss 0.933266944344631
Epoch 38: loss 0.9267039559113065
Epoch 39: loss 0.9194222158099595
Epoch 40: loss 0.9116508598363937
Epoch 41: loss 0.9030766105638891
Epoch 42: loss 0.8939006659989774
Epoch 43: loss 0.8838022306375104
Epoch 44: loss 0.8730405544443841
Epoch 45: loss 0.8614543957545616
Epoch 46: loss 0.8491601797632059
Epoch 47: loss 0.8359432390619064
Epoch 48: loss 0.8218784141696103
Epoch 49: loss 0.8071181070000792
-----------Time: 0:11:05.077740, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9847699999809265-------------


Epoch 0: loss 10.587161091438384
Epoch 1: loss 1.5298886078573428
Epoch 2: loss 1.0486676933293502
Epoch 3: loss 1.0103527644103218
Epoch 4: loss 1.004286335466218
Epoch 5: loss 1.0024088698116704
Epoch 6: loss 1.001984794270156
Epoch 7: loss 1.0014501491145247
Epoch 8: loss 1.0011965531152898
Epoch 9: loss 1.0009770458356526
Epoch 10: loss 1.0006077714333268
Epoch 11: loss 1.0003452027472806
Epoch 12: loss 0.9999163153218211
Epoch 13: loss 0.9994691165975412
Epoch 14: loss 0.9989546489430355
Epoch 15: loss 0.9982682952030383
Epoch 16: loss 0.9976900908822531
Epoch 17: loss 0.9964349951317545
Epoch 18: loss 0.9951989124688588
Epoch 19: loss 0.9936900275469215
Epoch 20: loss 0.9918463713711795
Epoch 21: loss 0.9895815400729819
Epoch 22: loss 0.9869983175191107
Epoch 23: loss 0.9841034846859434
Epoch 24: loss 0.9806828181046128
Epoch 25: loss 0.977306983955792
Epoch 26: loss 0.973296251016314
Epoch 27: loss 0.9691941989479407
Epoch 28: loss 0.964858912235372
Epoch 29: loss 0.9603746294683578
Epoch 30: loss 0.9553485707688293
Epoch 31: loss 0.9500079504039509
Epoch 32: loss 0.9442292398541696
Epoch 33: loss 0.9377244125895684
Epoch 34: loss 0.9307207565161117
Epoch 35: loss 0.9230679307475306
Epoch 36: loss 0.9144468252395663
Epoch 37: loss 0.9051959785901703
Epoch 38: loss 0.8948303707239743
Epoch 39: loss 0.8836517634924629
Epoch 40: loss 0.8716329730451593
Epoch 41: loss 0.8582143114921255
Epoch 42: loss 0.8439421663146878
Epoch 43: loss 0.828404749700594
Epoch 44: loss 0.8116881484211367
Epoch 45: loss 0.7938775662788559
Epoch 46: loss 0.7750498243749887
Epoch 47: loss 0.7549846586388955
Epoch 48: loss 0.7341136166328578
Epoch 49: loss 0.7122525064322273
-----------Time: 0:13:45.016199, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9851700663566589-------------


Epoch 0: loss 5.09260340069804
Epoch 1: loss 1.030878179484311
Epoch 2: loss 1.0199302446913499
Epoch 3: loss 1.0069633337581463
Epoch 4: loss 0.9934725988941908
Epoch 5: loss 0.9794902113904896
Epoch 6: loss 0.964231784979632
Epoch 7: loss 0.9467806579983707
Epoch 8: loss 0.9270240564492813
Epoch 9: loss 0.9048066907858576
Epoch 10: loss 0.8819434847956186
Epoch 11: loss 0.8602741463894815
Epoch 12: loss 0.8407713950749905
Epoch 13: loss 0.8234427867051115
Epoch 14: loss 0.809292189011177
Epoch 15: loss 0.7976293333073295
Epoch 16: loss 0.7883147266554146
Epoch 17: loss 0.7806111132486026
Epoch 18: loss 0.7742341234672322
Epoch 19: loss 0.7685769801557291
Epoch 20: loss 0.7643226278807043
Epoch 21: loss 0.7603285056926298
Epoch 22: loss 0.756815991862239
Epoch 23: loss 0.7538398677496368
Epoch 24: loss 0.7509455688787972
Epoch 25: loss 0.748572118624907
Epoch 26: loss 0.746326864494974
Epoch 27: loss 0.7443570692523658
Epoch 28: loss 0.7422765212547786
Epoch 29: loss 0.7406403420329937
Epoch 30: loss 0.7391156799738792
Epoch 31: loss 0.7376540633178782
Epoch 32: loss 0.7362225509292734
Epoch 33: loss 0.7348663051161447
Epoch 34: loss 0.7335233320160492
Epoch 35: loss 0.7323318212948914
Epoch 36: loss 0.731438889021845
Epoch 37: loss 0.730339517341871
Epoch 38: loss 0.7294220290247547
Epoch 39: loss 0.7283051592328744
Epoch 40: loss 0.7274760327921281
Epoch 41: loss 0.7265543226657062
Epoch 42: loss 0.7257123664791264
Epoch 43: loss 0.7249930570911881
Epoch 44: loss 0.7240727364757645
Epoch 45: loss 0.7233625927604755
Epoch 46: loss 0.7228231147912094
Epoch 47: loss 0.721893537848329
Epoch 48: loss 0.7214031447902071
Epoch 49: loss 0.7205947272281014
-----------Time: 0:07:10.172685, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0628347396850586-------------


Epoch 0: loss 3.4404260578072545
Epoch 1: loss 1.0554829047645415
Epoch 2: loss 1.0406936516477856
Epoch 3: loss 1.0234341561599813
Epoch 4: loss 1.004360550671184
Epoch 5: loss 0.9751424720068221
Epoch 6: loss 0.9329374898802398
Epoch 7: loss 0.880924116307591
Epoch 8: loss 0.8211414166738833
Epoch 9: loss 0.7602768063447993
Epoch 10: loss 0.7046684268021337
Epoch 11: loss 0.6594149738209866
Epoch 12: loss 0.6235631337675989
Epoch 13: loss 0.5966630422466147
Epoch 14: loss 0.5759356393802418
Epoch 15: loss 0.5597218131082352
Epoch 16: loss 0.5468698167943513
Epoch 17: loss 0.5364521225147626
Epoch 18: loss 0.5276007937827533
Epoch 19: loss 0.5204797156029515
Epoch 20: loss 0.5140093017800986
Epoch 21: loss 0.5085902601204987
Epoch 22: loss 0.5039074288528208
Epoch 23: loss 0.4995566098553257
Epoch 24: loss 0.4955641922336872
Epoch 25: loss 0.49244173146610093
Epoch 26: loss 0.489504061197703
Epoch 27: loss 0.48647632368431587
Epoch 28: loss 0.483818542623468
Epoch 29: loss 0.4814650479252407
Epoch 30: loss 0.4795136545147178
Epoch 31: loss 0.4774845737585106
Epoch 32: loss 0.4756042348375912
Epoch 33: loss 0.473814246259415
Epoch 34: loss 0.4720661365237166
Epoch 35: loss 0.4708179176145432
Epoch 36: loss 0.4691266398137519
Epoch 37: loss 0.46811588180655045
Epoch 38: loss 0.4665826045264363
Epoch 39: loss 0.4653019680803142
Epoch 40: loss 0.4641242715863834
Epoch 41: loss 0.4630238044158216
Epoch 42: loss 0.46202877624129524
Epoch 43: loss 0.46108141202781433
Epoch 44: loss 0.45999278323426074
Epoch 45: loss 0.45915941864884113
Epoch 46: loss 0.4582271131890957
Epoch 47: loss 0.4572725269605181
Epoch 48: loss 0.45671921729425685
Epoch 49: loss 0.45575096147451927
-----------Time: 0:10:07.249257, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.187422513961792-------------


Epoch 0: loss 2.8104317700687864
Epoch 1: loss 1.0768845728909988
Epoch 2: loss 1.0455147107377398
Epoch 3: loss 1.016638993746953
Epoch 4: loss 0.9746481431379728
Epoch 5: loss 0.9029684721969793
Epoch 6: loss 0.8005753701604404
Epoch 7: loss 0.6869364048781247
Epoch 8: loss 0.5824028841255798
Epoch 9: loss 0.49951064888065827
Epoch 10: loss 0.43780404137262124
Epoch 11: loss 0.3934758366645193
Epoch 12: loss 0.3609579587724949
Epoch 13: loss 0.3369689933092715
Epoch 14: loss 0.3182063252098409
Epoch 15: loss 0.3039404895519225
Epoch 16: loss 0.292314491457042
Epoch 17: loss 0.28259374450460734
Epoch 18: loss 0.2747141713151924
Epoch 19: loss 0.2681244364926969
Epoch 20: loss 0.262299742377472
Epoch 21: loss 0.2571582983877427
Epoch 22: loss 0.25266946028956516
Epoch 23: loss 0.24887890152758785
Epoch 24: loss 0.2452795427878017
Epoch 25: loss 0.24225527785876447
Epoch 26: loss 0.23939916530402217
Epoch 27: loss 0.23672619472546963
Epoch 28: loss 0.23441786153516475
Epoch 29: loss 0.23220664317465917
Epoch 30: loss 0.23000462016150508
Epoch 31: loss 0.22852478463804352
Epoch 32: loss 0.22644557119380074
Epoch 33: loss 0.2248819412834752
Epoch 34: loss 0.2232344547725197
Epoch 35: loss 0.22202550903035612
Epoch 36: loss 0.22051900895661863
Epoch 37: loss 0.21907312664456055
Epoch 38: loss 0.21826034602002647
Epoch 39: loss 0.21682231780698663
Epoch 40: loss 0.2157173695558274
Epoch 41: loss 0.21480829732380138
Epoch 42: loss 0.21369082957724958
Epoch 43: loss 0.21273193979973115
Epoch 44: loss 0.21167632667403496
Epoch 45: loss 0.21093699677182384
Epoch 46: loss 0.2100091156544181
Epoch 47: loss 0.20926266918571634
Epoch 48: loss 0.20837049844587155
Epoch 49: loss 0.2080024588304152
-----------Time: 0:09:32.887683, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.2719627618789673-------------


Epoch 0: loss 2.5938308187773074
Epoch 1: loss 1.0923055407468103
Epoch 2: loss 1.0541653301872722
Epoch 3: loss 0.9994109339400308
Epoch 4: loss 0.9056563592268242
Epoch 5: loss 0.7706485243191079
Epoch 6: loss 0.6201661028344713
Epoch 7: loss 0.48805615612188713
Epoch 8: loss 0.38890861828830203
Epoch 9: loss 0.3189566635393526
Epoch 10: loss 0.2716126113708671
Epoch 11: loss 0.23873729426910076
Epoch 12: loss 0.21590579793975365
Epoch 13: loss 0.1994478231667662
Epoch 14: loss 0.18738729108677993
Epoch 15: loss 0.1775665382463171
Epoch 16: loss 0.17013527749307847
Epoch 17: loss 0.16405682284597667
Epoch 18: loss 0.15899670874378874
Epoch 19: loss 0.15483206915484296
Epoch 20: loss 0.15147482855179636
Epoch 21: loss 0.14818157945608174
Epoch 22: loss 0.1454564032913097
Epoch 23: loss 0.1431600068062301
Epoch 24: loss 0.14080596645527266
Epoch 25: loss 0.1391430641846076
Epoch 26: loss 0.1371619169554223
Epoch 27: loss 0.13556030476916542
Epoch 28: loss 0.13380985245586674
Epoch 29: loss 0.13276727945463304
Epoch 30: loss 0.13143275595913767
Epoch 31: loss 0.1302211818869199
Epoch 32: loss 0.1290740534772589
Epoch 33: loss 0.1283081501941113
Epoch 34: loss 0.1271308053694078
Epoch 35: loss 0.12620636817450723
Epoch 36: loss 0.12535903865269923
Epoch 37: loss 0.1247627847960149
Epoch 38: loss 0.12379024675102776
Epoch 39: loss 0.12315143334672722
Epoch 40: loss 0.12228816045856365
Epoch 41: loss 0.12174975103989867
Epoch 42: loss 0.12109661407063778
Epoch 43: loss 0.12072410722845044
Epoch 44: loss 0.11999287907502099
Epoch 45: loss 0.11924877796725225
Epoch 46: loss 0.11882060876621572
Epoch 47: loss 0.11852933578760388
Epoch 48: loss 0.11787433985290545
Epoch 49: loss 0.11753690725596558
-----------Time: 0:12:25.527366, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.2281070947647095-------------


Epoch 0: loss 2.4711391760513406
Epoch 1: loss 1.095779624547305
Epoch 2: loss 1.0396424825584325
Epoch 3: loss 0.9485942088209331
Epoch 4: loss 0.803246949858611
Epoch 5: loss 0.6294120185445871
Epoch 6: loss 0.4706656143369073
Epoch 7: loss 0.34997807694330624
Epoch 8: loss 0.2687307673629501
Epoch 9: loss 0.21701536212742426
Epoch 10: loss 0.18480624428273154
Epoch 11: loss 0.16403931485870857
Epoch 12: loss 0.1510031339330087
Epoch 13: loss 0.14139003430803618
Epoch 14: loss 0.13473814752007582
Epoch 15: loss 0.1296088617009401
Epoch 16: loss 0.1255566061349295
Epoch 17: loss 0.12267716116939893
Epoch 18: loss 0.11993753032018432
Epoch 19: loss 0.11762758998920439
Epoch 20: loss 0.11577054612994875
Epoch 21: loss 0.11418041591231204
Epoch 22: loss 0.11269303799980097
Epoch 23: loss 0.11119238967243066
Epoch 24: loss 0.11013217943823325
Epoch 25: loss 0.109067375969715
Epoch 26: loss 0.10802814145554758
Epoch 27: loss 0.10692069868599188
Epoch 28: loss 0.10610559810299372
Epoch 29: loss 0.10522313087331886
Epoch 30: loss 0.10441693933827519
Epoch 31: loss 0.103552421434393
Epoch 32: loss 0.10318169702510914
Epoch 33: loss 0.10218224308720389
Epoch 34: loss 0.10187731649931227
Epoch 35: loss 0.10121114514342723
Epoch 36: loss 0.10063446385967238
Epoch 37: loss 0.10016135248300367
Epoch 38: loss 0.09956795848752086
Epoch 39: loss 0.09920411777053112
Epoch 40: loss 0.09880573354568281
Epoch 41: loss 0.09821176292327126
Epoch 42: loss 0.09791175301767614
Epoch 43: loss 0.09744969471102016
Epoch 44: loss 0.09697544541211962
Epoch 45: loss 0.09681953888036039
Epoch 46: loss 0.09655275200698737
Epoch 47: loss 0.09598248829642769
Epoch 48: loss 0.0959039821404439
Epoch 49: loss 0.09552238807649377
-----------Time: 0:12:01.312768, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.1591157913208008-------------


Epoch 0: loss 16.12928207391238
Epoch 1: loss 16.12927985230239
Epoch 2: loss 16.12927966639035
Epoch 3: loss 16.12927190190492
Epoch 4: loss 16.12927686889383
Epoch 5: loss 16.12927190683146
Epoch 6: loss 16.12927093189524
Epoch 7: loss 16.129265916159518
Epoch 8: loss 16.129268760587806
Epoch 9: loss 16.129263167150516
Epoch 10: loss 16.129265445026775
Epoch 11: loss 16.12926193395991
Epoch 12: loss 16.129261338367222
Epoch 13: loss 16.129257718916495
Epoch 14: loss 16.129257796963252
Epoch 15: loss 16.12925643231183
Epoch 16: loss 16.12925950310164
Epoch 17: loss 16.129252494451077
Epoch 18: loss 16.12924533800433
Epoch 19: loss 16.12924710403907
Epoch 20: loss 16.129243649757058
Epoch 21: loss 16.12924495632717
Epoch 22: loss 16.129250752789744
Epoch 23: loss 16.129232753807596
Epoch 24: loss 16.12924045554447
Epoch 25: loss 16.12923457481214
Epoch 26: loss 16.12922879183273
Epoch 27: loss 16.12923438371427
Epoch 28: loss 16.12923011966475
Epoch 29: loss 16.12923332917554
Epoch 30: loss 16.129227024760823
Epoch 31: loss 16.1292273475788
Epoch 32: loss 16.12922374731565
Epoch 33: loss 16.129217669781035
Epoch 34: loss 16.12922489571792
Epoch 35: loss 16.129216480669992
Epoch 36: loss 16.12921756528654
Epoch 37: loss 16.129206255248477
Epoch 38: loss 16.129206994747975
Epoch 39: loss 16.129208791119822
Epoch 40: loss 16.12920401937745
Epoch 41: loss 16.129201730610948
Epoch 42: loss 16.129195937000585
Epoch 43: loss 16.129199422397583
Epoch 44: loss 16.12919938531889
Epoch 45: loss 16.12919997650362
Epoch 46: loss 16.129196534667603
Epoch 47: loss 16.129192404153233
Epoch 48: loss 16.12919252757601
Epoch 49: loss 16.12919337027354
-----------Time: 0:09:27.368446, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.0171990394592285-------------


Epoch 0: loss 16.129169469815743
Epoch 1: loss 16.129167618992668
Epoch 2: loss 16.129169752962113
Epoch 3: loss 16.12916793455048
Epoch 4: loss 16.129173128938078
Epoch 5: loss 16.12916905780147
Epoch 6: loss 16.129168535847583
Epoch 7: loss 16.12915913289886
Epoch 8: loss 16.129160481992788
Epoch 9: loss 16.129158287089826
Epoch 10: loss 16.129154302815888
Epoch 11: loss 16.129155367466986
Epoch 12: loss 16.12914935423668
Epoch 13: loss 16.12915238742922
Epoch 14: loss 16.129147758297197
Epoch 15: loss 16.129146993387128
Epoch 16: loss 16.129140024926755
Epoch 17: loss 16.12914772018134
Epoch 18: loss 16.129138824925466
Epoch 19: loss 16.129141546968146
Epoch 20: loss 16.129137469867832
Epoch 21: loss 16.129132334339417
Epoch 22: loss 16.12912880226994
Epoch 23: loss 16.129135220513646
Epoch 24: loss 16.129133117659187
Epoch 25: loss 16.129124868557852
Epoch 26: loss 16.129124205290072
Epoch 27: loss 16.129124212290943
Epoch 28: loss 16.12911565178046
Epoch 29: loss 16.129117996035355
Epoch 30: loss 16.129116767511995
Epoch 31: loss 16.129113865780273
Epoch 32: loss 16.129115022479873
Epoch 33: loss 16.129115568547874
Epoch 34: loss 16.129110693866757
Epoch 35: loss 16.12910913604313
Epoch 36: loss 16.12910550414641
Epoch 37: loss 16.129104560584462
Epoch 38: loss 16.12910801019923
Epoch 39: loss 16.129101983226466
Epoch 40: loss 16.12910024441734
Epoch 41: loss 16.129101817020583
Epoch 42: loss 16.12909848823568
Epoch 43: loss 16.12908559003688
Epoch 44: loss 16.12909274752079
Epoch 45: loss 16.129094143027906
Epoch 46: loss 16.12908595900875
Epoch 47: loss 16.129083587528246
Epoch 48: loss 16.12908501466893
Epoch 49: loss 16.12907490100207
-----------Time: 0:07:58.417022, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017178058624268-------------


Epoch 0: loss 16.129185841743425
Epoch 1: loss 16.12916890507875
Epoch 2: loss 16.129170399116653
Epoch 3: loss 16.129173294106796
Epoch 4: loss 16.129167478456647
Epoch 5: loss 16.12916004508653
Epoch 6: loss 16.129158548196415
Epoch 7: loss 16.12916082762842
Epoch 8: loss 16.12916159072345
Epoch 9: loss 16.12915782243937
Epoch 10: loss 16.129156624512415
Epoch 11: loss 16.12915014948386
Epoch 12: loss 16.12915663047612
Epoch 13: loss 16.129152112580176
Epoch 14: loss 16.129150470486795
Epoch 15: loss 16.12914795795169
Epoch 16: loss 16.129145341959255
Epoch 17: loss 16.12913451601851
Epoch 18: loss 16.12914760842668
Epoch 19: loss 16.12914020824588
Epoch 20: loss 16.129135309969232
Epoch 21: loss 16.129135264333918
Epoch 22: loss 16.129127273227677
Epoch 23: loss 16.129130074094988
Epoch 24: loss 16.12912657080687
Epoch 25: loss 16.12913003260834
Epoch 26: loss 16.129122789039634
Epoch 27: loss 16.129118961118497
Epoch 28: loss 16.129127886192897
Epoch 29: loss 16.12911612265391
Epoch 30: loss 16.129117848498463
Epoch 31: loss 16.12911587917915
Epoch 32: loss 16.129114315132526
Epoch 33: loss 16.129113643048832
Epoch 34: loss 16.129110660158855
Epoch 35: loss 16.129103615207477
Epoch 36: loss 16.129103990661637
Epoch 37: loss 16.129110330858587
Epoch 38: loss 16.129101987115842
Epoch 39: loss 16.129098055218794
Epoch 40: loss 16.1291029579034
Epoch 41: loss 16.129092340173766
Epoch 42: loss 16.129103258940887
Epoch 43: loss 16.129084307321587
Epoch 44: loss 16.129088745355737
Epoch 45: loss 16.129087578284473
Epoch 46: loss 16.129087218387806
Epoch 47: loss 16.12908055641004
Epoch 48: loss 16.12908185312708
Epoch 49: loss 16.129079798500847
-----------Time: 0:11:19.678087, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017177581787109-------------


Epoch 0: loss 16.12917063636842
Epoch 1: loss 16.129170051665977
Epoch 2: loss 16.12917076705136
Epoch 3: loss 16.129175242682784
Epoch 4: loss 16.12916106721381
Epoch 5: loss 16.12916418052744
Epoch 6: loss 16.129160456322925
Epoch 7: loss 16.129158429181594
Epoch 8: loss 16.129159874472688
Epoch 9: loss 16.129162658226758
Epoch 10: loss 16.12915972408359
Epoch 11: loss 16.129153428484784
Epoch 12: loss 16.129157028229358
Epoch 13: loss 16.129154799359203
Epoch 14: loss 16.129142741783603
Epoch 15: loss 16.129151110936924
Epoch 16: loss 16.12914676080261
Epoch 17: loss 16.129144489667933
Epoch 18: loss 16.129143083270574
Epoch 19: loss 16.12914657489057
Epoch 20: loss 16.129142682146547
Epoch 21: loss 16.12913489147267
Epoch 22: loss 16.129138327604277
Epoch 23: loss 16.12912706164577
Epoch 24: loss 16.129128657844547
Epoch 25: loss 16.129133732180158
Epoch 26: loss 16.129129429236905
Epoch 27: loss 16.12912697219019
Epoch 28: loss 16.129126347297557
Epoch 29: loss 16.12912520278466
Epoch 30: loss 16.129117101220217
Epoch 31: loss 16.129121055157047
Epoch 32: loss 16.129114360249257
Epoch 33: loss 16.129109426190375
Epoch 34: loss 16.129114864312026
Epoch 35: loss 16.129107752982012
Epoch 36: loss 16.129102017712242
Epoch 37: loss 16.129098763862597
Epoch 38: loss 16.129104986341186
Epoch 39: loss 16.129097571121473
Epoch 40: loss 16.129096088492396
Epoch 41: loss 16.129091105945996
Epoch 42: loss 16.129093772500283
Epoch 43: loss 16.12909480966648
Epoch 44: loss 16.129091842593287
Epoch 45: loss 16.129088904820037
Epoch 46: loss 16.129089232564557
Epoch 47: loss 16.12908588822216
Epoch 48: loss 16.129084805161362
Epoch 49: loss 16.129077539034288
-----------Time: 0:10:44.584912, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017177104949951-------------


Epoch 0: loss 16.129170227724938
Epoch 1: loss 16.129166990988537
Epoch 2: loss 16.129165066008078
Epoch 3: loss 16.12916760421305
Epoch 4: loss 16.129161244569232
Epoch 5: loss 16.129157712499754
Epoch 6: loss 16.12915653687187
Epoch 7: loss 16.129160016305164
Epoch 8: loss 16.12915736193758
Epoch 9: loss 16.129153473082933
Epoch 10: loss 16.12915503816672
Epoch 11: loss 16.12914957696812
Epoch 12: loss 16.129145482754566
Epoch 13: loss 16.12914653340392
Epoch 14: loss 16.129134211869523
Epoch 15: loss 16.12914592743957
Epoch 16: loss 16.129139683439785
Epoch 17: loss 16.1291346941518
Epoch 18: loss 16.129141024754965
Epoch 19: loss 16.129140030371875
Epoch 20: loss 16.129133462257656
Epoch 21: loss 16.12912819656488
Epoch 22: loss 16.12912450477181
Epoch 23: loss 16.129123338997008
Epoch 24: loss 16.129126602959023
Epoch 25: loss 16.129124454987835
Epoch 26: loss 16.12912022905417
Epoch 27: loss 16.129112841319365
Epoch 28: loss 16.129109768195928
Epoch 29: loss 16.129115192315837
Epoch 30: loss 16.12911736440114
Epoch 31: loss 16.129115360077467
Epoch 32: loss 16.12910717216894
Epoch 33: loss 16.12910340310699
Epoch 34: loss 16.129108372948103
Epoch 35: loss 16.129105148916985
Epoch 36: loss 16.129094787108112
Epoch 37: loss 16.129096051673
Epoch 38: loss 16.129094033606872
Epoch 39: loss 16.12910296853435
Epoch 40: loss 16.129092832568418
Epoch 41: loss 16.12909061147701
Epoch 42: loss 16.129094635163266
Epoch 43: loss 16.12908750542355
Epoch 44: loss 16.12908827577874
Epoch 45: loss 16.129085913632732
Epoch 46: loss 16.12907670696771
Epoch 47: loss 16.129077562889112
Epoch 48: loss 16.12907394317909
Epoch 49: loss 16.129071762537166
-----------Time: 0:13:49.153977, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017177581787109-------------


Epoch 0: loss 16.129411984682733
Epoch 1: loss 16.129388555357675
Epoch 2: loss 16.129372090085035
Epoch 3: loss 16.129349839499362
Epoch 4: loss 16.129335483044887
Epoch 5: loss 16.12931660065643
Epoch 6: loss 16.129298759842126
Epoch 7: loss 16.129274211155455
Epoch 8: loss 16.12925129056042
Epoch 9: loss 16.129235937907815
Epoch 10: loss 16.129222665292016
Epoch 11: loss 16.129200976850417
Epoch 12: loss 16.12917956662865
Epoch 13: loss 16.129151769018872
Epoch 14: loss 16.129138340309563
Epoch 15: loss 16.1291158223943
Epoch 16: loss 16.129095982960738
Epoch 17: loss 16.129074817769485
Epoch 18: loss 16.12906550531351
Epoch 19: loss 16.12904128463065
Epoch 20: loss 16.129026335954276
Epoch 21: loss 16.12901085884173
Epoch 22: loss 16.128983436945408
Epoch 23: loss 16.128968643843965
Epoch 24: loss 16.128949408559713
Epoch 25: loss 16.12893172202388
Epoch 26: loss 16.128910528829138
Epoch 27: loss 16.12888721151803
Epoch 28: loss 16.128873033974727
Epoch 29: loss 16.1288479469988
Epoch 30: loss 16.128835709215572
Epoch 31: loss 16.12881209190414
Epoch 32: loss 16.128795594997936
Epoch 33: loss 16.128777544157476
Epoch 34: loss 16.128755812932773
Epoch 35: loss 16.128733668915924
Epoch 36: loss 16.12871293207431
Epoch 37: loss 16.128696591520907
Epoch 38: loss 16.1286758513085
Epoch 39: loss 16.128662064776854
Epoch 40: loss 16.128638681346402
Epoch 41: loss 16.128624318150347
Epoch 42: loss 16.128599237397417
Epoch 43: loss 16.12858798881145
Epoch 44: loss 16.12856450866525
Epoch 45: loss 16.12854296568621
Epoch 46: loss 16.12852254906966
Epoch 47: loss 16.128499787679633
Epoch 48: loss 16.128488167788166
Epoch 49: loss 16.128465676579935
-----------Time: 0:07:05.385346, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017104148864746-------------


Epoch 0: loss 16.12917148891903
Epoch 1: loss 16.129164543017023
Epoch 2: loss 16.129136616020762
Epoch 3: loss 16.12912407927438
Epoch 4: loss 16.129102816330214
Epoch 5: loss 16.12908342728607
Epoch 6: loss 16.129066478693986
Epoch 7: loss 16.12904141712863
Epoch 8: loss 16.129016682011333
Epoch 9: loss 16.129004731523143
Epoch 10: loss 16.128980505395155
Epoch 11: loss 16.12896969760482
Epoch 12: loss 16.12894678841861
Epoch 13: loss 16.128927883212498
Epoch 14: loss 16.128905774459298
Epoch 15: loss 16.128887156029638
Epoch 16: loss 16.128868236821784
Epoch 17: loss 16.12884582754968
Epoch 18: loss 16.128823739280513
Epoch 19: loss 16.128806833730824
Epoch 20: loss 16.12878849403958
Epoch 21: loss 16.12876209271479
Epoch 22: loss 16.1287550174263
Epoch 23: loss 16.1287310033988
Epoch 24: loss 16.12872183484964
Epoch 25: loss 16.128690640001988
Epoch 26: loss 16.128673259948762
Epoch 27: loss 16.128658414211138
Epoch 28: loss 16.12863671047134
Epoch 29: loss 16.128613365675328
Epoch 30: loss 16.12860057534181
Epoch 31: loss 16.128578704877544
Epoch 32: loss 16.12856118558476
Epoch 33: loss 16.1285352375016
Epoch 34: loss 16.12851419469596
Epoch 35: loss 16.128496161746614
Epoch 36: loss 16.12848614557338
Epoch 37: loss 16.128459382018296
Epoch 38: loss 16.128441329362797
Epoch 39: loss 16.12842583565559
Epoch 40: loss 16.128405305987926
Epoch 41: loss 16.128386449528623
Epoch 42: loss 16.1283652374056
Epoch 43: loss 16.12834348984553
Epoch 44: loss 16.12833002146461
Epoch 45: loss 16.128304534661115
Epoch 46: loss 16.128289180712056
Epoch 47: loss 16.128264960807066
Epoch 48: loss 16.128249673236642
Epoch 49: loss 16.128227233108845
-----------Time: 0:10:16.431465, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017065048217773-------------


Epoch 0: loss 16.129167694187217
Epoch 1: loss 16.12914341257084
Epoch 2: loss 16.129126393710745
Epoch 3: loss 16.129113148579847
Epoch 4: loss 16.129088363159994
Epoch 5: loss 16.129062728301022
Epoch 6: loss 16.12904840322083
Epoch 7: loss 16.12902692273105
Epoch 8: loss 16.129013546657927
Epoch 9: loss 16.128992120878667
Epoch 10: loss 16.128968381440917
Epoch 11: loss 16.128956496294197
Epoch 12: loss 16.1289347311023
Epoch 13: loss 16.12891780688362
Epoch 14: loss 16.12888855568542
Epoch 15: loss 16.12887106258108
Epoch 16: loss 16.128852926952284
Epoch 17: loss 16.128834437390527
Epoch 18: loss 16.128820825880673
Epoch 19: loss 16.128804536926285
Epoch 20: loss 16.128774724880465
Epoch 21: loss 16.128756049665956
Epoch 22: loss 16.128736309281763
Epoch 23: loss 16.12871510027024
Epoch 24: loss 16.12870297553813
Epoch 25: loss 16.12868374336537
Epoch 26: loss 16.128663697017153
Epoch 27: loss 16.12864759604914
Epoch 28: loss 16.12862101347956
Epoch 29: loss 16.12860623360199
Epoch 30: loss 16.128580779469228
Epoch 31: loss 16.128564377463725
Epoch 32: loss 16.12854357839214
Epoch 33: loss 16.12852137759044
Epoch 34: loss 16.128504370657758
Epoch 35: loss 16.128486895185244
Epoch 36: loss 16.128466642700246
Epoch 37: loss 16.128449230235578
Epoch 38: loss 16.128433189163914
Epoch 39: loss 16.12841408482189
Epoch 40: loss 16.12839257969942
Epoch 41: loss 16.12836936921643
Epoch 42: loss 16.128357359609765
Epoch 43: loss 16.12833337280788
Epoch 44: loss 16.128314839425848
Epoch 45: loss 16.128291303791254
Epoch 46: loss 16.12827097844533
Epoch 47: loss 16.12825388361281
Epoch 48: loss 16.12823509275427
Epoch 49: loss 16.128216692648095
-----------Time: 0:09:38.391635, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017069339752197-------------


Epoch 0: loss 16.129162479056294
Epoch 1: loss 16.12914753660292
Epoch 2: loss 16.129119037350215
Epoch 3: loss 16.12910430077433
Epoch 4: loss 16.12907547196206
Epoch 5: loss 16.129065385780105
Epoch 6: loss 16.129049333818195
Epoch 7: loss 16.12902179290705
Epoch 8: loss 16.129012972327146
Epoch 9: loss 16.128989479994242
Epoch 10: loss 16.128972102533936
Epoch 11: loss 16.12894550777765
Epoch 12: loss 16.128928280965734
Epoch 13: loss 16.12891240376633
Epoch 14: loss 16.128889969602234
Epoch 15: loss 16.128877833979878
Epoch 16: loss 16.128853948560987
Epoch 17: loss 16.128832838598836
Epoch 18: loss 16.12881312595884
Epoch 19: loss 16.128799891977483
Epoch 20: loss 16.12877543767293
Epoch 21: loss 16.12875402278392
Epoch 22: loss 16.128733570125842
Epoch 23: loss 16.128719222746575
Epoch 24: loss 16.12869751122803
Epoch 25: loss 16.128673696336442
Epoch 26: loss 16.12866092052325
Epoch 27: loss 16.1286392152277
Epoch 28: loss 16.12861502514124
Epoch 29: loss 16.12859832183896
Epoch 30: loss 16.128575727951272
Epoch 31: loss 16.128566786541505
Epoch 32: loss 16.128548494041322
Epoch 33: loss 16.12852334068676
Epoch 34: loss 16.1285053634851
Epoch 35: loss 16.128483446348355
Epoch 36: loss 16.128465316423974
Epoch 37: loss 16.128440990728034
Epoch 38: loss 16.128428669712218
Epoch 39: loss 16.128412247481975
Epoch 40: loss 16.1283944395977
Epoch 41: loss 16.12836768200632
Epoch 42: loss 16.128347360809062
Epoch 43: loss 16.128325813681357
Epoch 44: loss 16.128313904939105
Epoch 45: loss 16.128290548734267
Epoch 46: loss 16.128275823826502
Epoch 47: loss 16.12825961369675
Epoch 48: loss 16.128234641846266
Epoch 49: loss 16.12821414536792
-----------Time: 0:12:46.871311, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.0170674324035645-------------


Epoch 0: loss 16.129166058576125
Epoch 1: loss 16.129140594330995
Epoch 2: loss 16.129123895955253
Epoch 3: loss 16.129102115465155
Epoch 4: loss 16.12908735840524
Epoch 5: loss 16.129073155451366
Epoch 6: loss 16.129045547902294
Epoch 7: loss 16.129029229907253
Epoch 8: loss 16.1290084360215
Epoch 9: loss 16.128994189765933
Epoch 10: loss 16.12896814600419
Epoch 11: loss 16.128957195862796
Epoch 12: loss 16.12893430067833
Epoch 13: loss 16.12891140990182
Epoch 14: loss 16.128894086114865
Epoch 15: loss 16.128870757394928
Epoch 16: loss 16.128852993849506
Epoch 17: loss 16.12883423592099
Epoch 18: loss 16.128815860706805
Epoch 19: loss 16.12879528281091
Epoch 20: loss 16.1287778317118
Epoch 21: loss 16.128757352605987
Epoch 22: loss 16.12874456356893
Epoch 23: loss 16.128713778401924
Epoch 24: loss 16.12870062687528
Epoch 25: loss 16.128680682428637
Epoch 26: loss 16.128656233569213
Epoch 27: loss 16.12864325343428
Epoch 28: loss 16.12861869670957
Epoch 29: loss 16.128600278452986
Epoch 30: loss 16.128584108513422
Epoch 31: loss 16.128565912469696
Epoch 32: loss 16.128546256095966
Epoch 33: loss 16.12851790826952
Epoch 34: loss 16.128507504455417
Epoch 35: loss 16.12849209242505
Epoch 36: loss 16.128473037607712
Epoch 37: loss 16.128448971203323
Epoch 38: loss 16.12842818924498
Epoch 39: loss 16.12840335637477
Epoch 40: loss 16.128390652385335
Epoch 41: loss 16.128370769650083
Epoch 42: loss 16.12835025320629
Epoch 43: loss 16.128343966423394
Epoch 44: loss 16.128321621455598
Epoch 45: loss 16.12829653422038
Epoch 46: loss 16.128273977411382
Epoch 47: loss 16.128253877130522
Epoch 48: loss 16.128232958525533
Epoch 49: loss 16.12821260699116
-----------Time: 0:11:54.702539, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017068862915039-------------


Epoch 0: loss 16.12917036852025
Epoch 1: loss 16.128980510580988
Epoch 2: loss 16.128783588243476
Epoch 3: loss 16.12860158994964
Epoch 4: loss 16.128404626125484
Epoch 5: loss 16.128207189612834
Epoch 6: loss 16.12801447220637
Epoch 7: loss 16.12781978470272
Epoch 8: loss 16.127625699274045
Epoch 9: loss 16.127432892411996
Epoch 10: loss 16.127235752269584
Epoch 11: loss 16.127047158376623
Epoch 12: loss 16.126855648750194
Epoch 13: loss 16.1266633464695
Epoch 14: loss 16.12646632897199
Epoch 15: loss 16.126270237404604
Epoch 16: loss 16.126084302805765
Epoch 17: loss 16.12587858192813
Epoch 18: loss 16.12569725079145
Epoch 19: loss 16.125494734757357
Epoch 20: loss 16.12530083161064
Epoch 21: loss 16.125108962087545
Epoch 22: loss 16.124921604496684
Epoch 23: loss 16.12473175926271
Epoch 24: loss 16.124525330519145
Epoch 25: loss 16.12433802556447
Epoch 26: loss 16.124141061999605
Epoch 27: loss 16.123948643815588
Epoch 28: loss 16.12375328682116
Epoch 29: loss 16.123563221707947
Epoch 30: loss 16.123366028929354
Epoch 31: loss 16.123180128038417
Epoch 32: loss 16.122989426883034
Epoch 33: loss 16.122783661925837
Epoch 34: loss 16.12259622732539
Epoch 35: loss 16.122408041816616
Epoch 36: loss 16.122215391566662
Epoch 37: loss 16.122015241567954
Epoch 38: loss 16.121824997802868
Epoch 39: loss 16.121633881521717
Epoch 40: loss 16.121440085980996
Epoch 41: loss 16.12124921524894
Epoch 42: loss 16.12104860085907
Epoch 43: loss 16.1208587216579
Epoch 44: loss 16.120662290677874
Epoch 45: loss 16.120467759267736
Epoch 46: loss 16.120277457939924
Epoch 47: loss 16.120081639147244
Epoch 48: loss 16.119883358381312
Epoch 49: loss 16.11969141988666
-----------Time: 0:09:39.860421, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.0159711837768555-------------


Epoch 0: loss 16.129068278436623
Epoch 1: loss 16.12887719171471
Epoch 2: loss 16.128686397732956
Epoch 3: loss 16.128489348861173
Epoch 4: loss 16.12829340275634
Epoch 5: loss 16.12810173962932
Epoch 6: loss 16.127913559306375
Epoch 7: loss 16.127717119251145
Epoch 8: loss 16.127526915416958
Epoch 9: loss 16.12733443681801
Epoch 10: loss 16.127137822778153
Epoch 11: loss 16.126939264829552
Epoch 12: loss 16.12675384673948
Epoch 13: loss 16.126557322414495
Epoch 14: loss 16.12636459463637
Epoch 15: loss 16.126168437986806
Epoch 16: loss 16.125978632683726
Epoch 17: loss 16.12578855693956
Epoch 18: loss 16.12559222060095
Epoch 19: loss 16.12539722168815
Epoch 20: loss 16.12519798050632
Epoch 21: loss 16.125009069499793
Epoch 22: loss 16.12481235718844
Epoch 23: loss 16.12462792310979
Epoch 24: loss 16.124427681321876
Epoch 25: loss 16.12423778630392
Epoch 26: loss 16.124047302175565
Epoch 27: loss 16.123846522098393
Epoch 28: loss 16.12365435387143
Epoch 29: loss 16.123463453580143
Epoch 30: loss 16.123262622941116
Epoch 31: loss 16.123081086186236
Epoch 32: loss 16.122885588137205
Epoch 33: loss 16.12268946830704
Epoch 34: loss 16.12249898806806
Epoch 35: loss 16.122294916803256
Epoch 36: loss 16.122112698111604
Epoch 37: loss 16.121910523045898
Epoch 38: loss 16.12171825343594
Epoch 39: loss 16.121522408195524
Epoch 40: loss 16.121330719398635
Epoch 41: loss 16.12114393510139
Epoch 42: loss 16.120947736446595
Epoch 43: loss 16.12075227184617
Epoch 44: loss 16.1205586710334
Epoch 45: loss 16.12036204480684
Epoch 46: loss 16.120174117552448
Epoch 47: loss 16.11998027922862
Epoch 48: loss 16.1197856368417
Epoch 49: loss 16.11959497406147
-----------Time: 0:07:33.257097, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015995979309082-------------


Epoch 0: loss 16.12907388587566
Epoch 1: loss 16.128888287577958
Epoch 2: loss 16.1286879366283
Epoch 3: loss 16.12849533045791
Epoch 4: loss 16.128297390401073
Epoch 5: loss 16.128114310083603
Epoch 6: loss 16.127910569674818
Epoch 7: loss 16.127718283470198
Epoch 8: loss 16.12753008525614
Epoch 9: loss 16.12733613751128
Epoch 10: loss 16.127139302036436
Epoch 11: loss 16.126948934329988
Epoch 12: loss 16.12675467569456
Epoch 13: loss 16.12655979761162
Epoch 14: loss 16.12636779896133
Epoch 15: loss 16.12616835864359
Epoch 16: loss 16.125982824909485
Epoch 17: loss 16.12578105978372
Epoch 18: loss 16.125591946789076
Epoch 19: loss 16.125395644158367
Epoch 20: loss 16.125203318541434
Epoch 21: loss 16.125015618685733
Epoch 22: loss 16.124822039912743
Epoch 23: loss 16.124627825356878
Epoch 24: loss 16.12442931797013
Epoch 25: loss 16.124241215693946
Epoch 26: loss 16.124043128100215
Epoch 27: loss 16.123852848552893
Epoch 28: loss 16.12366002224398
Epoch 29: loss 16.123461117363288
Epoch 30: loss 16.12327147100593
Epoch 31: loss 16.123075314356363
Epoch 32: loss 16.122881825816833
Epoch 33: loss 16.12268724488201
Epoch 34: loss 16.122497256259805
Epoch 35: loss 16.12230293176432
Epoch 36: loss 16.122111190071955
Epoch 37: loss 16.121923365237727
Epoch 38: loss 16.121720492159174
Epoch 39: loss 16.121533347187384
Epoch 40: loss 16.121336218972385
Epoch 41: loss 16.121138728786384
Epoch 42: loss 16.12095124647629
Epoch 43: loss 16.120756486111713
Epoch 44: loss 16.120565154359287
Epoch 45: loss 16.120369310156036
Epoch 46: loss 16.120181077456202
Epoch 47: loss 16.11997849919214
Epoch 48: loss 16.11978620235657
Epoch 49: loss 16.119596617451304
-----------Time: 0:11:37.445094, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015992164611816-------------


Epoch 0: loss 16.12907594542843
Epoch 1: loss 16.128886015146826
Epoch 2: loss 16.128693554698287
Epoch 3: loss 16.12849316589206
Epoch 4: loss 16.128304867850755
Epoch 5: loss 16.12810813194387
Epoch 6: loss 16.127916912205393
Epoch 7: loss 16.12773220249983
Epoch 8: loss 16.127538160113552
Epoch 9: loss 16.127338188248135
Epoch 10: loss 16.127139618372127
Epoch 11: loss 16.126941927235176
Epoch 12: loss 16.126755539135416
Epoch 13: loss 16.12656406632839
Epoch 14: loss 16.126365648137934
Epoch 15: loss 16.126177662283652
Epoch 16: loss 16.125981031908427
Epoch 17: loss 16.12579711848721
Epoch 18: loss 16.12559596554879
Epoch 19: loss 16.125404126362803
Epoch 20: loss 16.12520925917011
Epoch 21: loss 16.125010141151765
Epoch 22: loss 16.124826302665806
Epoch 23: loss 16.124627833135623
Epoch 24: loss 16.124428110967973
Epoch 25: loss 16.12424601180972
Epoch 26: loss 16.124040916084013
Epoch 27: loss 16.12385406048159
Epoch 28: loss 16.123666103667965
Epoch 29: loss 16.123467410109885
Epoch 30: loss 16.123271521049197
Epoch 31: loss 16.1230770067523
Epoch 32: loss 16.12288411717625
Epoch 33: loss 16.122697314728597
Epoch 34: loss 16.122501126964043
Epoch 35: loss 16.122303559768454
Epoch 36: loss 16.122110370969953
Epoch 37: loss 16.121922887363404
Epoch 38: loss 16.121726930109038
Epoch 39: loss 16.121538080554608
Epoch 40: loss 16.121340173687088
Epoch 41: loss 16.121143470191644
Epoch 42: loss 16.120948145608658
Epoch 43: loss 16.120761025528857
Epoch 44: loss 16.1205613710363
Epoch 45: loss 16.120378283199376
Epoch 46: loss 16.120178967082285
Epoch 47: loss 16.11999010482257
Epoch 48: loss 16.119795873931338
Epoch 49: loss 16.119604223250306
-----------Time: 0:10:27.485130, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015989303588867-------------


Epoch 0: loss 16.12907872970108
Epoch 1: loss 16.12888642275314
Epoch 2: loss 16.128697855567207
Epoch 3: loss 16.12850342813298
Epoch 4: loss 16.128316131994218
Epoch 5: loss 16.128114990205333
Epoch 6: loss 16.127927251715192
Epoch 7: loss 16.12772921286827
Epoch 8: loss 16.127540325197984
Epoch 9: loss 16.127334552202747
Epoch 10: loss 16.12715468295182
Epoch 11: loss 16.126961455778172
Epoch 12: loss 16.126762648909683
Epoch 13: loss 16.126568130982708
Epoch 14: loss 16.12637601435476
Epoch 15: loss 16.12617239166434
Epoch 16: loss 16.125983568039064
Epoch 17: loss 16.12579319436891
Epoch 18: loss 16.125598370477906
Epoch 19: loss 16.125409493957154
Epoch 20: loss 16.125207652340382
Epoch 21: loss 16.125019919813944
Epoch 22: loss 16.124826503876047
Epoch 23: loss 16.124630326742448
Epoch 24: loss 16.12443225781771
Epoch 25: loss 16.12424424888648
Epoch 26: loss 16.124052784376783
Epoch 27: loss 16.123849300925922
Epoch 28: loss 16.123668809634566
Epoch 29: loss 16.123474665087418
Epoch 30: loss 16.123276658911237
Epoch 31: loss 16.12308146319615
Epoch 32: loss 16.122891357633456
Epoch 33: loss 16.122697342991376
Epoch 34: loss 16.122504249612167
Epoch 35: loss 16.12231365295128
Epoch 36: loss 16.122115089298266
Epoch 37: loss 16.121923031010923
Epoch 38: loss 16.121733755440477
Epoch 39: loss 16.12154258600456
Epoch 40: loss 16.121343376197007
Epoch 41: loss 16.121149710302056
Epoch 42: loss 16.120958024357375
Epoch 43: loss 16.120763132272693
Epoch 44: loss 16.120568977872466
Epoch 45: loss 16.12037663203079
Epoch 46: loss 16.120187446175223
Epoch 47: loss 16.11999423689269
Epoch 48: loss 16.119791433563563
Epoch 49: loss 16.119604368453572
-----------Time: 0:14:04.520374, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015989303588867-------------


Epoch 0: loss 16.128183365127455
Epoch 1: loss 16.126254694950585
Epoch 2: loss 16.12431321888271
Epoch 3: loss 16.12238026339441
Epoch 4: loss 16.120446329080522
Epoch 5: loss 16.118509879638605
Epoch 6: loss 16.116575315505543
Epoch 7: loss 16.114640941433183
Epoch 8: loss 16.112709244546064
Epoch 9: loss 16.110779276874286
Epoch 10: loss 16.108840810921997
Epoch 11: loss 16.106898410479747
Epoch 12: loss 16.104979987504887
Epoch 13: loss 16.103040130194145
Epoch 14: loss 16.1011028533529
Epoch 15: loss 16.099169703914523
Epoch 16: loss 16.097246062956536
Epoch 17: loss 16.095314243424514
Epoch 18: loss 16.09337408870637
Epoch 19: loss 16.091451449132343
Epoch 20: loss 16.089518931068888
Epoch 21: loss 16.087576496659445
Epoch 22: loss 16.085654389151678
Epoch 23: loss 16.083709817661287
Epoch 24: loss 16.081784104963823
Epoch 25: loss 16.07985380306524
Epoch 26: loss 16.077918984307875
Epoch 27: loss 16.075987529858356
Epoch 28: loss 16.074054235994588
Epoch 29: loss 16.072128140842093
Epoch 30: loss 16.07020441068781
Epoch 31: loss 16.068269122872035
Epoch 32: loss 16.06633760256246
Epoch 33: loss 16.064402192620364
Epoch 34: loss 16.06246863635355
Epoch 35: loss 16.060548134638346
Epoch 36: loss 16.058617233776282
Epoch 37: loss 16.056688723323006
Epoch 38: loss 16.054753227036827
Epoch 39: loss 16.05283033465354
Epoch 40: loss 16.050895340355794
Epoch 41: loss 16.048969268798828
Epoch 42: loss 16.047038801472237
Epoch 43: loss 16.045104406915843
Epoch 44: loss 16.043177012453395
Epoch 45: loss 16.041253983423463
Epoch 46: loss 16.039328521460927
Epoch 47: loss 16.037392432434263
Epoch 48: loss 16.03546110581548
Epoch 49: loss 16.03353794817694
-----------Time: 0:06:53.671741, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005183696746826-------------


Epoch 0: loss 16.12823663294607
Epoch 1: loss 16.12629926535278
Epoch 2: loss 16.12436902957354
Epoch 3: loss 16.122430872437487
Epoch 4: loss 16.12049619032677
Epoch 5: loss 16.11856202757702
Epoch 6: loss 16.116622303801428
Epoch 7: loss 16.114691721349388
Epoch 8: loss 16.112764647889875
Epoch 9: loss 16.11082700259534
Epoch 10: loss 16.10888890509634
Epoch 11: loss 16.10696242930385
Epoch 12: loss 16.10502398383559
Epoch 13: loss 16.103089712183394
Epoch 14: loss 16.10116181676967
Epoch 15: loss 16.09922943613074
Epoch 16: loss 16.097296135525394
Epoch 17: loss 16.0953647202289
Epoch 18: loss 16.09342909377836
Epoch 19: loss 16.091483846574196
Epoch 20: loss 16.089566869668303
Epoch 21: loss 16.087637115393466
Epoch 22: loss 16.085699182803893
Epoch 23: loss 16.083764286518356
Epoch 24: loss 16.081840673563857
Epoch 25: loss 16.079907757487874
Epoch 26: loss 16.077977851527486
Epoch 27: loss 16.076042820928926
Epoch 28: loss 16.074116152742103
Epoch 29: loss 16.072182908403025
Epoch 30: loss 16.07025273537234
Epoch 31: loss 16.068318309700963
Epoch 32: loss 16.066388406074196
Epoch 33: loss 16.064454490947877
Epoch 34: loss 16.062530750421935
Epoch 35: loss 16.060600531755938
Epoch 36: loss 16.058672105053834
Epoch 37: loss 16.05674395916448
Epoch 38: loss 16.05481591388024
Epoch 39: loss 16.052881611113058
Epoch 40: loss 16.050953133849102
Epoch 41: loss 16.049021528751194
Epoch 42: loss 16.047092088219134
Epoch 43: loss 16.045163533427004
Epoch 44: loss 16.043233962471295
Epoch 45: loss 16.041307646402398
Epoch 46: loss 16.039377560493673
Epoch 47: loss 16.03744643690017
Epoch 48: loss 16.035514082968266
Epoch 49: loss 16.03359350761426
-----------Time: 0:10:03.771158, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.005192756652832-------------


Epoch 0: loss 16.12821393974972
Epoch 1: loss 16.1262785152873
Epoch 2: loss 16.124355638461505
Epoch 3: loss 16.12241184121568
Epoch 4: loss 16.120480043204857
Epoch 5: loss 16.11854018122687
Epoch 6: loss 16.116611662216975
Epoch 7: loss 16.114685784610085
Epoch 8: loss 16.11274553965848
Epoch 9: loss 16.11081331640952
Epoch 10: loss 16.10888207276403
Epoch 11: loss 16.106948805866583
Epoch 12: loss 16.105009973794626
Epoch 13: loss 16.103078522715897
Epoch 14: loss 16.10115087725923
Epoch 15: loss 16.099208065839875
Epoch 16: loss 16.097278094797307
Epoch 17: loss 16.09534784812782
Epoch 18: loss 16.09342150975985
Epoch 19: loss 16.091481537323663
Epoch 20: loss 16.089556826788034
Epoch 21: loss 16.08762349117833
Epoch 22: loss 16.085691669831267
Epoch 23: loss 16.083764527140914
Epoch 24: loss 16.081831489197906
Epoch 25: loss 16.07989998185291
Epoch 26: loss 16.077956333440437
Epoch 27: loss 16.076039492662606
Epoch 28: loss 16.0740940411367
Epoch 29: loss 16.072168368370136
Epoch 30: loss 16.07024257658875
Epoch 31: loss 16.068310248586002
Epoch 32: loss 16.06637186431055
Epoch 33: loss 16.064448764234733
Epoch 34: loss 16.062520387575898
Epoch 35: loss 16.060587775648905
Epoch 36: loss 16.058657488011356
Epoch 37: loss 16.0567354935547
Epoch 38: loss 16.0547989976996
Epoch 39: loss 16.052871252934267
Epoch 40: loss 16.050939941095102
Epoch 41: loss 16.049006036599735
Epoch 42: loss 16.047075859420385
Epoch 43: loss 16.04515394845561
Epoch 44: loss 16.043220708524487
Epoch 45: loss 16.041294677676294
Epoch 46: loss 16.03937162868091
Epoch 47: loss 16.037428000493176
Epoch 48: loss 16.035505387885472
Epoch 49: loss 16.033577359195895
-----------Time: 0:09:20.486328, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.005203723907471-------------


Epoch 0: loss 16.128210325744114
Epoch 1: loss 16.126284588673247
Epoch 2: loss 16.1243431636858
Epoch 3: loss 16.122403848035106
Epoch 4: loss 16.12047762453329
Epoch 5: loss 16.11853369090011
Epoch 6: loss 16.116603187531993
Epoch 7: loss 16.114671480013925
Epoch 8: loss 16.112740536109463
Epoch 9: loss 16.110804534982638
Epoch 10: loss 16.108862577151058
Epoch 11: loss 16.106941176990635
Epoch 12: loss 16.104999511121342
Epoch 13: loss 16.103070374219687
Epoch 14: loss 16.101133126419093
Epoch 15: loss 16.09920547214651
Epoch 16: loss 16.097273594792476
Epoch 17: loss 16.095337694789354
Epoch 18: loss 16.09340795062689
Epoch 19: loss 16.091481562734234
Epoch 20: loss 16.08953946670026
Epoch 21: loss 16.08760606497121
Epoch 22: loss 16.085679285029734
Epoch 23: loss 16.083746853569657
Epoch 24: loss 16.0818170601418
Epoch 25: loss 16.079878780620135
Epoch 26: loss 16.077945920810418
Epoch 27: loss 16.076027521949673
Epoch 28: loss 16.07408852807979
Epoch 29: loss 16.07216322895235
Epoch 30: loss 16.070229164992682
Epoch 31: loss 16.06829807692212
Epoch 32: loss 16.066363533013796
Epoch 33: loss 16.064431967068913
Epoch 34: loss 16.06249927865091
Epoch 35: loss 16.060577857747163
Epoch 36: loss 16.058642623345445
Epoch 37: loss 16.056712184281633
Epoch 38: loss 16.05478004452455
Epoch 39: loss 16.052853660780556
Epoch 40: loss 16.050921748681453
Epoch 41: loss 16.048990176772865
Epoch 42: loss 16.04705734755955
Epoch 43: loss 16.045131732355713
Epoch 44: loss 16.04320067436297
Epoch 45: loss 16.041269069783645
Epoch 46: loss 16.039346090278396
Epoch 47: loss 16.037414821222335
Epoch 48: loss 16.035484204025227
Epoch 49: loss 16.033553584235204
-----------Time: 0:12:51.616298, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.0052008628845215-------------


Epoch 0: loss 16.12822276447829
Epoch 1: loss 16.126284945717707
Epoch 2: loss 16.124354491614984
Epoch 3: loss 16.12242004831178
Epoch 4: loss 16.120486735779025
Epoch 5: loss 16.118545094542426
Epoch 6: loss 16.116611957031466
Epoch 7: loss 16.114675759620937
Epoch 8: loss 16.11274984026811
Epoch 9: loss 16.110813039745437
Epoch 10: loss 16.108876072239006
Epoch 11: loss 16.106941058235105
Epoch 12: loss 16.105011715974545
Epoch 13: loss 16.103073404300726
Epoch 14: loss 16.10114070162169
Epoch 15: loss 16.099216738623596
Epoch 16: loss 16.097282953402345
Epoch 17: loss 16.09534347517585
Epoch 18: loss 16.093412078289056
Epoch 19: loss 16.091478241209494
Epoch 20: loss 16.08954737716683
Epoch 21: loss 16.087616984775494
Epoch 22: loss 16.085679186239652
Epoch 23: loss 16.08375189471595
Epoch 24: loss 16.081828728261495
Epoch 25: loss 16.07989111045186
Epoch 26: loss 16.077951513729126
Epoch 27: loss 16.076018421334894
Epoch 28: loss 16.074093405613112
Epoch 29: loss 16.07215511960916
Epoch 30: loss 16.070230013135337
Epoch 31: loss 16.0682994060506
Epoch 32: loss 16.066358190570725
Epoch 33: loss 16.064435829475823
Epoch 34: loss 16.062502027141328
Epoch 35: loss 16.060566520742775
Epoch 36: loss 16.05862831874928
Epoch 37: loss 16.056702769146206
Epoch 38: loss 16.054773157481723
Epoch 39: loss 16.052838102250465
Epoch 40: loss 16.050908168545877
Epoch 41: loss 16.048977737520104
Epoch 42: loss 16.047045123259487
Epoch 43: loss 16.045118217302313
Epoch 44: loss 16.043185718945022
Epoch 45: loss 16.04125076846755
Epoch 46: loss 16.039316279788327
Epoch 47: loss 16.037383693012615
Epoch 48: loss 16.035457277635054
Epoch 49: loss 16.033524224653135
-----------Time: 0:11:39.898255, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005193710327148-------------


Epoch 0: loss 16.119729516038174
Epoch 1: loss 16.10039392199576
Epoch 2: loss 16.081091480970773
Epoch 3: loss 16.061787032769907
Epoch 4: loss 16.042494726932976
Epoch 5: loss 16.02320656716791
Epoch 6: loss 16.003948686692038
Epoch 7: loss 15.9846978986179
Epoch 8: loss 15.965449217806178
Epoch 9: loss 15.94621812629596
Epoch 10: loss 15.927012282522428
Epoch 11: loss 15.907806111263671
Epoch 12: loss 15.888625250490156
Epoch 13: loss 15.869455742278522
Epoch 14: loss 15.850300383865996
Epoch 15: loss 15.831156332639328
Epoch 16: loss 15.812042431056078
Epoch 17: loss 15.792920743725492
Epoch 18: loss 15.773821413808701
Epoch 19: loss 15.754722455975282
Epoch 20: loss 15.73565469558243
Epoch 21: loss 15.716588579357285
Epoch 22: loss 15.69753472243657
Epoch 23: loss 15.678494903301013
Epoch 24: loss 15.65947041529686
Epoch 25: loss 15.640451635855445
Epoch 26: loss 15.62145430397378
Epoch 27: loss 15.602463754640448
Epoch 28: loss 15.583489454849184
Epoch 29: loss 15.564523552474022
Epoch 30: loss 15.545579575273639
Epoch 31: loss 15.526632710343279
Epoch 32: loss 15.507710836450972
Epoch 33: loss 15.488805044079813
Epoch 34: loss 15.469912855958341
Epoch 35: loss 15.451021772937453
Epoch 36: loss 15.432144687770824
Epoch 37: loss 15.41329003702758
Epoch 38: loss 15.394432854561654
Epoch 39: loss 15.375602772470529
Epoch 40: loss 15.356778071456917
Epoch 41: loss 15.337964632771726
Epoch 42: loss 15.319174246920264
Epoch 43: loss 15.300381053719201
Epoch 44: loss 15.281610097620655
Epoch 45: loss 15.262852497370494
Epoch 46: loss 15.244103262124987
Epoch 47: loss 15.225369310560533
Epoch 48: loss 15.206645741548273
Epoch 49: loss 15.187939414905426
-----------Time: 0:09:37.826101, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.8975813388824463-------------


Epoch 0: loss 16.11957383765158
Epoch 1: loss 16.100247893922027
Epoch 2: loss 16.080913282594583
Epoch 3: loss 16.06160993353059
Epoch 4: loss 16.0423160924284
Epoch 5: loss 16.023040341060405
Epoch 6: loss 16.003760058831883
Epoch 7: loss 15.984500272303835
Epoch 8: loss 15.965261150534351
Epoch 9: loss 15.946030642170825
Epoch 10: loss 15.926813769431266
Epoch 11: loss 15.907613566026797
Epoch 12: loss 15.888411449050697
Epoch 13: loss 15.869243315862144
Epoch 14: loss 15.850068576443514
Epoch 15: loss 15.830916875338282
Epoch 16: loss 15.811778283754466
Epoch 17: loss 15.792654085444523
Epoch 18: loss 15.773526053768322
Epoch 19: loss 15.754422837330505
Epoch 20: loss 15.73533029385142
Epoch 21: loss 15.716240489268964
Epoch 22: loss 15.697161267152488
Epoch 23: loss 15.678082883584882
Epoch 24: loss 15.659005977201229
Epoch 25: loss 15.639955574562524
Epoch 26: loss 15.620893463354644
Epoch 27: loss 15.601841905053508
Epoch 28: loss 15.582777920464189
Epoch 29: loss 15.563722768122895
Epoch 30: loss 15.544669600139823
Epoch 31: loss 15.525613152637243
Epoch 32: loss 15.506554214639337
Epoch 33: loss 15.487488280436862
Epoch 34: loss 15.46840441648308
Epoch 35: loss 15.449310855543956
Epoch 36: loss 15.430194828029299
Epoch 37: loss 15.411057977328941
Epoch 38: loss 15.391914354970496
Epoch 39: loss 15.372734082788796
Epoch 40: loss 15.353529545326088
Epoch 41: loss 15.334300284154912
Epoch 42: loss 15.315005527494263
Epoch 43: loss 15.295671305104142
Epoch 44: loss 15.27628472137866
Epoch 45: loss 15.256826348379944
Epoch 46: loss 15.237294081957073
Epoch 47: loss 15.217690344152404
Epoch 48: loss 15.197992131656898
Epoch 49: loss 15.178205297977268
-----------Time: 0:07:34.216403, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.896230936050415-------------


Epoch 0: loss 16.119586389436872
Epoch 1: loss 16.100267756432537
Epoch 2: loss 16.080931136632756
Epoch 3: loss 16.06162349058922
Epoch 4: loss 16.042316775920924
Epoch 5: loss 16.023042628011865
Epoch 6: loss 16.003764037401407
Epoch 7: loss 15.98449341067158
Epoch 8: loss 15.965232247823993
Epoch 9: loss 15.945966318679154
Epoch 10: loss 15.926720087653985
Epoch 11: loss 15.90747140813872
Epoch 12: loss 15.88821307908847
Epoch 13: loss 15.868950670604281
Epoch 14: loss 15.849667976705065
Epoch 15: loss 15.830368768786398
Epoch 16: loss 15.811031013548925
Epoch 17: loss 15.791653261034304
Epoch 18: loss 15.772213110786343
Epoch 19: loss 15.75269985924991
Epoch 20: loss 15.733082545976913
Epoch 21: loss 15.713332320633889
Epoch 22: loss 15.693440417092154
Epoch 23: loss 15.673376726935647
Epoch 24: loss 15.653072301689344
Epoch 25: loss 15.632504836834922
Epoch 26: loss 15.611628210369565
Epoch 27: loss 15.590365376921048
Epoch 28: loss 15.56872150261419
Epoch 29: loss 15.546590476272026
Epoch 30: loss 15.5239457023604
Epoch 31: loss 15.500688908863742
Epoch 32: loss 15.47678576402524
Epoch 33: loss 15.4521748323425
Epoch 34: loss 15.426804988263676
Epoch 35: loss 15.40059656290466
Epoch 36: loss 15.373519829526552
Epoch 37: loss 15.345524089630691
Epoch 38: loss 15.316562396926424
Epoch 39: loss 15.286567198186546
Epoch 40: loss 15.255519105144788
Epoch 41: loss 15.223379476359515
Epoch 42: loss 15.190117296672113
Epoch 43: loss 15.15569892430059
Epoch 44: loss 15.120085083057596
Epoch 45: loss 15.083265034383636
Epoch 46: loss 15.045245451406009
Epoch 47: loss 15.00596550899462
Epoch 48: loss 14.965422522704065
Epoch 49: loss 14.92360909857654
-----------Time: 0:11:38.814552, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.862053155899048-------------


Epoch 0: loss 16.119577171622314
Epoch 1: loss 16.1002475327289
Epoch 2: loss 16.080914265828135
Epoch 3: loss 16.06161145220119
Epoch 4: loss 16.042307773836935
Epoch 5: loss 16.022999790973675
Epoch 6: loss 16.003713202774687
Epoch 7: loss 15.984405630110658
Epoch 8: loss 15.965088241446466
Epoch 9: loss 15.94574278317566
Epoch 10: loss 15.926352330820272
Epoch 11: loss 15.90689796491315
Epoch 12: loss 15.88735319467392
Epoch 13: loss 15.867650660825463
Epoch 14: loss 15.84776192193708
Epoch 15: loss 15.8276015429992
Epoch 16: loss 15.807097046319527
Epoch 17: loss 15.786128990801863
Epoch 18: loss 15.764601911780236
Epoch 19: loss 15.742407731610578
Epoch 20: loss 15.719378327467702
Epoch 21: loss 15.695411157582104
Epoch 22: loss 15.670349523254941
Epoch 23: loss 15.644089914781365
Epoch 24: loss 15.61649463045785
Epoch 25: loss 15.58744197309244
Epoch 26: loss 15.55682435821359
Epoch 27: loss 15.524556739230464
Epoch 28: loss 15.490576233275238
Epoch 29: loss 15.454771141956655
Epoch 30: loss 15.417102785198633
Epoch 31: loss 15.377527701848223
Epoch 32: loss 15.335975447318164
Epoch 33: loss 15.292424817523468
Epoch 34: loss 15.246866609947782
Epoch 35: loss 15.19928149933268
Epoch 36: loss 15.149658356107015
Epoch 37: loss 15.097966872200232
Epoch 38: loss 15.044188740591824
Epoch 39: loss 14.988308565327497
Epoch 40: loss 14.930354643671846
Epoch 41: loss 14.870348628543525
Epoch 42: loss 14.808264792257187
Epoch 43: loss 14.744120393517615
Epoch 44: loss 14.677934222405472
Epoch 45: loss 14.609729522852355
Epoch 46: loss 14.53949963028759
Epoch 47: loss 14.467270209907253
Epoch 48: loss 14.393095098934205
Epoch 49: loss 14.31693315920848
-----------Time: 0:10:31.642267, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7807912826538086-------------


Epoch 0: loss 16.11957474880208
Epoch 1: loss 16.10024737741326
Epoch 2: loss 16.08090876703226
Epoch 3: loss 16.061587005156806
Epoch 4: loss 16.042287676148987
Epoch 5: loss 16.02297585110356
Epoch 6: loss 16.00361869152374
Epoch 7: loss 15.984211302244384
Epoch 8: loss 15.964737280741407
Epoch 9: loss 15.945116201517957
Epoch 10: loss 15.925301620531108
Epoch 11: loss 15.905144637793935
Epoch 12: loss 15.884530822241546
Epoch 13: loss 15.863238225234728
Epoch 14: loss 15.841066850016077
Epoch 15: loss 15.817768227088186
Epoch 16: loss 15.793085317627252
Epoch 17: loss 15.766782639790256
Epoch 18: loss 15.738638350469643
Epoch 19: loss 15.708434835343727
Epoch 20: loss 15.675995745822227
Epoch 21: loss 15.641186220999838
Epoch 22: loss 15.603873652177638
Epoch 23: loss 15.563980873153545
Epoch 24: loss 15.5214128546121
Epoch 25: loss 15.47615127241955
Epoch 26: loss 15.428101518609202
Epoch 27: loss 15.377279448340676
Epoch 28: loss 15.323655433665158
Epoch 29: loss 15.267161495339423
Epoch 30: loss 15.207887587824745
Epoch 31: loss 15.145734793728884
Epoch 32: loss 15.080781589454125
Epoch 33: loss 15.013015739291566
Epoch 34: loss 14.942465601439318
Epoch 35: loss 14.8691271314678
Epoch 36: loss 14.793045390877404
Epoch 37: loss 14.714245051777576
Epoch 38: loss 14.632700304287551
Epoch 39: loss 14.548460921753742
Epoch 40: loss 14.461578918838708
Epoch 41: loss 14.372127640824269
Epoch 42: loss 14.280073279722545
Epoch 43: loss 14.185523288284974
Epoch 44: loss 14.08842765617267
Epoch 45: loss 13.988860264104497
Epoch 46: loss 13.886856117736004
Epoch 47: loss 13.782461879433596
Epoch 48: loss 13.67570187618967
Epoch 49: loss 13.566607474502888
-----------Time: 0:14:02.331473, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.678509473800659-------------


Epoch 0: loss 16.03355242027544
Epoch 1: loss 15.841793607148611
Epoch 2: loss 15.651332559632245
Epoch 3: loss 15.462121139472435
Epoch 4: loss 15.274166280384488
Epoch 5: loss 15.087396815135596
Epoch 6: loss 14.901594931564622
Epoch 7: loss 14.716576142956733
Epoch 8: loss 14.532187568681145
Epoch 9: loss 14.347881885246455
Epoch 10: loss 14.163156866703169
Epoch 11: loss 13.977188094016196
Epoch 12: loss 13.788695713953844
Epoch 13: loss 13.596585356715972
Epoch 14: loss 13.39902709682977
Epoch 15: loss 13.194234238687322
Epoch 16: loss 12.980305290274027
Epoch 17: loss 12.755335635112122
Epoch 18: loss 12.517541627640178
Epoch 19: loss 12.26500030598477
Epoch 20: loss 11.996517821587318
Epoch 21: loss 11.711002772758032
Epoch 22: loss 11.408164993066254
Epoch 23: loss 11.087788590145474
Epoch 24: loss 10.750543967740182
Epoch 25: loss 10.397233706573354
Epoch 26: loss 10.029082991365119
Epoch 27: loss 9.647622250291949
Epoch 28: loss 9.254295176173631
Epoch 29: loss 8.850912560322415
Epoch 30: loss 8.439542569823729
Epoch 31: loss 8.02211787753808
Epoch 32: loss 7.600845786395962
Epoch 33: loss 7.178058784509755
Epoch 34: loss 6.755948933523073
Epoch 35: loss 6.336798292583198
Epoch 36: loss 5.923343441290593
Epoch 37: loss 5.5178497037270455
Epoch 38: loss 5.122743940923835
Epoch 39: loss 4.740919148682122
Epoch 40: loss 4.374587728122838
Epoch 41: loss 4.026203233305043
Epoch 42: loss 3.698106406887567
Epoch 43: loss 3.392428155176143
Epoch 44: loss 3.1108643474495885
Epoch 45: loss 2.8549158814940006
Epoch 46: loss 2.625383345441626
Epoch 47: loss 2.4224011697413936
Epoch 48: loss 2.245452871524879
Epoch 49: loss 2.093087959121011
-----------Time: 0:07:05.873992, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 1.434079647064209-------------


Epoch 0: loss 16.033491221246933
Epoch 1: loss 15.841686108761861
Epoch 2: loss 15.65091044751116
Epoch 3: loss 15.460480763900533
Epoch 4: loss 15.268118965943396
Epoch 5: loss 15.0683353272646
Epoch 6: loss 14.850624946876348
Epoch 7: loss 14.600944016535948
Epoch 8: loss 14.305695546717018
Epoch 9: loss 13.95559938735713
Epoch 10: loss 13.546584731927055
Epoch 11: loss 13.07887845093819
Epoch 12: loss 12.55547802211021
Epoch 13: loss 11.981392742953526
Epoch 14: loss 11.362597498963746
Epoch 15: loss 10.70628809345488
Epoch 16: loss 10.019134096764818
Epoch 17: loss 9.308392596413352
Epoch 18: loss 8.582784728034156
Epoch 19: loss 7.850628991824508
Epoch 20: loss 7.1213910119438895
Epoch 21: loss 6.405211801954168
Epoch 22: loss 5.712006925823508
Epoch 23: loss 5.052668520262087
Epoch 24: loss 4.437334257341585
Epoch 25: loss 3.8757521703620523
Epoch 26: loss 3.3765368232784096
Epoch 27: loss 2.9458982729924768
Epoch 28: loss 2.586523512707514
Epoch 29: loss 2.296136811124429
Epoch 30: loss 2.067733856891406
Epoch 31: loss 1.8906554504540254
Epoch 32: loss 1.753518862105894
Epoch 33: loss 1.6458401223963663
Epoch 34: loss 1.5597303745667528
Epoch 35: loss 1.4894104370155563
Epoch 36: loss 1.4310320215487105
Epoch 37: loss 1.381840026365667
Epoch 38: loss 1.3400289179385518
Epoch 39: loss 1.3040111740203315
Epoch 40: loss 1.272997863124803
Epoch 41: loss 1.2459718711256915
Epoch 42: loss 1.2224343814494625
Epoch 43: loss 1.2018139393126077
Epoch 44: loss 1.1837044855769139
Epoch 45: loss 1.1678011323233672
Epoch 46: loss 1.1536583184482871
Epoch 47: loss 1.1411856031728997
Epoch 48: loss 1.1300826625470042
Epoch 49: loss 1.1202363101213506
-----------Time: 0:10:21.248868, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.066864013671875-------------


Epoch 0: loss 16.033485928328545
Epoch 1: loss 15.841095265517616
Epoch 2: loss 15.64372721724435
Epoch 3: loss 15.409804750227812
Epoch 4: loss 15.0764209092344
Epoch 5: loss 14.603350656456504
Epoch 6: loss 13.988080921090122
Epoch 7: loss 13.242874060978767
Epoch 8: loss 12.384976259193193
Epoch 9: loss 11.43415174878376
Epoch 10: loss 10.412894207475755
Epoch 11: loss 9.344949975877174
Epoch 12: loss 8.256772287907063
Epoch 13: loss 7.17675283604695
Epoch 14: loss 6.1339009282639525
Epoch 15: loss 5.158172499297299
Epoch 16: loss 4.279064476846546
Epoch 17: loss 3.5229310638039832
Epoch 18: loss 2.9083915082834024
Epoch 19: loss 2.439122686475564
Epoch 20: loss 2.099814644580694
Epoch 21: loss 1.861463096849681
Epoch 22: loss 1.6924201727172743
Epoch 23: loss 1.5686203688626188
Epoch 24: loss 1.474316346055467
Epoch 25: loss 1.400440119964523
Epoch 26: loss 1.3411631066232612
Epoch 27: loss 1.2930274782393405
Epoch 28: loss 1.2534550659159982
Epoch 29: loss 1.2206912432078632
Epoch 30: loss 1.193419042928119
Epoch 31: loss 1.1705526857028647
Epoch 32: loss 1.1513123172596138
Epoch 33: loss 1.1350168681066928
Epoch 34: loss 1.1212086853221295
Epoch 35: loss 1.109381555830285
Epoch 36: loss 1.0992505290120629
Epoch 37: loss 1.0904861122872143
Epoch 38: loss 1.0829445682445773
Epoch 39: loss 1.0763961602288008
Epoch 40: loss 1.070740763748256
Epoch 41: loss 1.0657299565614209
Epoch 42: loss 1.0613735587522994
Epoch 43: loss 1.057544045810663
Epoch 44: loss 1.054230529727853
Epoch 45: loss 1.051167966442435
Epoch 46: loss 1.0485832551013134
Epoch 47: loss 1.0461650824598672
Epoch 48: loss 1.0441319564394875
Epoch 49: loss 1.0422745590432951
-----------Time: 0:09:23.396879, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0303548574447632-------------


Epoch 0: loss 16.033376019308268
Epoch 1: loss 15.837587393646075
Epoch 2: loss 15.587636829654182
Epoch 3: loss 15.151657423420792
Epoch 4: loss 14.473969513984917
Epoch 5: loss 13.582095540561644
Epoch 6: loss 12.513353150457451
Epoch 7: loss 11.307341432234333
Epoch 8: loss 10.007279146618659
Epoch 9: loss 8.659575610396782
Epoch 10: loss 7.31549023052089
Epoch 11: loss 6.028529687443785
Epoch 12: loss 4.853782450394374
Epoch 13: loss 3.8430984721616794
Epoch 14: loss 3.0364217893010315
Epoch 15: loss 2.4476197732733023
Epoch 16: loss 2.0506077644421783
Epoch 17: loss 1.7912361383308464
Epoch 18: loss 1.617112624930454
Epoch 19: loss 1.4936424989036532
Epoch 20: loss 1.4019498962173649
Epoch 21: loss 1.331658837408265
Epoch 22: loss 1.276775203106389
Epoch 23: loss 1.2333642028964946
Epoch 24: loss 1.198771179743342
Epoch 25: loss 1.17073288736556
Epoch 26: loss 1.1479254678872177
Epoch 27: loss 1.1294099330383516
Epoch 28: loss 1.1140462884830353
Epoch 29: loss 1.1013248918042227
Epoch 30: loss 1.09082893412226
Epoch 31: loss 1.0819300212180765
Epoch 32: loss 1.0743919240514372
Epoch 33: loss 1.068062355153388
Epoch 34: loss 1.0627278610537272
Epoch 35: loss 1.0581593708767718
Epoch 36: loss 1.0542281541473
Epoch 37: loss 1.0508218431939502
Epoch 38: loss 1.047899693542359
Epoch 39: loss 1.0453691529216425
Epoch 40: loss 1.043115998478024
Epoch 41: loss 1.0412329423356277
Epoch 42: loss 1.039515436813843
Epoch 43: loss 1.0381032682878288
Epoch 44: loss 1.0367193458811494
Epoch 45: loss 1.0355121462548666
Epoch 46: loss 1.0345515175122422
Epoch 47: loss 1.0337087124699544
Epoch 48: loss 1.0328367536582657
Epoch 49: loss 1.0321002649249689
-----------Time: 0:12:42.052655, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0252715349197388-------------


Epoch 0: loss 16.033312882853306
Epoch 1: loss 15.825909621823152
Epoch 2: loss 15.452363686045594
Epoch 3: loss 14.74269792708189
Epoch 4: loss 13.729247240478283
Epoch 5: loss 12.477161952242247
Epoch 6: loss 11.048804277696968
Epoch 7: loss 9.511405146984123
Epoch 8: loss 7.940663278718692
Epoch 9: loss 6.417387378935324
Epoch 10: loss 5.025795532867531
Epoch 11: loss 3.8469959482542517
Epoch 12: loss 2.940403025587205
Epoch 13: loss 2.318087794457913
Epoch 14: loss 1.9277337501721903
Epoch 15: loss 1.685655115932923
Epoch 16: loss 1.526444472976195
Epoch 17: loss 1.4146589831595449
Epoch 18: loss 1.3325655770859295
Epoch 19: loss 1.2707794445924878
Epoch 20: loss 1.2233508671185709
Epoch 21: loss 1.1864894940484925
Epoch 22: loss 1.157664680591156
Epoch 23: loss 1.1348786339317734
Epoch 24: loss 1.1165998602380176
Epoch 25: loss 1.101940300183947
Epoch 26: loss 1.0899417661920456
Epoch 27: loss 1.0802729402047386
Epoch 28: loss 1.0722858973045981
Epoch 29: loss 1.0655801263174458
Epoch 30: loss 1.0600317539469972
Epoch 31: loss 1.055488891158174
Epoch 32: loss 1.0515401081879414
Epoch 33: loss 1.0482857617397163
Epoch 34: loss 1.0454161339411598
Epoch 35: loss 1.0430759769247044
Epoch 36: loss 1.0410115025398716
Epoch 37: loss 1.0392400133927404
Epoch 38: loss 1.0376943748499012
Epoch 39: loss 1.0364404430753709
Epoch 40: loss 1.0352321420110004
Epoch 41: loss 1.0342846348306418
Epoch 42: loss 1.0334246460836565
Epoch 43: loss 1.0325957008714972
Epoch 44: loss 1.0319003471866648
Epoch 45: loss 1.0314344377087277
Epoch 46: loss 1.0308641655701798
Epoch 47: loss 1.0303852607563437
Epoch 48: loss 1.0299403067676187
Epoch 49: loss 1.0296522267426143
-----------Time: 0:11:46.512937, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.023856520652771-------------


Epoch 0: loss 15.194921628102588
Epoch 1: loss 13.251551608925219
Epoch 2: loss 10.015709303461254
Epoch 3: loss 5.631046280062283
Epoch 4: loss 2.5660620069879756
Epoch 5: loss 1.5090112657311041
Epoch 6: loss 1.2361504519484885
Epoch 7: loss 1.1324087884712635
Epoch 8: loss 1.084364053054372
Epoch 9: loss 1.0598572088590843
Epoch 10: loss 1.0466400785184282
Epoch 11: loss 1.0387211311375595
Epoch 12: loss 1.034149552052795
Epoch 13: loss 1.0309916738908922
Epoch 14: loss 1.0291570427950598
Epoch 15: loss 1.0276937217832973
Epoch 16: loss 1.0268147698299206
Epoch 17: loss 1.026185645575495
Epoch 18: loss 1.0255166454895999
Epoch 19: loss 1.0251987674853411
Epoch 20: loss 1.0248441200433962
Epoch 21: loss 1.0244025844410103
Epoch 22: loss 1.0243787613331201
Epoch 23: loss 1.0238419711881517
Epoch 24: loss 1.0237992329238355
Epoch 25: loss 1.0237182002796175
Epoch 26: loss 1.0234500569549185
Epoch 27: loss 1.023237346905998
Epoch 28: loss 1.023087296653144
Epoch 29: loss 1.0228083901200493
Epoch 30: loss 1.0228836123971852
Epoch 31: loss 1.022718324114798
Epoch 32: loss 1.022459998897005
Epoch 33: loss 1.0221513780061806
Epoch 34: loss 1.0223409463816067
Epoch 35: loss 1.021996687261095
Epoch 36: loss 1.021906067133598
Epoch 37: loss 1.0217879271461887
Epoch 38: loss 1.0216138180732208
Epoch 39: loss 1.021468919635532
Epoch 40: loss 1.0214543878837667
Epoch 41: loss 1.0212338386572464
Epoch 42: loss 1.0211763593442678
Epoch 43: loss 1.0209924289880725
Epoch 44: loss 1.0209424558575222
Epoch 45: loss 1.020610087928466
Epoch 46: loss 1.0205906019092839
Epoch 47: loss 1.0205386613068728
Epoch 48: loss 1.0204102985556842
Epoch 49: loss 1.020270060705322
-----------Time: 0:09:25.862230, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 1.0176078081130981-------------


Epoch 0: loss 15.070919197107411
Epoch 1: loss 9.503284599548971
Epoch 2: loss 3.1147550781697535
Epoch 3: loss 1.4245024941711726
Epoch 4: loss 1.1590979631955758
Epoch 5: loss 1.0831027019438761
Epoch 6: loss 1.05534357477492
Epoch 7: loss 1.0436025988867648
Epoch 8: loss 1.0377921367579663
Epoch 9: loss 1.0356134505392482
Epoch 10: loss 1.0336782801015147
Epoch 11: loss 1.0326538327888926
Epoch 12: loss 1.031948015409815
Epoch 13: loss 1.0315295957300312
Epoch 14: loss 1.0312325964808917
Epoch 15: loss 1.0307531905375207
Epoch 16: loss 1.03064500736179
Epoch 17: loss 1.0302351480466376
Epoch 18: loss 1.0301244541346346
Epoch 19: loss 1.0297668377234277
Epoch 20: loss 1.0296197889485135
Epoch 21: loss 1.0292630231756434
Epoch 22: loss 1.0293642415210824
Epoch 23: loss 1.0288540955922603
Epoch 24: loss 1.0289831918468288
Epoch 25: loss 1.0284135659438491
Epoch 26: loss 1.0283466092168538
Epoch 27: loss 1.0279998886416697
Epoch 28: loss 1.0279083677839493
Epoch 29: loss 1.0276653394872821
Epoch 30: loss 1.027645252073764
Epoch 31: loss 1.0273304217499581
Epoch 32: loss 1.0272634533548433
Epoch 33: loss 1.0269739738037043
Epoch 34: loss 1.0267461764546695
Epoch 35: loss 1.0265331864356995
Epoch 36: loss 1.0264009549249573
Epoch 37: loss 1.02627013585631
Epoch 38: loss 1.0258525620827927
Epoch 39: loss 1.025832806384194
Epoch 40: loss 1.0254786688390798
Epoch 41: loss 1.0255741929152273
Epoch 42: loss 1.0251885903406688
Epoch 43: loss 1.025087061688069
Epoch 44: loss 1.0248499822039654
Epoch 45: loss 1.024653569552612
Epoch 46: loss 1.0243022686894527
Epoch 47: loss 1.0242655321655876
Epoch 48: loss 1.023917347500673
Epoch 49: loss 1.0237364503532211
-----------Time: 0:07:55.737529, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 1.017886757850647-------------


Epoch 0: loss 13.552745614770057
Epoch 1: loss 3.8596229967116793
Epoch 2: loss 1.3464956305285003
Epoch 3: loss 1.1122495775607826
Epoch 4: loss 1.062182841821492
Epoch 5: loss 1.0476538952178707
Epoch 6: loss 1.0420413560812858
Epoch 7: loss 1.039970652897634
Epoch 8: loss 1.0386011864485594
Epoch 9: loss 1.0378255954639233
Epoch 10: loss 1.037914781676744
Epoch 11: loss 1.0369596373231078
Epoch 12: loss 1.037057190436653
Epoch 13: loss 1.0366612011774394
Epoch 14: loss 1.0362615070795482
Epoch 15: loss 1.0359307325294707
Epoch 16: loss 1.0357053841080854
Epoch 17: loss 1.0354774628825134
Epoch 18: loss 1.035153070842526
Epoch 19: loss 1.0348969409976594
Epoch 20: loss 1.0346434380840905
Epoch 21: loss 1.034371503301131
Epoch 22: loss 1.034139978717111
Epoch 23: loss 1.0339852284484095
Epoch 24: loss 1.0338057187376235
Epoch 25: loss 1.0333430493992914
Epoch 26: loss 1.0332812680181698
Epoch 27: loss 1.0328874799011194
Epoch 28: loss 1.0326639029758011
Epoch 29: loss 1.0325151749315826
Epoch 30: loss 1.0324646087850546
Epoch 31: loss 1.032102641526482
Epoch 32: loss 1.0319880017156897
Epoch 33: loss 1.0314981867723843
Epoch 34: loss 1.0313835479663467
Epoch 35: loss 1.0315370346263026
Epoch 36: loss 1.030872428738079
Epoch 37: loss 1.0306732646792787
Epoch 38: loss 1.03054525318711
Epoch 39: loss 1.030402418450858
Epoch 40: loss 1.0302900184652868
Epoch 41: loss 1.029875851636869
Epoch 42: loss 1.0298426577648174
Epoch 43: loss 1.0296639127696319
Epoch 44: loss 1.0294292845143904
Epoch 45: loss 1.0290867558894694
Epoch 46: loss 1.0290751598204135
Epoch 47: loss 1.0285812611324492
Epoch 48: loss 1.0284907230059046
Epoch 49: loss 1.0284716714945352
-----------Time: 0:11:28.546402, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 1.0188755989074707-------------


Epoch 0: loss 11.870267717134311
Epoch 1: loss 2.2117503796536484
Epoch 2: loss 1.1713484381449617
Epoch 3: loss 1.0725146301289237
Epoch 4: loss 1.0517328866575644
Epoch 5: loss 1.045452766310851
Epoch 6: loss 1.0435951229766247
Epoch 7: loss 1.0424830417162443
Epoch 8: loss 1.0417427098284344
Epoch 9: loss 1.0416484232049976
Epoch 10: loss 1.0412161311194454
Epoch 11: loss 1.0410544608940957
Epoch 12: loss 1.0402318151377024
Epoch 13: loss 1.0402910617656718
Epoch 14: loss 1.0399884041591207
Epoch 15: loss 1.0395323044907598
Epoch 16: loss 1.0391123044613977
Epoch 17: loss 1.0389237922776842
Epoch 18: loss 1.0387314824451241
Epoch 19: loss 1.0386554666548207
Epoch 20: loss 1.0380245394652274
Epoch 21: loss 1.0380003932962734
Epoch 22: loss 1.0376463350133445
Epoch 23: loss 1.0374140424563743
Epoch 24: loss 1.0370233592583862
Epoch 25: loss 1.0369682333889443
Epoch 26: loss 1.03665003486792
Epoch 27: loss 1.0363086865705404
Epoch 28: loss 1.036015073442148
Epoch 29: loss 1.0359194353587475
Epoch 30: loss 1.0356319435690329
Epoch 31: loss 1.035340403476573
Epoch 32: loss 1.035108815576798
Epoch 33: loss 1.0349162899650524
Epoch 34: loss 1.0347000656891543
Epoch 35: loss 1.034433833278347
Epoch 36: loss 1.0342377539948966
Epoch 37: loss 1.0340365139831338
Epoch 38: loss 1.0335054814718287
Epoch 39: loss 1.0336321842709335
Epoch 40: loss 1.033357198485348
Epoch 41: loss 1.033126475744237
Epoch 42: loss 1.0329616414003233
Epoch 43: loss 1.032570039111029
Epoch 44: loss 1.0327064690057062
Epoch 45: loss 1.0322817365652586
Epoch 46: loss 1.0320122461561407
Epoch 47: loss 1.0319077779638697
Epoch 48: loss 1.0314122540526833
Epoch 49: loss 1.031484420854414
-----------Time: 0:10:32.247517, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 1.0196497440338135-------------


Epoch 0: loss 10.522743702194106
Epoch 1: loss 1.658786968976794
Epoch 2: loss 1.1163806091870478
Epoch 3: loss 1.0612438103956914
Epoch 4: loss 1.0502675511449364
Epoch 5: loss 1.046823221114487
Epoch 6: loss 1.0460468849421456
Epoch 7: loss 1.0449889322834989
Epoch 8: loss 1.044897639764399
Epoch 9: loss 1.04450199076052
Epoch 10: loss 1.0439363855619415
Epoch 11: loss 1.0437653197290588
Epoch 12: loss 1.0435952430772288
Epoch 13: loss 1.0428521908807522
Epoch 14: loss 1.0427612955800991
Epoch 15: loss 1.0424902137042777
Epoch 16: loss 1.0420785873820433
Epoch 17: loss 1.0417418533236258
Epoch 18: loss 1.0417711446633995
Epoch 19: loss 1.0410647415933303
Epoch 20: loss 1.0409680374489327
Epoch 21: loss 1.0404642323490847
Epoch 22: loss 1.040232153723848
Epoch 23: loss 1.0401300332058765
Epoch 24: loss 1.0396962986166156
Epoch 25: loss 1.039243865719692
Epoch 26: loss 1.0391797115454018
Epoch 27: loss 1.0389074727268954
Epoch 28: loss 1.0385119822473874
Epoch 29: loss 1.0382004447966053
Epoch 30: loss 1.0380741696989102
Epoch 31: loss 1.0381256217474262
Epoch 32: loss 1.037579338404068
Epoch 33: loss 1.0372687821773292
Epoch 34: loss 1.0371897440037046
Epoch 35: loss 1.0368357324418715
Epoch 36: loss 1.0366464428534774
Epoch 37: loss 1.0361466494152376
Epoch 38: loss 1.036359863575546
Epoch 39: loss 1.0358460539057568
Epoch 40: loss 1.0357547786943675
Epoch 41: loss 1.0356734916527288
Epoch 42: loss 1.0351267211977848
Epoch 43: loss 1.0346595262333775
Epoch 44: loss 1.034570673663957
Epoch 45: loss 1.0342701683717295
Epoch 46: loss 1.0340345576283982
Epoch 47: loss 1.0335316559327175
Epoch 48: loss 1.0334526993224904
Epoch 49: loss 1.033081238590031
-----------Time: 0:13:45.301509, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 1.0192781686782837-------------


Epoch 0: loss 5.04854138870483
Epoch 1: loss 1.0758772085483077
Epoch 2: loss 1.0669963713765729
Epoch 3: loss 1.0625423153922116
Epoch 4: loss 1.0585595174222098
Epoch 5: loss 1.0541303647867424
Epoch 6: loss 1.0491310164551166
Epoch 7: loss 1.04570063930805
Epoch 8: loss 1.041726653280305
Epoch 9: loss 1.038438754262582
Epoch 10: loss 1.0354338136135461
Epoch 11: loss 1.032870998276907
Epoch 12: loss 1.0304751456464485
Epoch 13: loss 1.0275689407559176
Epoch 14: loss 1.0259845222125175
Epoch 15: loss 1.023697800889749
Epoch 16: loss 1.0219647624915291
Epoch 17: loss 1.020665337528594
Epoch 18: loss 1.0189984064992839
Epoch 19: loss 1.017531185321279
Epoch 20: loss 1.016282578131114
Epoch 21: loss 1.0150248750582411
Epoch 22: loss 1.013862447303686
Epoch 23: loss 1.0130126691823942
Epoch 24: loss 1.0121581322833078
Epoch 25: loss 1.0113505726440113
Epoch 26: loss 1.0106134434443832
Epoch 27: loss 1.0097900997399636
Epoch 28: loss 1.0096915188595417
Epoch 29: loss 1.0085925371540572
Epoch 30: loss 1.0078692481432225
Epoch 31: loss 1.0074802130671414
Epoch 32: loss 1.0067290820590564
Epoch 33: loss 1.0067295974172152
Epoch 34: loss 1.0062881474944805
Epoch 35: loss 1.0058136715747663
Epoch 36: loss 1.0059797091935236
Epoch 37: loss 1.0053625424767785
Epoch 38: loss 1.005041326223475
Epoch 39: loss 1.0046625272484822
Epoch 40: loss 1.0041985271286873
Epoch 41: loss 1.0039842635506582
Epoch 42: loss 1.0035342840541635
Epoch 43: loss 1.0032544230688778
Epoch 44: loss 1.002838351403714
Epoch 45: loss 1.002349922308914
Epoch 46: loss 1.0021066957838578
Epoch 47: loss 1.002030912902188
Epoch 48: loss 1.0018230285606935
Epoch 49: loss 1.0012460113738528
-----------Time: 0:07:09.118956, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0087560415267944-------------


Epoch 0: loss 3.443832119500397
Epoch 1: loss 1.1015255325445992
Epoch 2: loss 1.0951834218601872
Epoch 3: loss 1.0881089745914891
Epoch 4: loss 1.0813432411210182
Epoch 5: loss 1.0751464292204465
Epoch 6: loss 1.069111155744089
Epoch 7: loss 1.063979953965005
Epoch 8: loss 1.0592498311113572
Epoch 9: loss 1.0548752916611168
Epoch 10: loss 1.0506990063994264
Epoch 11: loss 1.0460801236716348
Epoch 12: loss 1.042559319164132
Epoch 13: loss 1.0384854931848213
Epoch 14: loss 1.0354168082690225
Epoch 15: loss 1.032080522741942
Epoch 16: loss 1.0292745704816826
Epoch 17: loss 1.0259036135906885
Epoch 18: loss 1.023301374983567
Epoch 19: loss 1.0205313067127662
Epoch 20: loss 1.018258174860069
Epoch 21: loss 1.0162764995755418
Epoch 22: loss 1.0146812623677661
Epoch 23: loss 1.0133913843089826
Epoch 24: loss 1.0120902088039787
Epoch 25: loss 1.0114278117880475
Epoch 26: loss 1.0102807999694392
Epoch 27: loss 1.0100411712118567
Epoch 28: loss 1.0091989685362484
Epoch 29: loss 1.0087347549708918
Epoch 30: loss 1.0082939186616071
Epoch 31: loss 1.008296498693546
Epoch 32: loss 1.0077104046657204
Epoch 33: loss 1.007337999091063
Epoch 34: loss 1.0069143845050992
Epoch 35: loss 1.0065982921445027
Epoch 36: loss 1.006276118943327
Epoch 37: loss 1.0057127431658963
Epoch 38: loss 1.0052010458633263
Epoch 39: loss 1.0045505625972158
Epoch 40: loss 1.004389169878646
Epoch 41: loss 1.0039139329544418
Epoch 42: loss 1.0034123515227102
Epoch 43: loss 1.0034299305011682
Epoch 44: loss 1.0030092926632133
Epoch 45: loss 1.002837268035006
Epoch 46: loss 1.0023974358160384
Epoch 47: loss 1.0023714003030084
Epoch 48: loss 1.0020441089459513
Epoch 49: loss 1.0021787614942959
-----------Time: 0:09:47.850050, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.0085619688034058-------------


Epoch 0: loss 2.8171582371077504
Epoch 1: loss 1.133134538158896
Epoch 2: loss 1.126758986708261
Epoch 3: loss 1.1171749541135896
Epoch 4: loss 1.1086336536340056
Epoch 5: loss 1.0993821109650639
Epoch 6: loss 1.090817680982483
Epoch 7: loss 1.082044859127612
Epoch 8: loss 1.0745890715026285
Epoch 9: loss 1.0670263419143526
Epoch 10: loss 1.0604029045667642
Epoch 11: loss 1.0545214871922026
Epoch 12: loss 1.0487242730951232
Epoch 13: loss 1.0433086705894947
Epoch 14: loss 1.0386745820815566
Epoch 15: loss 1.0350345422743195
Epoch 16: loss 1.0307595524015214
Epoch 17: loss 1.0270512383096952
Epoch 18: loss 1.024545227884922
Epoch 19: loss 1.0215439588366286
Epoch 20: loss 1.0194138676462905
Epoch 21: loss 1.017788947293005
Epoch 22: loss 1.016131815977107
Epoch 23: loss 1.0147077571671577
Epoch 24: loss 1.013770818839973
Epoch 25: loss 1.0123392880092568
Epoch 26: loss 1.011580142591102
Epoch 27: loss 1.0104598156650018
Epoch 28: loss 1.0098232050963625
Epoch 29: loss 1.0091304649731074
Epoch 30: loss 1.0084875689342399
Epoch 31: loss 1.0080330803577118
Epoch 32: loss 1.007330799974651
Epoch 33: loss 1.0069473062992873
Epoch 34: loss 1.006251479488407
Epoch 35: loss 1.0060170870588292
Epoch 36: loss 1.0053697469248728
Epoch 37: loss 1.0052250869381745
Epoch 38: loss 1.0049353704755886
Epoch 39: loss 1.0043349927731866
Epoch 40: loss 1.0043651390296082
Epoch 41: loss 1.0039495625626853
Epoch 42: loss 1.003585757607672
Epoch 43: loss 1.0033900167158987
Epoch 44: loss 1.003367809723615
Epoch 45: loss 1.0031118026046795
Epoch 46: loss 1.0029433665553016
Epoch 47: loss 1.00254691674568
Epoch 48: loss 1.0026710124492904
Epoch 49: loss 1.0024885741214897
-----------Time: 0:09:11.977602, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.0088293552398682-------------


Epoch 0: loss 2.618812040880104
Epoch 1: loss 1.1567567952661946
Epoch 2: loss 1.1453296729829923
Epoch 3: loss 1.1326548784987442
Epoch 4: loss 1.1202062412148392
Epoch 5: loss 1.1090720593119006
Epoch 6: loss 1.0977717564765885
Epoch 7: loss 1.0871300156865837
Epoch 8: loss 1.078250791729631
Epoch 9: loss 1.069822856302557
Epoch 10: loss 1.0630854897326656
Epoch 11: loss 1.0558279134645094
Epoch 12: loss 1.0496044412879209
Epoch 13: loss 1.0440744106668438
Epoch 14: loss 1.0388898906207331
Epoch 15: loss 1.0347702468848214
Epoch 16: loss 1.030995313598774
Epoch 17: loss 1.0277602463198978
Epoch 18: loss 1.0252856743958023
Epoch 19: loss 1.0226611310596891
Epoch 20: loss 1.0205199100875284
Epoch 21: loss 1.0187611457217705
Epoch 22: loss 1.0167178850157097
Epoch 23: loss 1.0158365567849341
Epoch 24: loss 1.0141815718520653
Epoch 25: loss 1.0128691189635248
Epoch 26: loss 1.0124581968447253
Epoch 27: loss 1.0112442719897476
Epoch 28: loss 1.010564672972859
Epoch 29: loss 1.009926205730879
Epoch 30: loss 1.0093992539661742
Epoch 31: loss 1.0083629270815992
Epoch 32: loss 1.0075994400047232
Epoch 33: loss 1.0070128553324642
Epoch 34: loss 1.0067021720366611
Epoch 35: loss 1.00569092475441
Epoch 36: loss 1.0054408466608515
Epoch 37: loss 1.0048319568220463
Epoch 38: loss 1.004556381407479
Epoch 39: loss 1.004186546335796
Epoch 40: loss 1.0037246432039906
Epoch 41: loss 1.0035056057930511
Epoch 42: loss 1.003236200042624
Epoch 43: loss 1.002825086015667
Epoch 44: loss 1.0024483085910285
Epoch 45: loss 1.0023111314019022
Epoch 46: loss 1.0021468803667386
Epoch 47: loss 1.0018353584248323
Epoch 48: loss 1.0018866193754767
Epoch 49: loss 1.001809076520008
-----------Time: 0:12:46.180575, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.0085859298706055-------------


Epoch 0: loss 2.511346272860875
Epoch 1: loss 1.1716804436654613
Epoch 2: loss 1.154056372721601
Epoch 3: loss 1.1378822204469015
Epoch 4: loss 1.124314799394343
Epoch 5: loss 1.1107221231018478
Epoch 6: loss 1.100057491243631
Epoch 7: loss 1.0899805115123622
Epoch 8: loss 1.0798431312571148
Epoch 9: loss 1.0714679279036987
Epoch 10: loss 1.062813630160461
Epoch 11: loss 1.0549698480936158
Epoch 12: loss 1.0486592528384429
Epoch 13: loss 1.0420946554686468
Epoch 14: loss 1.0364450367330662
Epoch 15: loss 1.0312457884894304
Epoch 16: loss 1.027208744004863
Epoch 17: loss 1.0236899260433554
Epoch 18: loss 1.0206788323896614
Epoch 19: loss 1.0188663971689877
Epoch 20: loss 1.0165616350046898
Epoch 21: loss 1.0151145350939557
Epoch 22: loss 1.0134745990212293
Epoch 23: loss 1.0125437868458218
Epoch 24: loss 1.0115157922303437
Epoch 25: loss 1.0108112875147317
Epoch 26: loss 1.0100422852742017
Epoch 27: loss 1.009443844252789
Epoch 28: loss 1.0086761864875566
Epoch 29: loss 1.0084175351134068
Epoch 30: loss 1.0078415849112894
Epoch 31: loss 1.0076913647576478
Epoch 32: loss 1.00725363088795
Epoch 33: loss 1.0072196271997744
Epoch 34: loss 1.0067317508011164
Epoch 35: loss 1.0061979578557774
Epoch 36: loss 1.0061270522319343
Epoch 37: loss 1.0058297610043054
Epoch 38: loss 1.005510621125313
Epoch 39: loss 1.0050661960751417
Epoch 40: loss 1.004941051634451
Epoch 41: loss 1.004711683950974
Epoch 42: loss 1.0043054732406445
Epoch 43: loss 1.0041262602566248
Epoch 44: loss 1.00380708765828
Epoch 45: loss 1.0034143033884608
Epoch 46: loss 1.0031591345753081
Epoch 47: loss 1.0030325477605542
Epoch 48: loss 1.0028506830342767
Epoch 49: loss 1.0025302405069547
-----------Time: 0:11:42.433506, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.0089973211288452-------------


Epoch 0: loss 16.1293846888021
Epoch 1: loss 16.129380223542338
Epoch 2: loss 16.129391847841763
Epoch 3: loss 16.12938515423043
Epoch 4: loss 16.12939082934456
Epoch 5: loss 16.12937172059458
Epoch 6: loss 16.129377497869577
Epoch 7: loss 16.12938178732967
Epoch 8: loss 16.129372650673364
Epoch 9: loss 16.129376706771062
Epoch 10: loss 16.129370601492255
Epoch 11: loss 16.12936584608525
Epoch 12: loss 16.129360721965664
Epoch 13: loss 16.12936515999981
Epoch 14: loss 16.129364821365048
Epoch 15: loss 16.129357908393068
Epoch 16: loss 16.129361751353112
Epoch 17: loss 16.12935531029175
Epoch 18: loss 16.129364025599287
Epoch 19: loss 16.12935563362831
Epoch 20: loss 16.12935081054621
Epoch 21: loss 16.129351660503907
Epoch 22: loss 16.129343604056196
Epoch 23: loss 16.12934305746961
Epoch 24: loss 16.129342331453273
Epoch 25: loss 16.12933878304843
Epoch 26: loss 16.129347188502567
Epoch 27: loss 16.129335651843686
Epoch 28: loss 16.12934272557643
Epoch 29: loss 16.129332276386304
Epoch 30: loss 16.129330880619897
Epoch 31: loss 16.12932796099706
Epoch 32: loss 16.129328057194222
Epoch 33: loss 16.129324581909593
Epoch 34: loss 16.12931930558587
Epoch 35: loss 16.12932522858272
Epoch 36: loss 16.129316911028415
Epoch 37: loss 16.129313180600903
Epoch 38: loss 16.129313948881762
Epoch 39: loss 16.129322206021133
Epoch 40: loss 16.12931754473696
Epoch 41: loss 16.129309930122048
Epoch 42: loss 16.12931476357581
Epoch 43: loss 16.129309973164446
Epoch 44: loss 16.129306156133556
Epoch 45: loss 16.129303438239543
Epoch 46: loss 16.129299612133448
Epoch 47: loss 16.129298116539793
Epoch 48: loss 16.129296653357585
Epoch 49: loss 16.12929269008626
-----------Time: 0:09:38.801651, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129199946166512
Epoch 1: loss 16.129190995422245
Epoch 2: loss 16.129186988330652
Epoch 3: loss 16.129185278043597
Epoch 4: loss 16.129190158169838
Epoch 5: loss 16.129188001382733
Epoch 6: loss 16.129188726361903
Epoch 7: loss 16.129180313906897
Epoch 8: loss 16.129182668014867
Epoch 9: loss 16.129182694203312
Epoch 10: loss 16.12917104449332
Epoch 11: loss 16.129179036636724
Epoch 12: loss 16.129168987792752
Epoch 13: loss 16.129171565928623
Epoch 14: loss 16.129163252004403
Epoch 15: loss 16.129172841643044
Epoch 16: loss 16.129161812158433
Epoch 17: loss 16.129169334206264
Epoch 18: loss 16.129158832120662
Epoch 19: loss 16.12915764638041
Epoch 20: loss 16.129158511117726
Epoch 21: loss 16.12916078354886
Epoch 22: loss 16.129156063664794
Epoch 23: loss 16.129157099793826
Epoch 24: loss 16.129151991750312
Epoch 25: loss 16.129148481461325
Epoch 26: loss 16.129144751293104
Epoch 27: loss 16.129141786553536
Epoch 28: loss 16.129148865990693
Epoch 29: loss 16.12914153400357
Epoch 30: loss 16.129142251463286
Epoch 31: loss 16.12914629407782
Epoch 32: loss 16.129135608932387
Epoch 33: loss 16.129129933558968
Epoch 34: loss 16.129130542375524
Epoch 35: loss 16.129132467355983
Epoch 36: loss 16.12913114107971
Epoch 37: loss 16.129118677972127
Epoch 38: loss 16.129125678066067
Epoch 39: loss 16.129124301227947
Epoch 40: loss 16.12911745515318
Epoch 41: loss 16.129116156102523
Epoch 42: loss 16.129117323173784
Epoch 43: loss 16.129115322739487
Epoch 44: loss 16.12911216949496
Epoch 45: loss 16.12911309698083
Epoch 46: loss 16.129105352720142
Epoch 47: loss 16.129107477095804
Epoch 48: loss 16.12911085177531
Epoch 49: loss 16.129092706812017
-----------Time: 0:07:46.888534, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017177581787109-------------


Epoch 0: loss 16.12917866947989
Epoch 1: loss 16.129177511743126
Epoch 2: loss 16.129173854695125
Epoch 3: loss 16.129165956155965
Epoch 4: loss 16.12916874794807
Epoch 5: loss 16.129169895831758
Epoch 6: loss 16.12916315943732
Epoch 7: loss 16.1291697809656
Epoch 8: loss 16.1291618253823
Epoch 9: loss 16.12916698320979
Epoch 10: loss 16.129156410078306
Epoch 11: loss 16.12915304058463
Epoch 12: loss 16.129150536087558
Epoch 13: loss 16.12915455614373
Epoch 14: loss 16.129157396941938
Epoch 15: loss 16.129146817328166
Epoch 16: loss 16.129147258642384
Epoch 17: loss 16.129139809196186
Epoch 18: loss 16.129142529423824
Epoch 19: loss 16.12914221231026
Epoch 20: loss 16.129136611612807
Epoch 21: loss 16.129141172032565
Epoch 22: loss 16.12913163165932
Epoch 23: loss 16.129130575564844
Epoch 24: loss 16.129132852922517
Epoch 25: loss 16.12912642301069
Epoch 26: loss 16.12912894410242
Epoch 27: loss 16.129123513500218
Epoch 28: loss 16.129124444875462
Epoch 29: loss 16.129123317216518
Epoch 30: loss 16.12911648540279
Epoch 31: loss 16.12911899327065
Epoch 32: loss 16.12911317554617
Epoch 33: loss 16.129111634057914
Epoch 34: loss 16.129118914186726
Epoch 35: loss 16.12911319395587
Epoch 36: loss 16.129108577269843
Epoch 37: loss 16.129101609587345
Epoch 38: loss 16.12910582774226
Epoch 39: loss 16.12910233352935
Epoch 40: loss 16.12910137415062
Epoch 41: loss 16.129101089707788
Epoch 42: loss 16.129096739573477
Epoch 43: loss 16.129097737586648
Epoch 44: loss 16.129089978805634
Epoch 45: loss 16.129089720032667
Epoch 46: loss 16.129087565060605
Epoch 47: loss 16.12908585840363
Epoch 48: loss 16.129089730663623
Epoch 49: loss 16.129077010598113
-----------Time: 0:11:36.248749, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017177104949951-------------


Epoch 0: loss 16.12917605737683
Epoch 1: loss 16.129171105945414
Epoch 2: loss 16.129167099890985
Epoch 3: loss 16.12916129746471
Epoch 4: loss 16.12916974907274
Epoch 5: loss 16.129162982600484
Epoch 6: loss 16.129161219677243
Epoch 7: loss 16.129162052003114
Epoch 8: loss 16.129157080087666
Epoch 9: loss 16.12915815225822
Epoch 10: loss 16.12915751725322
Epoch 11: loss 16.129145874284802
Epoch 12: loss 16.12914838448629
Epoch 13: loss 16.129148206093703
Epoch 14: loss 16.12914794291278
Epoch 15: loss 16.129148874028733
Epoch 16: loss 16.12914443210521
Epoch 17: loss 16.129131933993268
Epoch 18: loss 16.1291368849061
Epoch 19: loss 16.12913536545762
Epoch 20: loss 16.129128195787008
Epoch 21: loss 16.129130130879837
Epoch 22: loss 16.129127750064836
Epoch 23: loss 16.129126646779294
Epoch 24: loss 16.129127583858953
Epoch 25: loss 16.129126229838484
Epoch 26: loss 16.12912676138616
Epoch 27: loss 16.129121614708207
Epoch 28: loss 16.1291173226552
Epoch 29: loss 16.12912181954853
Epoch 30: loss 16.129119952390088
Epoch 31: loss 16.129104262658473
Epoch 32: loss 16.129107483578093
Epoch 33: loss 16.12910887156575
Epoch 34: loss 16.12910574399109
Epoch 35: loss 16.129110359380658
Epoch 36: loss 16.12909990759762
Epoch 37: loss 16.129101938109738
Epoch 38: loss 16.129098167751327
Epoch 39: loss 16.129095316062873
Epoch 40: loss 16.1291001904847
Epoch 41: loss 16.129092502230986
Epoch 42: loss 16.129084652179348
Epoch 43: loss 16.129085289258683
Epoch 44: loss 16.12908389427015
Epoch 45: loss 16.129083193664385
Epoch 46: loss 16.129083679058166
Epoch 47: loss 16.129082578624832
Epoch 48: loss 16.12907988458564
Epoch 49: loss 16.12907925321072
-----------Time: 0:10:40.708107, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017176628112793-------------


Epoch 0: loss 16.129181075705464
Epoch 1: loss 16.12917792272023
Epoch 2: loss 16.129175433521365
Epoch 3: loss 16.129169566012905
Epoch 4: loss 16.129162758054
Epoch 5: loss 16.12916231077608
Epoch 6: loss 16.129160933160083
Epoch 7: loss 16.12915895217265
Epoch 8: loss 16.129159153901472
Epoch 9: loss 16.12915998363443
Epoch 10: loss 16.12915936211259
Epoch 11: loss 16.129149221220114
Epoch 12: loss 16.12915096158499
Epoch 13: loss 16.12915057290696
Epoch 14: loss 16.129145328735383
Epoch 15: loss 16.129146551554328
Epoch 16: loss 16.129144098137694
Epoch 17: loss 16.129143205656185
Epoch 18: loss 16.129141635127272
Epoch 19: loss 16.129142917323982
Epoch 20: loss 16.129130319644084
Epoch 21: loss 16.129134100633447
Epoch 22: loss 16.129130023273845
Epoch 23: loss 16.129128591725202
Epoch 24: loss 16.129125409440025
Epoch 25: loss 16.12912452136647
Epoch 26: loss 16.129127067350186
Epoch 27: loss 16.12912286086339
Epoch 28: loss 16.12912407771863
Epoch 29: loss 16.129118233027828
Epoch 30: loss 16.12912143761208
Epoch 31: loss 16.12912319223799
Epoch 32: loss 16.129104631630348
Epoch 33: loss 16.129106561278054
Epoch 34: loss 16.12910721676709
Epoch 35: loss 16.129101159457218
Epoch 36: loss 16.12910266853403
Epoch 37: loss 16.129104436124518
Epoch 38: loss 16.12910495185541
Epoch 39: loss 16.12910106611226
Epoch 40: loss 16.129098068961248
Epoch 41: loss 16.12910129921536
Epoch 42: loss 16.129091889265766
Epoch 43: loss 16.129094147954447
Epoch 44: loss 16.129085741203852
Epoch 45: loss 16.12909460741907
Epoch 46: loss 16.129087072406662
Epoch 47: loss 16.129088437835957
Epoch 48: loss 16.12908330178896
Epoch 49: loss 16.12908578398696
-----------Time: 0:13:49.223757, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017177104949951-------------


Epoch 0: loss 16.12911166387644
Epoch 1: loss 16.129086942501598
Epoch 2: loss 16.12906665216003
Epoch 3: loss 16.12905127513402
Epoch 4: loss 16.129030273037152
Epoch 5: loss 16.129011248038346
Epoch 6: loss 16.128990444558802
Epoch 7: loss 16.128963446085578
Epoch 8: loss 16.128954592834937
Epoch 9: loss 16.12893380387572
Epoch 10: loss 16.12891743350379
Epoch 11: loss 16.128881138391375
Epoch 12: loss 16.128876020494786
Epoch 13: loss 16.1288605599769
Epoch 14: loss 16.128836891585035
Epoch 15: loss 16.12881566546027
Epoch 16: loss 16.128792533283328
Epoch 17: loss 16.12877125504096
Epoch 18: loss 16.12875539573267
Epoch 19: loss 16.12873846632816
Epoch 20: loss 16.128716055500306
Epoch 21: loss 16.128692867316392
Epoch 22: loss 16.128681830571615
Epoch 23: loss 16.12866115181131
Epoch 24: loss 16.128638467171584
Epoch 25: loss 16.128618373113717
Epoch 26: loss 16.128604155639515
Epoch 27: loss 16.12858979348063
Epoch 28: loss 16.128564767438217
Epoch 29: loss 16.12854552048584
Epoch 30: loss 16.12852432158669
Epoch 31: loss 16.128513277322458
Epoch 32: loss 16.128490072543883
Epoch 33: loss 16.128469881770272
Epoch 34: loss 16.12844712323245
Epoch 35: loss 16.12842896452671
Epoch 36: loss 16.12840643857341
Epoch 37: loss 16.128386444861373
Epoch 38: loss 16.128371540005276
Epoch 39: loss 16.128352323908597
Epoch 40: loss 16.128331033220235
Epoch 41: loss 16.128312482206375
Epoch 42: loss 16.128290903963688
Epoch 43: loss 16.128276376895375
Epoch 44: loss 16.128257068490903
Epoch 45: loss 16.12823336665043
Epoch 46: loss 16.12822035177043
Epoch 47: loss 16.12819936756468
Epoch 48: loss 16.128181946543393
Epoch 49: loss 16.12816295084453
-----------Time: 0:07:09.407686, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.0171098709106445-------------


Epoch 0: loss 16.12918739308476
Epoch 1: loss 16.12916723498188
Epoch 2: loss 16.129151152682862
Epoch 3: loss 16.129130403135964
Epoch 4: loss 16.129109567244978
Epoch 5: loss 16.12909183662958
Epoch 6: loss 16.129071046114618
Epoch 7: loss 16.129055519477383
Epoch 8: loss 16.129028583752714
Epoch 9: loss 16.129015533868355
Epoch 10: loss 16.128996460382027
Epoch 11: loss 16.128967879711773
Epoch 12: loss 16.128950601041545
Epoch 13: loss 16.12893721874542
Epoch 14: loss 16.128914868332497
Epoch 15: loss 16.128902346884317
Epoch 16: loss 16.12888131315388
Epoch 17: loss 16.128854228077277
Epoch 18: loss 16.128835411548874
Epoch 19: loss 16.128824350949277
Epoch 20: loss 16.128803379708103
Epoch 21: loss 16.12877808089098
Epoch 22: loss 16.128759538952327
Epoch 23: loss 16.1287442791261
Epoch 24: loss 16.128725063288712
Epoch 25: loss 16.128709755493546
Epoch 26: loss 16.12868472374672
Epoch 27: loss 16.128663988460854
Epoch 28: loss 16.12864778040543
Epoch 29: loss 16.128627133019403
Epoch 30: loss 16.128609880019038
Epoch 31: loss 16.12858458042404
Epoch 32: loss 16.128566629410827
Epoch 33: loss 16.128550759990166
Epoch 34: loss 16.128532123928682
Epoch 35: loss 16.12850861992766
Epoch 36: loss 16.12849362431952
Epoch 37: loss 16.128466788940678
Epoch 38: loss 16.128454981840715
Epoch 39: loss 16.12843107956787
Epoch 40: loss 16.128406476429976
Epoch 41: loss 16.12839587296138
Epoch 42: loss 16.128377586684195
Epoch 43: loss 16.12836076047772
Epoch 44: loss 16.128329692682925
Epoch 45: loss 16.128312742016508
Epoch 46: loss 16.128295673372435
Epoch 47: loss 16.12828150542292
Epoch 48: loss 16.128267334361905
Epoch 49: loss 16.128241257410842
-----------Time: 0:10:15.433184, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017078399658203-------------


Epoch 0: loss 16.129174605084867
Epoch 1: loss 16.129156779828055
Epoch 2: loss 16.12914143728782
Epoch 3: loss 16.12911642032061
Epoch 4: loss 16.129092940174413
Epoch 5: loss 16.129076180865155
Epoch 6: loss 16.129055794585717
Epoch 7: loss 16.129035568548453
Epoch 8: loss 16.129016309668668
Epoch 9: loss 16.128997134280763
Epoch 10: loss 16.128973962432212
Epoch 11: loss 16.12895822810245
Epoch 12: loss 16.128940857383725
Epoch 13: loss 16.128920349496546
Epoch 14: loss 16.128902319658703
Epoch 15: loss 16.128888170118888
Epoch 16: loss 16.128864679860317
Epoch 17: loss 16.128847795572536
Epoch 18: loss 16.12882200228643
Epoch 19: loss 16.12880352465208
Epoch 20: loss 16.12878625194556
Epoch 21: loss 16.128764344661892
Epoch 22: loss 16.128744270050895
Epoch 23: loss 16.128725647991153
Epoch 24: loss 16.12870103396301
Epoch 25: loss 16.128690999639364
Epoch 26: loss 16.12867112157136
Epoch 27: loss 16.128649160355053
Epoch 28: loss 16.128636340980883
Epoch 29: loss 16.128612426002757
Epoch 30: loss 16.128591227362893
Epoch 31: loss 16.128565249201912
Epoch 32: loss 16.128550501735784
Epoch 33: loss 16.128535531019633
Epoch 34: loss 16.12851638648742
Epoch 35: loss 16.12849515491753
Epoch 36: loss 16.12846896569323
Epoch 37: loss 16.128455935774323
Epoch 38: loss 16.128441560910147
Epoch 39: loss 16.128416439189152
Epoch 40: loss 16.1283996982896
Epoch 41: loss 16.12838168452783
Epoch 42: loss 16.128363267308412
Epoch 43: loss 16.12834101231478
Epoch 44: loss 16.12832071782455
Epoch 45: loss 16.12829974736125
Epoch 46: loss 16.12827728026713
Epoch 47: loss 16.128263013527533
Epoch 48: loss 16.128247748515474
Epoch 49: loss 16.12822016741414
-----------Time: 0:09:24.816173, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017068386077881-------------


Epoch 0: loss 16.129158839640116
Epoch 1: loss 16.129137738234586
Epoch 2: loss 16.129125902093968
Epoch 3: loss 16.129099749429773
Epoch 4: loss 16.129084079145024
Epoch 5: loss 16.129059905912513
Epoch 6: loss 16.12904414124564
Epoch 7: loss 16.129024786687275
Epoch 8: loss 16.129002395306287
Epoch 9: loss 16.128985984744165
Epoch 10: loss 16.128964324046763
Epoch 11: loss 16.1289495255002
Epoch 12: loss 16.128927545096317
Epoch 13: loss 16.12890553176241
Epoch 14: loss 16.128892524401866
Epoch 15: loss 16.12886395384398
Epoch 16: loss 16.128838492710347
Epoch 17: loss 16.12882747178236
Epoch 18: loss 16.128808701407852
Epoch 19: loss 16.12878823293299
Epoch 20: loss 16.128771555041283
Epoch 21: loss 16.12875082364479
Epoch 22: loss 16.12873085949199
Epoch 23: loss 16.128707716165515
Epoch 24: loss 16.12868674803584
Epoch 25: loss 16.128679281994984
Epoch 26: loss 16.128650747219332
Epoch 27: loss 16.12864252819582
Epoch 28: loss 16.12861160301138
Epoch 29: loss 16.12859569210407
Epoch 30: loss 16.12857736615528
Epoch 31: loss 16.1285576561082
Epoch 32: loss 16.12854212480372
Epoch 33: loss 16.128516203168296
Epoch 34: loss 16.128499009545695
Epoch 35: loss 16.12847969362177
Epoch 36: loss 16.12845836144676
Epoch 37: loss 16.12844762288727
Epoch 38: loss 16.128421246195177
Epoch 39: loss 16.128407327165547
Epoch 40: loss 16.128382895678655
Epoch 41: loss 16.128364469902614
Epoch 42: loss 16.12834950878025
Epoch 43: loss 16.128323558363466
Epoch 44: loss 16.1282965764849
Epoch 45: loss 16.128280297642885
Epoch 46: loss 16.128266475847585
Epoch 47: loss 16.128245681183955
Epoch 48: loss 16.12822314408112
Epoch 49: loss 16.1282037420724
-----------Time: 0:12:38.486030, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017068862915039-------------


Epoch 0: loss 16.12916133946994
Epoch 1: loss 16.129157007486032
Epoch 2: loss 16.12912593761691
Epoch 3: loss 16.12910840587813
Epoch 4: loss 16.129094197997716
Epoch 5: loss 16.129071281032758
Epoch 6: loss 16.12904892724905
Epoch 7: loss 16.12903107787812
Epoch 8: loss 16.129008482434685
Epoch 9: loss 16.128983590186714
Epoch 10: loss 16.12896745291788
Epoch 11: loss 16.128950134316757
Epoch 12: loss 16.12893299955334
Epoch 13: loss 16.12891268561624
Epoch 14: loss 16.128897322073392
Epoch 15: loss 16.128868671135127
Epoch 16: loss 16.12886072410845
Epoch 17: loss 16.128836141454585
Epoch 18: loss 16.128809042376236
Epoch 19: loss 16.12879913510545
Epoch 20: loss 16.128778135342206
Epoch 21: loss 16.128760437916128
Epoch 22: loss 16.128738690356055
Epoch 23: loss 16.128727195183824
Epoch 24: loss 16.128697936984754
Epoch 25: loss 16.12868124379484
Epoch 26: loss 16.12866883954644
Epoch 27: loss 16.12863802689453
Epoch 28: loss 16.1286237274842
Epoch 29: loss 16.128594674384743
Epoch 30: loss 16.128582988373932
Epoch 31: loss 16.128567440993375
Epoch 32: loss 16.12853768002799
Epoch 33: loss 16.128530180538522
Epoch 34: loss 16.12850803392876
Epoch 35: loss 16.12848973727991
Epoch 36: loss 16.128472669413714
Epoch 37: loss 16.12845174043706
Epoch 38: loss 16.128430222868595
Epoch 39: loss 16.128410987065756
Epoch 40: loss 16.12838690277025
Epoch 41: loss 16.12836949652858
Epoch 42: loss 16.128358108962342
Epoch 43: loss 16.128332888710556
Epoch 44: loss 16.128310007787128
Epoch 45: loss 16.128291234819702
Epoch 46: loss 16.128272527453042
Epoch 47: loss 16.12825155465612
Epoch 48: loss 16.128240951706108
Epoch 49: loss 16.12821688063447
-----------Time: 0:11:58.833382, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017068862915039-------------


Epoch 0: loss 16.129282082987583
Epoch 1: loss 16.129093224098657
Epoch 2: loss 16.12889339432501
Epoch 3: loss 16.12869938642451
Epoch 4: loss 16.12850675847363
Epoch 5: loss 16.128306215129644
Epoch 6: loss 16.12811809211013
Epoch 7: loss 16.127926124315536
Epoch 8: loss 16.127728934129856
Epoch 9: loss 16.127535009721235
Epoch 10: loss 16.12734470787484
Epoch 11: loss 16.12715707543494
Epoch 12: loss 16.126959905474
Epoch 13: loss 16.126773084616648
Epoch 14: loss 16.12657277722797
Epoch 15: loss 16.12637784884247
Epoch 16: loss 16.12618553541224
Epoch 17: loss 16.12599331610484
Epoch 18: loss 16.125801741136943
Epoch 19: loss 16.12560999685166
Epoch 20: loss 16.125409465953666
Epoch 21: loss 16.125217393664578
Epoch 22: loss 16.125026557936884
Epoch 23: loss 16.124835061793615
Epoch 24: loss 16.124635089150324
Epoch 25: loss 16.124442494388767
Epoch 26: loss 16.12424984387952
Epoch 27: loss 16.124050932257248
Epoch 28: loss 16.123861389616508
Epoch 29: loss 16.123672700044963
Epoch 30: loss 16.123480239596425
Epoch 31: loss 16.123277910511543
Epoch 32: loss 16.123091763293633
Epoch 33: loss 16.12289544640189
Epoch 34: loss 16.12269940021052
Epoch 35: loss 16.12251551271846
Epoch 36: loss 16.1223185950482
Epoch 37: loss 16.122124162946726
Epoch 38: loss 16.121923460657015
Epoch 39: loss 16.121737227354313
Epoch 40: loss 16.12154214261601
Epoch 41: loss 16.121347973954748
Epoch 42: loss 16.121154749374018
Epoch 43: loss 16.120959432310485
Epoch 44: loss 16.120763193984082
Epoch 45: loss 16.120571351686596
Epoch 46: loss 16.120373820013945
Epoch 47: loss 16.120179675466797
Epoch 48: loss 16.119998527130658
Epoch 49: loss 16.119794837543015
-----------Time: 0:09:14.081631, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016012191772461-------------


Epoch 0: loss 16.12906150055554
Epoch 1: loss 16.128865307864448
Epoch 2: loss 16.128671755279907
Epoch 3: loss 16.128469686783028
Epoch 4: loss 16.128277896603144
Epoch 5: loss 16.128089754396058
Epoch 6: loss 16.12788956887441
Epoch 7: loss 16.127701981032654
Epoch 8: loss 16.127505677883363
Epoch 9: loss 16.127307775164507
Epoch 10: loss 16.127118733215745
Epoch 11: loss 16.126928587462864
Epoch 12: loss 16.126726112396838
Epoch 13: loss 16.126539228790932
Epoch 14: loss 16.12635101994592
Epoch 15: loss 16.12615666770624
Epoch 16: loss 16.125962512528137
Epoch 17: loss 16.125767356225367
Epoch 18: loss 16.125571250656236
Epoch 19: loss 16.12537761872848
Epoch 20: loss 16.1251909459266
Epoch 21: loss 16.124997019184356
Epoch 22: loss 16.124803689071964
Epoch 23: loss 16.124604339506263
Epoch 24: loss 16.12440849452514
Epoch 25: loss 16.12421918291317
Epoch 26: loss 16.1240229987787
Epoch 27: loss 16.12383114247947
Epoch 28: loss 16.123642240288856
Epoch 29: loss 16.123441728319143
Epoch 30: loss 16.12325504281198
Epoch 31: loss 16.12305619367897
Epoch 32: loss 16.122862996323853
Epoch 33: loss 16.122670594475203
Epoch 34: loss 16.12247967214413
Epoch 35: loss 16.12228623520362
Epoch 36: loss 16.122093460493726
Epoch 37: loss 16.121901638161688
Epoch 38: loss 16.12171174755169
Epoch 39: loss 16.121514454167976
Epoch 40: loss 16.121314028283056
Epoch 41: loss 16.121129903798522
Epoch 42: loss 16.120936926322635
Epoch 43: loss 16.12073332152333
Epoch 44: loss 16.120545748201902
Epoch 45: loss 16.12035155568582
Epoch 46: loss 16.120153585032575
Epoch 47: loss 16.11996980877659
Epoch 48: loss 16.119775806839794
Epoch 49: loss 16.119577829704262
-----------Time: 0:07:59.331348, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.015983581542969-------------


Epoch 0: loss 16.129081032728617
Epoch 1: loss 16.12887791461955
Epoch 2: loss 16.128686892461232
Epoch 3: loss 16.12848970668351
Epoch 4: loss 16.128300429298022
Epoch 5: loss 16.128110687262083
Epoch 6: loss 16.127917714453442
Epoch 7: loss 16.127722659533667
Epoch 8: loss 16.127521621720696
Epoch 9: loss 16.127334439410923
Epoch 10: loss 16.127139961933427
Epoch 11: loss 16.12694679984196
Epoch 12: loss 16.126751929019182
Epoch 13: loss 16.12656227203087
Epoch 14: loss 16.126365538198318
Epoch 15: loss 16.126173959081754
Epoch 16: loss 16.12598173069915
Epoch 17: loss 16.125784167392933
Epoch 18: loss 16.125594213515797
Epoch 19: loss 16.125396767668647
Epoch 20: loss 16.12520182424424
Epoch 21: loss 16.125016930182387
Epoch 22: loss 16.12482297854815
Epoch 23: loss 16.124626006945245
Epoch 24: loss 16.12443339662619
Epoch 25: loss 16.124243467381753
Epoch 26: loss 16.124046602866258
Epoch 27: loss 16.123852666530226
Epoch 28: loss 16.123659699944582
Epoch 29: loss 16.123463080718896
Epoch 30: loss 16.123267561407957
Epoch 31: loss 16.12307935204436
Epoch 32: loss 16.122874227018706
Epoch 33: loss 16.122686011432112
Epoch 34: loss 16.122504535610744
Epoch 35: loss 16.122300623550956
Epoch 36: loss 16.122106981251537
Epoch 37: loss 16.121910579052873
Epoch 38: loss 16.12173177419375
Epoch 39: loss 16.121533494983566
Epoch 40: loss 16.12133299857135
Epoch 41: loss 16.12114030190821
Epoch 42: loss 16.12094744992943
Epoch 43: loss 16.12076503547266
Epoch 44: loss 16.120562371383095
Epoch 45: loss 16.120373978700375
Epoch 46: loss 16.120174824640504
Epoch 47: loss 16.11998327067522
Epoch 48: loss 16.119794716713155
Epoch 49: loss 16.11959239411056
-----------Time: 0:11:25.509084, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.015989780426025-------------


Epoch 0: loss 16.129081221492864
Epoch 1: loss 16.128892713943987
Epoch 2: loss 16.1286923025794
Epoch 3: loss 16.1285052468039
Epoch 4: loss 16.12830125799381
Epoch 5: loss 16.128113684413094
Epoch 6: loss 16.127925992076847
Epoch 7: loss 16.12772562297678
Epoch 8: loss 16.12753348534622
Epoch 9: loss 16.127341483843722
Epoch 10: loss 16.127146824084267
Epoch 11: loss 16.126955446177945
Epoch 12: loss 16.126763733526232
Epoch 13: loss 16.12656627860388
Epoch 14: loss 16.126376408477917
Epoch 15: loss 16.12618036876884
Epoch 16: loss 16.12598181470961
Epoch 17: loss 16.12578996463338
Epoch 18: loss 16.1255985639094
Epoch 19: loss 16.12540403223997
Epoch 20: loss 16.125206716816475
Epoch 21: loss 16.125022375823495
Epoch 22: loss 16.12482255823655
Epoch 23: loss 16.12463195924204
Epoch 24: loss 16.12444443700105
Epoch 25: loss 16.1242408366097
Epoch 26: loss 16.124046850230396
Epoch 27: loss 16.123857970338857
Epoch 28: loss 16.123664793467768
Epoch 29: loss 16.12347188107406
Epoch 30: loss 16.123289380532494
Epoch 31: loss 16.123089257759396
Epoch 32: loss 16.12288944250608
Epoch 33: loss 16.12270424351736
Epoch 34: loss 16.12251038289446
Epoch 35: loss 16.122307206963377
Epoch 36: loss 16.12212110875157
Epoch 37: loss 16.121928498173226
Epoch 38: loss 16.121728650508462
Epoch 39: loss 16.12153959922521
Epoch 40: loss 16.121341795037143
Epoch 41: loss 16.12115446907985
Epoch 42: loss 16.12095625910051
Epoch 43: loss 16.12076479070144
Epoch 44: loss 16.12057262014085
Epoch 45: loss 16.12037541828705
Epoch 46: loss 16.120181990940328
Epoch 47: loss 16.119996400162083
Epoch 48: loss 16.119799333399182
Epoch 49: loss 16.11960781832763
-----------Time: 0:10:58.546531, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.015987873077393-------------


Epoch 0: loss 16.12908321959354
Epoch 1: loss 16.12889089138369
Epoch 2: loss 16.12869542859831
Epoch 3: loss 16.12849387220224
Epoch 4: loss 16.128302820225393
Epoch 5: loss 16.128097548959307
Epoch 6: loss 16.12791467814941
Epoch 7: loss 16.127727666194183
Epoch 8: loss 16.127534743169523
Epoch 9: loss 16.12733528262704
Epoch 10: loss 16.127143023129452
Epoch 11: loss 16.126956743932148
Epoch 12: loss 16.12675158027205
Epoch 13: loss 16.126567665035793
Epoch 14: loss 16.126373470186085
Epoch 15: loss 16.126180994439345
Epoch 16: loss 16.125985011255825
Epoch 17: loss 16.125794430930306
Epoch 18: loss 16.125600231931934
Epoch 19: loss 16.12540141884045
Epoch 20: loss 16.125211297720266
Epoch 21: loss 16.12501951894921
Epoch 22: loss 16.124816888308253
Epoch 23: loss 16.12462859882357
Epoch 24: loss 16.12443488625614
Epoch 25: loss 16.124243203682248
Epoch 26: loss 16.124046028276187
Epoch 27: loss 16.12385949289883
Epoch 28: loss 16.123663913691544
Epoch 29: loss 16.12346912739781
Epoch 30: loss 16.123275249661667
Epoch 31: loss 16.12307706379644
Epoch 32: loss 16.12289056290486
Epoch 33: loss 16.12269688352675
Epoch 34: loss 16.122504060070042
Epoch 35: loss 16.12230741647095
Epoch 36: loss 16.122118199241104
Epoch 37: loss 16.121931521771977
Epoch 38: loss 16.121728200119044
Epoch 39: loss 16.12153501728425
Epoch 40: loss 16.121335157432785
Epoch 41: loss 16.121160324142316
Epoch 42: loss 16.120957113466165
Epoch 43: loss 16.120768604102246
Epoch 44: loss 16.12057554469023
Epoch 45: loss 16.120377147502392
Epoch 46: loss 16.120183130267392
Epoch 47: loss 16.11998806705029
Epoch 48: loss 16.119794970041
Epoch 49: loss 16.11959904260516
-----------Time: 0:13:33.865550, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.015989780426025-------------


Epoch 0: loss 16.128411235985645
Epoch 1: loss 16.12647296061264
Epoch 2: loss 16.124530084888985
Epoch 3: loss 16.122601754384043
Epoch 4: loss 16.120663342623686
Epoch 5: loss 16.118726242359983
Epoch 6: loss 16.11679561893988
Epoch 7: loss 16.114859097414914
Epoch 8: loss 16.112930130349223
Epoch 9: loss 16.110993327492928
Epoch 10: loss 16.109064982986244
Epoch 11: loss 16.107130763710938
Epoch 12: loss 16.105194253076217
Epoch 13: loss 16.103261215911086
Epoch 14: loss 16.10132584952997
Epoch 15: loss 16.09940062948645
Epoch 16: loss 16.097465110641902
Epoch 17: loss 16.095534521707574
Epoch 18: loss 16.093598596812463
Epoch 19: loss 16.091669913152433
Epoch 20: loss 16.089738184113163
Epoch 21: loss 16.087802272960502
Epoch 22: loss 16.085873284373612
Epoch 23: loss 16.08394682310145
Epoch 24: loss 16.082011369598373
Epoch 25: loss 16.080085823884666
Epoch 26: loss 16.078150131314782
Epoch 27: loss 16.07622695682229
Epoch 28: loss 16.07428558291528
Epoch 29: loss 16.0723637733335
Epoch 30: loss 16.070430861406187
Epoch 31: loss 16.06850670834739
Epoch 32: loss 16.066575568159227
Epoch 33: loss 16.064645380348924
Epoch 34: loss 16.06271856929246
Epoch 35: loss 16.06079007128518
Epoch 36: loss 16.0588596464824
Epoch 37: loss 16.056938076745304
Epoch 38: loss 16.055013017981125
Epoch 39: loss 16.053082517724505
Epoch 40: loss 16.051145844254695
Epoch 41: loss 16.04922159811023
Epoch 42: loss 16.04730192275719
Epoch 43: loss 16.045365920593195
Epoch 44: loss 16.04343712517851
Epoch 45: loss 16.041511579464803
Epoch 46: loss 16.039586708686997
Epoch 47: loss 16.03765813055863
Epoch 48: loss 16.035737872318187
Epoch 49: loss 16.033813513900483
-----------Time: 0:07:25.844407, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.005204677581787-------------


Epoch 0: loss 16.128201447601484
Epoch 1: loss 16.12626142927069
Epoch 2: loss 16.12432244058664
Epoch 3: loss 16.12238928518456
Epoch 4: loss 16.120461395475253
Epoch 5: loss 16.11852701051265
Epoch 6: loss 16.11659114447672
Epoch 7: loss 16.114660514574325
Epoch 8: loss 16.11272852002051
Epoch 9: loss 16.110799025037224
Epoch 10: loss 16.108853375412576
Epoch 11: loss 16.106929758827995
Epoch 12: loss 16.105000125642313
Epoch 13: loss 16.103064281646166
Epoch 14: loss 16.101127559170248
Epoch 15: loss 16.099202916309714
Epoch 16: loss 16.097273626685336
Epoch 17: loss 16.095338125991198
Epoch 18: loss 16.09341293654408
Epoch 19: loss 16.091478897994985
Epoch 20: loss 16.089543108450062
Epoch 21: loss 16.087615397651927
Epoch 22: loss 16.08568235504167
Epoch 23: loss 16.08375428901411
Epoch 24: loss 16.08182607570895
Epoch 25: loss 16.07988725349007
Epoch 26: loss 16.07796696491252
Epoch 27: loss 16.07604371937414
Epoch 28: loss 16.074109323780586
Epoch 29: loss 16.07218399250099
Epoch 30: loss 16.070255544277686
Epoch 31: loss 16.06833525232934
Epoch 32: loss 16.066389331745025
Epoch 33: loss 16.064466744288598
Epoch 34: loss 16.06254020652543
Epoch 35: loss 16.06060956080625
Epoch 36: loss 16.058682389334532
Epoch 37: loss 16.056759390641712
Epoch 38: loss 16.054831545011968
Epoch 39: loss 16.052904581751363
Epoch 40: loss 16.05097938089542
Epoch 41: loss 16.049047006998066
Epoch 42: loss 16.04712821971858
Epoch 43: loss 16.045201177633345
Epoch 44: loss 16.043275505644658
Epoch 45: loss 16.04135521188127
Epoch 46: loss 16.03942837256203
Epoch 47: loss 16.037503708180296
Epoch 48: loss 16.03557641924951
Epoch 49: loss 16.033653582873196
-----------Time: 0:09:35.318787, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.005206108093262-------------


Epoch 0: loss 16.128211611570904
Epoch 1: loss 16.126282875533985
Epoch 2: loss 16.12434562773339
Epoch 3: loss 16.122422151166244
Epoch 4: loss 16.12048212064875
Epoch 5: loss 16.11855728565318
Epoch 6: loss 16.1166090228883
Epoch 7: loss 16.114680075528767
Epoch 8: loss 16.11274867864197
Epoch 9: loss 16.110808321935707
Epoch 10: loss 16.10887842608769
Epoch 11: loss 16.10694903611748
Epoch 12: loss 16.105012148472845
Epoch 13: loss 16.103083137586882
Epoch 14: loss 16.10115640535505
Epoch 15: loss 16.099223908294213
Epoch 16: loss 16.097294901556914
Epoch 17: loss 16.09536968903285
Epoch 18: loss 16.093435429567354
Epoch 19: loss 16.091496677097904
Epoch 20: loss 16.08956885661944
Epoch 21: loss 16.08764431799411
Epoch 22: loss 16.085709728450475
Epoch 23: loss 16.083789072975378
Epoch 24: loss 16.0818568103141
Epoch 25: loss 16.07991656951116
Epoch 26: loss 16.077994176523397
Epoch 27: loss 16.076070116809557
Epoch 28: loss 16.074144164526704
Epoch 29: loss 16.072205974979205
Epoch 30: loss 16.070284406019983
Epoch 31: loss 16.068351373781386
Epoch 32: loss 16.066429322539886
Epoch 33: loss 16.06450042185283
Epoch 34: loss 16.06257774649657
Epoch 35: loss 16.060654358866422
Epoch 36: loss 16.05872351297417
Epoch 37: loss 16.05679768852205
Epoch 38: loss 16.054873102446365
Epoch 39: loss 16.052945753359936
Epoch 40: loss 16.051021866593562
Epoch 41: loss 16.049095870490433
Epoch 42: loss 16.047166537045783
Epoch 43: loss 16.045238893144866
Epoch 44: loss 16.04331575002665
Epoch 45: loss 16.04139189256022
Epoch 46: loss 16.039460834308187
Epoch 47: loss 16.037547708141084
Epoch 48: loss 16.035620571155142
Epoch 49: loss 16.03368898213331
-----------Time: 0:09:31.782036, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.0052056312561035-------------


Epoch 0: loss 16.128221086343387
Epoch 1: loss 16.126283110711416
Epoch 2: loss 16.124351486685224
Epoch 3: loss 16.122413831278315
Epoch 4: loss 16.12048939533244
Epoch 5: loss 16.118548503967002
Epoch 6: loss 16.116615025228363
Epoch 7: loss 16.114682727822018
Epoch 8: loss 16.112756685564996
Epoch 9: loss 16.110816596706904
Epoch 10: loss 16.108887412354893
Epoch 11: loss 16.106956137853707
Epoch 12: loss 16.105024925841786
Epoch 13: loss 16.10308219947006
Epoch 14: loss 16.101160494642066
Epoch 15: loss 16.09922909905173
Epoch 16: loss 16.097293635954866
Epoch 17: loss 16.09536566612447
Epoch 18: loss 16.09344128488911
Epoch 19: loss 16.091501611416078
Epoch 20: loss 16.089576016696267
Epoch 21: loss 16.087650023186058
Epoch 22: loss 16.085720240648445
Epoch 23: loss 16.083786827510565
Epoch 24: loss 16.08185402578216
Epoch 25: loss 16.079927141086895
Epoch 26: loss 16.077996447658073
Epoch 27: loss 16.07607507601972
Epoch 28: loss 16.074147743787243
Epoch 29: loss 16.072226070592816
Epoch 30: loss 16.0702914374882
Epoch 31: loss 16.068367006987447
Epoch 32: loss 16.06643558961662
Epoch 33: loss 16.06450717069326
Epoch 34: loss 16.062586479695224
Epoch 35: loss 16.06065784737492
Epoch 36: loss 16.058731586016542
Epoch 37: loss 16.056807571938013
Epoch 38: loss 16.05487878378349
Epoch 39: loss 16.052942916710393
Epoch 40: loss 16.051034845432035
Epoch 41: loss 16.049104502824676
Epoch 42: loss 16.047183099812045
Epoch 43: loss 16.045256143552315
Epoch 44: loss 16.043330659290707
Epoch 45: loss 16.04140654175485
Epoch 46: loss 16.039480292842466
Epoch 47: loss 16.037556906768067
Epoch 48: loss 16.035634398654857
Epoch 49: loss 16.033708215343232
-----------Time: 0:12:25.159338, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.005204677581787-------------


Epoch 0: loss 16.128210314075996
Epoch 1: loss 16.126280542428628
Epoch 2: loss 16.124343175872504
Epoch 3: loss 16.12241019860372
Epoch 4: loss 16.120477847783317
Epoch 5: loss 16.1185496637781
Epoch 6: loss 16.11661376403427
Epoch 7: loss 16.11467511735577
Epoch 8: loss 16.112748513473253
Epoch 9: loss 16.110811652794943
Epoch 10: loss 16.108876498773604
Epoch 11: loss 16.10694858598735
Epoch 12: loss 16.105020922120982
Epoch 13: loss 16.103086742776576
Epoch 14: loss 16.10115637838873
Epoch 15: loss 16.099228357477898
Epoch 16: loss 16.09729179498487
Epoch 17: loss 16.09536028582483
Epoch 18: loss 16.093436410985866
Epoch 19: loss 16.09150565714211
Epoch 20: loss 16.089573072959315
Epoch 21: loss 16.087654398990235
Epoch 22: loss 16.08571829959191
Epoch 23: loss 16.083790681620147
Epoch 24: loss 16.08186702121529
Epoch 25: loss 16.0799391924395
Epoch 26: loss 16.0780074029853
Epoch 27: loss 16.0760790037681
Epoch 28: loss 16.07414541171905
Epoch 29: loss 16.072224745872294
Epoch 30: loss 16.07030023162037
Epoch 31: loss 16.06836729402319
Epoch 32: loss 16.066444010109667
Epoch 33: loss 16.064517425155433
Epoch 34: loss 16.062591575033448
Epoch 35: loss 16.060665247555722
Epoch 36: loss 16.058736958018848
Epoch 37: loss 16.0568186758393
Epoch 38: loss 16.05488680133747
Epoch 39: loss 16.052963411892282
Epoch 40: loss 16.05104343161238
Epoch 41: loss 16.049112895054943
Epoch 42: loss 16.047186695926534
Epoch 43: loss 16.045264796111297
Epoch 44: loss 16.043335591275255
Epoch 45: loss 16.041413320932392
Epoch 46: loss 16.0394937238854
Epoch 47: loss 16.037556246871077
Epoch 48: loss 16.03563692752533
Epoch 49: loss 16.03371618077961
-----------Time: 0:12:11.271455, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.005201816558838-------------


Epoch 0: loss 16.119604420830466
Epoch 1: loss 16.100282871055498
Epoch 2: loss 16.080961517564234
Epoch 3: loss 16.06168090162749
Epoch 4: loss 16.04241797222658
Epoch 5: loss 16.0231576559659
Epoch 6: loss 16.0039166232177
Epoch 7: loss 15.984705147372438
Epoch 8: loss 15.965511983214935
Epoch 9: loss 15.94634228416472
Epoch 10: loss 15.927188482797943
Epoch 11: loss 15.908048373321401
Epoch 12: loss 15.888934627053787
Epoch 13: loss 15.86982684812188
Epoch 14: loss 15.850746722633648
Epoch 15: loss 15.831678467253228
Epoch 16: loss 15.81264166653059
Epoch 17: loss 15.793615675154038
Epoch 18: loss 15.774606412231048
Epoch 19: loss 15.755615831004853
Epoch 20: loss 15.736645818340058
Epoch 21: loss 15.717689338101195
Epoch 22: loss 15.698758341035745
Epoch 23: loss 15.6798479306821
Epoch 24: loss 15.660951045494224
Epoch 25: loss 15.642066424796607
Epoch 26: loss 15.623208141378763
Epoch 27: loss 15.604370509495613
Epoch 28: loss 15.58554010251218
Epoch 29: loss 15.566735420880466
Epoch 30: loss 15.547941358534132
Epoch 31: loss 15.529164428358301
Epoch 32: loss 15.51040682136656
Epoch 33: loss 15.491673721315552
Epoch 34: loss 15.472948208913134
Epoch 35: loss 15.454248046408276
Epoch 36: loss 15.435561277349077
Epoch 37: loss 15.416887606661756
Epoch 38: loss 15.398249172140167
Epoch 39: loss 15.379612696566227
Epoch 40: loss 15.360987131202785
Epoch 41: loss 15.342392051006025
Epoch 42: loss 15.323808263474797
Epoch 43: loss 15.305235926517655
Epoch 44: loss 15.28669062511243
Epoch 45: loss 15.268171774556677
Epoch 46: loss 15.249664109060465
Epoch 47: loss 15.23116895930802
Epoch 48: loss 15.2126954419902
Epoch 49: loss 15.194228850090354
-----------Time: 0:09:01.440965, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.8982272148132324-------------


Epoch 0: loss 16.119576750014254
Epoch 1: loss 16.1002640420811
Epoch 2: loss 16.08096566830335
Epoch 3: loss 16.061683654525865
Epoch 4: loss 16.04241643333124
Epoch 5: loss 16.023179487795723
Epoch 6: loss 16.00396503035694
Epoch 7: loss 15.984754029533796
Epoch 8: loss 15.965570940408712
Epoch 9: loss 15.94640447135332
Epoch 10: loss 15.927279336741595
Epoch 11: loss 15.90814642597632
Epoch 12: loss 15.889040335032394
Epoch 13: loss 15.86996475103564
Epoch 14: loss 15.850895987703076
Epoch 15: loss 15.831841757921111
Epoch 16: loss 15.812807334901978
Epoch 17: loss 15.793796496004951
Epoch 18: loss 15.774795490480622
Epoch 19: loss 15.755799393345313
Epoch 20: loss 15.736821453100706
Epoch 21: loss 15.717858054444736
Epoch 22: loss 15.698919063939421
Epoch 23: loss 15.679977541192839
Epoch 24: loss 15.661034389058166
Epoch 25: loss 15.64209510736844
Epoch 26: loss 15.623170543585607
Epoch 27: loss 15.604246757418128
Epoch 28: loss 15.585314149374286
Epoch 29: loss 15.56636101062561
Epoch 30: loss 15.547400511886323
Epoch 31: loss 15.528401851913344
Epoch 32: loss 15.509367605990336
Epoch 33: loss 15.49027992759858
Epoch 34: loss 15.471127144210426
Epoch 35: loss 15.451899954000849
Epoch 36: loss 15.432572692551647
Epoch 37: loss 15.413128498651982
Epoch 38: loss 15.393556370302152
Epoch 39: loss 15.373814837189716
Epoch 40: loss 15.353880948382528
Epoch 41: loss 15.333725600219278
Epoch 42: loss 15.313347593735845
Epoch 43: loss 15.292701193014521
Epoch 44: loss 15.271772443762028
Epoch 45: loss 15.250514039791039
Epoch 46: loss 15.228923688964263
Epoch 47: loss 15.20699340717633
Epoch 48: loss 15.184711561830488
Epoch 49: loss 15.16203234648173
-----------Time: 0:08:05.116299, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.89369535446167-------------


Epoch 0: loss 16.1195770891676
Epoch 1: loss 16.10026646930929
Epoch 2: loss 16.08095620960694
Epoch 3: loss 16.06169382445899
Epoch 4: loss 16.042440864040255
Epoch 5: loss 16.02320201841627
Epoch 6: loss 16.003990058992272
Epoch 7: loss 15.984795238480352
Epoch 8: loss 15.965625685151988
Epoch 9: loss 15.946452268898131
Epoch 10: loss 15.927310279039276
Epoch 11: loss 15.908161595828382
Epoch 12: loss 15.889017125845863
Epoch 13: loss 15.869871107374216
Epoch 14: loss 15.850684403984232
Epoch 15: loss 15.83144677936415
Epoch 16: loss 15.812127775832192
Epoch 17: loss 15.792657892622334
Epoch 18: loss 15.773025801286806
Epoch 19: loss 15.75312408719003
Epoch 20: loss 15.732865508319632
Epoch 21: loss 15.712159875296198
Epoch 22: loss 15.690902103737297
Epoch 23: loss 15.669027829701776
Epoch 24: loss 15.646441450061975
Epoch 25: loss 15.623080745995725
Epoch 26: loss 15.59890384347365
Epoch 27: loss 15.573878789738899
Epoch 28: loss 15.548022226641656
Epoch 29: loss 15.521343446413157
Epoch 30: loss 15.493838278867422
Epoch 31: loss 15.465530811411456
Epoch 32: loss 15.436400965026309
Epoch 33: loss 15.406479194800838
Epoch 34: loss 15.375773099014589
Epoch 35: loss 15.34428203903248
Epoch 36: loss 15.312018435178988
Epoch 37: loss 15.278992043817219
Epoch 38: loss 15.245212907827439
Epoch 39: loss 15.210663081122975
Epoch 40: loss 15.175334488846413
Epoch 41: loss 15.139280495272828
Epoch 42: loss 15.102478531665813
Epoch 43: loss 15.064953704188866
Epoch 44: loss 15.026680108319812
Epoch 45: loss 14.987699851969003
Epoch 46: loss 14.947993784121937
Epoch 47: loss 14.907576095805082
Epoch 48: loss 14.866439335497908
Epoch 49: loss 14.824593779183783
-----------Time: 0:11:14.918734, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.8492047786712646-------------


Epoch 0: loss 16.119580384503895
Epoch 1: loss 16.100266119265697
Epoch 2: loss 16.08096396579504
Epoch 3: loss 16.061689326009912
Epoch 4: loss 16.042446273121257
Epoch 5: loss 16.02321541678847
Epoch 6: loss 16.004000275338583
Epoch 7: loss 15.984790681949965
Epoch 8: loss 15.965555101843746
Epoch 9: loss 15.946288996121364
Epoch 10: loss 15.926935136285795
Epoch 11: loss 15.907406705822098
Epoch 12: loss 15.887583797687158
Epoch 13: loss 15.86726345712559
Epoch 14: loss 15.846210028828326
Epoch 15: loss 15.82415329496001
Epoch 16: loss 15.800925715064276
Epoch 17: loss 15.776399040948185
Epoch 18: loss 15.750545158147682
Epoch 19: loss 15.723389026067256
Epoch 20: loss 15.694965766525062
Epoch 21: loss 15.665265896969869
Epoch 22: loss 15.634305847929248
Epoch 23: loss 15.602121344852085
Epoch 24: loss 15.568719074608127
Epoch 25: loss 15.534113346979371
Epoch 26: loss 15.498306970093289
Epoch 27: loss 15.461326763252645
Epoch 28: loss 15.423158833875547
Epoch 29: loss 15.383831587091874
Epoch 30: loss 15.343336534474194
Epoch 31: loss 15.301705376747964
Epoch 32: loss 15.258911072138277
Epoch 33: loss 15.21500828060007
Epoch 34: loss 15.169960597599895
Epoch 35: loss 15.123797993092124
Epoch 36: loss 15.076548503494056
Epoch 37: loss 15.028214769949402
Epoch 38: loss 14.978806723324483
Epoch 39: loss 14.928307894457287
Epoch 40: loss 14.876766131195184
Epoch 41: loss 14.824168257378831
Epoch 42: loss 14.770535165684064
Epoch 43: loss 14.715901654073893
Epoch 44: loss 14.660277546586519
Epoch 45: loss 14.603644813643388
Epoch 46: loss 14.5460052010545
Epoch 47: loss 14.487388649214992
Epoch 48: loss 14.427832490922057
Epoch 49: loss 14.36732554189403
-----------Time: 0:11:07.234192, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.7885875701904297-------------


Epoch 0: loss 16.119584729193086
Epoch 1: loss 16.100272134311048
Epoch 2: loss 16.080972992511725
Epoch 3: loss 16.06170536915591
Epoch 4: loss 16.04245066707584
Epoch 5: loss 16.023204867252517
Epoch 6: loss 16.003943891354137
Epoch 7: loss 15.984622488597477
Epoch 8: loss 15.96511290940725
Epoch 9: loss 15.945244506755039
Epoch 10: loss 15.92465127687729
Epoch 11: loss 15.902888053004155
Epoch 12: loss 15.879543956820896
Epoch 13: loss 15.854467981856047
Epoch 14: loss 15.827560673983866
Epoch 15: loss 15.798885024456046
Epoch 16: loss 15.768484941703461
Epoch 17: loss 15.736374713468319
Epoch 18: loss 15.702599299608721
Epoch 19: loss 15.66717290567146
Epoch 20: loss 15.630119804457
Epoch 21: loss 15.591439180234127
Epoch 22: loss 15.551160909093678
Epoch 23: loss 15.509287293803892
Epoch 24: loss 15.465822770583882
Epoch 25: loss 15.420797460295702
Epoch 26: loss 15.374215257498413
Epoch 27: loss 15.32612297779973
Epoch 28: loss 15.276513293101905
Epoch 29: loss 15.225360005364722
Epoch 30: loss 15.172738722966637
Epoch 31: loss 15.11863168780735
Epoch 32: loss 15.063080803475476
Epoch 33: loss 15.006106001193787
Epoch 34: loss 14.947701882252945
Epoch 35: loss 14.88787580301348
Epoch 36: loss 14.82663869676284
Epoch 37: loss 14.764059517939756
Epoch 38: loss 14.700110199530531
Epoch 39: loss 14.63482621158187
Epoch 40: loss 14.568197745094485
Epoch 41: loss 14.500312584777964
Epoch 42: loss 14.431124670600164
Epoch 43: loss 14.360659478474338
Epoch 44: loss 14.288976452025205
Epoch 45: loss 14.216105035882466
Epoch 46: loss 14.142021973668525
Epoch 47: loss 14.066768893085788
Epoch 48: loss 13.990343047199591
Epoch 49: loss 13.912766577693157
-----------Time: 0:13:08.924173, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.727449655532837-------------


Epoch 0: loss 16.033931246783954
Epoch 1: loss 15.842644729842434
Epoch 2: loss 15.653148033490577
Epoch 3: loss 15.46546262136421
Epoch 4: loss 15.279419612728947
Epoch 5: loss 15.09514333995676
Epoch 6: loss 14.912374740971892
Epoch 7: loss 14.730732428502492
Epoch 8: loss 14.549051825931501
Epoch 9: loss 14.36576032353329
Epoch 10: loss 14.178096133642317
Epoch 11: loss 13.982312195971325
Epoch 12: loss 13.774958348390912
Epoch 13: loss 13.55460797372624
Epoch 14: loss 13.321522974591984
Epoch 15: loss 13.076886862889134
Epoch 16: loss 12.821607386437105
Epoch 17: loss 12.556768209147284
Epoch 18: loss 12.283193983676448
Epoch 19: loss 12.001717896225532
Epoch 20: loss 11.713298837797613
Epoch 21: loss 11.418784498844282
Epoch 22: loss 11.118906745578752
Epoch 23: loss 10.81460366785818
Epoch 24: loss 10.507000591522848
Epoch 25: loss 10.196929922824712
Epoch 26: loss 9.88539048988836
Epoch 27: loss 9.573010525021493
Epoch 28: loss 9.261071762097407
Epoch 29: loss 8.950319102045114
Epoch 30: loss 8.641796921957699
Epoch 31: loss 8.336264603419302
Epoch 32: loss 8.034751009591057
Epoch 33: loss 7.738014131735822
Epoch 34: loss 7.446987380016881
Epoch 35: loss 7.16248527436622
Epoch 36: loss 6.8851763221995865
Epoch 37: loss 6.615864959989572
Epoch 38: loss 6.355282150966567
Epoch 39: loss 6.103971776785962
Epoch 40: loss 5.862417132132335
Epoch 41: loss 5.631265866101728
Epoch 42: loss 5.410738792543894
Epoch 43: loss 5.201287652436257
Epoch 44: loss 5.003119604298445
Epoch 45: loss 4.816275327862962
Epoch 46: loss 4.640903671517717
Epoch 47: loss 4.476882519768657
Epoch 48: loss 4.323822136026417
Epoch 49: loss 4.181784410655596
-----------Time: 0:07:26.021995, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.0390303134918213-------------


Epoch 0: loss 16.033636414661167
Epoch 1: loss 15.842478361643158
Epoch 2: loss 15.6529007473998
Epoch 3: loss 15.463310522252675
Epoch 4: loss 15.264484998258059
Epoch 5: loss 15.03370984706495
Epoch 6: loss 14.757962045882175
Epoch 7: loss 14.442530412917684
Epoch 8: loss 14.092653903318139
Epoch 9: loss 13.711517396214347
Epoch 10: loss 13.302286555159021
Epoch 11: loss 12.868344768225466
Epoch 12: loss 12.412911418470888
Epoch 13: loss 11.939480448365536
Epoch 14: loss 11.451809809738167
Epoch 15: loss 10.953583590034041
Epoch 16: loss 10.448540719713186
Epoch 17: loss 9.940492661119869
Epoch 18: loss 9.433529096689478
Epoch 19: loss 8.93127902141403
Epoch 20: loss 8.437464915779119
Epoch 21: loss 7.95586580112617
Epoch 22: loss 7.489784444655459
Epoch 23: loss 7.042695280000397
Epoch 24: loss 6.617223902070179
Epoch 25: loss 6.216206381280763
Epoch 26: loss 5.84163125122676
Epoch 27: loss 5.4950122056673765
Epoch 28: loss 5.17734933482881
Epoch 29: loss 4.889020890240568
Epoch 30: loss 4.62937246288146
Epoch 31: loss 4.397950580614037
Epoch 32: loss 4.193133813521991
Epoch 33: loss 4.012887148055905
Epoch 34: loss 3.855111377663169
Epoch 35: loss 3.717604387253766
Epoch 36: loss 3.597719254021803
Epoch 37: loss 3.4937792170495037
Epoch 38: loss 3.4031814096283823
Epoch 39: loss 3.324550689336332
Epoch 40: loss 3.255651807383132
Epoch 41: loss 3.1954695277398146
Epoch 42: loss 3.1432739594761867
Epoch 43: loss 3.097050567500419
Epoch 44: loss 3.0564227308379106
Epoch 45: loss 3.0206842450359193
Epoch 46: loss 2.9891039801395864
Epoch 47: loss 2.960995202806088
Epoch 48: loss 2.9360958141370466
Epoch 49: loss 2.913742132456553
-----------Time: 0:09:19.559166, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.715693712234497-------------


Epoch 0: loss 16.033538487244066
Epoch 1: loss 15.841937628824338
Epoch 2: loss 15.635949848137711
Epoch 3: loss 15.349966991459825
Epoch 4: loss 14.967146040888958
Epoch 5: loss 14.507346458445431
Epoch 6: loss 13.980618466236722
Epoch 7: loss 13.39600010524955
Epoch 8: loss 12.762839770563405
Epoch 9: loss 12.090934670185382
Epoch 10: loss 11.39082500764765
Epoch 11: loss 10.67356908755694
Epoch 12: loss 9.949809320728827
Epoch 13: loss 9.230771559486575
Epoch 14: loss 8.527162334204627
Epoch 15: loss 7.849607628282482
Epoch 16: loss 7.2076176706379425
Epoch 17: loss 6.609670157922618
Epoch 18: loss 6.062665162882512
Epoch 19: loss 5.571270589662545
Epoch 20: loss 5.138205209071381
Epoch 21: loss 4.763493959640536
Epoch 22: loss 4.444468015040179
Epoch 23: loss 4.176891562729961
Epoch 24: loss 3.9540925179570183
Epoch 25: loss 3.770120265757409
Epoch 26: loss 3.6186708530827407
Epoch 27: loss 3.4935844877999136
Epoch 28: loss 3.390267048924432
Epoch 29: loss 3.3047805426884374
Epoch 30: loss 3.2336603327896882
Epoch 31: loss 3.1736481815916875
Epoch 32: loss 3.1233396026477536
Epoch 33: loss 3.0810123951685306
Epoch 34: loss 3.0448162477258887
Epoch 35: loss 3.013584390679671
Epoch 36: loss 2.9870461742148056
Epoch 37: loss 2.96438681042149
Epoch 38: loss 2.944772771417349
Epoch 39: loss 2.9272544332792605
Epoch 40: loss 2.912261052712466
Epoch 41: loss 2.898987207806843
Epoch 42: loss 2.8872037751963764
Epoch 43: loss 2.876678665673493
Epoch 44: loss 2.8675139028783594
Epoch 45: loss 2.859240387560817
Epoch 46: loss 2.851602054407183
Epoch 47: loss 2.845013983551739
Epoch 48: loss 2.8389512270024575
Epoch 49: loss 2.8332691478495886
-----------Time: 0:09:45.293562, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.6938509941101074-------------


Epoch 0: loss 16.033525170289412
Epoch 1: loss 15.836229688386155
Epoch 2: loss 15.543930245845974
Epoch 3: loss 15.094424379980389
Epoch 4: loss 14.52699793208352
Epoch 5: loss 13.86029838776705
Epoch 6: loss 13.11121860595421
Epoch 7: loss 12.298080318060952
Epoch 8: loss 11.439806738516895
Epoch 9: loss 10.556483437018008
Epoch 10: loss 9.668705062285397
Epoch 11: loss 8.796368041590805
Epoch 12: loss 7.959460063107705
Epoch 13: loss 7.175439142675229
Epoch 14: loss 6.459890658600036
Epoch 15: loss 5.82408518231129
Epoch 16: loss 5.274393752711568
Epoch 17: loss 4.812192077024791
Epoch 18: loss 4.432511117226277
Epoch 19: loss 4.126748204231262
Epoch 20: loss 3.8833494113151503
Epoch 21: loss 3.690855417580887
Epoch 22: loss 3.5385719319930082
Epoch 23: loss 3.4176686093365127
Epoch 24: loss 3.3211504805664966
Epoch 25: loss 3.2434061897909467
Epoch 26: loss 3.18067073698858
Epoch 27: loss 3.1291209313453314
Epoch 28: loss 3.087197539856928
Epoch 29: loss 3.0516143584782953
Epoch 30: loss 3.022682933524745
Epoch 31: loss 2.998001402263216
Epoch 32: loss 2.9772673405791963
Epoch 33: loss 2.9594814122663107
Epoch 34: loss 2.9446091621449746
Epoch 35: loss 2.931742723056841
Epoch 36: loss 2.9202024085551517
Epoch 37: loss 2.910096399741305
Epoch 38: loss 2.901682087232912
Epoch 39: loss 2.893772305322121
Epoch 40: loss 2.886898394898917
Epoch 41: loss 2.8807971144318905
Epoch 42: loss 2.8754031437126564
Epoch 43: loss 2.870312889676304
Epoch 44: loss 2.8658364383081434
Epoch 45: loss 2.8614418598022584
Epoch 46: loss 2.8571984217567508
Epoch 47: loss 2.8535664482827157
Epoch 48: loss 2.850385283568166
Epoch 49: loss 2.84670541531499
-----------Time: 0:12:01.595561, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.6980578899383545-------------


Epoch 0: loss 16.033519693014735
Epoch 1: loss 15.813409413767612
Epoch 2: loss 15.402684915188928
Epoch 3: loss 14.80497105216254
Epoch 4: loss 14.065578402352761
Epoch 5: loss 13.210565463040691
Epoch 6: loss 12.268722401422414
Epoch 7: loss 11.26928656201054
Epoch 8: loss 10.243611763066866
Epoch 9: loss 9.223598321927119
Epoch 10: loss 8.239552820681748
Epoch 11: loss 7.320269080722636
Epoch 12: loss 6.49003259833576
Epoch 13: loss 5.766375019603478
Epoch 14: loss 5.157946979111468
Epoch 15: loss 4.663939185671472
Epoch 16: loss 4.27441429508452
Epoch 17: loss 3.9735788325371466
Epoch 18: loss 3.7430013244472295
Epoch 19: loss 3.567136908886937
Epoch 20: loss 3.4315510970085064
Epoch 21: loss 3.327079251060154
Epoch 22: loss 3.2452265726865277
Epoch 23: loss 3.180977767222987
Epoch 24: loss 3.129506992412171
Epoch 25: loss 3.087821398694078
Epoch 26: loss 3.0544585915866787
Epoch 27: loss 3.026967890888793
Epoch 28: loss 3.004150296372522
Epoch 29: loss 2.985216665488342
Epoch 30: loss 2.969448962050848
Epoch 31: loss 2.9560699934801242
Epoch 32: loss 2.944891314262797
Epoch 33: loss 2.93483549612511
Epoch 34: loss 2.9261992492385374
Epoch 35: loss 2.9191027752013876
Epoch 36: loss 2.9121258763789872
Epoch 37: loss 2.906598619302373
Epoch 38: loss 2.900900021592452
Epoch 39: loss 2.896523976987182
Epoch 40: loss 2.892279944386153
Epoch 41: loss 2.8873962249491383
Epoch 42: loss 2.8842429732291057
Epoch 43: loss 2.8802440933976374
Epoch 44: loss 2.8768163086474754
Epoch 45: loss 2.8738636939405033
Epoch 46: loss 2.8709388327637466
Epoch 47: loss 2.867874014371112
Epoch 48: loss 2.8649789009358715
Epoch 49: loss 2.8622627613530205
-----------Time: 0:12:24.759712, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.7025353908538818-------------


Epoch 0: loss 15.205406363208764
Epoch 1: loss 13.30154286447331
Epoch 2: loss 10.397807940897442
Epoch 3: loss 7.259847507845
Epoch 4: loss 4.913572499906323
Epoch 5: loss 3.6098764861258816
Epoch 6: loss 3.0303452164015736
Epoch 7: loss 2.787956590883235
Epoch 8: loss 2.6761192806782184
Epoch 9: loss 2.6189079127148354
Epoch 10: loss 2.584963071858384
Epoch 11: loss 2.5634811804176607
Epoch 12: loss 2.5460376778395686
Epoch 13: loss 2.532599688289346
Epoch 14: loss 2.5189890307923775
Epoch 15: loss 2.5072722430459957
Epoch 16: loss 2.496583506132741
Epoch 17: loss 2.48495463202867
Epoch 18: loss 2.4737338393262185
Epoch 19: loss 2.463141318603078
Epoch 20: loss 2.45169483785593
Epoch 21: loss 2.440568682387706
Epoch 22: loss 2.4294390158526205
Epoch 23: loss 2.4183764022092835
Epoch 24: loss 2.4065556267864876
Epoch 25: loss 2.3942900732394596
Epoch 26: loss 2.381164433419413
Epoch 27: loss 2.367542283926534
Epoch 28: loss 2.3526032323938404
Epoch 29: loss 2.3362161702536706
Epoch 30: loss 2.317899698404724
Epoch 31: loss 2.2975465553230796
Epoch 32: loss 2.2754941063448424
Epoch 33: loss 2.2531640274554507
Epoch 34: loss 2.2328009480623656
Epoch 35: loss 2.2165190920095976
Epoch 36: loss 2.2035053333489385
Epoch 37: loss 2.1928965982177324
Epoch 38: loss 2.184132123006434
Epoch 39: loss 2.1771681472748243
Epoch 40: loss 2.1714565973038127
Epoch 41: loss 2.1667449070164015
Epoch 42: loss 2.1622464220571804
Epoch 43: loss 2.1587638530217803
Epoch 44: loss 2.155824713631776
Epoch 45: loss 2.1534230793021565
Epoch 46: loss 2.1516914591638856
Epoch 47: loss 2.149510834384315
Epoch 48: loss 2.1477565213413974
Epoch 49: loss 2.1466442498258433
-----------Time: 0:08:21.749572, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 20, rmse: 1.4696574211120605-------------


Epoch 0: loss 15.05117012587107
Epoch 1: loss 10.549304088604975
Epoch 2: loss 5.83015895505649
Epoch 3: loss 3.651178426493634
Epoch 4: loss 3.015228857677744
Epoch 5: loss 2.8250756989749246
Epoch 6: loss 2.7535840985955202
Epoch 7: loss 2.7151015502334355
Epoch 8: loss 2.6939694416659106
Epoch 9: loss 2.6741948245724236
Epoch 10: loss 2.657259892354263
Epoch 11: loss 2.6411503900192947
Epoch 12: loss 2.626758685184601
Epoch 13: loss 2.610230519630262
Epoch 14: loss 2.5979940190530457
Epoch 15: loss 2.582571109848997
Epoch 16: loss 2.5700158325468085
Epoch 17: loss 2.5552944859581967
Epoch 18: loss 2.5427881798840137
Epoch 19: loss 2.52905897487435
Epoch 20: loss 2.5148411655050054
Epoch 21: loss 2.5011357933104588
Epoch 22: loss 2.487057009781749
Epoch 23: loss 2.4725374477981807
Epoch 24: loss 2.456526562595834
Epoch 25: loss 2.440190794676656
Epoch 26: loss 2.421480965620801
Epoch 27: loss 2.400615819288765
Epoch 28: loss 2.376744919363865
Epoch 29: loss 2.3489399734497587
Epoch 30: loss 2.3167037276096356
Epoch 31: loss 2.285078488367287
Epoch 32: loss 2.2585004590463873
Epoch 33: loss 2.2375592245817053
Epoch 34: loss 2.2210444065460115
Epoch 35: loss 2.2075276664251087
Epoch 36: loss 2.196386084438602
Epoch 37: loss 2.187537320545667
Epoch 38: loss 2.180224427715341
Epoch 39: loss 2.17404019268652
Epoch 40: loss 2.1688908284743755
Epoch 41: loss 2.1646541634806478
Epoch 42: loss 2.160751454430861
Epoch 43: loss 2.1579614703003123
Epoch 44: loss 2.1550944271523256
Epoch 45: loss 2.152869890182157
Epoch 46: loss 2.150729559536146
Epoch 47: loss 2.1491968178308287
Epoch 48: loss 2.14762797013387
Epoch 49: loss 2.1463423717793333
-----------Time: 0:08:15.089513, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 50, rmse: 1.469559907913208-------------


Epoch 0: loss 14.083533322052698
Epoch 1: loss 6.949501388354299
Epoch 2: loss 3.6418189973919337
Epoch 3: loss 3.006645169706692
Epoch 4: loss 2.8665421294284426
Epoch 5: loss 2.819980217037543
Epoch 6: loss 2.7900848968836716
Epoch 7: loss 2.7698139011892304
Epoch 8: loss 2.748004863935038
Epoch 9: loss 2.7288388737911373
Epoch 10: loss 2.711740377287686
Epoch 11: loss 2.692686267752178
Epoch 12: loss 2.677526024776415
Epoch 13: loss 2.6600254493410525
Epoch 14: loss 2.644886000315867
Epoch 15: loss 2.627455048745193
Epoch 16: loss 2.6137049657096676
Epoch 17: loss 2.5975401540888985
Epoch 18: loss 2.583936454259293
Epoch 19: loss 2.568056976717409
Epoch 20: loss 2.5533379925665094
Epoch 21: loss 2.5375168118676523
Epoch 22: loss 2.521217492529074
Epoch 23: loss 2.5043494685828005
Epoch 24: loss 2.486056823196328
Epoch 25: loss 2.465869436256259
Epoch 26: loss 2.4412254988012787
Epoch 27: loss 2.412831343471388
Epoch 28: loss 2.377186420950441
Epoch 29: loss 2.336604378773117
Epoch 30: loss 2.2996254439519888
Epoch 31: loss 2.2707209434763382
Epoch 32: loss 2.247500929383884
Epoch 33: loss 2.229241813830282
Epoch 34: loss 2.2147036526694515
Epoch 35: loss 2.2026312498374763
Epoch 36: loss 2.1928013227697685
Epoch 37: loss 2.184491524526514
Epoch 38: loss 2.177864190351062
Epoch 39: loss 2.172286496271317
Epoch 40: loss 2.167230220966329
Epoch 41: loss 2.163479489888621
Epoch 42: loss 2.160200074971403
Epoch 43: loss 2.157060338110299
Epoch 44: loss 2.154602079724669
Epoch 45: loss 2.152283063590883
Epoch 46: loss 2.1503158625278087
Epoch 47: loss 2.148642148415377
Epoch 48: loss 2.147279031319486
Epoch 49: loss 2.146131866000281
-----------Time: 0:10:46.009094, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 100, rmse: 1.469472885131836-------------


Epoch 0: loss 13.090492283280224
Epoch 1: loss 5.258048949194966
Epoch 2: loss 3.2151494817671535
Epoch 3: loss 2.9412583488041193
Epoch 4: loss 2.875537360408113
Epoch 5: loss 2.8440561683492986
Epoch 6: loss 2.82121549876505
Epoch 7: loss 2.7962151214180593
Epoch 8: loss 2.7767575469076666
Epoch 9: loss 2.756157663479131
Epoch 10: loss 2.737697126814825
Epoch 11: loss 2.717707443068765
Epoch 12: loss 2.7018746420246806
Epoch 13: loss 2.6832247275642365
Epoch 14: loss 2.6663140816945234
Epoch 15: loss 2.6505754909287202
Epoch 16: loss 2.634010155541927
Epoch 17: loss 2.6180157577168237
Epoch 18: loss 2.6026712084283252
Epoch 19: loss 2.5880221627599975
Epoch 20: loss 2.571224987993038
Epoch 21: loss 2.5542958354483227
Epoch 22: loss 2.5378744484873943
Epoch 23: loss 2.5191785986492206
Epoch 24: loss 2.500170602300622
Epoch 25: loss 2.4761059898407063
Epoch 26: loss 2.4481983598060357
Epoch 27: loss 2.413024074134132
Epoch 28: loss 2.3688936443546664
Epoch 29: loss 2.3258451306022594
Epoch 30: loss 2.291421032425111
Epoch 31: loss 2.264157283915457
Epoch 32: loss 2.242872127117574
Epoch 33: loss 2.225885679275851
Epoch 34: loss 2.2116071860254816
Epoch 35: loss 2.2006648697886795
Epoch 36: loss 2.1911656741800356
Epoch 37: loss 2.183287404586985
Epoch 38: loss 2.1768662530874154
Epoch 39: loss 2.1713139304588385
Epoch 40: loss 2.16667835562692
Epoch 41: loss 2.1628549062343576
Epoch 42: loss 2.1595468448840647
Epoch 43: loss 2.156461601836841
Epoch 44: loss 2.154204206274799
Epoch 45: loss 2.1520523018588054
Epoch 46: loss 2.150038723650025
Epoch 47: loss 2.14859962644883
Epoch 48: loss 2.14703787329276
Epoch 49: loss 2.1458500346909273
-----------Time: 0:11:17.202739, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 150, rmse: 1.4694424867630005-------------


Epoch 0: loss 12.221655480689234
Epoch 1: loss 4.405188720079788
Epoch 2: loss 3.089940092821624
Epoch 3: loss 2.9353191392327083
Epoch 4: loss 2.8908945372598596
Epoch 5: loss 2.864131407545856
Epoch 6: loss 2.837998384882277
Epoch 7: loss 2.8152444953176885
Epoch 8: loss 2.795177917042266
Epoch 9: loss 2.773772451980014
Epoch 10: loss 2.7540443477713588
Epoch 11: loss 2.7329737305187414
Epoch 12: loss 2.716756357972942
Epoch 13: loss 2.6984739040407946
Epoch 14: loss 2.6809255639906486
Epoch 15: loss 2.6639005642601044
Epoch 16: loss 2.6473116870157223
Epoch 17: loss 2.6319786009417725
Epoch 18: loss 2.615457373769987
Epoch 19: loss 2.5992210396221633
Epoch 20: loss 2.5818738759439106
Epoch 21: loss 2.5652383563958026
Epoch 22: loss 2.548829859433581
Epoch 23: loss 2.5291561682046657
Epoch 24: loss 2.5069490046265206
Epoch 25: loss 2.4830714068443895
Epoch 26: loss 2.4509033784067715
Epoch 27: loss 2.4099782564965455
Epoch 28: loss 2.3616153137265115
Epoch 29: loss 2.3196912229612123
Epoch 30: loss 2.28691400880072
Epoch 31: loss 2.260752065989425
Epoch 32: loss 2.240287454313659
Epoch 33: loss 2.2236757459946466
Epoch 34: loss 2.210183448250881
Epoch 35: loss 2.199159445323135
Epoch 36: loss 2.190026439325261
Epoch 37: loss 2.182513776228699
Epoch 38: loss 2.1763798780516477
Epoch 39: loss 2.1708304845748745
Epoch 40: loss 2.166346081512787
Epoch 41: loss 2.162641480735231
Epoch 42: loss 2.1591249201983587
Epoch 43: loss 2.156405057577804
Epoch 44: loss 2.153973135240315
Epoch 45: loss 2.1518665823423064
Epoch 46: loss 2.1498091629564016
Epoch 47: loss 2.1485944155962198
Epoch 48: loss 2.1471872345269407
Epoch 49: loss 2.1458924291184442
-----------Time: 0:13:02.552788, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.0001, embedding_dim: 200, rmse: 1.4694385528564453-------------


Epoch 0: loss 6.521420283833556
Epoch 1: loss 2.54155447322302
Epoch 2: loss 2.4047406987044524
Epoch 3: loss 2.2829934772815053
Epoch 4: loss 2.177929955795448
Epoch 5: loss 2.1473666671309024
Epoch 6: loss 2.142576236061067
Epoch 7: loss 2.13988998426958
Epoch 8: loss 2.1407765987257776
Epoch 9: loss 2.139371475783167
Epoch 10: loss 2.140796252409358
Epoch 11: loss 2.140370240835083
Epoch 12: loss 2.1391449914865355
Epoch 13: loss 2.1399177175750546
Epoch 14: loss 2.1404570655177118
Epoch 15: loss 2.140608347162337
Epoch 16: loss 2.1399942485128425
Epoch 17: loss 2.1396165463573067
Epoch 18: loss 2.1396250306684856
Epoch 19: loss 2.1401259206830967
Epoch 20: loss 2.139930850983703
Epoch 21: loss 2.1405031606838585
Epoch 22: loss 2.1401784418392857
Epoch 23: loss 2.139770697711148
Epoch 24: loss 2.139726287551131
Epoch 25: loss 2.1403464876872937
Epoch 26: loss 2.1392494365532415
Epoch 27: loss 2.1408138489710242
Epoch 28: loss 2.1404804271028506
Epoch 29: loss 2.1398492259717363
Epoch 30: loss 2.1395824783972386
Epoch 31: loss 2.1399745867264017
Epoch 32: loss 2.1388853868756232
Epoch 33: loss 2.140989855458517
Epoch 34: loss 2.1400104840531693
Epoch 35: loss 2.140558004703387
Epoch 36: loss 2.139630030036426
Epoch 37: loss 2.1400601400701555
Epoch 38: loss 2.139308149883318
Epoch 39: loss 2.1398418120808937
Epoch 40: loss 2.140369706305555
Epoch 41: loss 2.1403503780328688
Epoch 42: loss 2.1393602652482326
Epoch 43: loss 2.1408637109627344
Epoch 44: loss 2.139061608477868
Epoch 45: loss 2.1404973637999367
Epoch 46: loss 2.140199128248693
Epoch 47: loss 2.1394446795890354
Epoch 48: loss 2.141061466468813
Epoch 49: loss 2.1392816870091984
-----------Time: 0:07:41.126511, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.001, embedding_dim: 20, rmse: 1.4661850929260254-------------


Epoch 0: loss 5.164333085381123
Epoch 1: loss 2.628277292713126
Epoch 2: loss 2.4646789613400157
Epoch 3: loss 2.28118659190473
Epoch 4: loss 2.1692603383782507
Epoch 5: loss 2.147494804087769
Epoch 6: loss 2.141805824538558
Epoch 7: loss 2.14108394296225
Epoch 8: loss 2.140068088911875
Epoch 9: loss 2.1402570445165483
Epoch 10: loss 2.1395644095360162
Epoch 11: loss 2.1399389357266054
Epoch 12: loss 2.139626534883583
Epoch 13: loss 2.1405648689609693
Epoch 14: loss 2.139807038491287
Epoch 15: loss 2.1408320409309094
Epoch 16: loss 2.1389572649562143
Epoch 17: loss 2.1404507832724224
Epoch 18: loss 2.140396261552289
Epoch 19: loss 2.1395858170028093
Epoch 20: loss 2.1388856173533988
Epoch 21: loss 2.1412876157283525
Epoch 22: loss 2.1396469145834995
Epoch 23: loss 2.140552312054668
Epoch 24: loss 2.139307761529401
Epoch 25: loss 2.1404813319007103
Epoch 26: loss 2.1406435625847426
Epoch 27: loss 2.1398250785063246
Epoch 28: loss 2.139523859384463
Epoch 29: loss 2.1394302572745096
Epoch 30: loss 2.1403060599213517
Epoch 31: loss 2.140859139717552
Epoch 32: loss 2.139053727214008
Epoch 33: loss 2.1401330102002123
Epoch 34: loss 2.140624694295264
Epoch 35: loss 2.1389010508401425
Epoch 36: loss 2.1400579203076178
Epoch 37: loss 2.1404199833582123
Epoch 38: loss 2.1404452689514657
Epoch 39: loss 2.1393877397807413
Epoch 40: loss 2.1397900878896916
Epoch 41: loss 2.1409319430152123
Epoch 42: loss 2.1393668336865694
Epoch 43: loss 2.140011619750153
Epoch 44: loss 2.139741621631977
Epoch 45: loss 2.1396378348416905
Epoch 46: loss 2.140354844653911
Epoch 47: loss 2.1398880264887414
Epoch 48: loss 2.140329358887582
Epoch 49: loss 2.1403038668982552
-----------Time: 0:09:04.863111, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4653136730194092-------------


Epoch 0: loss 4.626599658825528
Epoch 1: loss 2.6860032846514073
Epoch 2: loss 2.4951006976270755
Epoch 3: loss 2.2704014468932034
Epoch 4: loss 2.164803015433815
Epoch 5: loss 2.147000881139839
Epoch 6: loss 2.141467386286436
Epoch 7: loss 2.1411951129497426
Epoch 8: loss 2.140160377774957
Epoch 9: loss 2.140366516209219
Epoch 10: loss 2.139612233398919
Epoch 11: loss 2.139202425310053
Epoch 12: loss 2.1406846782187525
Epoch 13: loss 2.1401091521689914
Epoch 14: loss 2.13950834350783
Epoch 15: loss 2.1410325735142983
Epoch 16: loss 2.1393850035094384
Epoch 17: loss 2.1404459591855685
Epoch 18: loss 2.140201636084141
Epoch 19: loss 2.1393839215506283
Epoch 20: loss 2.1399293034993287
Epoch 21: loss 2.1401395845374314
Epoch 22: loss 2.140429161922513
Epoch 23: loss 2.1388747752393438
Epoch 24: loss 2.1398725535896825
Epoch 25: loss 2.140037177826413
Epoch 26: loss 2.1401221567097406
Epoch 27: loss 2.139895826432289
Epoch 28: loss 2.140535808082765
Epoch 29: loss 2.1408771156875774
Epoch 30: loss 2.1389812624538767
Epoch 31: loss 2.1399381614496296
Epoch 32: loss 2.1400058408545823
Epoch 33: loss 2.1409071640776447
Epoch 34: loss 2.1398398972128057
Epoch 35: loss 2.1398500250758783
Epoch 36: loss 2.139634504792741
Epoch 37: loss 2.140143826287878
Epoch 38: loss 2.1394491207022717
Epoch 39: loss 2.1400796834251814
Epoch 40: loss 2.1398954921406594
Epoch 41: loss 2.140819732814849
Epoch 42: loss 2.1402455153123006
Epoch 43: loss 2.1396087130299724
Epoch 44: loss 2.1395417286884273
Epoch 45: loss 2.1399520312462768
Epoch 46: loss 2.1409684903210695
Epoch 47: loss 2.1395029935777155
Epoch 48: loss 2.1403601006232504
Epoch 49: loss 2.1389273863697
-----------Time: 0:10:08.206393, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.001, embedding_dim: 100, rmse: 1.4661716222763062-------------


Epoch 0: loss 4.432276172959461
Epoch 1: loss 2.7200066698446683
Epoch 2: loss 2.510565232556433
Epoch 3: loss 2.2610322425325258
Epoch 4: loss 2.1635370969513046
Epoch 5: loss 2.146901259090409
Epoch 6: loss 2.1415517082870363
Epoch 7: loss 2.140464707747175
Epoch 8: loss 2.140676825898312
Epoch 9: loss 2.1399028531360265
Epoch 10: loss 2.139765179176704
Epoch 11: loss 2.1401340057825253
Epoch 12: loss 2.1398971265201143
Epoch 13: loss 2.1400201413344924
Epoch 14: loss 2.1404272616071633
Epoch 15: loss 2.139221932234616
Epoch 16: loss 2.140065369008351
Epoch 17: loss 2.1402032280370165
Epoch 18: loss 2.1406554086108516
Epoch 19: loss 2.1401551523696085
Epoch 20: loss 2.13930407492216
Epoch 21: loss 2.140107451313666
Epoch 22: loss 2.140011588894459
Epoch 23: loss 2.140033937233048
Epoch 24: loss 2.139461330190309
Epoch 25: loss 2.1409910228214812
Epoch 26: loss 2.139853067181563
Epoch 27: loss 2.139799934615902
Epoch 28: loss 2.1401323763944324
Epoch 29: loss 2.139465875279456
Epoch 30: loss 2.1399024606334445
Epoch 31: loss 2.139644854965905
Epoch 32: loss 2.140107036803714
Epoch 33: loss 2.1407522295336805
Epoch 34: loss 2.1402907398422495
Epoch 35: loss 2.140229614647009
Epoch 36: loss 2.1391783437046943
Epoch 37: loss 2.1405001208793863
Epoch 38: loss 2.139890535361356
Epoch 39: loss 2.13984576780035
Epoch 40: loss 2.1398779209571535
Epoch 41: loss 2.1401063476716002
Epoch 42: loss 2.140097341017596
Epoch 43: loss 2.1398644612300908
Epoch 44: loss 2.1407108091996894
Epoch 45: loss 2.139914613077341
Epoch 46: loss 2.1400468657689617
Epoch 47: loss 2.1395091500341925
Epoch 48: loss 2.139495274824834
Epoch 49: loss 2.141461369199167
-----------Time: 0:11:50.787834, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4646002054214478-------------


Epoch 0: loss 4.335519258842448
Epoch 1: loss 2.7430333335351658
Epoch 2: loss 2.5181579307540076
Epoch 3: loss 2.2570056750308956
Epoch 4: loss 2.162887520022597
Epoch 5: loss 2.146308690293319
Epoch 6: loss 2.141810922923459
Epoch 7: loss 2.140236375123149
Epoch 8: loss 2.14054966008553
Epoch 9: loss 2.140444117988692
Epoch 10: loss 2.1397810228626777
Epoch 11: loss 2.1407019226949067
Epoch 12: loss 2.1393758024515956
Epoch 13: loss 2.14027224603459
Epoch 14: loss 2.1402534665848783
Epoch 15: loss 2.140008179178181
Epoch 16: loss 2.1395318872128706
Epoch 17: loss 2.140378840012418
Epoch 18: loss 2.1401486250289827
Epoch 19: loss 2.1389215698444435
Epoch 20: loss 2.140177252015191
Epoch 21: loss 2.1408147407394833
Epoch 22: loss 2.139550695865293
Epoch 23: loss 2.140152205488745
Epoch 24: loss 2.1409658254197654
Epoch 25: loss 2.1390462632798966
Epoch 26: loss 2.14061252934055
Epoch 27: loss 2.139622868176969
Epoch 28: loss 2.1396838889101266
Epoch 29: loss 2.1405025369904496
Epoch 30: loss 2.1400995032174097
Epoch 31: loss 2.1394871204945627
Epoch 32: loss 2.1403298225332823
Epoch 33: loss 2.1406165610972536
Epoch 34: loss 2.139215242836772
Epoch 35: loss 2.1399095881367716
Epoch 36: loss 2.1403577883908644
Epoch 37: loss 2.139449124850936
Epoch 38: loss 2.1401322816881945
Epoch 39: loss 2.140025544160083
Epoch 40: loss 2.1404610466153287
Epoch 41: loss 2.13963756294209
Epoch 42: loss 2.140064908765852
Epoch 43: loss 2.140409021321816
Epoch 44: loss 2.1398602153958026
Epoch 45: loss 2.140077155786341
Epoch 46: loss 2.139576319834018
Epoch 47: loss 2.1401671608418726
Epoch 48: loss 2.1398685263705812
Epoch 49: loss 2.1404931979895028
-----------Time: 0:12:42.883251, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 256, learning_rate: 0.001, embedding_dim: 200, rmse: 1.4654115438461304-------------


Epoch 0: loss 16.12921093831314
Epoch 1: loss 16.129211570465934
Epoch 2: loss 16.129207020417837
Epoch 3: loss 16.129209248769406
Epoch 4: loss 16.129205526898517
Epoch 5: loss 16.129203583767648
Epoch 6: loss 16.129201906151327
Epoch 7: loss 16.129209015407014
Epoch 8: loss 16.129201043229052
Epoch 9: loss 16.129197850831506
Epoch 10: loss 16.129195047889862
Epoch 11: loss 16.129197698368074
Epoch 12: loss 16.129193121872238
Epoch 13: loss 16.12919436439734
Epoch 14: loss 16.12919137580295
Epoch 15: loss 16.129188774071547
Epoch 16: loss 16.129184231283613
Epoch 17: loss 16.12918950112505
Epoch 18: loss 16.129187484873967
Epoch 19: loss 16.129186068104946
Epoch 20: loss 16.129184881068234
Epoch 21: loss 16.12918947260298
Epoch 22: loss 16.12918478979761
Epoch 23: loss 16.12918070491855
Epoch 24: loss 16.1291848084666
Epoch 25: loss 16.129182041307192
Epoch 26: loss 16.129178890396293
Epoch 27: loss 16.129179092125117
Epoch 28: loss 16.129180530156045
Epoch 29: loss 16.129171423836855
Epoch 30: loss 16.12917673516494
Epoch 31: loss 16.1291738954039
Epoch 32: loss 16.12917211095946
Epoch 33: loss 16.129171661866497
Epoch 34: loss 16.129169408104353
Epoch 35: loss 16.129169926168867
Epoch 36: loss 16.129161049063406
Epoch 37: loss 16.129165206025515
Epoch 38: loss 16.1291643934058
Epoch 39: loss 16.129159511983104
Epoch 40: loss 16.129160811033763
Epoch 41: loss 16.129160367645213
Epoch 42: loss 16.129159655112037
Epoch 43: loss 16.129157060122218
Epoch 44: loss 16.1291550786162
Epoch 45: loss 16.129152569711177
Epoch 46: loss 16.129153307654924
Epoch 47: loss 16.129153500049252
Epoch 48: loss 16.12914990211972
Epoch 49: loss 16.12915160462803
-----------Time: 0:05:14.476095, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017196178436279-------------


Epoch 0: loss 16.129205273829964
Epoch 1: loss 16.129206753866125
Epoch 2: loss 16.12920383165037
Epoch 3: loss 16.12920280381867
Epoch 4: loss 16.12920360917822
Epoch 5: loss 16.12919690752885
Epoch 6: loss 16.129201393791227
Epoch 7: loss 16.129196133284285
Epoch 8: loss 16.12919466258262
Epoch 9: loss 16.129195183758636
Epoch 10: loss 16.129190318412014
Epoch 11: loss 16.12919458738807
Epoch 12: loss 16.12919064926803
Epoch 13: loss 16.129183156779437
Epoch 14: loss 16.129189139672633
Epoch 15: loss 16.129184228690697
Epoch 16: loss 16.129186665512673
Epoch 17: loss 16.12918352756635
Epoch 18: loss 16.129184013997296
Epoch 19: loss 16.129176714421614
Epoch 20: loss 16.12917879964425
Epoch 21: loss 16.129175144151997
Epoch 22: loss 16.129181713562676
Epoch 23: loss 16.129178274838157
Epoch 24: loss 16.129174869302954
Epoch 25: loss 16.12917653447328
Epoch 26: loss 16.129173401712787
Epoch 27: loss 16.12917132789898
Epoch 28: loss 16.1291730822656
Epoch 29: loss 16.12916685563835
Epoch 30: loss 16.129164397554465
Epoch 31: loss 16.12916469625833
Epoch 32: loss 16.12916332356887
Epoch 33: loss 16.129165118903554
Epoch 34: loss 16.12916008398026
Epoch 35: loss 16.12916474500514
Epoch 36: loss 16.129163026939338
Epoch 37: loss 16.1291592298739
Epoch 38: loss 16.12915979720381
Epoch 39: loss 16.129157347417255
Epoch 40: loss 16.129151527877735
Epoch 41: loss 16.12915151543174
Epoch 42: loss 16.1291541244233
Epoch 43: loss 16.129156004805616
Epoch 44: loss 16.12914712095857
Epoch 45: loss 16.129149601341524
Epoch 46: loss 16.129151287255176
Epoch 47: loss 16.129145588026933
Epoch 48: loss 16.129139906430517
Epoch 49: loss 16.129144466850278
-----------Time: 0:03:44.331328, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129193303894905
Epoch 1: loss 16.12919233051443
Epoch 2: loss 16.129193760766615
Epoch 3: loss 16.12919358289261
Epoch 4: loss 16.12918800760573
Epoch 5: loss 16.129187972860663
Epoch 6: loss 16.129187599480833
Epoch 7: loss 16.129183654100625
Epoch 8: loss 16.129183098698128
Epoch 9: loss 16.129182935344453
Epoch 10: loss 16.129181389966824
Epoch 11: loss 16.12917796939271
Epoch 12: loss 16.12917979428663
Epoch 13: loss 16.129174052534577
Epoch 14: loss 16.129175909062063
Epoch 15: loss 16.129175389960384
Epoch 16: loss 16.129170748123077
Epoch 17: loss 16.12917130715566
Epoch 18: loss 16.129174033347002
Epoch 19: loss 16.12916975088778
Epoch 20: loss 16.129171402574947
Epoch 21: loss 16.12916911614207
Epoch 22: loss 16.129168559702407
Epoch 23: loss 16.129164408963295
Epoch 24: loss 16.129163714580525
Epoch 25: loss 16.12916028156042
Epoch 26: loss 16.12916313273029
Epoch 27: loss 16.129160873263736
Epoch 28: loss 16.129164437485365
Epoch 29: loss 16.129153631250777
Epoch 30: loss 16.129155032462307
Epoch 31: loss 16.129156535834706
Epoch 32: loss 16.12915176642596
Epoch 33: loss 16.129152894862777
Epoch 34: loss 16.129153698147995
Epoch 35: loss 16.12915023971732
Epoch 36: loss 16.12914768828848
Epoch 37: loss 16.129146370050243
Epoch 38: loss 16.129147416550936
Epoch 39: loss 16.129147739109623
Epoch 40: loss 16.129139842644797
Epoch 41: loss 16.129145415338762
Epoch 42: loss 16.129142521126493
Epoch 43: loss 16.129142614990034
Epoch 44: loss 16.12914109709731
Epoch 45: loss 16.12913513702177
Epoch 46: loss 16.12913103606663
Epoch 47: loss 16.12913405110876
Epoch 48: loss 16.12913349622485
Epoch 49: loss 16.129136362433627
-----------Time: 0:05:12.838363, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017182350158691-------------


Epoch 0: loss 16.1291740027506
Epoch 1: loss 16.129170823317626
Epoch 2: loss 16.129167419856756
Epoch 3: loss 16.12917424596607
Epoch 4: loss 16.129165833511063
Epoch 5: loss 16.12916654500707
Epoch 6: loss 16.129165796691662
Epoch 7: loss 16.129162110084422
Epoch 8: loss 16.129162882254654
Epoch 9: loss 16.129157915265747
Epoch 10: loss 16.12915762122913
Epoch 11: loss 16.129158908352377
Epoch 12: loss 16.129153979738618
Epoch 13: loss 16.12915826582792
Epoch 14: loss 16.129155344130748
Epoch 15: loss 16.129152739806432
Epoch 16: loss 16.129150144298027
Epoch 17: loss 16.12915241828491
Epoch 18: loss 16.129145817240662
Epoch 19: loss 16.129144361577907
Epoch 20: loss 16.129148575584157
Epoch 21: loss 16.12914784127049
Epoch 22: loss 16.12914719822745
Epoch 23: loss 16.12914668586735
Epoch 24: loss 16.129141409802916
Epoch 25: loss 16.129140702974155
Epoch 26: loss 16.129138878080234
Epoch 27: loss 16.129140272031602
Epoch 28: loss 16.129134747565864
Epoch 29: loss 16.12913711074904
Epoch 30: loss 16.12913363209362
Epoch 31: loss 16.12913396243105
Epoch 32: loss 16.12913588896726
Epoch 33: loss 16.129128695701112
Epoch 34: loss 16.12912419439983
Epoch 35: loss 16.129123742195365
Epoch 36: loss 16.129127678241076
Epoch 37: loss 16.12912922050721
Epoch 38: loss 16.129128580575667
Epoch 39: loss 16.12912342274818
Epoch 40: loss 16.129122626723124
Epoch 41: loss 16.12912084124152
Epoch 42: loss 16.12911991038486
Epoch 43: loss 16.12911857295905
Epoch 44: loss 16.129117016691172
Epoch 45: loss 16.129114567423205
Epoch 46: loss 16.129114689808816
Epoch 47: loss 16.129112343738882
Epoch 48: loss 16.129116573821207
Epoch 49: loss 16.129109538722908
-----------Time: 0:06:22.899378, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12917720448264
Epoch 1: loss 16.12917590854348
Epoch 2: loss 16.129174108541548
Epoch 3: loss 16.12917544804169
Epoch 4: loss 16.129173205169796
Epoch 5: loss 16.129175458413354
Epoch 6: loss 16.129166940685977
Epoch 7: loss 16.129166557453065
Epoch 8: loss 16.129167935328358
Epoch 9: loss 16.12916642780729
Epoch 10: loss 16.129166766442054
Epoch 11: loss 16.129168388569983
Epoch 12: loss 16.129161303687706
Epoch 13: loss 16.12915660843634
Epoch 14: loss 16.129163050794162
Epoch 15: loss 16.12915878700393
Epoch 16: loss 16.12915576833172
Epoch 17: loss 16.129157769025312
Epoch 18: loss 16.12915659028593
Epoch 19: loss 16.129148628479633
Epoch 20: loss 16.129150561757424
Epoch 21: loss 16.129146274630955
Epoch 22: loss 16.12915267446496
Epoch 23: loss 16.12914120962984
Epoch 24: loss 16.129147035392357
Epoch 25: loss 16.129144900385747
Epoch 26: loss 16.1291431745412
Epoch 27: loss 16.129145497793473
Epoch 28: loss 16.129138456472177
Epoch 29: loss 16.129138063904772
Epoch 30: loss 16.129137389746745
Epoch 31: loss 16.129136494153734
Epoch 32: loss 16.129141509889454
Epoch 33: loss 16.129138739618547
Epoch 34: loss 16.12913535482667
Epoch 35: loss 16.12913119319731
Epoch 36: loss 16.12913165162477
Epoch 37: loss 16.12913221739893
Epoch 38: loss 16.12912382776158
Epoch 39: loss 16.12912279526263
Epoch 40: loss 16.129129379712218
Epoch 41: loss 16.129127142026153
Epoch 42: loss 16.129118888257572
Epoch 43: loss 16.12912285178819
Epoch 44: loss 16.129123556542616
Epoch 45: loss 16.129121962418175
Epoch 46: loss 16.12911697105586
Epoch 47: loss 16.129122012202153
Epoch 48: loss 16.129117167598856
Epoch 49: loss 16.129119596642084
-----------Time: 0:07:45.450493, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017181873321533-------------


Epoch 0: loss 16.12939013833258
Epoch 1: loss 16.12937804549333
Epoch 2: loss 16.129365298720792
Epoch 3: loss 16.12935121711536
Epoch 4: loss 16.12933889869246
Epoch 5: loss 16.129327831610574
Epoch 6: loss 16.129315744994322
Epoch 7: loss 16.12930088032841
Epoch 8: loss 16.129291186973152
Epoch 9: loss 16.129281290851903
Epoch 10: loss 16.12926240561124
Epoch 11: loss 16.12925369497095
Epoch 12: loss 16.12924395131313
Epoch 13: loss 16.129231905664938
Epoch 14: loss 16.129216486115116
Epoch 15: loss 16.129198865698626
Epoch 16: loss 16.12919026136787
Epoch 17: loss 16.129176973713165
Epoch 18: loss 16.129169910611374
Epoch 19: loss 16.129154868071463
Epoch 20: loss 16.129141029162923
Epoch 21: loss 16.12913469518897
Epoch 22: loss 16.129112379521114
Epoch 23: loss 16.129107313482834
Epoch 24: loss 16.129091719170507
Epoch 25: loss 16.12908054214901
Epoch 26: loss 16.129061888196404
Epoch 27: loss 16.129054973927964
Epoch 28: loss 16.129041388087977
Epoch 29: loss 16.12902695747612
Epoch 30: loss 16.129016918225933
Epoch 31: loss 16.129008864111846
Epoch 32: loss 16.12898955467021
Epoch 33: loss 16.128982830721764
Epoch 34: loss 16.12897068550562
Epoch 35: loss 16.12895437554862
Epoch 36: loss 16.12894354857071
Epoch 37: loss 16.128930732308035
Epoch 38: loss 16.128919596254597
Epoch 39: loss 16.12890581853886
Epoch 40: loss 16.128890440994272
Epoch 41: loss 16.12887914884732
Epoch 42: loss 16.128868065170778
Epoch 43: loss 16.128855987370436
Epoch 44: loss 16.128842580441614
Epoch 45: loss 16.1288300353979
Epoch 46: loss 16.128825723379446
Epoch 47: loss 16.128807096911746
Epoch 48: loss 16.12879626371084
Epoch 49: loss 16.128778044849454
-----------Time: 0:03:25.009946, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017126560211182-------------


Epoch 0: loss 16.129158318723395
Epoch 1: loss 16.129153441449365
Epoch 2: loss 16.129134895362046
Epoch 3: loss 16.129121341674207
Epoch 4: loss 16.12910631469179
Epoch 5: loss 16.129100366025078
Epoch 6: loss 16.12908973351583
Epoch 7: loss 16.129078356839834
Epoch 8: loss 16.12906076909408
Epoch 9: loss 16.129043732083577
Epoch 10: loss 16.12903229836344
Epoch 11: loss 16.129023283314872
Epoch 12: loss 16.129009496264644
Epoch 13: loss 16.128999052001056
Epoch 14: loss 16.128993533758315
Epoch 15: loss 16.1289721282038
Epoch 16: loss 16.128965626727503
Epoch 17: loss 16.128951314352594
Epoch 18: loss 16.12893645331677
Epoch 19: loss 16.128926841897638
Epoch 20: loss 16.12891007714326
Epoch 21: loss 16.12890056632925
Epoch 22: loss 16.12888470857671
Epoch 23: loss 16.128870185657064
Epoch 24: loss 16.128865620051474
Epoch 25: loss 16.128855493160742
Epoch 26: loss 16.128840801183006
Epoch 27: loss 16.128827657175815
Epoch 28: loss 16.128815020342895
Epoch 29: loss 16.128798511249983
Epoch 30: loss 16.128781975709334
Epoch 31: loss 16.12877133179126
Epoch 32: loss 16.128762478540615
Epoch 33: loss 16.12875124706789
Epoch 34: loss 16.12874241248624
Epoch 35: loss 16.128725434075626
Epoch 36: loss 16.128713091797906
Epoch 37: loss 16.128697684694075
Epoch 38: loss 16.12868966117639
Epoch 39: loss 16.128677860299423
Epoch 40: loss 16.12866286443199
Epoch 41: loss 16.12864888032018
Epoch 42: loss 16.128640660778085
Epoch 43: loss 16.128629372779802
Epoch 44: loss 16.1286169501217
Epoch 45: loss 16.12859820645422
Epoch 46: loss 16.128586327271204
Epoch 47: loss 16.128580770134732
Epoch 48: loss 16.128566517915463
Epoch 49: loss 16.128550470361507
-----------Time: 0:04:18.557209, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017118453979492-------------


Epoch 0: loss 16.1291697135498
Epoch 1: loss 16.129164796344867
Epoch 2: loss 16.129154385789185
Epoch 3: loss 16.12913549899277
Epoch 4: loss 16.129123452307415
Epoch 5: loss 16.12911181270979
Epoch 6: loss 16.12910814477154
Epoch 7: loss 16.12908266211671
Epoch 8: loss 16.129070315690324
Epoch 9: loss 16.12906366849218
Epoch 10: loss 16.129046025258035
Epoch 11: loss 16.129036604677484
Epoch 12: loss 16.1290231936
Epoch 13: loss 16.129010655816447
Epoch 14: loss 16.12900467603475
Epoch 15: loss 16.128991153461897
Epoch 16: loss 16.128977069782135
Epoch 17: loss 16.12895446111483
Epoch 18: loss 16.12895200614245
Epoch 19: loss 16.128937032574086
Epoch 20: loss 16.12892754924498
Epoch 21: loss 16.128915158739034
Epoch 22: loss 16.128904791225747
Epoch 23: loss 16.12888892413871
Epoch 24: loss 16.128874956621562
Epoch 25: loss 16.128859884522413
Epoch 26: loss 16.12884743126791
Epoch 27: loss 16.128841969550727
Epoch 28: loss 16.12882444507211
Epoch 29: loss 16.12881628672282
Epoch 30: loss 16.12880161289549
Epoch 31: loss 16.12878857416067
Epoch 32: loss 16.12877204069435
Epoch 33: loss 16.128768964459418
Epoch 34: loss 16.128752577233534
Epoch 35: loss 16.12874037704758
Epoch 36: loss 16.12872810685291
Epoch 37: loss 16.128710347974735
Epoch 38: loss 16.1287004912658
Epoch 39: loss 16.12868682660118
Epoch 40: loss 16.1286776860555
Epoch 41: loss 16.128665244728406
Epoch 42: loss 16.12865348741242
Epoch 43: loss 16.12864081220435
Epoch 44: loss 16.12862619334683
Epoch 45: loss 16.1286173152042
Epoch 46: loss 16.128600522964916
Epoch 47: loss 16.128592225635355
Epoch 48: loss 16.128576469525104
Epoch 49: loss 16.128569605559225
-----------Time: 0:05:48.200629, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017109394073486-------------


Epoch 0: loss 16.129169356246045
Epoch 1: loss 16.129156405670347
Epoch 2: loss 16.129149919751548
Epoch 3: loss 16.12913788966085
Epoch 4: loss 16.129125656804163
Epoch 5: loss 16.129105297231753
Epoch 6: loss 16.129099281149237
Epoch 7: loss 16.129085068860867
Epoch 8: loss 16.12907557567868
Epoch 9: loss 16.12905828300671
Epoch 10: loss 16.129047813851138
Epoch 11: loss 16.12903556802987
Epoch 12: loss 16.129023258422883
Epoch 13: loss 16.12901010041395
Epoch 14: loss 16.129001516307937
Epoch 15: loss 16.128982648699097
Epoch 16: loss 16.12897138507422
Epoch 17: loss 16.12896343986258
Epoch 18: loss 16.128945982281184
Epoch 19: loss 16.12893520768017
Epoch 20: loss 16.12892144137326
Epoch 21: loss 16.128907469707446
Epoch 22: loss 16.128894981707873
Epoch 23: loss 16.128888668477245
Epoch 24: loss 16.128870599486376
Epoch 25: loss 16.128863773895645
Epoch 26: loss 16.12884811476043
Epoch 27: loss 16.128836055110497
Epoch 28: loss 16.12882152389352
Epoch 29: loss 16.12881093598242
Epoch 30: loss 16.128796071835094
Epoch 31: loss 16.128787510546736
Epoch 32: loss 16.128776921598465
Epoch 33: loss 16.12876047162403
Epoch 34: loss 16.128749339200674
Epoch 35: loss 16.12873600072482
Epoch 36: loss 16.128722675213545
Epoch 37: loss 16.128711754890677
Epoch 38: loss 16.12870328902161
Epoch 39: loss 16.12868734777719
Epoch 40: loss 16.128672930129913
Epoch 41: loss 16.12866341983449
Epoch 42: loss 16.12865158213812
Epoch 43: loss 16.128639406844155
Epoch 44: loss 16.128623857129973
Epoch 45: loss 16.128611576563642
Epoch 46: loss 16.128597981389156
Epoch 47: loss 16.128589973428962
Epoch 48: loss 16.128575591045337
Epoch 49: loss 16.128564946090094
-----------Time: 0:06:45.404215, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017111301422119-------------


Epoch 0: loss 16.129170777682315
Epoch 1: loss 16.12915908933788
Epoch 2: loss 16.129147635393004
Epoch 3: loss 16.129132847477393
Epoch 4: loss 16.12912094029089
Epoch 5: loss 16.129109725931404
Epoch 6: loss 16.129097684950462
Epoch 7: loss 16.12909065607516
Epoch 8: loss 16.12907588164271
Epoch 9: loss 16.129057680931734
Epoch 10: loss 16.129051061737076
Epoch 11: loss 16.129032900438418
Epoch 12: loss 16.129026576317543
Epoch 13: loss 16.12901298010589
Epoch 14: loss 16.128996800313246
Epoch 15: loss 16.128987997365165
Epoch 16: loss 16.128972737020355
Epoch 17: loss 16.12895937313393
Epoch 18: loss 16.128953173473
Epoch 19: loss 16.12893529687646
Epoch 20: loss 16.128926017350512
Epoch 21: loss 16.12891240091412
Epoch 22: loss 16.12890337756822
Epoch 23: loss 16.128887627680967
Epoch 24: loss 16.12887596734002
Epoch 25: loss 16.12886532290336
Epoch 26: loss 16.12884995417468
Epoch 27: loss 16.128841160561095
Epoch 28: loss 16.128830419149395
Epoch 29: loss 16.128813034688214
Epoch 30: loss 16.128798618596687
Epoch 31: loss 16.128790112796718
Epoch 32: loss 16.128777486854045
Epoch 33: loss 16.128762543882086
Epoch 34: loss 16.128749657610694
Epoch 35: loss 16.128741708768978
Epoch 36: loss 16.128727623533464
Epoch 37: loss 16.128713194477356
Epoch 38: loss 16.128706964738605
Epoch 39: loss 16.128686946912456
Epoch 40: loss 16.128676384411925
Epoch 41: loss 16.128666148100162
Epoch 42: loss 16.128649631228505
Epoch 43: loss 16.12863760424931
Epoch 44: loss 16.12862535116788
Epoch 45: loss 16.12861019350252
Epoch 46: loss 16.12860695080241
Epoch 47: loss 16.128591005409326
Epoch 48: loss 16.12857722924934
Epoch 49: loss 16.128570891126724
-----------Time: 0:06:26.717686, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017111778259277-------------


Epoch 0: loss 16.129196646681553
Epoch 1: loss 16.129067500821268
Epoch 2: loss 16.128951069581372
Epoch 3: loss 16.128823434353652
Epoch 4: loss 16.128700339839536
Epoch 5: loss 16.128579277133994
Epoch 6: loss 16.128451968095042
Epoch 7: loss 16.128327192853106
Epoch 8: loss 16.128207722197672
Epoch 9: loss 16.128082012468994
Epoch 10: loss 16.12795602840986
Epoch 11: loss 16.127829251437163
Epoch 12: loss 16.127708880002892
Epoch 13: loss 16.127581746245028
Epoch 14: loss 16.12746541249875
Epoch 15: loss 16.127335635004258
Epoch 16: loss 16.12721552545445
Epoch 17: loss 16.127088125144873
Epoch 18: loss 16.126968128127597
Epoch 19: loss 16.12684162341103
Epoch 20: loss 16.126721559496534
Epoch 21: loss 16.126594049247338
Epoch 22: loss 16.12646995283068
Epoch 23: loss 16.126347108792196
Epoch 24: loss 16.126217019110676
Epoch 25: loss 16.126099063755053
Epoch 26: loss 16.12596782022614
Epoch 27: loss 16.125854404028374
Epoch 28: loss 16.125728764826995
Epoch 29: loss 16.125605676017294
Epoch 30: loss 16.125483073638534
Epoch 31: loss 16.12535292639429
Epoch 32: loss 16.1252295886647
Epoch 33: loss 16.125106479630254
Epoch 34: loss 16.124981520291318
Epoch 35: loss 16.12485801039214
Epoch 36: loss 16.124739689435437
Epoch 37: loss 16.12460711107363
Epoch 38: loss 16.124486237132334
Epoch 39: loss 16.124360916859562
Epoch 40: loss 16.12423658604334
Epoch 41: loss 16.124115812707167
Epoch 42: loss 16.12398977315964
Epoch 43: loss 16.12386855487917
Epoch 44: loss 16.123742764251528
Epoch 45: loss 16.123619170341886
Epoch 46: loss 16.123495832612296
Epoch 47: loss 16.12337054034301
Epoch 48: loss 16.12324665654542
Epoch 49: loss 16.123122944917416
-----------Time: 0:04:03.317507, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016420364379883-------------


Epoch 0: loss 16.12909691278023
Epoch 1: loss 16.12897599424079
Epoch 2: loss 16.12884950404455
Epoch 3: loss 16.12872877012069
Epoch 4: loss 16.128601387961524
Epoch 5: loss 16.128477996817875
Epoch 6: loss 16.128355063064518
Epoch 7: loss 16.12823212931116
Epoch 8: loss 16.128101166335703
Epoch 9: loss 16.127984297411672
Epoch 10: loss 16.127857595114943
Epoch 11: loss 16.12773774226377
Epoch 12: loss 16.127610702888028
Epoch 13: loss 16.12748814095875
Epoch 14: loss 16.127368160017554
Epoch 15: loss 16.127237773187915
Epoch 16: loss 16.12711313900058
Epoch 17: loss 16.126996324527777
Epoch 18: loss 16.126867324907927
Epoch 19: loss 16.12674438130149
Epoch 20: loss 16.12662313968478
Epoch 21: loss 16.126498026326665
Epoch 22: loss 16.12637149723669
Epoch 23: loss 16.126245138760556
Epoch 24: loss 16.126125281242135
Epoch 25: loss 16.12600114437599
Epoch 26: loss 16.125878293595928
Epoch 27: loss 16.125751103312506
Epoch 28: loss 16.1256254158829
Epoch 29: loss 16.125506860526635
Epoch 30: loss 16.125379655204306
Epoch 31: loss 16.125253337696236
Epoch 32: loss 16.125134881389343
Epoch 33: loss 16.125011478836864
Epoch 34: loss 16.124888489076532
Epoch 35: loss 16.124762858691067
Epoch 36: loss 16.12463524679959
Epoch 37: loss 16.12450993586131
Epoch 38: loss 16.124388423544225
Epoch 39: loss 16.124261381056986
Epoch 40: loss 16.124139149983723
Epoch 41: loss 16.12401471700663
Epoch 42: loss 16.12389333589107
Epoch 43: loss 16.123771867653545
Epoch 44: loss 16.123639282550158
Epoch 45: loss 16.12352172961502
Epoch 46: loss 16.123392718586345
Epoch 47: loss 16.12327279883795
Epoch 48: loss 16.123146047275828
Epoch 49: loss 16.12302405578796
-----------Time: 0:05:06.874456, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.0164361000061035-------------


Epoch 0: loss 16.129126810651552
Epoch 1: loss 16.128999285882035
Epoch 2: loss 16.128870693349917
Epoch 3: loss 16.128748830989238
Epoch 4: loss 16.128627326969482
Epoch 5: loss 16.128504033319455
Epoch 6: loss 16.128379218665202
Epoch 7: loss 16.1282512484328
Epoch 8: loss 16.12813152004157
Epoch 9: loss 16.1280065218089
Epoch 10: loss 16.127881118562833
Epoch 11: loss 16.12775798100632
Epoch 12: loss 16.12763248234096
Epoch 13: loss 16.12751307910133
Epoch 14: loss 16.127387075854617
Epoch 15: loss 16.127265687997475
Epoch 16: loss 16.127138398146098
Epoch 17: loss 16.127017816167086
Epoch 18: loss 16.126890855097393
Epoch 19: loss 16.1267681894515
Epoch 20: loss 16.1266460792081
Epoch 21: loss 16.12651837345308
Epoch 22: loss 16.12638820546551
Epoch 23: loss 16.12627006082706
Epoch 24: loss 16.126149715840526
Epoch 25: loss 16.126024071971898
Epoch 26: loss 16.125902754642055
Epoch 27: loss 16.12577627481748
Epoch 28: loss 16.125653146076875
Epoch 29: loss 16.125529406445384
Epoch 30: loss 16.125406128871436
Epoch 31: loss 16.12528356694216
Epoch 32: loss 16.125154511833916
Epoch 33: loss 16.125032471599237
Epoch 34: loss 16.124906914333987
Epoch 35: loss 16.124783080838956
Epoch 36: loss 16.12466038355949
Epoch 37: loss 16.124536584290944
Epoch 38: loss 16.124407938344767
Epoch 39: loss 16.124283942014642
Epoch 40: loss 16.12415728276031
Epoch 41: loss 16.124038897499304
Epoch 42: loss 16.12391787939191
Epoch 43: loss 16.123788941483447
Epoch 44: loss 16.123669087595108
Epoch 45: loss 16.123542419524863
Epoch 46: loss 16.123418454309725
Epoch 47: loss 16.123294120382006
Epoch 48: loss 16.12317016605711
Epoch 49: loss 16.123046031783883
-----------Time: 0:05:51.577212, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016418933868408-------------


Epoch 0: loss 16.129113891709427
Epoch 1: loss 16.128990226753903
Epoch 2: loss 16.12887061867395
Epoch 3: loss 16.128744651209473
Epoch 4: loss 16.128621848657637
Epoch 5: loss 16.12849480668898
Epoch 6: loss 16.128369238014905
Epoch 7: loss 16.128245874874743
Epoch 8: loss 16.128118553389797
Epoch 9: loss 16.12799852266462
Epoch 10: loss 16.127870856321916
Epoch 11: loss 16.127748745041348
Epoch 12: loss 16.12762749201581
Epoch 13: loss 16.127502163964294
Epoch 14: loss 16.1273757152547
Epoch 15: loss 16.127250698871624
Epoch 16: loss 16.127134738505177
Epoch 17: loss 16.127004768616352
Epoch 18: loss 16.126885034520708
Epoch 19: loss 16.126758270512592
Epoch 20: loss 16.12663985776668
Epoch 21: loss 16.126505384502234
Epoch 22: loss 16.126385087225344
Epoch 23: loss 16.126263445262484
Epoch 24: loss 16.126134054630977
Epoch 25: loss 16.126014036351794
Epoch 26: loss 16.125892192141528
Epoch 27: loss 16.125766553458732
Epoch 28: loss 16.12563985168059
Epoch 29: loss 16.125513062780485
Epoch 30: loss 16.12539320163198
Epoch 31: loss 16.12527020876015
Epoch 32: loss 16.12514172979773
Epoch 33: loss 16.125018903391073
Epoch 34: loss 16.124896508964135
Epoch 35: loss 16.12477369085481
Epoch 36: loss 16.124645219152555
Epoch 37: loss 16.124527761118124
Epoch 38: loss 16.124402904977224
Epoch 39: loss 16.12427647493662
Epoch 40: loss 16.124152523723225
Epoch 41: loss 16.124027537936552
Epoch 42: loss 16.123900873496392
Epoch 43: loss 16.12378474459044
Epoch 44: loss 16.123662418616473
Epoch 45: loss 16.123535183216323
Epoch 46: loss 16.12340800952756
Epoch 47: loss 16.123286531436957
Epoch 48: loss 16.123166972622396
Epoch 49: loss 16.123034221053835
-----------Time: 0:05:26.479492, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016418933868408-------------


Epoch 0: loss 16.129116265264265
Epoch 1: loss 16.1289892818955
Epoch 2: loss 16.128864309073403
Epoch 3: loss 16.12874305086203
Epoch 4: loss 16.128615720561175
Epoch 5: loss 16.128491937887286
Epoch 6: loss 16.1283700236683
Epoch 7: loss 16.12824719881739
Epoch 8: loss 16.12812303239201
Epoch 9: loss 16.127997585584964
Epoch 10: loss 16.127873717863448
Epoch 11: loss 16.127745612280858
Epoch 12: loss 16.127627091151076
Epoch 13: loss 16.12750041582067
Epoch 14: loss 16.127379289329408
Epoch 15: loss 16.12725212134506
Epoch 16: loss 16.12712592051819
Epoch 17: loss 16.127001704308828
Epoch 18: loss 16.126878319906762
Epoch 19: loss 16.126753077940037
Epoch 20: loss 16.12663269768985
Epoch 21: loss 16.126509650366796
Epoch 22: loss 16.126388990081736
Epoch 23: loss 16.126263702998283
Epoch 24: loss 16.126135632679343
Epoch 25: loss 16.126011711543768
Epoch 26: loss 16.125890517636705
Epoch 27: loss 16.12576683245644
Epoch 28: loss 16.125642251683164
Epoch 29: loss 16.12552180661009
Epoch 30: loss 16.12539172885598
Epoch 31: loss 16.12527134082705
Epoch 32: loss 16.125145372325406
Epoch 33: loss 16.125020658276274
Epoch 34: loss 16.124893720024236
Epoch 35: loss 16.124773896213718
Epoch 36: loss 16.12465140481174
Epoch 37: loss 16.124527560426465
Epoch 38: loss 16.124401605926565
Epoch 39: loss 16.124276636215964
Epoch 40: loss 16.124154911798392
Epoch 41: loss 16.12403103837246
Epoch 42: loss 16.123904995713435
Epoch 43: loss 16.123784953060845
Epoch 44: loss 16.123663513863974
Epoch 45: loss 16.123534055816666
Epoch 46: loss 16.123406468298594
Epoch 47: loss 16.12328247196847
Epoch 48: loss 16.123163106585405
Epoch 49: loss 16.12303874672853
-----------Time: 0:07:05.765879, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016420364379883-------------


Epoch 0: loss 16.128706222646194
Epoch 1: loss 16.12746484465026
Epoch 2: loss 16.126221864751138
Epoch 3: loss 16.12498389903199
Epoch 4: loss 16.123745692690278
Epoch 5: loss 16.122504950995804
Epoch 6: loss 16.121262636957383
Epoch 7: loss 16.12002503269065
Epoch 8: loss 16.11878755755111
Epoch 9: loss 16.117548316636118
Epoch 10: loss 16.11630317580123
Epoch 11: loss 16.115068499973248
Epoch 12: loss 16.113830610485813
Epoch 13: loss 16.11258831200488
Epoch 14: loss 16.111343879554504
Epoch 15: loss 16.110109125939058
Epoch 16: loss 16.108870729795935
Epoch 17: loss 16.107632126738157
Epoch 18: loss 16.106392893601917
Epoch 19: loss 16.105156101436314
Epoch 20: loss 16.103912317733393
Epoch 21: loss 16.102675872499884
Epoch 22: loss 16.101434895887266
Epoch 23: loss 16.100197980298887
Epoch 24: loss 16.09895672105848
Epoch 25: loss 16.097718235200485
Epoch 26: loss 16.096483753322584
Epoch 27: loss 16.09523976685367
Epoch 28: loss 16.094004830696974
Epoch 29: loss 16.09276415071389
Epoch 30: loss 16.091529995024754
Epoch 31: loss 16.09028739161696
Epoch 32: loss 16.089050916824217
Epoch 33: loss 16.087817188447556
Epoch 34: loss 16.08657384294735
Epoch 35: loss 16.0853354353954
Epoch 36: loss 16.084099175296057
Epoch 37: loss 16.08286385020204
Epoch 38: loss 16.081624780938053
Epoch 39: loss 16.08038754123524
Epoch 40: loss 16.079146662634827
Epoch 41: loss 16.077908823449953
Epoch 42: loss 16.076667431452275
Epoch 43: loss 16.075437329527215
Epoch 44: loss 16.074197689821826
Epoch 45: loss 16.072956858412475
Epoch 46: loss 16.07171651447124
Epoch 47: loss 16.070481874425493
Epoch 48: loss 16.069242510087726
Epoch 49: loss 16.06801126987277
-----------Time: 0:04:52.766481, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009506702423096-------------


Epoch 0: loss 16.128552821098687
Epoch 1: loss 16.127310428235635
Epoch 2: loss 16.12607924921348
Epoch 3: loss 16.124835982537906
Epoch 4: loss 16.123593721394958
Epoch 5: loss 16.122357609091797
Epoch 6: loss 16.12111621605695
Epoch 7: loss 16.11987541731834
Epoch 8: loss 16.118641970013716
Epoch 9: loss 16.11740395502917
Epoch 10: loss 16.116162488355528
Epoch 11: loss 16.114919940436128
Epoch 12: loss 16.11368282726759
Epoch 13: loss 16.11244206949704
Epoch 14: loss 16.111207406633635
Epoch 15: loss 16.10995957798255
Epoch 16: loss 16.108727774067766
Epoch 17: loss 16.107489005063396
Epoch 18: loss 16.106249521451517
Epoch 19: loss 16.105016136376868
Epoch 20: loss 16.103771182231892
Epoch 21: loss 16.10253337208767
Epoch 22: loss 16.10130034846544
Epoch 23: loss 16.10005757288806
Epoch 24: loss 16.09882051520791
Epoch 25: loss 16.097579446806087
Epoch 26: loss 16.0963410065834
Epoch 27: loss 16.095098968949767
Epoch 28: loss 16.09386502380537
Epoch 29: loss 16.092626471050153
Epoch 30: loss 16.091393060046347
Epoch 31: loss 16.090152610832742
Epoch 32: loss 16.088914513912066
Epoch 33: loss 16.087675587258435
Epoch 34: loss 16.086436790250577
Epoch 35: loss 16.085199759536753
Epoch 36: loss 16.083959837203572
Epoch 37: loss 16.082727887048353
Epoch 38: loss 16.0814849050749
Epoch 39: loss 16.080251469179103
Epoch 40: loss 16.079008501725976
Epoch 41: loss 16.077771120449977
Epoch 42: loss 16.07653798896246
Epoch 43: loss 16.07529939057193
Epoch 44: loss 16.07405862242972
Epoch 45: loss 16.072825091114634
Epoch 46: loss 16.07158781614817
Epoch 47: loss 16.070354028134453
Epoch 48: loss 16.069107878655437
Epoch 49: loss 16.067869811294
-----------Time: 0:04:36.729143, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.009516716003418-------------


Epoch 0: loss 16.12855629145678
Epoch 1: loss 16.12731448925987
Epoch 2: loss 16.126072842637893
Epoch 3: loss 16.124836121518175
Epoch 4: loss 16.123593967721927
Epoch 5: loss 16.122358674261477
Epoch 6: loss 16.121115478631786
Epoch 7: loss 16.11987637047407
Epoch 8: loss 16.118641652122278
Epoch 9: loss 16.117399310598948
Epoch 10: loss 16.116163276083252
Epoch 11: loss 16.114922586765672
Epoch 12: loss 16.1136814799887
Epoch 13: loss 16.11243965704847
Epoch 14: loss 16.11120206885781
Epoch 15: loss 16.10996706787823
Epoch 16: loss 16.108722111658924
Epoch 17: loss 16.107483187079623
Epoch 18: loss 16.106247921104078
Epoch 19: loss 16.105014944154327
Epoch 20: loss 16.103768997441303
Epoch 21: loss 16.102527789540627
Epoch 22: loss 16.101296194096243
Epoch 23: loss 16.100056607804913
Epoch 24: loss 16.098820164127154
Epoch 25: loss 16.097579068240425
Epoch 26: loss 16.096341934847146
Epoch 27: loss 16.095097043450725
Epoch 28: loss 16.093865615508683
Epoch 29: loss 16.09262401141052
Epoch 30: loss 16.091393137833812
Epoch 31: loss 16.090150170899268
Epoch 32: loss 16.088912520997223
Epoch 33: loss 16.087672396416636
Epoch 34: loss 16.086436773137336
Epoch 35: loss 16.085196247692014
Epoch 36: loss 16.08396426797756
Epoch 37: loss 16.08272094685076
Epoch 38: loss 16.081485625905405
Epoch 39: loss 16.080252947140934
Epoch 40: loss 16.079008917629622
Epoch 41: loss 16.077770391840726
Epoch 42: loss 16.076534812122404
Epoch 43: loss 16.0752984711241
Epoch 44: loss 16.07406160117103
Epoch 45: loss 16.07282570459844
Epoch 46: loss 16.07158818486075
Epoch 47: loss 16.070344300034126
Epoch 48: loss 16.069106399656444
Epoch 49: loss 16.067870474043197
-----------Time: 0:04:55.988500, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009511947631836-------------


Epoch 0: loss 16.128550061718027
Epoch 1: loss 16.127318321070376
Epoch 2: loss 16.12607251593054
Epoch 3: loss 16.124831694374272
Epoch 4: loss 16.123599722438563
Epoch 5: loss 16.122353815137853
Epoch 6: loss 16.1211166594455
Epoch 7: loss 16.11987771464146
Epoch 8: loss 16.1186412004364
Epoch 9: loss 16.117399029008325
Epoch 10: loss 16.11615794141893
Epoch 11: loss 16.114919926952965
Epoch 12: loss 16.11368125233072
Epoch 13: loss 16.11244155921127
Epoch 14: loss 16.1112027227911
Epoch 15: loss 16.109963747909237
Epoch 16: loss 16.10872233568682
Epoch 17: loss 16.10748147731115
Epoch 18: loss 16.10624312524759
Epoch 19: loss 16.10500790853744
Epoch 20: loss 16.103765296832318
Epoch 21: loss 16.102528829299736
Epoch 22: loss 16.10128972943935
Epoch 23: loss 16.100050748853075
Epoch 24: loss 16.09881396757772
Epoch 25: loss 16.097572794940692
Epoch 26: loss 16.096330625586955
Epoch 27: loss 16.095098754256362
Epoch 28: loss 16.09385521791758
Epoch 29: loss 16.092616061013054
Epoch 30: loss 16.091378020617935
Epoch 31: loss 16.090141821192812
Epoch 32: loss 16.088903239915524
Epoch 33: loss 16.087659201588302
Epoch 34: loss 16.086425394905593
Epoch 35: loss 16.085186644570214
Epoch 36: loss 16.083945248942456
Epoch 37: loss 16.082707789360406
Epoch 38: loss 16.081470263399723
Epoch 39: loss 16.080230964403427
Epoch 40: loss 16.078994970337213
Epoch 41: loss 16.077760849393147
Epoch 42: loss 16.07652054901289
Epoch 43: loss 16.075275229785415
Epoch 44: loss 16.074038510221445
Epoch 45: loss 16.07280008296334
Epoch 46: loss 16.071565686133066
Epoch 47: loss 16.070330956891024
Epoch 48: loss 16.069085549504425
Epoch 49: loss 16.067854024587344
-----------Time: 0:06:06.089190, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.009506702423096-------------


Epoch 0: loss 16.128560931738335
Epoch 1: loss 16.127321977081216
Epoch 2: loss 16.12607853253164
Epoch 3: loss 16.124839007433113
Epoch 4: loss 16.12360193886272
Epoch 5: loss 16.122363119037207
Epoch 6: loss 16.121119991860485
Epoch 7: loss 16.119882581025248
Epoch 8: loss 16.118644468028496
Epoch 9: loss 16.117401475164797
Epoch 10: loss 16.1161643485131
Epoch 11: loss 16.114923030154223
Epoch 12: loss 16.11368847100744
Epoch 13: loss 16.11244317615337
Epoch 14: loss 16.111211746136995
Epoch 15: loss 16.109965960703317
Epoch 16: loss 16.108725038023344
Epoch 17: loss 16.107484779129734
Epoch 18: loss 16.10625169638903
Epoch 19: loss 16.105010798601043
Epoch 20: loss 16.103772855180967
Epoch 21: loss 16.102532900177053
Epoch 22: loss 16.101295223308686
Epoch 23: loss 16.100051684376986
Epoch 24: loss 16.09881197310713
Epoch 25: loss 16.097574034354295
Epoch 26: loss 16.096339911854482
Epoch 27: loss 16.095099091335378
Epoch 28: loss 16.09386059354997
Epoch 29: loss 16.092620920136675
Epoch 30: loss 16.091389355288698
Epoch 31: loss 16.09014249327651
Epoch 32: loss 16.088907556601228
Epoch 33: loss 16.087664727091205
Epoch 34: loss 16.086428716430333
Epoch 35: loss 16.085184086918378
Epoch 36: loss 16.08394523286638
Epoch 37: loss 16.082713644682162
Epoch 38: loss 16.081472020359257
Epoch 39: loss 16.080232851008738
Epoch 40: loss 16.078993361692444
Epoch 41: loss 16.077756959501333
Epoch 42: loss 16.07651843111952
Epoch 43: loss 16.07527975546011
Epoch 44: loss 16.074038970204654
Epoch 45: loss 16.072806300256094
Epoch 46: loss 16.07156563220042
Epoch 47: loss 16.070325999236612
Epoch 48: loss 16.069083924264994
Epoch 49: loss 16.06784794990494
-----------Time: 0:07:59.457441, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009505748748779-------------


Epoch 0: loss 16.123201220887324
Epoch 1: loss 16.110823101294695
Epoch 2: loss 16.098433026287335
Epoch 3: loss 16.08605810220375
Epoch 4: loss 16.073686849169917
Epoch 5: loss 16.061321891216302
Epoch 6: loss 16.048957455994447
Epoch 7: loss 16.036599545066547
Epoch 8: loss 16.02425363363294
Epoch 9: loss 16.011905499552178
Epoch 10: loss 15.99956912434315
Epoch 11: loss 15.987231159676929
Epoch 12: loss 15.974905578775079
Epoch 13: loss 15.962579286895801
Epoch 14: loss 15.95025265119593
Epoch 15: loss 15.937940951726436
Epoch 16: loss 15.925633662806185
Epoch 17: loss 15.913327808805365
Epoch 18: loss 15.901025654376364
Epoch 19: loss 15.88873237290416
Epoch 20: loss 15.876439554008082
Epoch 21: loss 15.864151826560853
Epoch 22: loss 15.851866033947163
Epoch 23: loss 15.839583413506281
Epoch 24: loss 15.827317115986975
Epoch 25: loss 15.815060373361245
Epoch 26: loss 15.802801996680172
Epoch 27: loss 15.790545534607896
Epoch 28: loss 15.778305234177855
Epoch 29: loss 15.766062550339898
Epoch 30: loss 15.753830136521605
Epoch 31: loss 15.741608430925695
Epoch 32: loss 15.729375649431985
Epoch 33: loss 15.717155535886183
Epoch 34: loss 15.704941199356087
Epoch 35: loss 15.692730013736893
Epoch 36: loss 15.680519381445864
Epoch 37: loss 15.668313864977092
Epoch 38: loss 15.656119693032162
Epoch 39: loss 15.643933942358153
Epoch 40: loss 15.631752571636985
Epoch 41: loss 15.619574917600879
Epoch 42: loss 15.607399879297917
Epoch 43: loss 15.595226317401034
Epoch 44: loss 15.583061788703127
Epoch 45: loss 15.570903835120312
Epoch 46: loss 15.55875326979089
Epoch 47: loss 15.54659642856882
Epoch 48: loss 15.534453945875557
Epoch 49: loss 15.522315498795958
-----------Time: 0:03:49.358332, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9405734539031982-------------


Epoch 0: loss 16.123060291263318
Epoch 1: loss 16.110671776153914
Epoch 2: loss 16.09829239225569
Epoch 3: loss 16.085914246733907
Epoch 4: loss 16.073536788853307
Epoch 5: loss 16.061164578515076
Epoch 6: loss 16.048806087292686
Epoch 7: loss 16.036439392085693
Epoch 8: loss 16.024087717638125
Epoch 9: loss 16.011740170593427
Epoch 10: loss 15.999402043610697
Epoch 11: loss 15.98706996382547
Epoch 12: loss 15.974739775312797
Epoch 13: loss 15.962411451106359
Epoch 14: loss 15.950085120851933
Epoch 15: loss 15.937774040570655
Epoch 16: loss 15.925459243085736
Epoch 17: loss 15.913155563503844
Epoch 18: loss 15.900844964467682
Epoch 19: loss 15.88854570476953
Epoch 20: loss 15.876250391488751
Epoch 21: loss 15.863963561708866
Epoch 22: loss 15.851681951467857
Epoch 23: loss 15.839398696281263
Epoch 24: loss 15.827114763306561
Epoch 25: loss 15.8148381402792
Epoch 26: loss 15.802571592802844
Epoch 27: loss 15.790294301840454
Epoch 28: loss 15.778027233188084
Epoch 29: loss 15.765770138444429
Epoch 30: loss 15.753515794265523
Epoch 31: loss 15.741266020359456
Epoch 32: loss 15.72901107669849
Epoch 33: loss 15.716771225361411
Epoch 34: loss 15.704516286886276
Epoch 35: loss 15.692267245738126
Epoch 36: loss 15.680020789208134
Epoch 37: loss 15.667774677535903
Epoch 38: loss 15.655529018068131
Epoch 39: loss 15.643292402171
Epoch 40: loss 15.631055448157687
Epoch 41: loss 15.618810340462332
Epoch 42: loss 15.60656421841844
Epoch 43: loss 15.594321064744195
Epoch 44: loss 15.582059193331629
Epoch 45: loss 15.569806345783542
Epoch 46: loss 15.557547604538552
Epoch 47: loss 15.545290439267596
Epoch 48: loss 15.533030604849438
Epoch 49: loss 15.520758854947445
-----------Time: 0:04:03.462415, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9403271675109863-------------


Epoch 0: loss 16.123021075490897
Epoch 1: loss 16.110635518898064
Epoch 2: loss 16.09825357008784
Epoch 3: loss 16.08586938462872
Epoch 4: loss 16.073486020605223
Epoch 5: loss 16.06111632124635
Epoch 6: loss 16.04875090227236
Epoch 7: loss 16.036385681915696
Epoch 8: loss 16.02402218947792
Epoch 9: loss 16.011667940783852
Epoch 10: loss 15.999302793028823
Epoch 11: loss 15.986945628342
Epoch 12: loss 15.974593297886809
Epoch 13: loss 15.962230845726722
Epoch 14: loss 15.949873278619414
Epoch 15: loss 15.937516484200922
Epoch 16: loss 15.925160364459039
Epoch 17: loss 15.912790762593787
Epoch 18: loss 15.900416482071828
Epoch 19: loss 15.88804124268972
Epoch 20: loss 15.875656353513335
Epoch 21: loss 15.863261037186607
Epoch 22: loss 15.850844836999956
Epoch 23: loss 15.838411834202363
Epoch 24: loss 15.825953311930538
Epoch 25: loss 15.813476626285722
Epoch 26: loss 15.800961687877296
Epoch 27: loss 15.788408659021773
Epoch 28: loss 15.77581544360627
Epoch 29: loss 15.763173376625812
Epoch 30: loss 15.750471599468918
Epoch 31: loss 15.737712322855332
Epoch 32: loss 15.724886331563413
Epoch 33: loss 15.711972621940543
Epoch 34: loss 15.698954797944925
Epoch 35: loss 15.685833998903624
Epoch 36: loss 15.672600298098988
Epoch 37: loss 15.659232795076438
Epoch 38: loss 15.645718516442619
Epoch 39: loss 15.632048313354526
Epoch 40: loss 15.618209914061737
Epoch 41: loss 15.60418332666726
Epoch 42: loss 15.589959730072602
Epoch 43: loss 15.575506000300994
Epoch 44: loss 15.560820839338941
Epoch 45: loss 15.545879319934109
Epoch 46: loss 15.530675163600936
Epoch 47: loss 15.515180116895102
Epoch 48: loss 15.49938932380448
Epoch 49: loss 15.4832684515351
-----------Time: 0:05:31.588338, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.935310125350952-------------


Epoch 0: loss 16.12302241187954
Epoch 1: loss 16.110636401526495
Epoch 2: loss 16.098248032138944
Epoch 3: loss 16.085865525333308
Epoch 4: loss 16.073487828904483
Epoch 5: loss 16.061115294451813
Epoch 6: loss 16.048734285314165
Epoch 7: loss 16.03636482917076
Epoch 8: loss 16.023988728940715
Epoch 9: loss 16.01161671307114
Epoch 10: loss 15.999235433233116
Epoch 11: loss 15.986851674567882
Epoch 12: loss 15.974455881663287
Epoch 13: loss 15.962045478198501
Epoch 14: loss 15.949619351294198
Epoch 15: loss 15.937178147104916
Epoch 16: loss 15.924700743741091
Epoch 17: loss 15.912183367992109
Epoch 18: loss 15.899615662197759
Epoch 19: loss 15.886982609228701
Epoch 20: loss 15.87426785089971
Epoch 21: loss 15.861453120639753
Epoch 22: loss 15.84852141670198
Epoch 23: loss 15.835454525410837
Epoch 24: loss 15.822216672829923
Epoch 25: loss 15.808786308720036
Epoch 26: loss 15.7951316314913
Epoch 27: loss 15.781204187850578
Epoch 28: loss 15.766981572933727
Epoch 29: loss 15.752415179947008
Epoch 30: loss 15.737477387598897
Epoch 31: loss 15.722112596002592
Epoch 32: loss 15.706297576265403
Epoch 33: loss 15.689972536793347
Epoch 34: loss 15.673114531840106
Epoch 35: loss 15.655682383833403
Epoch 36: loss 15.637628740432017
Epoch 37: loss 15.618931131948914
Epoch 38: loss 15.599543640962303
Epoch 39: loss 15.579450746280076
Epoch 40: loss 15.558613727376667
Epoch 41: loss 15.53700370798946
Epoch 42: loss 15.514597487747832
Epoch 43: loss 15.491363494794221
Epoch 44: loss 15.467309683156182
Epoch 45: loss 15.44238364793737
Epoch 46: loss 15.416579320159796
Epoch 47: loss 15.389896583142262
Epoch 48: loss 15.362305530716117
Epoch 49: loss 15.333772602257618
-----------Time: 0:06:51.354107, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.9155209064483643-------------


Epoch 0: loss 16.123028726147336
Epoch 1: loss 16.11064707137373
Epoch 2: loss 16.09825618685815
Epoch 3: loss 16.085865889897224
Epoch 4: loss 16.073486025791055
Epoch 5: loss 16.061099289421676
Epoch 6: loss 16.04870112088791
Epoch 7: loss 16.036316081322425
Epoch 8: loss 16.023907319691727
Epoch 9: loss 16.011489261940422
Epoch 10: loss 15.999051961126115
Epoch 11: loss 15.986564144095109
Epoch 12: loss 15.974041000664915
Epoch 13: loss 15.961449636072487
Epoch 14: loss 15.948763139485147
Epoch 15: loss 15.93596472384944
Epoch 16: loss 15.923001239065115
Epoch 17: loss 15.909840557353531
Epoch 18: loss 15.896432910294815
Epoch 19: loss 15.882713552789237
Epoch 20: loss 15.868622677166739
Epoch 21: loss 15.854087312043912
Epoch 22: loss 15.839043316988922
Epoch 23: loss 15.82340329778525
Epoch 24: loss 15.807101196799348
Epoch 25: loss 15.790060302107927
Epoch 26: loss 15.772219927042578
Epoch 27: loss 15.753519134977838
Epoch 28: loss 15.733881432832487
Epoch 29: loss 15.713272516334621
Epoch 30: loss 15.691622896386333
Epoch 31: loss 15.668902157051788
Epoch 32: loss 15.64507606458638
Epoch 33: loss 15.620096569673207
Epoch 34: loss 15.593936742810596
Epoch 35: loss 15.56657373885735
Epoch 36: loss 15.537985022784964
Epoch 37: loss 15.508143484106267
Epoch 38: loss 15.477045976059019
Epoch 39: loss 15.44467022031335
Epoch 40: loss 15.410998251076428
Epoch 41: loss 15.376024722793687
Epoch 42: loss 15.33974310235526
Epoch 43: loss 15.302155369192832
Epoch 44: loss 15.26323823685099
Epoch 45: loss 15.22298705571368
Epoch 46: loss 15.181419927442509
Epoch 47: loss 15.13852340828925
Epoch 48: loss 15.094301986590619
Epoch 49: loss 15.048757819652298
-----------Time: 0:06:36.486614, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.877990245819092-------------


Epoch 0: loss 16.067829011250215
Epoch 1: loss 15.944553427621033
Epoch 2: loss 15.821808050610956
Epoch 3: loss 15.699578984267301
Epoch 4: loss 15.577841154578719
Epoch 5: loss 15.456532046871382
Epoch 6: loss 15.335645931320647
Epoch 7: loss 15.215211721527115
Epoch 8: loss 15.095085794864756
Epoch 9: loss 14.975192681416797
Epoch 10: loss 14.855452507458802
Epoch 11: loss 14.735733398346229
Epoch 12: loss 14.61589845073424
Epoch 13: loss 14.495670963720888
Epoch 14: loss 14.374791704357285
Epoch 15: loss 14.252911765786731
Epoch 16: loss 14.129633702293177
Epoch 17: loss 14.004464975577973
Epoch 18: loss 13.876873687922533
Epoch 19: loss 13.746256147927081
Epoch 20: loss 13.61201016134124
Epoch 21: loss 13.473510545644507
Epoch 22: loss 13.330104377232148
Epoch 23: loss 13.181182842659133
Epoch 24: loss 13.026208849041407
Epoch 25: loss 12.864649973854284
Epoch 26: loss 12.696010194957871
Epoch 27: loss 12.51993127929704
Epoch 28: loss 12.336142911283526
Epoch 29: loss 12.14442932352934
Epoch 30: loss 11.944591460505409
Epoch 31: loss 11.73665847695347
Epoch 32: loss 11.52066451794042
Epoch 33: loss 11.296647642279787
Epoch 34: loss 11.064740459966945
Epoch 35: loss 10.825182588545118
Epoch 36: loss 10.57826325269806
Epoch 37: loss 10.324308263147053
Epoch 38: loss 10.063680268163717
Epoch 39: loss 9.796924077667187
Epoch 40: loss 9.524434666843632
Epoch 41: loss 9.246773640444383
Epoch 42: loss 8.964417237182749
Epoch 43: loss 8.67792078176875
Epoch 44: loss 8.388002671775382
Epoch 45: loss 8.095174618814871
Epoch 46: loss 7.800300293476962
Epoch 47: loss 7.504008645788362
Epoch 48: loss 7.206905745630228
Epoch 49: loss 6.90980804971277
-----------Time: 0:03:45.653683, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.607569694519043-------------


Epoch 0: loss 16.06779271095197
Epoch 1: loss 15.944484949241584
Epoch 2: loss 15.82158524138309
Epoch 3: loss 15.698988783806806
Epoch 4: loss 15.576232351123672
Epoch 5: loss 15.452622997041239
Epoch 6: loss 15.326811938262491
Epoch 7: loss 15.196505352101163
Epoch 8: loss 15.058156951605593
Epoch 9: loss 14.906982697242626
Epoch 10: loss 14.73774184786541
Epoch 11: loss 14.545683649196905
Epoch 12: loss 14.327120522171858
Epoch 13: loss 14.079759959749323
Epoch 14: loss 13.802800941882152
Epoch 15: loss 13.496453714085506
Epoch 16: loss 13.161566937079956
Epoch 17: loss 12.799216988684368
Epoch 18: loss 12.410969377925307
Epoch 19: loss 11.998550744857912
Epoch 20: loss 11.564202544091511
Epoch 21: loss 11.10989999900764
Epoch 22: loss 10.63795420110971
Epoch 23: loss 10.151015378133701
Epoch 24: loss 9.651633706541409
Epoch 25: loss 9.142686703854634
Epoch 26: loss 8.627176245118433
Epoch 27: loss 8.108128813183002
Epoch 28: loss 7.588635328219208
Epoch 29: loss 7.071952169261722
Epoch 30: loss 6.561255884403894
Epoch 31: loss 6.059908478463584
Epoch 32: loss 5.571373199236788
Epoch 33: loss 5.099204497988163
Epoch 34: loss 4.6467210345452346
Epoch 35: loss 4.217404318348489
Epoch 36: loss 3.8145130610193747
Epoch 37: loss 3.4408046321547374
Epoch 38: loss 3.0992250751061827
Epoch 39: loss 2.791651771015418
Epoch 40: loss 2.519361834347151
Epoch 41: loss 2.2828362235042827
Epoch 42: loss 2.0812275873830357
Epoch 43: loss 1.9123595911501023
Epoch 44: loss 1.7731163379872474
Epoch 45: loss 1.659562316000494
Epoch 46: loss 1.5673815910423365
Epoch 47: loss 1.4923497152561853
Epoch 48: loss 1.4307488323619275
Epoch 49: loss 1.379480066864694
-----------Time: 0:04:40.534174, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.1750203371047974-------------


Epoch 0: loss 16.067768838497656
Epoch 1: loss 15.944313679913453
Epoch 2: loss 15.820304012869025
Epoch 3: loss 15.692185267084898
Epoch 4: loss 15.548864189342936
Epoch 5: loss 15.370526296149913
Epoch 6: loss 15.137119839280892
Epoch 7: loss 14.836188296639056
Epoch 8: loss 14.46398515115295
Epoch 9: loss 14.02267691543272
Epoch 10: loss 13.517188030717943
Epoch 11: loss 12.953606226964125
Epoch 12: loss 12.339096761644372
Epoch 13: loss 11.680470791246789
Epoch 14: loss 10.985014573720047
Epoch 15: loss 10.260107208426197
Epoch 16: loss 9.514217925888484
Epoch 17: loss 8.755772548113393
Epoch 18: loss 7.993177611261299
Epoch 19: loss 7.235832974079707
Epoch 20: loss 6.493216048640489
Epoch 21: loss 5.77556636736664
Epoch 22: loss 5.092409611007582
Epoch 23: loss 4.45325262265208
Epoch 24: loss 3.8669002473581218
Epoch 25: loss 3.3421936571889757
Epoch 26: loss 2.8852583636013693
Epoch 27: loss 2.4998093405905593
Epoch 28: loss 2.185575463449002
Epoch 29: loss 1.9380385526306543
Epoch 30: loss 1.7485769926821038
Epoch 31: loss 1.6059696183637668
Epoch 32: loss 1.49857309293721
Epoch 33: loss 1.4162599320253515
Epoch 34: loss 1.3515663289583009
Epoch 35: loss 1.2992958255667217
Epoch 36: loss 1.2561136674725406
Epoch 37: loss 1.2197920927345396
Epoch 38: loss 1.188942036270901
Epoch 39: loss 1.1625442131827097
Epoch 40: loss 1.1398418177853076
Epoch 41: loss 1.1202578886233314
Epoch 42: loss 1.1033071887240324
Epoch 43: loss 1.0886179047994733
Epoch 44: loss 1.0758480792009293
Epoch 45: loss 1.0647376942984625
Epoch 46: loss 1.0550449483416662
Epoch 47: loss 1.0465880193549566
Epoch 48: loss 1.0391838660766535
Epoch 49: loss 1.0326979568368913
-----------Time: 0:06:16.793885, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0245991945266724-------------


Epoch 0: loss 16.06781945428231
Epoch 1: loss 15.944053539444145
Epoch 2: loss 15.81505432720091
Epoch 3: loss 15.653118287564102
Epoch 4: loss 15.402725452570373
Epoch 5: loss 15.029457388396105
Epoch 6: loss 14.536302289086363
Epoch 7: loss 13.936768971557782
Epoch 8: loss 13.244874559581895
Epoch 9: loss 12.474147225152286
Epoch 10: loss 11.63846771194599
Epoch 11: loss 10.751636462084557
Epoch 12: loss 9.828417752604565
Epoch 13: loss 8.884330384169926
Epoch 14: loss 7.936003944029816
Epoch 15: loss 7.000801240704253
Epoch 16: loss 6.095788536932625
Epoch 17: loss 5.2393007304371535
Epoch 18: loss 4.448604774189877
Epoch 19: loss 3.7402971470206894
Epoch 20: loss 3.1281333247107486
Epoch 21: loss 2.621322590738227
Epoch 22: loss 2.2216997381913526
Epoch 23: loss 1.9214594156084792
Epoch 24: loss 1.7045378290226176
Epoch 25: loss 1.5506422467177299
Epoch 26: loss 1.4402351025461566
Epoch 27: loss 1.3582565198455445
Epoch 28: loss 1.2949128064725501
Epoch 29: loss 1.2444269467852178
Epoch 30: loss 1.2033156852737725
Epoch 31: loss 1.169375565144081
Epoch 32: loss 1.1411306359574742
Epoch 33: loss 1.1175083447047198
Epoch 34: loss 1.097684788470556
Epoch 35: loss 1.0810022184290011
Epoch 36: loss 1.0669382127359421
Epoch 37: loss 1.0550320656031225
Epoch 38: loss 1.0449338568948805
Epoch 39: loss 1.036356372603779
Epoch 40: loss 1.0290512058189085
Epoch 41: loss 1.0228092689239312
Epoch 42: loss 1.0174719723363879
Epoch 43: loss 1.012892615671972
Epoch 44: loss 1.0089526684988186
Epoch 45: loss 1.0055505349754055
Epoch 46: loss 1.0026098762665707
Epoch 47: loss 1.0000686475995963
Epoch 48: loss 0.9978621513251055
Epoch 49: loss 0.9959416254309612
-----------Time: 0:05:36.008103, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0070390701293945-------------


Epoch 0: loss 16.06774286941188
Epoch 1: loss 15.942708436012786
Epoch 2: loss 15.792918693766508
Epoch 3: loss 15.532351430049728
Epoch 4: loss 15.094768238949218
Epoch 5: loss 14.489616726973317
Epoch 6: loss 13.7438342051379
Epoch 7: loss 12.882751929494205
Epoch 8: loss 11.927575720724299
Epoch 9: loss 10.90017336668561
Epoch 10: loss 9.824030520411663
Epoch 11: loss 8.722882405126011
Epoch 12: loss 7.6227641307899265
Epoch 13: loss 6.550802358406403
Epoch 14: loss 5.533824239495398
Epoch 15: loss 4.598816739663668
Epoch 16: loss 3.770889961829712
Epoch 17: loss 3.0714137437487503
Epoch 18: loss 2.512527279000752
Epoch 19: loss 2.0929998904223543
Epoch 20: loss 1.7966049895978868
Epoch 21: loss 1.5949495053926586
Epoch 22: loss 1.4577219976945828
Epoch 23: loss 1.3608496429610344
Epoch 24: loss 1.2888566061100797
Epoch 25: loss 1.2331480169957458
Epoch 26: loss 1.1889037049419533
Epoch 27: loss 1.1532145713644872
Epoch 28: loss 1.1242352473858972
Epoch 29: loss 1.1005652292128683
Epoch 30: loss 1.0811568602976298
Epoch 31: loss 1.065192374280771
Epoch 32: loss 1.0520289449862905
Epoch 33: loss 1.041131022282435
Epoch 34: loss 1.0320888422247247
Epoch 35: loss 1.0245452381917612
Epoch 36: loss 1.0182473016796454
Epoch 37: loss 1.012964169767773
Epoch 38: loss 1.0085242760123085
Epoch 39: loss 1.004770647675658
Epoch 40: loss 1.001605084954947
Epoch 41: loss 0.9989064468385863
Epoch 42: loss 0.9966228846430195
Epoch 43: loss 0.9946678151508723
Epoch 44: loss 0.9929942100057141
Epoch 45: loss 0.9915646692538403
Epoch 46: loss 0.9903288033748853
Epoch 47: loss 0.9892672056928804
Epoch 48: loss 0.9883476416010647
Epoch 49: loss 0.9875510568655075
-----------Time: 0:06:56.341097, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0028659105300903-------------


Epoch 0: loss 15.52563319691113
Epoch 1: loss 14.334051585443776
Epoch 2: loss 12.973016061103493
Epoch 3: loss 10.881338973613198
Epoch 4: loss 7.975784501412305
Epoch 5: loss 4.986281320152885
Epoch 6: loss 2.771816802063735
Epoch 7: loss 1.6626279391044505
Epoch 8: loss 1.2785650793140904
Epoch 9: loss 1.1386762508365886
Epoch 10: loss 1.0708174584610686
Epoch 11: loss 1.034279009387051
Epoch 12: loss 1.01383116667915
Epoch 13: loss 1.002015271479828
Epoch 14: loss 0.9949782609032055
Epoch 15: loss 0.990648362641752
Epoch 16: loss 0.9878682195654118
Epoch 17: loss 0.9860521226294862
Epoch 18: loss 0.9847739105836537
Epoch 19: loss 0.983847272046044
Epoch 20: loss 0.983138357380799
Epoch 21: loss 0.9825638647699174
Epoch 22: loss 0.9820857662555639
Epoch 23: loss 0.9816531258928445
Epoch 24: loss 0.9812702045225462
Epoch 25: loss 0.9808953341872488
Epoch 26: loss 0.9805296778095488
Epoch 27: loss 0.9801671076170967
Epoch 28: loss 0.9798174306969075
Epoch 29: loss 0.9794542737925033
Epoch 30: loss 0.9790982433866715
Epoch 31: loss 0.9786933333475736
Epoch 32: loss 0.9783107402727875
Epoch 33: loss 0.9779074504493436
Epoch 34: loss 0.9774799876850931
Epoch 35: loss 0.9770562882783816
Epoch 36: loss 0.9765945887876762
Epoch 37: loss 0.9761020055343561
Epoch 38: loss 0.9756254182683831
Epoch 39: loss 0.9750927558990197
Epoch 40: loss 0.9745682226762362
Epoch 41: loss 0.9739952992005735
Epoch 42: loss 0.9733907367756601
Epoch 43: loss 0.9727720185555214
Epoch 44: loss 0.9721066213743138
Epoch 45: loss 0.9714365903290152
Epoch 46: loss 0.9707180349122836
Epoch 47: loss 0.9699641546197013
Epoch 48: loss 0.9691772070379605
Epoch 49: loss 0.9683516923340201
-----------Time: 0:04:21.587626, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9983540773391724-------------


Epoch 0: loss 15.511592036294445
Epoch 1: loss 13.13546454173965
Epoch 2: loss 7.577547754782448
Epoch 3: loss 3.055244166154846
Epoch 4: loss 1.4805582704720386
Epoch 5: loss 1.1572030063165537
Epoch 6: loss 1.058120975486606
Epoch 7: loss 1.0190222872995436
Epoch 8: loss 1.002484110919855
Epoch 9: loss 0.9949532099688033
Epoch 10: loss 0.9913537606653666
Epoch 11: loss 0.9893920669872
Epoch 12: loss 0.9882814193173813
Epoch 13: loss 0.9875898115851427
Epoch 14: loss 0.9870774418909696
Epoch 15: loss 0.9866766166985197
Epoch 16: loss 0.9863465860396899
Epoch 17: loss 0.9860037874436495
Epoch 18: loss 0.9857415070736
Epoch 19: loss 0.9854375019612813
Epoch 20: loss 0.9850961296308164
Epoch 21: loss 0.9847013425930743
Epoch 22: loss 0.9843738081988075
Epoch 23: loss 0.9839656067127893
Epoch 24: loss 0.9835063982684046
Epoch 25: loss 0.9830381246349487
Epoch 26: loss 0.9824878910172563
Epoch 27: loss 0.9819406842070472
Epoch 28: loss 0.981313241701401
Epoch 29: loss 0.9806274122165038
Epoch 30: loss 0.979887774160467
Epoch 31: loss 0.9790632541313192
Epoch 32: loss 0.9781857075219312
Epoch 33: loss 0.9772136389658204
Epoch 34: loss 0.9761466458480341
Epoch 35: loss 0.9750881571689334
Epoch 36: loss 0.9738743317198533
Epoch 37: loss 0.9726044121418652
Epoch 38: loss 0.9712987228409631
Epoch 39: loss 0.9698856702117962
Epoch 40: loss 0.9683912473894319
Epoch 41: loss 0.9668630980615579
Epoch 42: loss 0.9652609472173657
Epoch 43: loss 0.9635274050156665
Epoch 44: loss 0.9617965805135128
Epoch 45: loss 0.9599622434867083
Epoch 46: loss 0.9581267401665173
Epoch 47: loss 0.9562194761923956
Epoch 48: loss 0.9542098301399525
Epoch 49: loss 0.952203572801172
-----------Time: 0:05:42.573826, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9938191771507263-------------


Epoch 0: loss 15.152094706171813
Epoch 1: loss 8.00680272782737
Epoch 2: loss 2.2129357880778517
Epoch 3: loss 1.2086833144673321
Epoch 4: loss 1.0555412668518038
Epoch 5: loss 1.0129744592018917
Epoch 6: loss 0.9994197529965992
Epoch 7: loss 0.9945824256145546
Epoch 8: loss 0.9926255392989366
Epoch 9: loss 0.9916421053460138
Epoch 10: loss 0.9911784327114451
Epoch 11: loss 0.9907541103891991
Epoch 12: loss 0.9905220953065977
Epoch 13: loss 0.9902814510012529
Epoch 14: loss 0.9900547149973502
Epoch 15: loss 0.9898185055498328
Epoch 16: loss 0.989558102936768
Epoch 17: loss 0.9892440945185547
Epoch 18: loss 0.9889291705094712
Epoch 19: loss 0.9886160748947438
Epoch 20: loss 0.9881989973404279
Epoch 21: loss 0.9877777250970816
Epoch 22: loss 0.9872328620555934
Epoch 23: loss 0.986706192789031
Epoch 24: loss 0.986085622125245
Epoch 25: loss 0.9853243800270097
Epoch 26: loss 0.9845326459556899
Epoch 27: loss 0.9835996780465517
Epoch 28: loss 0.9825910784875393
Epoch 29: loss 0.9814592251121641
Epoch 30: loss 0.9802251618280561
Epoch 31: loss 0.9787828663304294
Epoch 32: loss 0.9772921591559592
Epoch 33: loss 0.9756683642738471
Epoch 34: loss 0.9739298408870272
Epoch 35: loss 0.9720414215744417
Epoch 36: loss 0.9701088909677607
Epoch 37: loss 0.9680163878795568
Epoch 38: loss 0.9658228643501887
Epoch 39: loss 0.9635386295523444
Epoch 40: loss 0.9611423286878266
Epoch 41: loss 0.9586853454332108
Epoch 42: loss 0.956108209165041
Epoch 43: loss 0.953387236128429
Epoch 44: loss 0.9506037617715044
Epoch 45: loss 0.947689902173105
Epoch 46: loss 0.9446796232620746
Epoch 47: loss 0.941449943024416
Epoch 48: loss 0.9381375270994413
Epoch 49: loss 0.9346188416359151
-----------Time: 0:04:41.401528, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9913724660873413-------------


Epoch 0: loss 14.349405701488434
Epoch 1: loss 4.8424226157103885
Epoch 2: loss 1.3745795899982878
Epoch 3: loss 1.0715086337922384
Epoch 4: loss 1.0147510094445578
Epoch 5: loss 1.0003113259174954
Epoch 6: loss 0.9960465104181913
Epoch 7: loss 0.9946357303173922
Epoch 8: loss 0.9937767311241913
Epoch 9: loss 0.9934653993078131
Epoch 10: loss 0.9931695801917465
Epoch 11: loss 0.9929969170418947
Epoch 12: loss 0.9927412983579049
Epoch 13: loss 0.9925772585967366
Epoch 14: loss 0.9922507459732293
Epoch 15: loss 0.9921214744213679
Epoch 16: loss 0.9918592318430616
Epoch 17: loss 0.9915507990682559
Epoch 18: loss 0.9911996879740991
Epoch 19: loss 0.9908440702631786
Epoch 20: loss 0.9904070599951648
Epoch 21: loss 0.9898972033546826
Epoch 22: loss 0.9894057557659346
Epoch 23: loss 0.9887002868260814
Epoch 24: loss 0.9879983584475037
Epoch 25: loss 0.9871148886143869
Epoch 26: loss 0.98615513611514
Epoch 27: loss 0.9850133775108985
Epoch 28: loss 0.9837566532150048
Epoch 29: loss 0.9822312248926438
Epoch 30: loss 0.9806570261635814
Epoch 31: loss 0.9785718242392195
Epoch 32: loss 0.9764941881181365
Epoch 33: loss 0.9741093412633173
Epoch 34: loss 0.9714921097057938
Epoch 35: loss 0.9686244827952963
Epoch 36: loss 0.96537055908953
Epoch 37: loss 0.9619305995639865
Epoch 38: loss 0.9583484803353268
Epoch 39: loss 0.9544619150366583
Epoch 40: loss 0.9502983413486782
Epoch 41: loss 0.9458594992473244
Epoch 42: loss 0.9412582984367358
Epoch 43: loss 0.9363413017168195
Epoch 44: loss 0.9312387530475676
Epoch 45: loss 0.925794267699795
Epoch 46: loss 0.919979820308509
Epoch 47: loss 0.9137474508179213
Epoch 48: loss 0.9072802794764002
Epoch 49: loss 0.9002936216007438
-----------Time: 0:05:46.980353, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.988189697265625-------------


Epoch 0: loss 13.282559113245803
Epoch 1: loss 3.1028948164038064
Epoch 2: loss 1.176739947960518
Epoch 3: loss 1.031737198351258
Epoch 4: loss 1.0048678502108235
Epoch 5: loss 0.9983684170045173
Epoch 6: loss 0.9966201560208174
Epoch 7: loss 0.9958125988772021
Epoch 8: loss 0.9954608436056036
Epoch 9: loss 0.9951771493837067
Epoch 10: loss 0.9950658645300583
Epoch 11: loss 0.9948567746449191
Epoch 12: loss 0.9946524234100682
Epoch 13: loss 0.9943860868450868
Epoch 14: loss 0.9942226603732542
Epoch 15: loss 0.9939300478509485
Epoch 16: loss 0.9936224363497121
Epoch 17: loss 0.9933104222724463
Epoch 18: loss 0.9928715780028965
Epoch 19: loss 0.9924210936819619
Epoch 20: loss 0.9919328236463214
Epoch 21: loss 0.9912230784378342
Epoch 22: loss 0.9905926910932457
Epoch 23: loss 0.9896620948590812
Epoch 24: loss 0.9887546434747324
Epoch 25: loss 0.9875087344886299
Epoch 26: loss 0.9861662144762073
Epoch 27: loss 0.9844943116124782
Epoch 28: loss 0.982642285716216
Epoch 29: loss 0.9805647504595455
Epoch 30: loss 0.978158489396353
Epoch 31: loss 0.9754195584428901
Epoch 32: loss 0.9724147014802017
Epoch 33: loss 0.9690445670620004
Epoch 34: loss 0.9654606322739162
Epoch 35: loss 0.961535984922713
Epoch 36: loss 0.9572302348073116
Epoch 37: loss 0.9527083791229244
Epoch 38: loss 0.9477888407624241
Epoch 39: loss 0.9425312149712675
Epoch 40: loss 0.9369474718465696
Epoch 41: loss 0.9308054095185795
Epoch 42: loss 0.9243097724701932
Epoch 43: loss 0.9173018878863647
Epoch 44: loss 0.9096778432204063
Epoch 45: loss 0.901338539802879
Epoch 46: loss 0.8923130749748608
Epoch 47: loss 0.8827328880303835
Epoch 48: loss 0.8723021757025768
Epoch 49: loss 0.8610468190801475
-----------Time: 0:07:22.500643, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9862901568412781-------------


Epoch 0: loss 7.097800847821549
Epoch 1: loss 1.031560659019632
Epoch 2: loss 1.0116261629192256
Epoch 3: loss 1.003403320754592
Epoch 4: loss 0.9927422465546873
Epoch 5: loss 0.9820271598184284
Epoch 6: loss 0.9701923713728161
Epoch 7: loss 0.957035984469212
Epoch 8: loss 0.9419527749058473
Epoch 9: loss 0.9249565575419723
Epoch 10: loss 0.9061500089786959
Epoch 11: loss 0.8868250209653812
Epoch 12: loss 0.8674927403448457
Epoch 13: loss 0.848258989864617
Epoch 14: loss 0.8305202658698895
Epoch 15: loss 0.8142922297322666
Epoch 16: loss 0.7998357152018358
Epoch 17: loss 0.7875153493984942
Epoch 18: loss 0.77675952633935
Epoch 19: loss 0.7674275038357725
Epoch 20: loss 0.7598192411443991
Epoch 21: loss 0.75322739691628
Epoch 22: loss 0.7472538860224069
Epoch 23: loss 0.7420762274627001
Epoch 24: loss 0.7376881277450988
Epoch 25: loss 0.7337344813891375
Epoch 26: loss 0.7301631313811962
Epoch 27: loss 0.7271725908448478
Epoch 28: loss 0.7240275633989824
Epoch 29: loss 0.7216018610119884
Epoch 30: loss 0.7189910733032123
Epoch 31: loss 0.716666265212044
Epoch 32: loss 0.714228884625396
Epoch 33: loss 0.712429560838412
Epoch 34: loss 0.71048975603421
Epoch 35: loss 0.7086998216479676
Epoch 36: loss 0.7070755389088065
Epoch 37: loss 0.7053361564890854
Epoch 38: loss 0.7038495295264269
Epoch 39: loss 0.7023080800654384
Epoch 40: loss 0.7008781648641569
Epoch 41: loss 0.699553338323617
Epoch 42: loss 0.6982949857740314
Epoch 43: loss 0.6971785342842421
Epoch 44: loss 0.69597080267273
Epoch 45: loss 0.6948969795886688
Epoch 46: loss 0.6936880281400655
Epoch 47: loss 0.6926430523103836
Epoch 48: loss 0.6916996607837501
Epoch 49: loss 0.6906534398684624
-----------Time: 0:04:54.119180, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.119323492050171-------------


Epoch 0: loss 4.692924243202023
Epoch 1: loss 1.0340243840165733
Epoch 2: loss 1.0235222614997752
Epoch 3: loss 1.0087030360788156
Epoch 4: loss 0.9934387865955898
Epoch 5: loss 0.9754015404352226
Epoch 6: loss 0.948311234019644
Epoch 7: loss 0.9117896454109972
Epoch 8: loss 0.8664218445651358
Epoch 9: loss 0.8142801167979197
Epoch 10: loss 0.7592946520118755
Epoch 11: loss 0.7057287354972325
Epoch 12: loss 0.6577978255049958
Epoch 13: loss 0.616894207874545
Epoch 14: loss 0.5838411006997499
Epoch 15: loss 0.557147871514518
Epoch 16: loss 0.5360480541765463
Epoch 17: loss 0.5186227777848236
Epoch 18: loss 0.5043919949410984
Epoch 19: loss 0.49211974753965304
Epoch 20: loss 0.4816753339547058
Epoch 21: loss 0.4726944396585275
Epoch 22: loss 0.46478437875003
Epoch 23: loss 0.45771170226434704
Epoch 24: loss 0.4513331510697842
Epoch 25: loss 0.445771073601439
Epoch 26: loss 0.4404946077511711
Epoch 27: loss 0.4356549915540082
Epoch 28: loss 0.4313167126007609
Epoch 29: loss 0.4271531248786256
Epoch 30: loss 0.42350182392598495
Epoch 31: loss 0.41981995817174594
Epoch 32: loss 0.41665418383671965
Epoch 33: loss 0.4136462989969705
Epoch 34: loss 0.4106699413031972
Epoch 35: loss 0.40800807912301734
Epoch 36: loss 0.4053770805941515
Epoch 37: loss 0.4030242232831682
Epoch 38: loss 0.4007804155706257
Epoch 39: loss 0.39840318673392106
Epoch 40: loss 0.3966106435992006
Epoch 41: loss 0.39448813119292453
Epoch 42: loss 0.3925133138133366
Epoch 43: loss 0.3907234637616704
Epoch 44: loss 0.38894417844692475
Epoch 45: loss 0.38735766639976543
Epoch 46: loss 0.3855905841393857
Epoch 47: loss 0.3841281294044818
Epoch 48: loss 0.3826172699611948
Epoch 49: loss 0.3811971125491227
-----------Time: 0:03:46.356103, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3790972232818604-------------


Epoch 0: loss 3.630020905993048
Epoch 1: loss 1.0480710959032606
Epoch 2: loss 1.0261276166940785
Epoch 3: loss 1.0008922129172615
Epoch 4: loss 0.9678069220216718
Epoch 5: loss 0.9208937973805003
Epoch 6: loss 0.8527611672003157
Epoch 7: loss 0.7641813241314019
Epoch 8: loss 0.6627699024266299
Epoch 9: loss 0.5636195320516517
Epoch 10: loss 0.4775079667827498
Epoch 11: loss 0.40906718945360626
Epoch 12: loss 0.35755320521331857
Epoch 13: loss 0.3180681569476436
Epoch 14: loss 0.287852302290876
Epoch 15: loss 0.26421147217045277
Epoch 16: loss 0.24506690941017695
Epoch 17: loss 0.22955606854305505
Epoch 18: loss 0.21650665789405052
Epoch 19: loss 0.20521243117534965
Epoch 20: loss 0.19581931295879124
Epoch 21: loss 0.18737761233895286
Epoch 22: loss 0.17988827726547973
Epoch 23: loss 0.17354847344125723
Epoch 24: loss 0.16767371155811822
Epoch 25: loss 0.1623377329535689
Epoch 26: loss 0.15774590511922282
Epoch 27: loss 0.15320720006228142
Epoch 28: loss 0.14942950371282135
Epoch 29: loss 0.14559392399312102
Epoch 30: loss 0.14232039893610182
Epoch 31: loss 0.13913206275210166
Epoch 32: loss 0.13629419130109588
Epoch 33: loss 0.13353843220052283
Epoch 34: loss 0.13099864899885272
Epoch 35: loss 0.12876822419825165
Epoch 36: loss 0.1264490959492308
Epoch 37: loss 0.12442344266266976
Epoch 38: loss 0.12247991633599968
Epoch 39: loss 0.1205582621914009
Epoch 40: loss 0.11883631020849202
Epoch 41: loss 0.11713162539379955
Epoch 42: loss 0.11560205148979656
Epoch 43: loss 0.11409956419991371
Epoch 44: loss 0.1125857312274796
Epoch 45: loss 0.11131632747356499
Epoch 46: loss 0.1099545629396265
Epoch 47: loss 0.10871000319330378
Epoch 48: loss 0.10747468893281009
Epoch 49: loss 0.10641447425925778
-----------Time: 0:05:17.059377, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.592604398727417-------------


Epoch 0: loss 3.2201261109408637
Epoch 1: loss 1.0632215119886166
Epoch 2: loss 1.037194579402152
Epoch 3: loss 1.0023116033125208
Epoch 4: loss 0.9451328736080171
Epoch 5: loss 0.8542227021956845
Epoch 6: loss 0.7278526884955904
Epoch 7: loss 0.5838319190725676
Epoch 8: loss 0.45163530720648004
Epoch 9: loss 0.3469436100282508
Epoch 10: loss 0.27037092333516716
Epoch 11: loss 0.216431360754583
Epoch 12: loss 0.17849092798883853
Epoch 13: loss 0.1507801239000448
Epoch 14: loss 0.13035983033174403
Epoch 15: loss 0.11477096367459248
Epoch 16: loss 0.10230212185841789
Epoch 17: loss 0.09235868310477825
Epoch 18: loss 0.084336774981803
Epoch 19: loss 0.07761907895957082
Epoch 20: loss 0.07193865889785664
Epoch 21: loss 0.06727202492500207
Epoch 22: loss 0.06300524199840295
Epoch 23: loss 0.059494101965537595
Epoch 24: loss 0.05636290921472609
Epoch 25: loss 0.05357405065663681
Epoch 26: loss 0.05123649828861173
Epoch 27: loss 0.049007861238416606
Epoch 28: loss 0.047099083402741534
Epoch 29: loss 0.04532856216665323
Epoch 30: loss 0.04377595333928418
Epoch 31: loss 0.04230684859049165
Epoch 32: loss 0.04106394915885579
Epoch 33: loss 0.03984910889127094
Epoch 34: loss 0.03875203305463418
Epoch 35: loss 0.03771793928003428
Epoch 36: loss 0.03682890946330131
Epoch 37: loss 0.035845300141172994
Epoch 38: loss 0.035111280871213874
Epoch 39: loss 0.03430128894653121
Epoch 40: loss 0.0336781785939377
Epoch 41: loss 0.03298144578091018
Epoch 42: loss 0.03232917601020845
Epoch 43: loss 0.031835199071903665
Epoch 44: loss 0.031137922785815626
Epoch 45: loss 0.03066709365081923
Epoch 46: loss 0.030264000560786493
Epoch 47: loss 0.02975803077658471
Epoch 48: loss 0.02921926420913273
Epoch 49: loss 0.028887622925397687
-----------Time: 0:06:26.586725, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4706836938858032-------------


Epoch 0: loss 3.0512635050227033
Epoch 1: loss 1.066804375384021
Epoch 2: loss 1.0270325702257037
Epoch 3: loss 0.9647159793248573
Epoch 4: loss 0.8627350128105375
Epoch 5: loss 0.7160283633390803
Epoch 6: loss 0.5486408952074637
Epoch 7: loss 0.39651053925128393
Epoch 8: loss 0.279898714928474
Epoch 9: loss 0.19916297144511266
Epoch 10: loss 0.14614604326403613
Epoch 11: loss 0.1121523164459841
Epoch 12: loss 0.08935643341179579
Epoch 13: loss 0.0737485689875255
Epoch 14: loss 0.06268911380898116
Epoch 15: loss 0.054887553384247
Epoch 16: loss 0.048947175807720164
Epoch 17: loss 0.04454641557861243
Epoch 18: loss 0.0411013956726116
Epoch 19: loss 0.03844797064597319
Epoch 20: loss 0.03636194672688736
Epoch 21: loss 0.03465845990701173
Epoch 22: loss 0.03330826131062802
Epoch 23: loss 0.03217075147998761
Epoch 24: loss 0.031105662141523976
Epoch 25: loss 0.03039283750152348
Epoch 26: loss 0.02950101337978508
Epoch 27: loss 0.028968251402965377
Epoch 28: loss 0.02838457633537109
Epoch 29: loss 0.027884799265923094
Epoch 30: loss 0.027316526093030508
Epoch 31: loss 0.0268825601367052
Epoch 32: loss 0.026541950256078056
Epoch 33: loss 0.026155168882380238
Epoch 34: loss 0.02572477572354984
Epoch 35: loss 0.025448140456802562
Epoch 36: loss 0.025126070842600632
Epoch 37: loss 0.024805669677345047
Epoch 38: loss 0.024601866367063876
Epoch 39: loss 0.024288929742621687
Epoch 40: loss 0.02399136091806825
Epoch 41: loss 0.02377142031262302
Epoch 42: loss 0.023568746653578408
Epoch 43: loss 0.023248362448623546
Epoch 44: loss 0.023129747334772507
Epoch 45: loss 0.022924816032315547
Epoch 46: loss 0.022603688118427327
Epoch 47: loss 0.02252248489582033
Epoch 48: loss 0.022356983734058776
Epoch 49: loss 0.022094320803637605
-----------Time: 0:07:36.994470, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2695587873458862-------------


Epoch 0: loss 16.129215472803743
Epoch 1: loss 16.129210856895593
Epoch 2: loss 16.12920781281281
Epoch 3: loss 16.129200039770762
Epoch 4: loss 16.129207730358097
Epoch 5: loss 16.129204504252648
Epoch 6: loss 16.129199407617964
Epoch 7: loss 16.129200331733045
Epoch 8: loss 16.12920072637478
Epoch 9: loss 16.129197641842516
Epoch 10: loss 16.12919725912819
Epoch 11: loss 16.129193603117354
Epoch 12: loss 16.129194366471673
Epoch 13: loss 16.129196323604283
Epoch 14: loss 16.129188065168453
Epoch 15: loss 16.12918985272439
Epoch 16: loss 16.129191778223433
Epoch 17: loss 16.129190457910866
Epoch 18: loss 16.129181939146324
Epoch 19: loss 16.129184965078696
Epoch 20: loss 16.12918566205438
Epoch 21: loss 16.129185445286645
Epoch 22: loss 16.12918385375512
Epoch 23: loss 16.12917927311062
Epoch 24: loss 16.129178720301038
Epoch 25: loss 16.129177237153378
Epoch 26: loss 16.12917844960066
Epoch 27: loss 16.12917361796194
Epoch 28: loss 16.129178603101256
Epoch 29: loss 16.129177259971033
Epoch 30: loss 16.129177414508796
Epoch 31: loss 16.129171221070862
Epoch 32: loss 16.12917105460569
Epoch 33: loss 16.129165023484262
Epoch 34: loss 16.129165223138756
Epoch 35: loss 16.12916816557925
Epoch 36: loss 16.12916710040957
Epoch 37: loss 16.12915720117682
Epoch 38: loss 16.129161879314942
Epoch 39: loss 16.1291645712798
Epoch 40: loss 16.129161067732397
Epoch 41: loss 16.129154593741006
Epoch 42: loss 16.129155601347964
Epoch 43: loss 16.129157587002645
Epoch 44: loss 16.129158308351734
Epoch 45: loss 16.129157806881878
Epoch 46: loss 16.12915468812313
Epoch 47: loss 16.12915063332189
Epoch 48: loss 16.129149425541854
Epoch 49: loss 16.129148424157894
-----------Time: 0:03:27.748228, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017197608947754-------------


Epoch 0: loss 16.129201543661743
Epoch 1: loss 16.12919845083215
Epoch 2: loss 16.129199859822425
Epoch 3: loss 16.129193919453044
Epoch 4: loss 16.129195794649522
Epoch 5: loss 16.129197619543444
Epoch 6: loss 16.12919357563245
Epoch 7: loss 16.12919019084057
Epoch 8: loss 16.12919476474349
Epoch 9: loss 16.129191529303547
Epoch 10: loss 16.129184348483395
Epoch 11: loss 16.12918720276476
Epoch 12: loss 16.129188730510567
Epoch 13: loss 16.1291834985257
Epoch 14: loss 16.129183094030882
Epoch 15: loss 16.12918003802069
Epoch 16: loss 16.129177308717846
Epoch 17: loss 16.12918361779981
Epoch 18: loss 16.129174696614783
Epoch 19: loss 16.12917628192331
Epoch 20: loss 16.12917774329048
Epoch 21: loss 16.12917118891871
Epoch 22: loss 16.129180334131636
Epoch 23: loss 16.129170170421506
Epoch 24: loss 16.1291669614293
Epoch 25: loss 16.12917510785118
Epoch 26: loss 16.129166263935033
Epoch 27: loss 16.12916837404966
Epoch 28: loss 16.12917280067498
Epoch 29: loss 16.12916041068761
Epoch 30: loss 16.129164370588143
Epoch 31: loss 16.12916283817509
Epoch 32: loss 16.129163431434154
Epoch 33: loss 16.12916267741433
Epoch 34: loss 16.129156634884076
Epoch 35: loss 16.129159045258316
Epoch 36: loss 16.12915805839468
Epoch 37: loss 16.129155671875264
Epoch 38: loss 16.12915216366061
Epoch 39: loss 16.12915318475073
Epoch 40: loss 16.12915147757517
Epoch 41: loss 16.12914917662197
Epoch 42: loss 16.12914829658645
Epoch 43: loss 16.129147503154314
Epoch 44: loss 16.129145998226164
Epoch 45: loss 16.12914297021946
Epoch 46: loss 16.129143320263047
Epoch 47: loss 16.12914268292442
Epoch 48: loss 16.129142525793743
Epoch 49: loss 16.12914339960626
-----------Time: 0:04:24.061591, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017189979553223-------------


Epoch 0: loss 16.129187431978494
Epoch 1: loss 16.12918894053672
Epoch 2: loss 16.129189158860207
Epoch 3: loss 16.129186214863964
Epoch 4: loss 16.12918388953735
Epoch 5: loss 16.12918304528407
Epoch 6: loss 16.12918685894417
Epoch 7: loss 16.129182169397218
Epoch 8: loss 16.12918387190553
Epoch 9: loss 16.129174452880726
Epoch 10: loss 16.12917625132691
Epoch 11: loss 16.129179084864955
Epoch 12: loss 16.12917373360597
Epoch 13: loss 16.12917215296469
Epoch 14: loss 16.129169269642667
Epoch 15: loss 16.12917384095267
Epoch 16: loss 16.129168507325513
Epoch 17: loss 16.129170215019652
Epoch 18: loss 16.129167268430493
Epoch 19: loss 16.129164916656144
Epoch 20: loss 16.129168368863827
Epoch 21: loss 16.129164705592824
Epoch 22: loss 16.12915796504972
Epoch 23: loss 16.129161151742856
Epoch 24: loss 16.129160431430936
Epoch 25: loss 16.12915969245002
Epoch 26: loss 16.12915461967016
Epoch 27: loss 16.12915662866108
Epoch 28: loss 16.129155683284093
Epoch 29: loss 16.12915590368191
Epoch 30: loss 16.129157893485257
Epoch 31: loss 16.129148129602694
Epoch 32: loss 16.129143516806042
Epoch 33: loss 16.129146709722175
Epoch 34: loss 16.129149708688228
Epoch 35: loss 16.1291453650362
Epoch 36: loss 16.12914574826911
Epoch 37: loss 16.129142060106123
Epoch 38: loss 16.129144690359592
Epoch 39: loss 16.129141744289015
Epoch 40: loss 16.129137699859438
Epoch 41: loss 16.129135998906875
Epoch 42: loss 16.129135783694892
Epoch 43: loss 16.129135357419585
Epoch 44: loss 16.12913867272133
Epoch 45: loss 16.129128973661654
Epoch 46: loss 16.129134168827125
Epoch 47: loss 16.129132270294406
Epoch 48: loss 16.12913057660201
Epoch 49: loss 16.12912673130834
-----------Time: 0:05:54.723767, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017179489135742-------------


Epoch 0: loss 16.129179475876608
Epoch 1: loss 16.12917552323624
Epoch 2: loss 16.129171628158595
Epoch 3: loss 16.129170820206127
Epoch 4: loss 16.129170598771147
Epoch 5: loss 16.1291701154517
Epoch 6: loss 16.129162860474164
Epoch 7: loss 16.129166765923472
Epoch 8: loss 16.12916299219427
Epoch 9: loss 16.12916245649793
Epoch 10: loss 16.12916037490538
Epoch 11: loss 16.129164101443518
Epoch 12: loss 16.129161400662746
Epoch 13: loss 16.12915893894878
Epoch 14: loss 16.129156452342826
Epoch 15: loss 16.129157803251797
Epoch 16: loss 16.129156509905552
Epoch 17: loss 16.12915216677211
Epoch 18: loss 16.129156873432304
Epoch 19: loss 16.12914537800078
Epoch 20: loss 16.129149899008222
Epoch 21: loss 16.129145936514774
Epoch 22: loss 16.12914985492866
Epoch 23: loss 16.12914737402712
Epoch 24: loss 16.129146994424293
Epoch 25: loss 16.12914416140483
Epoch 26: loss 16.129148126491195
Epoch 27: loss 16.129144488112182
Epoch 28: loss 16.129141091911478
Epoch 29: loss 16.129138399428037
Epoch 30: loss 16.12913744264222
Epoch 31: loss 16.12913542327964
Epoch 32: loss 16.129134664592566
Epoch 33: loss 16.129132409274675
Epoch 34: loss 16.129134302621566
Epoch 35: loss 16.129130150326702
Epoch 36: loss 16.129129933558968
Epoch 37: loss 16.129132246439582
Epoch 38: loss 16.12912526008809
Epoch 39: loss 16.12912257797631
Epoch 40: loss 16.129125525084056
Epoch 41: loss 16.129123979187842
Epoch 42: loss 16.129121834846732
Epoch 43: loss 16.129116767771286
Epoch 44: loss 16.12911921081626
Epoch 45: loss 16.129118053338786
Epoch 46: loss 16.12911752645836
Epoch 47: loss 16.129120038474884
Epoch 48: loss 16.12911309205429
Epoch 49: loss 16.129113566039244
-----------Time: 0:06:34.109266, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017182350158691-------------


Epoch 0: loss 16.129176305259552
Epoch 1: loss 16.129177894716744
Epoch 2: loss 16.129172475004793
Epoch 3: loss 16.129173554694802
Epoch 4: loss 16.129174141212285
Epoch 5: loss 16.129170876213102
Epoch 6: loss 16.129168825735537
Epoch 7: loss 16.129166182517487
Epoch 8: loss 16.129167891767377
Epoch 9: loss 16.12916914829422
Epoch 10: loss 16.129166340166748
Epoch 11: loss 16.129168020894568
Epoch 12: loss 16.129162316480496
Epoch 13: loss 16.129162153126817
Epoch 14: loss 16.129161178190596
Epoch 15: loss 16.12916024629677
Epoch 16: loss 16.12915572943799
Epoch 17: loss 16.129159358482507
Epoch 18: loss 16.129153046807627
Epoch 19: loss 16.12915037247459
Epoch 20: loss 16.12915294723967
Epoch 21: loss 16.12915218907118
Epoch 22: loss 16.129154926152772
Epoch 23: loss 16.129151655449174
Epoch 24: loss 16.129146478952695
Epoch 25: loss 16.129145693817886
Epoch 26: loss 16.129146356567084
Epoch 27: loss 16.129148191832666
Epoch 28: loss 16.12914810678504
Epoch 29: loss 16.129141739103183
Epoch 30: loss 16.129143101939565
Epoch 31: loss 16.12913913840895
Epoch 32: loss 16.129139118702792
Epoch 33: loss 16.129136515934224
Epoch 34: loss 16.1291387655477
Epoch 35: loss 16.129138239704442
Epoch 36: loss 16.12913643503526
Epoch 37: loss 16.129130431917325
Epoch 38: loss 16.12913802034379
Epoch 39: loss 16.129130211000923
Epoch 40: loss 16.129127857670827
Epoch 41: loss 16.12912793856979
Epoch 42: loss 16.12912471039001
Epoch 43: loss 16.129128148595946
Epoch 44: loss 16.129124551184997
Epoch 45: loss 16.129122073913543
Epoch 46: loss 16.129122200447817
Epoch 47: loss 16.129117775378244
Epoch 48: loss 16.1291200747757
Epoch 49: loss 16.129121193359442
-----------Time: 0:06:40.381879, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017180442810059-------------


Epoch 0: loss 16.129299073066317
Epoch 1: loss 16.129288031395003
Epoch 2: loss 16.129276760509963
Epoch 3: loss 16.129264137678785
Epoch 4: loss 16.12924822080777
Epoch 5: loss 16.129243590379293
Epoch 6: loss 16.129226360455878
Epoch 7: loss 16.129212072972955
Epoch 8: loss 16.129199282120855
Epoch 9: loss 16.129190871221596
Epoch 10: loss 16.129178659626813
Epoch 11: loss 16.129166759181892
Epoch 12: loss 16.129153788900037
Epoch 13: loss 16.129142180935983
Epoch 14: loss 16.129127645051756
Epoch 15: loss 16.129112421007765
Epoch 16: loss 16.129104991267724
Epoch 17: loss 16.129094746140048
Epoch 18: loss 16.129082038779828
Epoch 19: loss 16.129067077657464
Epoch 20: loss 16.129053212301184
Epoch 21: loss 16.129037993443017
Epoch 22: loss 16.12902803353605
Epoch 23: loss 16.129013605517105
Epoch 24: loss 16.12900563541348
Epoch 25: loss 16.1289947093862
Epoch 26: loss 16.128984077914115
Epoch 27: loss 16.128971013768723
Epoch 28: loss 16.128953089462538
Epoch 29: loss 16.128941425491508
Epoch 30: loss 16.128929476299774
Epoch 31: loss 16.1289169883002
Epoch 32: loss 16.12890514334367
Epoch 33: loss 16.128894826132942
Epoch 34: loss 16.128881508918997
Epoch 35: loss 16.128866291616585
Epoch 36: loss 16.128856453576642
Epoch 37: loss 16.128846907498982
Epoch 38: loss 16.128832289160044
Epoch 39: loss 16.12882372527877
Epoch 40: loss 16.128804541852826
Epoch 41: loss 16.12879291055253
Epoch 42: loss 16.128783187119453
Epoch 43: loss 16.128769605428126
Epoch 44: loss 16.128761747338448
Epoch 45: loss 16.128745224762376
Epoch 46: loss 16.12873102128992
Epoch 47: loss 16.128719271752676
Epoch 48: loss 16.128710644604265
Epoch 49: loss 16.128691420210256
-----------Time: 0:04:16.372006, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.0171380043029785-------------


Epoch 0: loss 16.12919338946112
Epoch 1: loss 16.12919209767062
Epoch 2: loss 16.12917781174345
Epoch 3: loss 16.129161300057625
Epoch 4: loss 16.129152201517176
Epoch 5: loss 16.129144909720242
Epoch 6: loss 16.129127074091766
Epoch 7: loss 16.129112681855062
Epoch 8: loss 16.12910690587652
Epoch 9: loss 16.129084077848567
Epoch 10: loss 16.12907541128784
Epoch 11: loss 16.129061083355438
Epoch 12: loss 16.129047883859855
Epoch 13: loss 16.129035111676746
Epoch 14: loss 16.12902508124247
Epoch 15: loss 16.129010444234545
Epoch 16: loss 16.128996830391067
Epoch 17: loss 16.128989540668464
Epoch 18: loss 16.128976264422583
Epoch 19: loss 16.128966860436694
Epoch 20: loss 16.128951436219623
Epoch 21: loss 16.12894211209553
Epoch 22: loss 16.128925286666927
Epoch 23: loss 16.128912818373514
Epoch 24: loss 16.128904710326783
Epoch 25: loss 16.12889008057902
Epoch 26: loss 16.128873904416455
Epoch 27: loss 16.12886695021712
Epoch 28: loss 16.12884989401904
Epoch 29: loss 16.128839427974963
Epoch 30: loss 16.12882702450444
Epoch 31: loss 16.128815342901582
Epoch 32: loss 16.12880073752722
Epoch 33: loss 16.128796246079013
Epoch 34: loss 16.128779389016845
Epoch 35: loss 16.128768298080136
Epoch 36: loss 16.128754630822602
Epoch 37: loss 16.128746660718974
Epoch 38: loss 16.128730770814283
Epoch 39: loss 16.12871316491812
Epoch 40: loss 16.128708744515798
Epoch 41: loss 16.128692208456567
Epoch 42: loss 16.12867626098915
Epoch 43: loss 16.128668260807704
Epoch 44: loss 16.1286552122198
Epoch 45: loss 16.128640496905827
Epoch 46: loss 16.12862632247402
Epoch 47: loss 16.128614339574387
Epoch 48: loss 16.12860772608414
Epoch 49: loss 16.12859461215477
-----------Time: 0:05:04.200801, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017107963562012-------------


Epoch 0: loss 16.12915722969889
Epoch 1: loss 16.12914268240584
Epoch 2: loss 16.129125566570703
Epoch 3: loss 16.12911248583065
Epoch 4: loss 16.12910533716265
Epoch 5: loss 16.129095801456653
Epoch 6: loss 16.129082470759545
Epoch 7: loss 16.12906763617146
Epoch 8: loss 16.129054334515004
Epoch 9: loss 16.129041860517177
Epoch 10: loss 16.129029633364905
Epoch 11: loss 16.129018828686064
Epoch 12: loss 16.12900441778037
Epoch 13: loss 16.128991319408488
Epoch 14: loss 16.128982323547497
Epoch 15: loss 16.128973043502967
Epoch 16: loss 16.1289573553271
Epoch 17: loss 16.128947035004874
Epoch 18: loss 16.128932546830296
Epoch 19: loss 16.128919904811543
Epoch 20: loss 16.12890891188704
Epoch 21: loss 16.12889857911882
Epoch 22: loss 16.12888143579878
Epoch 23: loss 16.128866425929605
Epoch 24: loss 16.128859267408526
Epoch 25: loss 16.12884372858459
Epoch 26: loss 16.12883146979875
Epoch 27: loss 16.12882001896537
Epoch 28: loss 16.128811349293148
Epoch 29: loss 16.128799255935313
Epoch 30: loss 16.12878957865613
Epoch 31: loss 16.128773066970304
Epoch 32: loss 16.128766697214115
Epoch 33: loss 16.128750538164795
Epoch 34: loss 16.12873628801986
Epoch 35: loss 16.12872478014234
Epoch 36: loss 16.128707533105683
Epoch 37: loss 16.1287025375947
Epoch 38: loss 16.128678423480665
Epoch 39: loss 16.128673096595087
Epoch 40: loss 16.128657558289735
Epoch 41: loss 16.128641196474426
Epoch 42: loss 16.12863885559032
Epoch 43: loss 16.128626113485033
Epoch 44: loss 16.128610125568134
Epoch 45: loss 16.12860110429657
Epoch 46: loss 16.12858283616979
Epoch 47: loss 16.128575319826375
Epoch 48: loss 16.128560082817803
Epoch 49: loss 16.12854752429093
-----------Time: 0:05:30.008442, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.01711368560791-------------


Epoch 0: loss 16.129173755905043
Epoch 1: loss 16.12915736556766
Epoch 2: loss 16.129149067200935
Epoch 3: loss 16.12912708446343
Epoch 4: loss 16.129116781773032
Epoch 5: loss 16.129109182456318
Epoch 6: loss 16.129098363257153
Epoch 7: loss 16.129084322101207
Epoch 8: loss 16.12907576755443
Epoch 9: loss 16.129057268658173
Epoch 10: loss 16.12904729682379
Epoch 11: loss 16.1290318762368
Epoch 12: loss 16.12901711528751
Epoch 13: loss 16.129007567654103
Epoch 14: loss 16.128997938603145
Epoch 15: loss 16.128985595806842
Epoch 16: loss 16.128972301410553
Epoch 17: loss 16.12896150451046
Epoch 18: loss 16.128945097059837
Epoch 19: loss 16.128932953918024
Epoch 20: loss 16.128921021839535
Epoch 21: loss 16.12890564377636
Epoch 22: loss 16.128896527604088
Epoch 23: loss 16.12888627417908
Epoch 24: loss 16.12887586725348
Epoch 25: loss 16.12885974657931
Epoch 26: loss 16.12885243092755
Epoch 27: loss 16.12883837265836
Epoch 28: loss 16.1288220155103
Epoch 29: loss 16.12880843018889
Epoch 30: loss 16.128797159303847
Epoch 31: loss 16.128785408729442
Epoch 32: loss 16.12877330085128
Epoch 33: loss 16.128764498940363
Epoch 34: loss 16.128746923640605
Epoch 35: loss 16.128739597617187
Epoch 36: loss 16.12872450736763
Epoch 37: loss 16.12870882334043
Epoch 38: loss 16.128693989789504
Epoch 39: loss 16.12868824129587
Epoch 40: loss 16.128674555887926
Epoch 41: loss 16.128666507996833
Epoch 42: loss 16.128651452492345
Epoch 43: loss 16.128641374348426
Epoch 44: loss 16.128626784012976
Epoch 45: loss 16.1286074512351
Epoch 46: loss 16.128598113109263
Epoch 47: loss 16.128586469362972
Epoch 48: loss 16.128574979117282
Epoch 49: loss 16.12856226501548
-----------Time: 0:05:32.517194, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017114162445068-------------


Epoch 0: loss 16.129161365399092
Epoch 1: loss 16.129152305233795
Epoch 2: loss 16.129145358294622
Epoch 3: loss 16.129128155337526
Epoch 4: loss 16.12911887425583
Epoch 5: loss 16.129109657478438
Epoch 6: loss 16.129094283045344
Epoch 7: loss 16.12908144292785
Epoch 8: loss 16.1290683528533
Epoch 9: loss 16.12905843184006
Epoch 10: loss 16.12904861143194
Epoch 11: loss 16.12903600623259
Epoch 12: loss 16.129016548476187
Epoch 13: loss 16.129009323057886
Epoch 14: loss 16.128995847157515
Epoch 15: loss 16.12898074186905
Epoch 16: loss 16.12897038265309
Epoch 17: loss 16.128957245387483
Epoch 18: loss 16.128941664558315
Epoch 19: loss 16.128933015629414
Epoch 20: loss 16.128921261943507
Epoch 21: loss 16.1289073914014
Epoch 22: loss 16.128891047736495
Epoch 23: loss 16.128882666915054
Epoch 24: loss 16.128873005193366
Epoch 25: loss 16.12885601329959
Epoch 26: loss 16.128844523053896
Epoch 27: loss 16.12883055916683
Epoch 28: loss 16.128819313173775
Epoch 29: loss 16.1288084882702
Epoch 30: loss 16.128797339770767
Epoch 31: loss 16.128779228256086
Epoch 32: loss 16.128776337155315
Epoch 33: loss 16.128759620110582
Epoch 34: loss 16.128746826146983
Epoch 35: loss 16.128729908929174
Epoch 36: loss 16.12872740209848
Epoch 37: loss 16.12871061867511
Epoch 38: loss 16.128698863433456
Epoch 39: loss 16.128676467903805
Epoch 40: loss 16.128672652169374
Epoch 41: loss 16.128657481539438
Epoch 42: loss 16.128649951712863
Epoch 43: loss 16.128634341843043
Epoch 44: loss 16.12862481547154
Epoch 45: loss 16.1286044844212
Epoch 46: loss 16.128598226160378
Epoch 47: loss 16.128585560805387
Epoch 48: loss 16.128575841520973
Epoch 49: loss 16.1285625497176
-----------Time: 0:07:12.416219, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017111778259277-------------


Epoch 0: loss 16.12925096618669
Epoch 1: loss 16.129135704351675
Epoch 2: loss 16.129005434721822
Epoch 3: loss 16.128880751269094
Epoch 4: loss 16.128756361334396
Epoch 5: loss 16.128628899832016
Epoch 6: loss 16.12850686219025
Epoch 7: loss 16.128381380638135
Epoch 8: loss 16.12825882700619
Epoch 9: loss 16.12813490846353
Epoch 10: loss 16.128011228469095
Epoch 11: loss 16.127888047870183
Epoch 12: loss 16.127762022324397
Epoch 13: loss 16.12763684673631
Epoch 14: loss 16.127514667521357
Epoch 15: loss 16.127389283462865
Epoch 16: loss 16.127266750055657
Epoch 17: loss 16.12714613851741
Epoch 18: loss 16.12702422377984
Epoch 19: loss 16.12689686806841
Epoch 20: loss 16.126770634570804
Epoch 21: loss 16.12664801300447
Epoch 22: loss 16.126526539581118
Epoch 23: loss 16.12640710159642
Epoch 24: loss 16.126273680018496
Epoch 25: loss 16.126152558194484
Epoch 26: loss 16.12602950309268
Epoch 27: loss 16.125903441764663
Epoch 28: loss 16.125781238694888
Epoch 29: loss 16.125661228713035
Epoch 30: loss 16.125534537825136
Epoch 31: loss 16.125408647629538
Epoch 32: loss 16.12528533323619
Epoch 33: loss 16.12516233932719
Epoch 34: loss 16.12504078085621
Epoch 35: loss 16.12491320267263
Epoch 36: loss 16.124790350855402
Epoch 37: loss 16.12466410646755
Epoch 38: loss 16.124546986549298
Epoch 39: loss 16.12442239696011
Epoch 40: loss 16.124294916788738
Epoch 41: loss 16.12417215261205
Epoch 42: loss 16.124047099409577
Epoch 43: loss 16.123924784844437
Epoch 44: loss 16.12380168047724
Epoch 45: loss 16.123679073949816
Epoch 46: loss 16.123549260154505
Epoch 47: loss 16.123432495465675
Epoch 48: loss 16.12330628737864
Epoch 49: loss 16.123180940139548
-----------Time: 0:05:02.510800, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016448974609375-------------


Epoch 0: loss 16.129129917482892
Epoch 1: loss 16.129008130835345
Epoch 2: loss 16.128881157319658
Epoch 3: loss 16.128759441717996
Epoch 4: loss 16.12863268808154
Epoch 5: loss 16.128517738952137
Epoch 6: loss 16.128384955231425
Epoch 7: loss 16.128266654499463
Epoch 8: loss 16.128141780726736
Epoch 9: loss 16.128011869437803
Epoch 10: loss 16.127886252016914
Epoch 11: loss 16.127769805219526
Epoch 12: loss 16.127646157377242
Epoch 13: loss 16.127521149291493
Epoch 14: loss 16.127393420718818
Epoch 15: loss 16.127273518602248
Epoch 16: loss 16.127146793487864
Epoch 17: loss 16.12701979559877
Epoch 18: loss 16.12689776573575
Epoch 19: loss 16.126770218667158
Epoch 20: loss 16.12665151862621
Epoch 21: loss 16.126525140962503
Epoch 22: loss 16.126397859408456
Epoch 23: loss 16.126274190822848
Epoch 24: loss 16.12614899397285
Epoch 25: loss 16.126035611482987
Epoch 26: loss 16.125904266311785
Epoch 27: loss 16.125780951918436
Epoch 28: loss 16.125659452565927
Epoch 29: loss 16.125531671616354
Epoch 30: loss 16.125414172613862
Epoch 31: loss 16.12528563142147
Epoch 32: loss 16.12516541452496
Epoch 33: loss 16.125040050691208
Epoch 34: loss 16.12491655790527
Epoch 35: loss 16.124788096574676
Epoch 36: loss 16.124666988752406
Epoch 37: loss 16.124543701325376
Epoch 38: loss 16.12441659608958
Epoch 39: loss 16.124293266138736
Epoch 40: loss 16.12416782607327
Epoch 41: loss 16.12404577754126
Epoch 42: loss 16.123925330393856
Epoch 43: loss 16.12379946197875
Epoch 44: loss 16.12367670350648
Epoch 45: loss 16.123554629563895
Epoch 46: loss 16.12342706227056
Epoch 47: loss 16.123304245716984
Epoch 48: loss 16.123176707464303
Epoch 49: loss 16.123050764373232
-----------Time: 0:04:14.765132, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016412258148193-------------


Epoch 0: loss 16.1291053786493
Epoch 1: loss 16.128982864429666
Epoch 2: loss 16.12885452444752
Epoch 3: loss 16.128732041861454
Epoch 4: loss 16.128607944407626
Epoch 5: loss 16.128485251276825
Epoch 6: loss 16.128358603431323
Epoch 7: loss 16.12823717564328
Epoch 8: loss 16.128116360301878
Epoch 9: loss 16.1279923717505
Epoch 10: loss 16.127872484672842
Epoch 11: loss 16.127740843909276
Epoch 12: loss 16.127620958387364
Epoch 13: loss 16.12749624174532
Epoch 14: loss 16.12737111957129
Epoch 15: loss 16.127245558157377
Epoch 16: loss 16.127123172546355
Epoch 17: loss 16.12700390776841
Epoch 18: loss 16.12687673615398
Epoch 19: loss 16.12675692427087
Epoch 20: loss 16.12662406431844
Epoch 21: loss 16.12650160506862
Epoch 22: loss 16.126379383848437
Epoch 23: loss 16.126256941193276
Epoch 24: loss 16.126134234579315
Epoch 25: loss 16.12600998258772
Epoch 26: loss 16.125882855571437
Epoch 27: loss 16.125759421903975
Epoch 28: loss 16.1256394248867
Epoch 29: loss 16.125511440652552
Epoch 30: loss 16.125383563765112
Epoch 31: loss 16.125262995787843
Epoch 32: loss 16.12514088295153
Epoch 33: loss 16.12501735075328
Epoch 34: loss 16.12489599556687
Epoch 35: loss 16.124771151871965
Epoch 36: loss 16.124639775585777
Epoch 37: loss 16.12451989006387
Epoch 38: loss 16.124395515686665
Epoch 39: loss 16.124269898784362
Epoch 40: loss 16.12414753702816
Epoch 41: loss 16.12402579134868
Epoch 42: loss 16.123901821984877
Epoch 43: loss 16.123774547690992
Epoch 44: loss 16.12365368723286
Epoch 45: loss 16.123531175606143
Epoch 46: loss 16.12340737530043
Epoch 47: loss 16.123282448632228
Epoch 48: loss 16.123163891720214
Epoch 49: loss 16.123026763828893
-----------Time: 0:05:03.937864, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016419410705566-------------


Epoch 0: loss 16.129116298972168
Epoch 1: loss 16.128989726839798
Epoch 2: loss 16.128867937080752
Epoch 3: loss 16.128741958207446
Epoch 4: loss 16.128616393682034
Epoch 5: loss 16.128493487932165
Epoch 6: loss 16.12836922556891
Epoch 7: loss 16.12824251290052
Epoch 8: loss 16.12812114215662
Epoch 9: loss 16.12800178714522
Epoch 10: loss 16.127874358313573
Epoch 11: loss 16.127751371146157
Epoch 12: loss 16.12762780679575
Epoch 13: loss 16.12750079023767
Epoch 14: loss 16.127377001859365
Epoch 15: loss 16.127256981505848
Epoch 16: loss 16.127133003326133
Epoch 17: loss 16.127008861792742
Epoch 18: loss 16.126881341171888
Epoch 19: loss 16.126758389786705
Epoch 20: loss 16.126636454824393
Epoch 21: loss 16.126509696520692
Epoch 22: loss 16.126386305895625
Epoch 23: loss 16.126261527023605
Epoch 24: loss 16.126144646172165
Epoch 25: loss 16.12601451759691
Epoch 26: loss 16.125889971050118
Epoch 27: loss 16.12576582847956
Epoch 28: loss 16.12563849766012
Epoch 29: loss 16.125518940919893
Epoch 30: loss 16.12539528944753
Epoch 31: loss 16.125272822937543
Epoch 32: loss 16.12514398822712
Epoch 33: loss 16.12502550287957
Epoch 34: loss 16.124902807155856
Epoch 35: loss 16.1247773022675
Epoch 36: loss 16.124652035927372
Epoch 37: loss 16.124522439418374
Epoch 38: loss 16.124398794169007
Epoch 39: loss 16.12428272904878
Epoch 40: loss 16.124155055445907
Epoch 41: loss 16.124030494897376
Epoch 42: loss 16.12390733711612
Epoch 43: loss 16.12377984242442
Epoch 44: loss 16.12365970642687
Epoch 45: loss 16.123537352449418
Epoch 46: loss 16.123409726556194
Epoch 47: loss 16.12328855287387
Epoch 48: loss 16.12316128791448
Epoch 49: loss 16.12303405873733
-----------Time: 0:06:11.586922, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016422748565674-------------


Epoch 0: loss 16.12911603501337
Epoch 1: loss 16.128988881549347
Epoch 2: loss 16.128860830936567
Epoch 3: loss 16.12874538552314
Epoch 4: loss 16.12861876360679
Epoch 5: loss 16.128489788360348
Epoch 6: loss 16.128370035077126
Epoch 7: loss 16.128244428546484
Epoch 8: loss 16.128120185370804
Epoch 9: loss 16.12799781791019
Epoch 10: loss 16.12787348553822
Epoch 11: loss 16.127748780823584
Epoch 12: loss 16.127625601261837
Epoch 13: loss 16.127495291701084
Epoch 14: loss 16.127378670659773
Epoch 15: loss 16.127257473122626
Epoch 16: loss 16.127129807298505
Epoch 17: loss 16.127006819612504
Epoch 18: loss 16.126878800633293
Epoch 19: loss 16.126757584945736
Epoch 20: loss 16.126629474177317
Epoch 21: loss 16.12651094008296
Epoch 22: loss 16.126391152573255
Epoch 23: loss 16.126259477583204
Epoch 24: loss 16.126139538647234
Epoch 25: loss 16.126016873519923
Epoch 26: loss 16.12589144278895
Epoch 27: loss 16.125767309034305
Epoch 28: loss 16.125645166120172
Epoch 29: loss 16.125524005402426
Epoch 30: loss 16.125395432057882
Epoch 31: loss 16.125270961742807
Epoch 32: loss 16.125144022453604
Epoch 33: loss 16.125019538136783
Epoch 34: loss 16.12489745589687
Epoch 35: loss 16.12477211176928
Epoch 36: loss 16.124654803605363
Epoch 37: loss 16.124523633196667
Epoch 38: loss 16.124401124162866
Epoch 39: loss 16.124276352032428
Epoch 40: loss 16.124155552248517
Epoch 41: loss 16.124029557817718
Epoch 42: loss 16.123910118795855
Epoch 43: loss 16.123781032054044
Epoch 44: loss 16.123660105217276
Epoch 45: loss 16.123531048553286
Epoch 46: loss 16.123412114631357
Epoch 47: loss 16.12328684673548
Epoch 48: loss 16.123161149452795
Epoch 49: loss 16.123038379571696
-----------Time: 0:08:05.173662, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016420364379883-------------


Epoch 0: loss 16.12878398366309
Epoch 1: loss 16.127539762794616
Epoch 2: loss 16.126300473651398
Epoch 3: loss 16.125056976206345
Epoch 4: loss 16.123818423451127
Epoch 5: loss 16.122583224891386
Epoch 6: loss 16.12134521353692
Epoch 7: loss 16.120100899842072
Epoch 8: loss 16.118857957799516
Epoch 9: loss 16.11762624567394
Epoch 10: loss 16.116379586946323
Epoch 11: loss 16.115140719929748
Epoch 12: loss 16.11390311981168
Epoch 13: loss 16.112659519687174
Epoch 14: loss 16.11142315794554
Epoch 15: loss 16.110185936393137
Epoch 16: loss 16.10895132486946
Epoch 17: loss 16.10770804419214
Epoch 18: loss 16.10646450681619
Epoch 19: loss 16.105231238941318
Epoch 20: loss 16.103994667692117
Epoch 21: loss 16.10274733739939
Epoch 22: loss 16.101511406600313
Epoch 23: loss 16.10027285643801
Epoch 24: loss 16.099037791672707
Epoch 25: loss 16.097795155075595
Epoch 26: loss 16.096556776045716
Epoch 27: loss 16.095320789239665
Epoch 28: loss 16.094083194307427
Epoch 29: loss 16.09284064379511
Epoch 30: loss 16.09160377799071
Epoch 31: loss 16.090368847019846
Epoch 32: loss 16.089130751136334
Epoch 33: loss 16.0878922357191
Epoch 34: loss 16.086650381663883
Epoch 35: loss 16.08541386849599
Epoch 36: loss 16.08417712352145
Epoch 37: loss 16.08294007050855
Epoch 38: loss 16.081698712737357
Epoch 39: loss 16.080467000093197
Epoch 40: loss 16.07922123695859
Epoch 41: loss 16.077985616790787
Epoch 42: loss 16.076750032923805
Epoch 43: loss 16.075508592179315
Epoch 44: loss 16.074275528626185
Epoch 45: loss 16.073038659191702
Epoch 46: loss 16.071797577306718
Epoch 47: loss 16.070558752813955
Epoch 48: loss 16.06932607145657
Epoch 49: loss 16.068090619309693
-----------Time: 0:03:31.421616, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009541988372803-------------


Epoch 0: loss 16.128562987920315
Epoch 1: loss 16.127329563951932
Epoch 2: loss 16.126089122517072
Epoch 3: loss 16.12484574019747
Epoch 4: loss 16.12360992659817
Epoch 5: loss 16.122365970207078
Epoch 6: loss 16.12112553966246
Epoch 7: loss 16.119886048271837
Epoch 8: loss 16.11864766250038
Epoch 9: loss 16.117413748989552
Epoch 10: loss 16.11617151118284
Epoch 11: loss 16.11492737121333
Epoch 12: loss 16.11369597023761
Epoch 13: loss 16.11244810684146
Epoch 14: loss 16.11121481563035
Epoch 15: loss 16.109971724754445
Epoch 16: loss 16.108738373387695
Epoch 17: loss 16.107491102732027
Epoch 18: loss 16.106253205465844
Epoch 19: loss 16.10501519151846
Epoch 20: loss 16.10378321698984
Epoch 21: loss 16.102533437947724
Epoch 22: loss 16.10129609297254
Epoch 23: loss 16.100059685595596
Epoch 24: loss 16.098825305878567
Epoch 25: loss 16.097581423644854
Epoch 26: loss 16.096338452561646
Epoch 27: loss 16.0951094515885
Epoch 28: loss 16.09386557920787
Epoch 29: loss 16.092632413493867
Epoch 30: loss 16.09139306730651
Epoch 31: loss 16.090152847825216
Epoch 32: loss 16.088915425581153
Epoch 33: loss 16.087678242403893
Epoch 34: loss 16.086443597172316
Epoch 35: loss 16.085201778899332
Epoch 36: loss 16.0839619841376
Epoch 37: loss 16.0827312604314
Epoch 38: loss 16.081488383730317
Epoch 39: loss 16.080252916025948
Epoch 40: loss 16.07901426992577
Epoch 41: loss 16.077772243700966
Epoch 42: loss 16.076539057762226
Epoch 43: loss 16.075298489274505
Epoch 44: loss 16.0740585218246
Epoch 45: loss 16.072816658434885
Epoch 46: loss 16.071584870077594
Epoch 47: loss 16.07034521637046
Epoch 48: loss 16.069109404845495
Epoch 49: loss 16.067875302051835
-----------Time: 0:04:08.216279, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.009507179260254-------------


Epoch 0: loss 16.128543013655147
Epoch 1: loss 16.127302362194133
Epoch 2: loss 16.126065230875188
Epoch 3: loss 16.12482691459386
Epoch 4: loss 16.12358605725536
Epoch 5: loss 16.122344028437638
Epoch 6: loss 16.121107398588546
Epoch 7: loss 16.119862400364006
Epoch 8: loss 16.118631084954497
Epoch 9: loss 16.117384853020774
Epoch 10: loss 16.116150181341457
Epoch 11: loss 16.11490860317245
Epoch 12: loss 16.1136669586248
Epoch 13: loss 16.112429936726887
Epoch 14: loss 16.11118905657073
Epoch 15: loss 16.109956091029805
Epoch 16: loss 16.108708954687156
Epoch 17: loss 16.10747548975071
Epoch 18: loss 16.10623712627832
Epoch 19: loss 16.104997462199524
Epoch 20: loss 16.103758946263707
Epoch 21: loss 16.10252152038956
Epoch 22: loss 16.101280317156128
Epoch 23: loss 16.100039176671256
Epoch 24: loss 16.098802217003314
Epoch 25: loss 16.097557405987274
Epoch 26: loss 16.096326383577214
Epoch 27: loss 16.095087741625704
Epoch 28: loss 16.093850680315477
Epoch 29: loss 16.09260863438451
Epoch 30: loss 16.091369601421345
Epoch 31: loss 16.09013305973138
Epoch 32: loss 16.088895036449504
Epoch 33: loss 16.08765627833538
Epoch 34: loss 16.08641434182545
Epoch 35: loss 16.08517977489992
Epoch 36: loss 16.083938449799465
Epoch 37: loss 16.082694406286407
Epoch 38: loss 16.081461812050982
Epoch 39: loss 16.080222239761394
Epoch 40: loss 16.07899065728159
Epoch 41: loss 16.07774719873027
Epoch 42: loss 16.076517710289014
Epoch 43: loss 16.07527464949093
Epoch 44: loss 16.074040406161252
Epoch 45: loss 16.072797232830634
Epoch 46: loss 16.071558469012096
Epoch 47: loss 16.070325892408494
Epoch 48: loss 16.069087063248485
Epoch 49: loss 16.067843474014225
-----------Time: 0:05:37.007607, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009511947631836-------------


Epoch 0: loss 16.12855183371647
Epoch 1: loss 16.127312265575547
Epoch 2: loss 16.126074517661298
Epoch 3: loss 16.12484000155691
Epoch 4: loss 16.12359720938487
Epoch 5: loss 16.12235655688669
Epoch 6: loss 16.121118806898107
Epoch 7: loss 16.119880993642386
Epoch 8: loss 16.11863773993139
Epoch 9: loss 16.117404842324852
Epoch 10: loss 16.116158632171615
Epoch 11: loss 16.11492052228636
Epoch 12: loss 16.113683508685778
Epoch 13: loss 16.11244221366314
Epoch 14: loss 16.111206229450005
Epoch 15: loss 16.109963683086352
Epoch 16: loss 16.10872166878896
Epoch 17: loss 16.107485114134416
Epoch 18: loss 16.106251357754267
Epoch 19: loss 16.10501255919066
Epoch 20: loss 16.103772231844083
Epoch 21: loss 16.102533854369952
Epoch 22: loss 16.10129681276588
Epoch 23: loss 16.100055061908698
Epoch 24: loss 16.09881156705656
Epoch 25: loss 16.097582449920804
Epoch 26: loss 16.096342353862287
Epoch 27: loss 16.095100406980695
Epoch 28: loss 16.09385937850977
Epoch 29: loss 16.092628020057866
Epoch 30: loss 16.091385770323743
Epoch 31: loss 16.090145338741966
Epoch 32: loss 16.08890757682597
Epoch 33: loss 16.08767198155016
Epoch 34: loss 16.086431010123373
Epoch 35: loss 16.085193811388624
Epoch 36: loss 16.083952963903197
Epoch 37: loss 16.0827111736337
Epoch 38: loss 16.08147956315041
Epoch 39: loss 16.080241305468977
Epoch 40: loss 16.079008033964023
Epoch 41: loss 16.077761590448393
Epoch 42: loss 16.076529858616656
Epoch 43: loss 16.075288721243282
Epoch 44: loss 16.07404904782999
Epoch 45: loss 16.072814970446903
Epoch 46: loss 16.071574698070133
Epoch 47: loss 16.070336550328314
Epoch 48: loss 16.069102063783166
Epoch 49: loss 16.06786128163921
-----------Time: 0:06:56.903521, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.00950813293457-------------


Epoch 0: loss 16.128560921885256
Epoch 1: loss 16.127315506201324
Epoch 2: loss 16.1260793098877
Epoch 3: loss 16.124834943297376
Epoch 4: loss 16.123597298581164
Epoch 5: loss 16.122360469077577
Epoch 6: loss 16.121123367317868
Epoch 7: loss 16.119880650340377
Epoch 8: loss 16.118644666645825
Epoch 9: loss 16.117401490203708
Epoch 10: loss 16.116166412473827
Epoch 11: loss 16.114928802502682
Epoch 12: loss 16.113685491747542
Epoch 13: loss 16.11244205290238
Epoch 14: loss 16.11120421319892
Epoch 15: loss 16.10996450296623
Epoch 16: loss 16.10873290129885
Epoch 17: loss 16.10748885674863
Epoch 18: loss 16.10625163830772
Epoch 19: loss 16.105017810363105
Epoch 20: loss 16.10377037116793
Epoch 21: loss 16.102534793005358
Epoch 22: loss 16.101293857360808
Epoch 23: loss 16.10005584393201
Epoch 24: loss 16.098816718661052
Epoch 25: loss 16.09758042381664
Epoch 26: loss 16.096337451177682
Epoch 27: loss 16.09510727820674
Epoch 28: loss 16.093866597705073
Epoch 29: loss 16.0926249324141
Epoch 30: loss 16.091383255714305
Epoch 31: loss 16.090146111949363
Epoch 32: loss 16.0889128959328
Epoch 33: loss 16.08767037134964
Epoch 34: loss 16.086430827582124
Epoch 35: loss 16.085192522191043
Epoch 36: loss 16.08395189458485
Epoch 37: loss 16.082712095155866
Epoch 38: loss 16.081471488811584
Epoch 39: loss 16.08023973571794
Epoch 40: loss 16.078995537667122
Epoch 41: loss 16.077755498134163
Epoch 42: loss 16.076521442012982
Epoch 43: loss 16.075281881132224
Epoch 44: loss 16.07404903486541
Epoch 45: loss 16.072807343126705
Epoch 46: loss 16.071569222351208
Epoch 47: loss 16.0703328844644
Epoch 48: loss 16.06909628884179
Epoch 49: loss 16.06785362527836
-----------Time: 0:06:32.049971, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009509086608887-------------


Epoch 0: loss 16.12310608889241
Epoch 1: loss 16.110725687534153
Epoch 2: loss 16.09833433681237
Epoch 3: loss 16.0859696106654
Epoch 4: loss 16.07359922262817
Epoch 5: loss 16.06123052672759
Epoch 6: loss 16.04886784120511
Epoch 7: loss 16.036517321123515
Epoch 8: loss 16.024158973548644
Epoch 9: loss 16.011810846728043
Epoch 10: loss 15.999470932189377
Epoch 11: loss 15.987127957491852
Epoch 12: loss 15.974798510553008
Epoch 13: loss 15.962473473644827
Epoch 14: loss 15.95015067079263
Epoch 15: loss 15.937840477807033
Epoch 16: loss 15.925531270129321
Epoch 17: loss 15.913232191416672
Epoch 18: loss 15.900930383401178
Epoch 19: loss 15.888635861478207
Epoch 20: loss 15.87634210394672
Epoch 21: loss 15.864059377196304
Epoch 22: loss 15.851787767311748
Epoch 23: loss 15.839510397524727
Epoch 24: loss 15.827246934580634
Epoch 25: loss 15.81498119142666
Epoch 26: loss 15.802719002122656
Epoch 27: loss 15.790471354925872
Epoch 28: loss 15.778211222322431
Epoch 29: loss 15.765978474018041
Epoch 30: loss 15.753736470561108
Epoch 31: loss 15.741504568584332
Epoch 32: loss 15.729279262465974
Epoch 33: loss 15.717063524205766
Epoch 34: loss 15.70484539372173
Epoch 35: loss 15.69263979064958
Epoch 36: loss 15.68042967383015
Epoch 37: loss 15.668230376209884
Epoch 38: loss 15.656031216532721
Epoch 39: loss 15.643845879688024
Epoch 40: loss 15.631664811300803
Epoch 41: loss 15.61948439217962
Epoch 42: loss 15.607308775137922
Epoch 43: loss 15.595130438646459
Epoch 44: loss 15.582970269676652
Epoch 45: loss 15.570817503999146
Epoch 46: loss 15.558664434431947
Epoch 47: loss 15.546504933915752
Epoch 48: loss 15.534359450700686
Epoch 49: loss 15.522224178905393
-----------Time: 0:03:54.075306, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9404819011688232-------------


Epoch 0: loss 16.123088173920724
Epoch 1: loss 16.11070200125117
Epoch 2: loss 16.098322500671753
Epoch 3: loss 16.085948243486033
Epoch 4: loss 16.073580040757978
Epoch 5: loss 16.061209237335685
Epoch 6: loss 16.048849786215985
Epoch 7: loss 16.03649084019622
Epoch 8: loss 16.024132924082487
Epoch 9: loss 16.01178687522436
Epoch 10: loss 15.999438614090737
Epoch 11: loss 15.987096190128462
Epoch 12: loss 15.974765711209255
Epoch 13: loss 15.962437071185711
Epoch 14: loss 15.950103504104158
Epoch 15: loss 15.937788406878207
Epoch 16: loss 15.925462748188892
Epoch 17: loss 15.913152690553483
Epoch 18: loss 15.900849972424655
Epoch 19: loss 15.888543855502204
Epoch 20: loss 15.87623531679669
Epoch 21: loss 15.863945152527487
Epoch 22: loss 15.851651121702707
Epoch 23: loss 15.839366206013034
Epoch 24: loss 15.827069908461537
Epoch 25: loss 15.81479001939783
Epoch 26: loss 15.802508907515177
Epoch 27: loss 15.790235889677511
Epoch 28: loss 15.777958377798722
Epoch 29: loss 15.76568638675559
Epoch 30: loss 15.753422018883992
Epoch 31: loss 15.74115974816244
Epoch 32: loss 15.728899862404553
Epoch 33: loss 15.716629021578733
Epoch 34: loss 15.70436833616571
Epoch 35: loss 15.692099733544536
Epoch 36: loss 15.679836819779943
Epoch 37: loss 15.667569806615965
Epoch 38: loss 15.655302421109322
Epoch 39: loss 15.643043918931141
Epoch 40: loss 15.63078114622115
Epoch 41: loss 15.618512524412402
Epoch 42: loss 15.606237635526922
Epoch 43: loss 15.593967431521143
Epoch 44: loss 15.58168657737429
Epoch 45: loss 15.569401356239174
Epoch 46: loss 15.557117355330085
Epoch 47: loss 15.54482536409263
Epoch 48: loss 15.532520677422362
Epoch 49: loss 15.520208618056197
-----------Time: 0:04:49.875184, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.940235137939453-------------


Epoch 0: loss 16.123030437471556
Epoch 1: loss 16.110641291246523
Epoch 2: loss 16.09825429143693
Epoch 3: loss 16.085872003473362
Epoch 4: loss 16.073495306354168
Epoch 5: loss 16.061123130242418
Epoch 6: loss 16.048757200982664
Epoch 7: loss 16.03639449004961
Epoch 8: loss 16.024033489922196
Epoch 9: loss 16.01167460716957
Epoch 10: loss 15.999318920963157
Epoch 11: loss 15.986966957146217
Epoch 12: loss 15.974613518997533
Epoch 13: loss 15.962267232628346
Epoch 14: loss 15.94991040761345
Epoch 15: loss 15.937561370160932
Epoch 16: loss 15.925209323370696
Epoch 17: loss 15.91284893205984
Epoch 18: loss 15.90048656650313
Epoch 19: loss 15.888118153230335
Epoch 20: loss 15.875735426026884
Epoch 21: loss 15.863355788541528
Epoch 22: loss 15.850950958807875
Epoch 23: loss 15.83853352750495
Epoch 24: loss 15.826102185210434
Epoch 25: loss 15.813641944354226
Epoch 26: loss 15.801158076333508
Epoch 27: loss 15.788643743111557
Epoch 28: loss 15.776088591695416
Epoch 29: loss 15.76348882657539
Epoch 30: loss 15.750843886125988
Epoch 31: loss 15.738123279735406
Epoch 32: loss 15.725340467746522
Epoch 33: loss 15.712478899061194
Epoch 34: loss 15.699540366421191
Epoch 35: loss 15.686499531337786
Epoch 36: loss 15.67333860848506
Epoch 37: loss 15.660067910406497
Epoch 38: loss 15.646656513991985
Epoch 39: loss 15.633097508084585
Epoch 40: loss 15.619368788079765
Epoch 41: loss 15.605464980419464
Epoch 42: loss 15.591363236332407
Epoch 43: loss 15.57704388233162
Epoch 44: loss 15.562501981506015
Epoch 45: loss 15.547710338839375
Epoch 46: loss 15.532651964512254
Epoch 47: loss 15.517316621175727
Epoch 48: loss 15.501670542847144
Epoch 49: loss 15.485714054678107
-----------Time: 0:06:21.649394, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9356331825256348-------------


Epoch 0: loss 16.123022863565417
Epoch 1: loss 16.11062982278132
Epoch 2: loss 16.098243249765616
Epoch 3: loss 16.085856129644746
Epoch 4: loss 16.07348498551336
Epoch 5: loss 16.06110934993377
Epoch 6: loss 16.048730818067575
Epoch 7: loss 16.03636333565144
Epoch 8: loss 16.023980508880033
Epoch 9: loss 16.01159201658829
Epoch 10: loss 15.999205548326369
Epoch 11: loss 15.98681816580245
Epoch 12: loss 15.974417689573901
Epoch 13: loss 15.962004709788287
Epoch 14: loss 15.949562618303325
Epoch 15: loss 15.937102675113813
Epoch 16: loss 15.924603736031115
Epoch 17: loss 15.91206122767089
Epoch 18: loss 15.899458611380995
Epoch 19: loss 15.886788726566017
Epoch 20: loss 15.874024978210382
Epoch 21: loss 15.8611616593071
Epoch 22: loss 15.84817282306947
Epoch 23: loss 15.835025645261746
Epoch 24: loss 15.821698645134864
Epoch 25: loss 15.808155961111877
Epoch 26: loss 15.794379579690307
Epoch 27: loss 15.78031925120517
Epoch 28: loss 15.765941196709239
Epoch 29: loss 15.751214235356608
Epoch 30: loss 15.73610444092245
Epoch 31: loss 15.720561277483908
Epoch 32: loss 15.704546832986471
Epoch 33: loss 15.688032357701793
Epoch 34: loss 15.670978727646663
Epoch 35: loss 15.653331038872789
Epoch 36: loss 15.635076143224321
Epoch 37: loss 15.616187294259417
Epoch 38: loss 15.596599860966627
Epoch 39: loss 15.576309519400086
Epoch 40: loss 15.555275449809852
Epoch 41: loss 15.533474189940842
Epoch 42: loss 15.510871896541527
Epoch 43: loss 15.487442997760784
Epoch 44: loss 15.463186098091494
Epoch 45: loss 15.438072749102096
Epoch 46: loss 15.412074235290726
Epoch 47: loss 15.38518696132077
Epoch 48: loss 15.357395961402615
Epoch 49: loss 15.32868777000755
-----------Time: 0:05:18.629126, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.914867877960205-------------


Epoch 0: loss 16.123034396334923
Epoch 1: loss 16.110652009321985
Epoch 2: loss 16.09826170147081
Epoch 3: loss 16.085880066403362
Epoch 4: loss 16.073506736444223
Epoch 5: loss 16.06112367319892
Epoch 6: loss 16.048741837439813
Epoch 7: loss 16.036360250600595
Epoch 8: loss 16.023965143781435
Epoch 9: loss 16.011560231593073
Epoch 10: loss 15.999140109362454
Epoch 11: loss 15.986689912423678
Epoch 12: loss 15.97420593550051
Epoch 13: loss 15.96166276127959
Epoch 14: loss 15.949054797879375
Epoch 15: loss 15.93634739046579
Epoch 16: loss 15.923522772381913
Epoch 17: loss 15.910536958965158
Epoch 18: loss 15.897338484991757
Epoch 19: loss 15.88388999484686
Epoch 20: loss 15.870129004971627
Epoch 21: loss 15.855998047542935
Epoch 22: loss 15.841421862670169
Epoch 23: loss 15.826335029539822
Epoch 24: loss 15.810659704680285
Epoch 25: loss 15.794329062955025
Epoch 26: loss 15.77726826504432
Epoch 27: loss 15.75940492867516
Epoch 28: loss 15.740687802280013
Epoch 29: loss 15.721044434614322
Epoch 30: loss 15.700433483196381
Epoch 31: loss 15.678788380106873
Epoch 32: loss 15.656081442861469
Epoch 33: loss 15.632248020223978
Epoch 34: loss 15.607270543117638
Epoch 35: loss 15.58111520355457
Epoch 36: loss 15.553759861148006
Epoch 37: loss 15.525164068490671
Epoch 38: loss 15.495310010178832
Epoch 39: loss 15.464198035737494
Epoch 40: loss 15.431787890153712
Epoch 41: loss 15.398083903596348
Epoch 42: loss 15.363078339842755
Epoch 43: loss 15.326757114176
Epoch 44: loss 15.28910202536653
Epoch 45: loss 15.250125451474297
Epoch 46: loss 15.209826419118826
Epoch 47: loss 15.16820714472428
Epoch 48: loss 15.125252210815164
Epoch 49: loss 15.080965129754802
-----------Time: 0:06:57.007090, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.8821980953216553-------------


Epoch 0: loss 16.067864557528637
Epoch 1: loss 15.944607421974569
Epoch 2: loss 15.821889672479012
Epoch 3: loss 15.699759395696859
Epoch 4: loss 15.578123650128196
Epoch 5: loss 15.456969546390138
Epoch 6: loss 15.336293450770915
Epoch 7: loss 15.216028675558517
Epoch 8: loss 15.09618480001992
Epoch 9: loss 14.976704387965572
Epoch 10: loss 14.857539646258621
Epoch 11: loss 14.738646056095888
Epoch 12: loss 14.619839602932455
Epoch 13: loss 14.501036183998727
Epoch 14: loss 14.382057800832294
Epoch 15: loss 14.262639377608515
Epoch 16: loss 14.142541271892172
Epoch 17: loss 14.021499873374454
Epoch 18: loss 13.899158926357583
Epoch 19: loss 13.775083092258571
Epoch 20: loss 13.648851735362415
Epoch 21: loss 13.520068058181938
Epoch 22: loss 13.388224163024304
Epoch 23: loss 13.25282840199805
Epoch 24: loss 13.113380127202046
Epoch 25: loss 12.969388008117676
Epoch 26: loss 12.820330987487427
Epoch 27: loss 12.665739139828622
Epoch 28: loss 12.505218692030708
Epoch 29: loss 12.338343516065091
Epoch 30: loss 12.164746013265903
Epoch 31: loss 11.984090923290657
Epoch 32: loss 11.79611839659516
Epoch 33: loss 11.600705778423764
Epoch 34: loss 11.397713108384266
Epoch 35: loss 11.187115028800363
Epoch 36: loss 10.968959961331624
Epoch 37: loss 10.743297616316612
Epoch 38: loss 10.510240415310717
Epoch 39: loss 10.270200381400858
Epoch 40: loss 10.023455064928097
Epoch 41: loss 9.770162771680292
Epoch 42: loss 9.5109265401611
Epoch 43: loss 9.246178094429318
Epoch 44: loss 8.9763674090387
Epoch 45: loss 8.702000149441128
Epoch 46: loss 8.423680855956915
Epoch 47: loss 8.141839607180689
Epoch 48: loss 7.857298603130463
Epoch 49: loss 7.570677950953971
-----------Time: 0:04:44.103233, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.732347249984741-------------


Epoch 0: loss 16.06780981071103
Epoch 1: loss 15.944546451122623
Epoch 2: loss 15.821665701625008
Epoch 3: loss 15.699023785572976
Epoch 4: loss 15.576232273854789
Epoch 5: loss 15.452461430921446
Epoch 6: loss 15.326242466086988
Epoch 7: loss 15.195087601919418
Epoch 8: loss 15.055456503763867
Epoch 9: loss 14.902931842938786
Epoch 10: loss 14.732836857122075
Epoch 11: loss 14.540740540521568
Epoch 12: loss 14.323182783806175
Epoch 13: loss 14.077695208700407
Epoch 14: loss 13.803223100675714
Epoch 15: loss 13.499404689940244
Epoch 16: loss 13.16676755597113
Epoch 17: loss 12.806444345964822
Epoch 18: loss 12.419885985419567
Epoch 19: loss 12.008964950329199
Epoch 20: loss 11.575619762912272
Epoch 21: loss 11.122116220068712
Epoch 22: loss 10.651029735625341
Epoch 23: loss 10.16482129649276
Epoch 24: loss 9.666053779751403
Epoch 25: loss 9.157748159083937
Epoch 26: loss 8.642805820058518
Epoch 27: loss 8.124226205046893
Epoch 28: loss 7.605070866599816
Epoch 29: loss 7.088515122731526
Epoch 30: loss 6.577829767790354
Epoch 31: loss 6.0763746640680925
Epoch 32: loss 5.587597163751632
Epoch 33: loss 5.115086638713544
Epoch 34: loss 4.662106585541518
Epoch 35: loss 4.231915616677986
Epoch 36: loss 3.8280507781830995
Epoch 37: loss 3.453542353602581
Epoch 38: loss 3.1108540129959747
Epoch 39: loss 2.8021207483259474
Epoch 40: loss 2.5286277084651365
Epoch 41: loss 2.290713079472221
Epoch 42: loss 2.087790083275858
Epoch 43: loss 1.9177539538533355
Epoch 44: loss 1.7773995902500703
Epoch 45: loss 1.6629162870586534
Epoch 46: loss 1.5699367243806717
Epoch 47: loss 1.494304559748091
Epoch 48: loss 1.4322490965691257
Epoch 49: loss 1.380662008042307
-----------Time: 0:05:15.381695, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.175711989402771-------------


Epoch 0: loss 16.067742761546597
Epoch 1: loss 15.944369158451812
Epoch 2: loss 15.82070159030518
Epoch 3: loss 15.693751946034931
Epoch 4: loss 15.553762506440387
Epoch 5: loss 15.381627670379357
Epoch 6: loss 15.156854817243683
Epoch 7: loss 14.866455753838776
Epoch 8: loss 14.50548319948828
Epoch 9: loss 14.075029857525559
Epoch 10: loss 13.580168514552486
Epoch 11: loss 13.026896370954654
Epoch 12: loss 12.42136551544289
Epoch 13: loss 11.770594079316862
Epoch 14: loss 11.08159383656604
Epoch 15: loss 10.362097844408542
Epoch 16: loss 9.620090670271889
Epoch 17: loss 8.863990363639097
Epoch 18: loss 8.102697968029211
Epoch 19: loss 7.34515787687815
Epoch 20: loss 6.601141422711487
Epoch 21: loss 5.880258021632117
Epoch 22: loss 5.192204168920221
Epoch 23: loss 4.5463465627864235
Epoch 24: loss 3.952243943134036
Epoch 25: loss 3.4181576820091424
Epoch 26: loss 2.950814244668078
Epoch 27: loss 2.554486460371987
Epoch 28: loss 2.2297903533990517
Epoch 29: loss 1.9726057096691867
Epoch 30: loss 1.7747939334996956
Epoch 31: loss 1.6256866689088747
Epoch 32: loss 1.5135360353339167
Epoch 33: loss 1.4278698451756264
Epoch 34: loss 1.3608089525975762
Epoch 35: loss 1.3068622221046975
Epoch 36: loss 1.2624224566324305
Epoch 37: loss 1.2251517143244326
Epoch 38: loss 1.1935515264054237
Epoch 39: loss 1.1665224139621686
Epoch 40: loss 1.1432838255844406
Epoch 41: loss 1.1232475493834548
Epoch 42: loss 1.1059133781564825
Epoch 43: loss 1.0908721104920591
Epoch 44: loss 1.0778174066491721
Epoch 45: loss 1.0664474127278372
Epoch 46: loss 1.056544084403747
Epoch 47: loss 1.047898476962618
Epoch 48: loss 1.0403406090422647
Epoch 49: loss 1.0337119783442457
-----------Time: 0:04:50.842787, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0250568389892578-------------


Epoch 0: loss 16.06775775793261
Epoch 1: loss 15.944011689269589
Epoch 2: loss 15.815065784775868
Epoch 3: loss 15.654216592632084
Epoch 4: loss 15.409526101527751
Epoch 5: loss 15.048976722912272
Epoch 6: loss 14.571682058254488
Epoch 7: loss 13.987871520606118
Epoch 8: loss 13.310738402258254
Epoch 9: loss 12.553547618569338
Epoch 10: loss 11.72995041737808
Epoch 11: loss 10.853488199214302
Epoch 12: loss 9.938490204346964
Epoch 13: loss 9.000599460280284
Epoch 14: loss 8.055630611038001
Epoch 15: loss 7.12071808840413
Epoch 16: loss 6.213434211063022
Epoch 17: loss 5.351580623442094
Epoch 18: loss 4.552680445210061
Epoch 19: loss 3.8336146879999973
Epoch 20: loss 3.2084931599957716
Epoch 21: loss 2.687263043436296
Epoch 22: loss 2.2728098903161276
Epoch 23: loss 1.959186975822428
Epoch 24: loss 1.7315126017035836
Epoch 25: loss 1.5698132612187945
Epoch 26: loss 1.4541245342402953
Epoch 27: loss 1.368731010601921
Epoch 28: loss 1.303166041236782
Epoch 29: loss 1.2510945825164508
Epoch 30: loss 1.2087883647529245
Epoch 31: loss 1.1738815709843202
Epoch 32: loss 1.1448653158187347
Epoch 33: loss 1.1206401979035174
Epoch 34: loss 1.1003143223424914
Epoch 35: loss 1.0832393552506339
Epoch 36: loss 1.068824156851403
Epoch 37: loss 1.0566264749416001
Epoch 38: loss 1.0462932107304672
Epoch 39: loss 1.0375090893550436
Epoch 40: loss 1.0300299917763507
Epoch 41: loss 1.023645729977129
Epoch 42: loss 1.018178494421278
Epoch 43: loss 1.013489419445517
Epoch 44: loss 1.0094623150483235
Epoch 45: loss 1.005990627830997
Epoch 46: loss 1.002987787119392
Epoch 47: loss 1.0003842643384637
Epoch 48: loss 0.9981349092452924
Epoch 49: loss 0.9961745025931396
-----------Time: 0:05:56.794823, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.007152795791626-------------


Epoch 0: loss 16.06778150696415
Epoch 1: loss 15.942747066823474
Epoch 2: loss 15.792606664391041
Epoch 3: loss 15.528713583622114
Epoch 4: loss 15.083265627124117
Epoch 5: loss 14.469843515953077
Epoch 6: loss 13.717884714023908
Epoch 7: loss 12.851333755588584
Epoch 8: loss 11.891547199952466
Epoch 9: loss 10.860845559572901
Epoch 10: loss 9.782160837278216
Epoch 11: loss 8.679617387046628
Epoch 12: loss 7.579544308264661
Epoch 13: loss 6.508571536596733
Epoch 14: loss 5.4935966966721335
Epoch 15: loss 4.5620309857974695
Epoch 16: loss 3.739145783382372
Epoch 17: loss 3.0451589973806494
Epoch 18: loss 2.492201855676598
Epoch 19: loss 2.0783755452559007
Epoch 20: loss 1.7866929401970222
Epoch 21: loss 1.588448862720923
Epoch 22: loss 1.453371573622425
Epoch 23: loss 1.35773037904757
Epoch 24: loss 1.28646541951207
Epoch 25: loss 1.2312436044183745
Epoch 26: loss 1.1873489706784632
Epoch 27: loss 1.151965392582309
Epoch 28: loss 1.1231998781460404
Epoch 29: loss 1.0997205578872469
Epoch 30: loss 1.0804898726544736
Epoch 31: loss 1.0646558047118817
Epoch 32: loss 1.051572265930964
Epoch 33: loss 1.0407547124252816
Epoch 34: loss 1.0317647522269804
Epoch 35: loss 1.0242839704200326
Epoch 36: loss 1.01802912965425
Epoch 37: loss 1.0127809164056056
Epoch 38: loss 1.0083672207875898
Epoch 39: loss 1.0046393442503974
Epoch 40: loss 1.0014872972313122
Epoch 41: loss 0.9988109537153674
Epoch 42: loss 0.9965409941489182
Epoch 43: loss 0.9945963736200151
Epoch 44: loss 0.9929326419080969
Epoch 45: loss 0.9915116419929599
Epoch 46: loss 0.9902848612361138
Epoch 47: loss 0.9892318621804496
Epoch 48: loss 0.9883180216254587
Epoch 49: loss 0.987525632097515
-----------Time: 0:07:53.408968, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0028678178787231-------------


Epoch 0: loss 15.525412534617288
Epoch 1: loss 14.335459197325964
Epoch 2: loss 12.983906302003513
Epoch 3: loss 10.8937827696808
Epoch 4: loss 7.989658706275065
Epoch 5: loss 5.042771383625713
Epoch 6: loss 2.838223268766647
Epoch 7: loss 1.6959760499558025
Epoch 8: loss 1.2902445261280067
Epoch 9: loss 1.1441967456544333
Epoch 10: loss 1.0739002173656091
Epoch 11: loss 1.0360120568280637
Epoch 12: loss 1.0147395310290421
Epoch 13: loss 1.0023456781762266
Epoch 14: loss 0.9949124577318993
Epoch 15: loss 0.9902501795005384
Epoch 16: loss 0.9872100475107994
Epoch 17: loss 0.9851078919057551
Epoch 18: loss 0.9835890667343866
Epoch 19: loss 0.9824167777170884
Epoch 20: loss 0.9814293519383608
Epoch 21: loss 0.9805776281419042
Epoch 22: loss 0.9798083199373466
Epoch 23: loss 0.9790314586672075
Epoch 24: loss 0.9782838297966319
Epoch 25: loss 0.9775210472797168
Epoch 26: loss 0.9767406087909593
Epoch 27: loss 0.9758993163907443
Epoch 28: loss 0.9750779534954944
Epoch 29: loss 0.9741805798661262
Epoch 30: loss 0.9732417862268814
Epoch 31: loss 0.9722856799826277
Epoch 32: loss 0.9712685993859922
Epoch 33: loss 0.9702337180680072
Epoch 34: loss 0.9691436261894782
Epoch 35: loss 0.9680077322738984
Epoch 36: loss 0.9668664907294166
Epoch 37: loss 0.9656792352418656
Epoch 38: loss 0.964490837477808
Epoch 39: loss 0.9632653509423161
Epoch 40: loss 0.9620297601716943
Epoch 41: loss 0.9607585595638095
Epoch 42: loss 0.9594915156263317
Epoch 43: loss 0.9582278853269165
Epoch 44: loss 0.9569577004288512
Epoch 45: loss 0.9556713120777883
Epoch 46: loss 0.954395755663587
Epoch 47: loss 0.9531306378085566
Epoch 48: loss 0.9518697818963018
Epoch 49: loss 0.950607958891393
-----------Time: 0:04:14.288536, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9944906830787659-------------


Epoch 0: loss 15.511480185180465
Epoch 1: loss 13.012645740104539
Epoch 2: loss 7.31651093066549
Epoch 3: loss 2.9141402105846894
Epoch 4: loss 1.450545936839097
Epoch 5: loss 1.1498801853748817
Epoch 6: loss 1.0553104695643725
Epoch 7: loss 1.0178925895120476
Epoch 8: loss 1.0020770374168968
Epoch 9: loss 0.9948606854097035
Epoch 10: loss 0.9913440455944399
Epoch 11: loss 0.9894768993708023
Epoch 12: loss 0.9884589733864835
Epoch 13: loss 0.9877914032772742
Epoch 14: loss 0.9872946616357406
Epoch 15: loss 0.986972237715843
Epoch 16: loss 0.986675866632892
Epoch 17: loss 0.9864075052277947
Epoch 18: loss 0.9861858391839567
Epoch 19: loss 0.9858891334528506
Epoch 20: loss 0.9856995175298366
Epoch 21: loss 0.985404944017959
Epoch 22: loss 0.98513869888566
Epoch 23: loss 0.9848698621343309
Epoch 24: loss 0.9845485337212269
Epoch 25: loss 0.9842167142748249
Epoch 26: loss 0.9839254817345533
Epoch 27: loss 0.9835344516044728
Epoch 28: loss 0.9831484633013241
Epoch 29: loss 0.982736536670859
Epoch 30: loss 0.9823058661136244
Epoch 31: loss 0.9818229225422132
Epoch 32: loss 0.9812724363745523
Epoch 33: loss 0.9807001633641511
Epoch 34: loss 0.98007106784247
Epoch 35: loss 0.9794374535173226
Epoch 36: loss 0.9787116720664754
Epoch 37: loss 0.9779242868654231
Epoch 38: loss 0.9770934639773076
Epoch 39: loss 0.976183534316747
Epoch 40: loss 0.975190840381977
Epoch 41: loss 0.9741144185483682
Epoch 42: loss 0.9729698484134518
Epoch 43: loss 0.9716918404300684
Epoch 44: loss 0.9703525926121686
Epoch 45: loss 0.9689270711385926
Epoch 46: loss 0.9673645272405333
Epoch 47: loss 0.9656793654386345
Epoch 48: loss 0.9639427009129278
Epoch 49: loss 0.9620024743061989
-----------Time: 0:03:56.352890, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.996015191078186-------------


Epoch 0: loss 15.17462548436906
Epoch 1: loss 8.182377543120102
Epoch 2: loss 2.280314647483722
Epoch 3: loss 1.2169765871202511
Epoch 4: loss 1.0574956612784037
Epoch 5: loss 1.0134198492217674
Epoch 6: loss 0.9994856941434208
Epoch 7: loss 0.9943866854520386
Epoch 8: loss 0.9921908699474365
Epoch 9: loss 0.9911526881721501
Epoch 10: loss 0.9903850974499887
Epoch 11: loss 0.9898903901451239
Epoch 12: loss 0.9893405574245775
Epoch 13: loss 0.988800027566218
Epoch 14: loss 0.9881428340130879
Epoch 15: loss 0.9873401198197863
Epoch 16: loss 0.9865860909531465
Epoch 17: loss 0.9856614066635285
Epoch 18: loss 0.9846529960956438
Epoch 19: loss 0.98352842096922
Epoch 20: loss 0.982303310860234
Epoch 21: loss 0.9810890867503976
Epoch 22: loss 0.9797089983419985
Epoch 23: loss 0.9782825803030698
Epoch 24: loss 0.9767555552957627
Epoch 25: loss 0.97521954155798
Epoch 26: loss 0.9735923990275044
Epoch 27: loss 0.9720100934509353
Epoch 28: loss 0.9703257056667209
Epoch 29: loss 0.9686236342637029
Epoch 30: loss 0.9668531523183871
Epoch 31: loss 0.9651391927543835
Epoch 32: loss 0.9633333677828085
Epoch 33: loss 0.961590017456928
Epoch 34: loss 0.959743674939712
Epoch 35: loss 0.9578830331742213
Epoch 36: loss 0.9559884216229251
Epoch 37: loss 0.9540117165198334
Epoch 38: loss 0.9520294265036612
Epoch 39: loss 0.9499048703776033
Epoch 40: loss 0.9476994091301525
Epoch 41: loss 0.9454323824104114
Epoch 42: loss 0.9429518748652099
Epoch 43: loss 0.9403901398019858
Epoch 44: loss 0.9376446038824894
Epoch 45: loss 0.9347583990242249
Epoch 46: loss 0.9316509842807796
Epoch 47: loss 0.9283237679293779
Epoch 48: loss 0.9248461256926963
Epoch 49: loss 0.9210526959088394
-----------Time: 0:05:25.804439, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9900690913200378-------------


Epoch 0: loss 14.281041511444373
Epoch 1: loss 4.708976256063025
Epoch 2: loss 1.3618628998500228
Epoch 3: loss 1.0698403226350428
Epoch 4: loss 1.0142910138234422
Epoch 5: loss 1.0003109688406213
Epoch 6: loss 0.9960489405634093
Epoch 7: loss 0.9945906167318148
Epoch 8: loss 0.993864670695657
Epoch 9: loss 0.9935595570471444
Epoch 10: loss 0.9933092723792503
Epoch 11: loss 0.9930132478070428
Epoch 12: loss 0.9929214106946617
Epoch 13: loss 0.9927069244662208
Epoch 14: loss 0.9925479042004994
Epoch 15: loss 0.9923271157937312
Epoch 16: loss 0.9921131823100494
Epoch 17: loss 0.9918499246053296
Epoch 18: loss 0.9916068430242492
Epoch 19: loss 0.9912508598418908
Epoch 20: loss 0.9910077937534804
Epoch 21: loss 0.9905796348238328
Epoch 22: loss 0.9901674421602387
Epoch 23: loss 0.9896354883050322
Epoch 24: loss 0.989068439954516
Epoch 25: loss 0.9885326662486243
Epoch 26: loss 0.987730301321039
Epoch 27: loss 0.986914519643965
Epoch 28: loss 0.9859170343903111
Epoch 29: loss 0.9848385105125278
Epoch 30: loss 0.9835237144062609
Epoch 31: loss 0.9820144889859028
Epoch 32: loss 0.9804332068654881
Epoch 33: loss 0.9785217239585761
Epoch 34: loss 0.9764129406670762
Epoch 35: loss 0.9740555453002291
Epoch 36: loss 0.9714340218193445
Epoch 37: loss 0.9686419626433542
Epoch 38: loss 0.9654781155575871
Epoch 39: loss 0.9620239894438075
Epoch 40: loss 0.958253661240489
Epoch 41: loss 0.95431760596866
Epoch 42: loss 0.9500478425059647
Epoch 43: loss 0.9454670103105791
Epoch 44: loss 0.9405867627549911
Epoch 45: loss 0.9354361790622817
Epoch 46: loss 0.9298205142308993
Epoch 47: loss 0.9239503343251816
Epoch 48: loss 0.9175617352589892
Epoch 49: loss 0.9107305771828735
-----------Time: 0:06:46.302235, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.989553689956665-------------


Epoch 0: loss 13.244007148452269
Epoch 1: loss 3.0790929310732786
Epoch 2: loss 1.1754209790753567
Epoch 3: loss 1.0315762388116319
Epoch 4: loss 1.0047523117765518
Epoch 5: loss 0.9984911785558749
Epoch 6: loss 0.9965011862194233
Epoch 7: loss 0.9958816756367229
Epoch 8: loss 0.9955483865906455
Epoch 9: loss 0.9952650560251457
Epoch 10: loss 0.9950791916294938
Epoch 11: loss 0.9949010970671582
Epoch 12: loss 0.9947481400750136
Epoch 13: loss 0.9945102435146226
Epoch 14: loss 0.9943478100321875
Epoch 15: loss 0.9941111865284781
Epoch 16: loss 0.9938682810711822
Epoch 17: loss 0.9935260852955813
Epoch 18: loss 0.9932111492618522
Epoch 19: loss 0.9928146303186214
Epoch 20: loss 0.992413745683584
Epoch 21: loss 0.9919728807874061
Epoch 22: loss 0.9913429468789134
Epoch 23: loss 0.9906796765042233
Epoch 24: loss 0.9899747468259686
Epoch 25: loss 0.9890021949179993
Epoch 26: loss 0.9879643464944102
Epoch 27: loss 0.9867308784464639
Epoch 28: loss 0.9852240486531364
Epoch 29: loss 0.9835141522201135
Epoch 30: loss 0.9816701848134844
Epoch 31: loss 0.9793669855938196
Epoch 32: loss 0.9767448561485855
Epoch 33: loss 0.9738194224885004
Epoch 34: loss 0.9703746509979833
Epoch 35: loss 0.96666737514193
Epoch 36: loss 0.9625357668836717
Epoch 37: loss 0.9579335196048038
Epoch 38: loss 0.9529840212726541
Epoch 39: loss 0.9476264669170452
Epoch 40: loss 0.9419173708812513
Epoch 41: loss 0.9356462919174037
Epoch 42: loss 0.9289290608758185
Epoch 43: loss 0.921718366642631
Epoch 44: loss 0.9139992454053268
Epoch 45: loss 0.9056788917920588
Epoch 46: loss 0.89661429380839
Epoch 47: loss 0.8868949099556787
Epoch 48: loss 0.8765165079152085
Epoch 49: loss 0.8651991428208778
-----------Time: 0:06:52.522208, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9860711097717285-------------


Epoch 0: loss 7.037368996205571
Epoch 1: loss 1.0316217515765487
Epoch 2: loss 1.0141591684464335
Epoch 3: loss 1.0099114869326726
Epoch 4: loss 1.0043413563225825
Epoch 5: loss 0.9952493414987748
Epoch 6: loss 0.9827982374285148
Epoch 7: loss 0.9682167019449932
Epoch 8: loss 0.9515195749841869
Epoch 9: loss 0.9333882481264899
Epoch 10: loss 0.9131134897875617
Epoch 11: loss 0.8913495665402954
Epoch 12: loss 0.8698076090778976
Epoch 13: loss 0.8491371834064191
Epoch 14: loss 0.8302485783700388
Epoch 15: loss 0.813080816634522
Epoch 16: loss 0.7986742184173808
Epoch 17: loss 0.7861758637130098
Epoch 18: loss 0.7756913168849084
Epoch 19: loss 0.7667515443160912
Epoch 20: loss 0.7592915717903332
Epoch 21: loss 0.75273502022109
Epoch 22: loss 0.7475227024517609
Epoch 23: loss 0.7425238407779609
Epoch 24: loss 0.7384126446310627
Epoch 25: loss 0.7342294849670082
Epoch 26: loss 0.7310302574197646
Epoch 27: loss 0.7278084752999161
Epoch 28: loss 0.7250217317303216
Epoch 29: loss 0.7223724059399993
Epoch 30: loss 0.7198476902877202
Epoch 31: loss 0.7175451027368189
Epoch 32: loss 0.715417750248642
Epoch 33: loss 0.7132529025171165
Epoch 34: loss 0.7116462169878505
Epoch 35: loss 0.7098608497953597
Epoch 36: loss 0.7080634926699503
Epoch 37: loss 0.70638715912947
Epoch 38: loss 0.7050537745093055
Epoch 39: loss 0.703468599871749
Epoch 40: loss 0.7021769116338406
Epoch 41: loss 0.7007621123195666
Epoch 42: loss 0.6994669004715676
Epoch 43: loss 0.6982427680861891
Epoch 44: loss 0.6970024361371865
Epoch 45: loss 0.6959180631606459
Epoch 46: loss 0.694708322072042
Epoch 47: loss 0.6936193561022408
Epoch 48: loss 0.6927235394537999
Epoch 49: loss 0.6914223652614595
-----------Time: 0:03:40.224355, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.121705412864685-------------


Epoch 0: loss 4.7291846834102875
Epoch 1: loss 1.0335965263837055
Epoch 2: loss 1.0206808960910982
Epoch 3: loss 1.0004802372482822
Epoch 4: loss 0.9806842589067207
Epoch 5: loss 0.9606260613230923
Epoch 6: loss 0.9371494970417593
Epoch 7: loss 0.9047345830863944
Epoch 8: loss 0.8627116025109989
Epoch 9: loss 0.8126313143479169
Epoch 10: loss 0.7580786719796708
Epoch 11: loss 0.7047405486653978
Epoch 12: loss 0.6569901494437939
Epoch 13: loss 0.616582429136771
Epoch 14: loss 0.5834789520505331
Epoch 15: loss 0.5572399455221403
Epoch 16: loss 0.5360529195393733
Epoch 17: loss 0.5186548871699722
Epoch 18: loss 0.5042510457081663
Epoch 19: loss 0.49214930455602984
Epoch 20: loss 0.4817260212390043
Epoch 21: loss 0.47278988815118855
Epoch 22: loss 0.46457260223171387
Epoch 23: loss 0.45776134786105405
Epoch 24: loss 0.45128548166361626
Epoch 25: loss 0.44561116309059645
Epoch 26: loss 0.440386660352487
Epoch 27: loss 0.4354657225468808
Epoch 28: loss 0.4310938136418142
Epoch 29: loss 0.4270670257585732
Epoch 30: loss 0.4231499054141509
Epoch 31: loss 0.4197791124764443
Epoch 32: loss 0.4163751351146221
Epoch 33: loss 0.41328084611451904
Epoch 34: loss 0.4105205042424443
Epoch 35: loss 0.40768394953469467
Epoch 36: loss 0.4051761476970225
Epoch 37: loss 0.4027520453009416
Epoch 38: loss 0.4003637249279178
Epoch 39: loss 0.39821414880418077
Epoch 40: loss 0.3962626609386083
Epoch 41: loss 0.3939397812797169
Epoch 42: loss 0.39229955037629366
Epoch 43: loss 0.3905390531651153
Epoch 44: loss 0.3887167645414735
Epoch 45: loss 0.38697801764533857
Epoch 46: loss 0.38542526336452637
Epoch 47: loss 0.38398625360940575
Epoch 48: loss 0.38243713401270146
Epoch 49: loss 0.38103654334051185
-----------Time: 0:04:34.373734, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3828212022781372-------------


Epoch 0: loss 3.721065273908508
Epoch 1: loss 1.0480472058171224
Epoch 2: loss 1.0253273722527791
Epoch 3: loss 0.9967448285508895
Epoch 4: loss 0.9579041954808549
Epoch 5: loss 0.9052309540706591
Epoch 6: loss 0.8356107844160328
Epoch 7: loss 0.7484912886251374
Epoch 8: loss 0.6512231101414119
Epoch 9: loss 0.5556159512124158
Epoch 10: loss 0.4722216091543647
Epoch 11: loss 0.4057208304012386
Epoch 12: loss 0.3550689520720492
Epoch 13: loss 0.31651409356862453
Epoch 14: loss 0.28687133886783
Epoch 15: loss 0.2633495286406869
Epoch 16: loss 0.2444840081121171
Epoch 17: loss 0.22909170351902255
Epoch 18: loss 0.216114186913245
Epoch 19: loss 0.2051841888805652
Epoch 20: loss 0.19569438125466185
Epoch 21: loss 0.18733609842502144
Epoch 22: loss 0.18009335632765663
Epoch 23: loss 0.17361205366560142
Epoch 24: loss 0.1677394004304685
Epoch 25: loss 0.16253640207747133
Epoch 26: loss 0.15789285582116014
Epoch 27: loss 0.15336453234812306
Epoch 28: loss 0.14943249931618724
Epoch 29: loss 0.14590419755560863
Epoch 30: loss 0.14242869210816908
Epoch 31: loss 0.13940428822924741
Epoch 32: loss 0.1363916001999229
Epoch 33: loss 0.13367867588089366
Epoch 34: loss 0.13125928629507766
Epoch 35: loss 0.128915661825409
Epoch 36: loss 0.12657853800959404
Epoch 37: loss 0.12450126541526244
Epoch 38: loss 0.12257300280419428
Epoch 39: loss 0.12075403206305636
Epoch 40: loss 0.11895882388928326
Epoch 41: loss 0.11734503863734873
Epoch 42: loss 0.11562314612133598
Epoch 43: loss 0.11412637831887322
Epoch 44: loss 0.11275514173096066
Epoch 45: loss 0.11137863067487713
Epoch 46: loss 0.11004455332492862
Epoch 47: loss 0.10879122186705364
Epoch 48: loss 0.10756354949975416
Epoch 49: loss 0.10648596316754266
-----------Time: 0:06:11.380738, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5901737213134766-------------


Epoch 0: loss 3.223391461398305
Epoch 1: loss 1.0598904014670893
Epoch 2: loss 1.0320996110565057
Epoch 3: loss 0.9948334496523
Epoch 4: loss 0.9373237258588055
Epoch 5: loss 0.8492584985646947
Epoch 6: loss 0.725117352975977
Epoch 7: loss 0.582583122145812
Epoch 8: loss 0.4504605896969474
Epoch 9: loss 0.3458687368620343
Epoch 10: loss 0.26996258640172627
Epoch 11: loss 0.21635773311236944
Epoch 12: loss 0.17857075619140095
Epoch 13: loss 0.151203405454374
Epoch 14: loss 0.13046246038596873
Epoch 15: loss 0.11491792896167943
Epoch 16: loss 0.10249326447386532
Epoch 17: loss 0.09265732354382707
Epoch 18: loss 0.08441921248383792
Epoch 19: loss 0.07786139108082599
Epoch 20: loss 0.07222729757663152
Epoch 21: loss 0.06736665715368757
Epoch 22: loss 0.0632247419672339
Epoch 23: loss 0.05968273696836536
Epoch 24: loss 0.05659501023829599
Epoch 25: loss 0.05365774601033487
Epoch 26: loss 0.051430013393370465
Epoch 27: loss 0.049258632123543235
Epoch 28: loss 0.04723875773457842
Epoch 29: loss 0.04548895425077623
Epoch 30: loss 0.043936753584767826
Epoch 31: loss 0.042396811311613804
Epoch 32: loss 0.04118237825634202
Epoch 33: loss 0.03997144506471419
Epoch 34: loss 0.03885204238739106
Epoch 35: loss 0.03783279102014063
Epoch 36: loss 0.036892889758299525
Epoch 37: loss 0.03598329233225433
Epoch 38: loss 0.03520621967468008
Epoch 39: loss 0.03441871115416208
Epoch 40: loss 0.033714789533587565
Epoch 41: loss 0.03303850996305496
Epoch 42: loss 0.032446720867806186
Epoch 43: loss 0.03185470941992575
Epoch 44: loss 0.031222523011311752
Epoch 45: loss 0.030749246690780847
Epoch 46: loss 0.030224107021527682
Epoch 47: loss 0.029777720265694127
Epoch 48: loss 0.029399242189314265
Epoch 49: loss 0.02883612826697492
-----------Time: 0:05:52.836993, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4700117111206055-------------


Epoch 0: loss 3.0200839951480454
Epoch 1: loss 1.0683175514352392
Epoch 2: loss 1.0307968507777614
Epoch 3: loss 0.9703384807344492
Epoch 4: loss 0.8756900715270465
Epoch 5: loss 0.7348237293060867
Epoch 6: loss 0.565580435506412
Epoch 7: loss 0.40674829489514513
Epoch 8: loss 0.2853562554436705
Epoch 9: loss 0.20199772302322636
Epoch 10: loss 0.14768713840717462
Epoch 11: loss 0.1129298931703806
Epoch 12: loss 0.0897176862634058
Epoch 13: loss 0.07381055919760397
Epoch 14: loss 0.06252384370333673
Epoch 15: loss 0.05461038379256916
Epoch 16: loss 0.04882992307345072
Epoch 17: loss 0.04414523629924407
Epoch 18: loss 0.04092336047791216
Epoch 19: loss 0.03833853546580651
Epoch 20: loss 0.03625500775962792
Epoch 21: loss 0.03446147335635507
Epoch 22: loss 0.03321051536013472
Epoch 23: loss 0.03198185292958338
Epoch 24: loss 0.03108082370059752
Epoch 25: loss 0.03021440888051594
Epoch 26: loss 0.029530160024745948
Epoch 27: loss 0.028893879057192297
Epoch 28: loss 0.02834567414468044
Epoch 29: loss 0.027737566247144883
Epoch 30: loss 0.027274296740034537
Epoch 31: loss 0.026871645190085062
Epoch 32: loss 0.026488456792031507
Epoch 33: loss 0.026147700262789238
Epoch 34: loss 0.02569683075818178
Epoch 35: loss 0.02543716934216353
Epoch 36: loss 0.025091200357970498
Epoch 37: loss 0.0247919171463137
Epoch 38: loss 0.024480266896617874
Epoch 39: loss 0.024290630063158524
Epoch 40: loss 0.023971327730920925
Epoch 41: loss 0.02380752954439601
Epoch 42: loss 0.023502599455055624
Epoch 43: loss 0.023236911156421902
Epoch 44: loss 0.023085622010573282
Epoch 45: loss 0.022893118906549426
Epoch 46: loss 0.0226612732628722
Epoch 47: loss 0.022429612872877997
Epoch 48: loss 0.02230401552682859
Epoch 49: loss 0.022117193627245662
-----------Time: 0:06:57.802337, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2652958631515503-------------


Epoch 0: loss 16.129371646696487
Epoch 1: loss 16.129376459147633
Epoch 2: loss 16.129374271245545
Epoch 3: loss 16.12937569734906
Epoch 4: loss 16.129368433037033
Epoch 5: loss 16.12936880900978
Epoch 6: loss 16.129370338829915
Epoch 7: loss 16.12937211082836
Epoch 8: loss 16.129367108575803
Epoch 9: loss 16.129366594659952
Epoch 10: loss 16.12936363458763
Epoch 11: loss 16.129357052212374
Epoch 12: loss 16.129363022659575
Epoch 13: loss 16.129361457057204
Epoch 14: loss 16.129361326374262
Epoch 15: loss 16.12935666690513
Epoch 16: loss 16.12935779793487
Epoch 17: loss 16.129356682462625
Epoch 18: loss 16.12935483163955
Epoch 19: loss 16.129349783233096
Epoch 20: loss 16.12935295333157
Epoch 21: loss 16.129350090752872
Epoch 22: loss 16.129351864307065
Epoch 23: loss 16.129350329301097
Epoch 24: loss 16.12934799930724
Epoch 25: loss 16.1293444169352
Epoch 26: loss 16.12934734329962
Epoch 27: loss 16.129344633702935
Epoch 28: loss 16.1293398793331
Epoch 29: loss 16.129338887283634
Epoch 30: loss 16.129344667410837
Epoch 31: loss 16.129340404657775
Epoch 32: loss 16.12933735850066
Epoch 33: loss 16.1293327980809
Epoch 34: loss 16.12933392288764
Epoch 35: loss 16.12933578304521
Epoch 36: loss 16.129332334467612
Epoch 37: loss 16.12933088243494
Epoch 38: loss 16.129328442501464
Epoch 39: loss 16.129327082257998
Epoch 40: loss 16.129329289866245
Epoch 41: loss 16.129326887789336
Epoch 42: loss 16.129322112676174
Epoch 43: loss 16.12932561207492
Epoch 44: loss 16.129318123216407
Epoch 45: loss 16.129319944998826
Epoch 46: loss 16.12932092667663
Epoch 47: loss 16.129319353295514
Epoch 48: loss 16.129317166949175
Epoch 49: loss 16.129320445431517
-----------Time: 0:04:29.613162, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129192869840853
Epoch 1: loss 16.12918717527986
Epoch 2: loss 16.129184375449714
Epoch 3: loss 16.129191931205447
Epoch 4: loss 16.129185661017214
Epoch 5: loss 16.129182279336835
Epoch 6: loss 16.129174869302954
Epoch 7: loss 16.129184719788892
Epoch 8: loss 16.129177060835126
Epoch 9: loss 16.129180167666462
Epoch 10: loss 16.129177864120344
Epoch 11: loss 16.129176123236885
Epoch 12: loss 16.129174819518976
Epoch 13: loss 16.129173804651856
Epoch 14: loss 16.129175975959285
Epoch 15: loss 16.12917250819411
Epoch 16: loss 16.12917111527991
Epoch 17: loss 16.12917106549593
Epoch 18: loss 16.129167509571634
Epoch 19: loss 16.1291633256432
Epoch 20: loss 16.129162765573458
Epoch 21: loss 16.129166300754434
Epoch 22: loss 16.12915755744341
Epoch 23: loss 16.129160509736984
Epoch 24: loss 16.12915682779699
Epoch 25: loss 16.129162717863814
Epoch 26: loss 16.129158740331455
Epoch 27: loss 16.129154995124324
Epoch 28: loss 16.129158050615935
Epoch 29: loss 16.129150622431645
Epoch 30: loss 16.129154370231692
Epoch 31: loss 16.12914792009512
Epoch 32: loss 16.129147808081175
Epoch 33: loss 16.129147548789625
Epoch 34: loss 16.12914512856231
Epoch 35: loss 16.12914424334096
Epoch 36: loss 16.129145295546067
Epoch 37: loss 16.129145730637287
Epoch 38: loss 16.129139599429323
Epoch 39: loss 16.12914160634591
Epoch 40: loss 16.129135794066553
Epoch 41: loss 16.129136222934775
Epoch 42: loss 16.129139262868893
Epoch 43: loss 16.129137629332135
Epoch 44: loss 16.129137281362876
Epoch 45: loss 16.129133881532088
Epoch 46: loss 16.129134103485654
Epoch 47: loss 16.12913333079684
Epoch 48: loss 16.129129695529326
Epoch 49: loss 16.12913038731918
-----------Time: 0:05:22.007936, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12917168105407
Epoch 1: loss 16.129168301448022
Epoch 2: loss 16.129167168862537
Epoch 3: loss 16.129163650794805
Epoch 4: loss 16.129164860130587
Epoch 5: loss 16.12916521691576
Epoch 6: loss 16.12915920187041
Epoch 7: loss 16.1291618684247
Epoch 8: loss 16.129161832642463
Epoch 9: loss 16.12915734482434
Epoch 10: loss 16.12915739149682
Epoch 11: loss 16.129154310594636
Epoch 12: loss 16.12915424629033
Epoch 13: loss 16.129156906103038
Epoch 14: loss 16.12915929054812
Epoch 15: loss 16.12915505320563
Epoch 16: loss 16.12915025371906
Epoch 17: loss 16.12914497661746
Epoch 18: loss 16.129143636598737
Epoch 19: loss 16.129141963649666
Epoch 20: loss 16.129142429337286
Epoch 21: loss 16.129144676876432
Epoch 22: loss 16.129140767797043
Epoch 23: loss 16.129139266498974
Epoch 24: loss 16.129139781970572
Epoch 25: loss 16.129137804613222
Epoch 26: loss 16.129138932012875
Epoch 27: loss 16.129136194931288
Epoch 28: loss 16.12913539164607
Epoch 29: loss 16.129134119043147
Epoch 30: loss 16.129129853697172
Epoch 31: loss 16.129133641946698
Epoch 32: loss 16.129131981962203
Epoch 33: loss 16.129126043667153
Epoch 34: loss 16.129130799074158
Epoch 35: loss 16.129122839342195
Epoch 36: loss 16.129126715232264
Epoch 37: loss 16.129124963458562
Epoch 38: loss 16.129122479445524
Epoch 39: loss 16.12912035273624
Epoch 40: loss 16.12912402586032
Epoch 41: loss 16.12911890640798
Epoch 42: loss 16.129119342017784
Epoch 43: loss 16.129113504846437
Epoch 44: loss 16.129117266648226
Epoch 45: loss 16.12911378280698
Epoch 46: loss 16.129117621359065
Epoch 47: loss 16.129111704325922
Epoch 48: loss 16.129109164824495
Epoch 49: loss 16.12910935514449
-----------Time: 0:04:54.848979, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.0171799659729-------------


Epoch 0: loss 16.12917728434444
Epoch 1: loss 16.12917581934719
Epoch 2: loss 16.12917501761772
Epoch 3: loss 16.129174400503835
Epoch 4: loss 16.1291697135498
Epoch 5: loss 16.129172346396185
Epoch 6: loss 16.129169509228056
Epoch 7: loss 16.129170740344332
Epoch 8: loss 16.129167721672122
Epoch 9: loss 16.129166195482064
Epoch 10: loss 16.129165035930257
Epoch 11: loss 16.129166256156285
Epoch 12: loss 16.129166629536115
Epoch 13: loss 16.12916044906276
Epoch 14: loss 16.129161499193533
Epoch 15: loss 16.129160066348433
Epoch 16: loss 16.12915841466127
Epoch 17: loss 16.129153831423853
Epoch 18: loss 16.12915234412753
Epoch 19: loss 16.12915224611532
Epoch 20: loss 16.129150894687772
Epoch 21: loss 16.12915057005475
Epoch 22: loss 16.12914932078807
Epoch 23: loss 16.129148148271685
Epoch 24: loss 16.12914562951358
Epoch 25: loss 16.129145049219094
Epoch 26: loss 16.129143247661414
Epoch 27: loss 16.129138602194026
Epoch 28: loss 16.129139984736565
Epoch 29: loss 16.129139577648832
Epoch 30: loss 16.129143919745108
Epoch 31: loss 16.129140150683156
Epoch 32: loss 16.129142887764743
Epoch 33: loss 16.12913620219145
Epoch 34: loss 16.129134070296338
Epoch 35: loss 16.129131383517308
Epoch 36: loss 16.129128413073325
Epoch 37: loss 16.129130848858136
Epoch 38: loss 16.12913772890009
Epoch 39: loss 16.12912784781775
Epoch 40: loss 16.129128645917138
Epoch 41: loss 16.129125183337795
Epoch 42: loss 16.129126616701477
Epoch 43: loss 16.129126536839678
Epoch 44: loss 16.129125465965583
Epoch 45: loss 16.129117683589037
Epoch 46: loss 16.12912022049755
Epoch 47: loss 16.129115338037685
Epoch 48: loss 16.129119600272166
Epoch 49: loss 16.129115070448808
-----------Time: 0:05:51.591799, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017182350158691-------------


Epoch 0: loss 16.12918058305152
Epoch 1: loss 16.12917394933654
Epoch 2: loss 16.12917411528313
Epoch 3: loss 16.12917481796323
Epoch 4: loss 16.129171619861264
Epoch 5: loss 16.129171467397835
Epoch 6: loss 16.12917294691541
Epoch 7: loss 16.129170540171256
Epoch 8: loss 16.12916845650437
Epoch 9: loss 16.12916639202506
Epoch 10: loss 16.12916202970404
Epoch 11: loss 16.12916606894779
Epoch 12: loss 16.129166519077916
Epoch 13: loss 16.1291614665228
Epoch 14: loss 16.12915671682021
Epoch 15: loss 16.129159734455254
Epoch 16: loss 16.1291558118927
Epoch 17: loss 16.12915590212616
Epoch 18: loss 16.129155067725957
Epoch 19: loss 16.12915678579176
Epoch 20: loss 16.129153804976113
Epoch 21: loss 16.129153594431376
Epoch 22: loss 16.129152375242516
Epoch 23: loss 16.129148206871577
Epoch 24: loss 16.129152380946927
Epoch 25: loss 16.129150581982163
Epoch 26: loss 16.129145858208727
Epoch 27: loss 16.129144077394372
Epoch 28: loss 16.12914303452376
Epoch 29: loss 16.12914082587835
Epoch 30: loss 16.129141688800622
Epoch 31: loss 16.129135893115926
Epoch 32: loss 16.12914165146264
Epoch 33: loss 16.129141534781443
Epoch 34: loss 16.129140997010772
Epoch 35: loss 16.12913395050364
Epoch 36: loss 16.12913186424384
Epoch 37: loss 16.129130421545664
Epoch 38: loss 16.129132849551723
Epoch 39: loss 16.129124039862063
Epoch 40: loss 16.12912533320831
Epoch 41: loss 16.129127903824724
Epoch 42: loss 16.12912866666046
Epoch 43: loss 16.129122900016416
Epoch 44: loss 16.129128563981006
Epoch 45: loss 16.129119607013745
Epoch 46: loss 16.129118021186635
Epoch 47: loss 16.12911902360776
Epoch 48: loss 16.12912184469981
Epoch 49: loss 16.12912273873707
-----------Time: 0:07:21.577949, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129246910866865
Epoch 1: loss 16.129236673517937
Epoch 2: loss 16.12922157497105
Epoch 3: loss 16.12920721021925
Epoch 4: loss 16.129202995175834
Epoch 5: loss 16.129185940533503
Epoch 6: loss 16.129178479159897
Epoch 7: loss 16.12916020481012
Epoch 8: loss 16.129151190280137
Epoch 9: loss 16.129138141692234
Epoch 10: loss 16.129121286185814
Epoch 11: loss 16.129111631205706
Epoch 12: loss 16.12909701338535
Epoch 13: loss 16.129085402309798
Epoch 14: loss 16.12907360298858
Epoch 15: loss 16.129062288023974
Epoch 16: loss 16.129047067091477
Epoch 17: loss 16.129035968894605
Epoch 18: loss 16.129021562137574
Epoch 19: loss 16.12901066100228
Epoch 20: loss 16.128999650964534
Epoch 21: loss 16.12899033358202
Epoch 22: loss 16.128974426045502
Epoch 23: loss 16.128961575556346
Epoch 24: loss 16.1289498690615
Epoch 25: loss 16.128935841907296
Epoch 26: loss 16.12892630464555
Epoch 27: loss 16.12891338207334
Epoch 28: loss 16.12889912674257
Epoch 29: loss 16.12888885879724
Epoch 30: loss 16.128879254638274
Epoch 31: loss 16.12886573673267
Epoch 32: loss 16.128851555559283
Epoch 33: loss 16.128837938085724
Epoch 34: loss 16.12882559840092
Epoch 35: loss 16.12881513806126
Epoch 36: loss 16.128800726118392
Epoch 37: loss 16.12878530293849
Epoch 38: loss 16.12877756671584
Epoch 39: loss 16.12876507923485
Epoch 40: loss 16.128756968595205
Epoch 41: loss 16.128737923112364
Epoch 42: loss 16.12872421592393
Epoch 43: loss 16.128715575292357
Epoch 44: loss 16.12870220051569
Epoch 45: loss 16.12869117440187
Epoch 46: loss 16.128678687958047
Epoch 47: loss 16.128667226234423
Epoch 48: loss 16.128654611700576
Epoch 49: loss 16.128643906589694
-----------Time: 0:05:04.387969, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017138481140137-------------


Epoch 0: loss 16.129184730160553
Epoch 1: loss 16.12917052772526
Epoch 2: loss 16.129162772315038
Epoch 3: loss 16.12915283989297
Epoch 4: loss 16.12913499285567
Epoch 5: loss 16.129125472707162
Epoch 6: loss 16.129105260412352
Epoch 7: loss 16.12909889687916
Epoch 8: loss 16.129085063675035
Epoch 9: loss 16.129076528834414
Epoch 10: loss 16.129060654487215
Epoch 11: loss 16.12904782474138
Epoch 12: loss 16.129039598976288
Epoch 13: loss 16.129025873637445
Epoch 14: loss 16.12901310197292
Epoch 15: loss 16.129000114577824
Epoch 16: loss 16.12898863107371
Epoch 17: loss 16.128973293460017
Epoch 18: loss 16.12896203657672
Epoch 19: loss 16.1289526979323
Epoch 20: loss 16.128936575183797
Epoch 21: loss 16.12892282132288
Epoch 22: loss 16.12891270895248
Epoch 23: loss 16.128900294073123
Epoch 24: loss 16.12888636596829
Epoch 25: loss 16.1288735165163
Epoch 26: loss 16.128860679510304
Epoch 27: loss 16.128853437497344
Epoch 28: loss 16.128842296258078
Epoch 29: loss 16.1288289370389
Epoch 30: loss 16.12881311973584
Epoch 31: loss 16.128797847982202
Epoch 32: loss 16.1287933072686
Epoch 33: loss 16.12877724519432
Epoch 34: loss 16.128761661253655
Epoch 35: loss 16.12875461889519
Epoch 36: loss 16.128742083704555
Epoch 37: loss 16.128727482997444
Epoch 38: loss 16.12871219413056
Epoch 39: loss 16.128701691785672
Epoch 40: loss 16.128687446826564
Epoch 41: loss 16.1286763361837
Epoch 42: loss 16.12866307030948
Epoch 43: loss 16.128658209111524
Epoch 44: loss 16.128638646601335
Epoch 45: loss 16.128629883065567
Epoch 46: loss 16.128618630849516
Epoch 47: loss 16.128601041548016
Epoch 48: loss 16.128593077667386
Epoch 49: loss 16.128578012828402
-----------Time: 0:03:48.029226, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017116546630859-------------


Epoch 0: loss 16.12917326791835
Epoch 1: loss 16.1291542073966
Epoch 2: loss 16.129147041096772
Epoch 3: loss 16.12913364609536
Epoch 4: loss 16.129116893268396
Epoch 5: loss 16.12910461892506
Epoch 6: loss 16.129096885813908
Epoch 7: loss 16.129083292195173
Epoch 8: loss 16.12907133781761
Epoch 9: loss 16.12905559778343
Epoch 10: loss 16.129041287482856
Epoch 11: loss 16.129026047881368
Epoch 12: loss 16.129019983570625
Epoch 13: loss 16.129009987362835
Epoch 14: loss 16.128997201696563
Epoch 15: loss 16.128983171949443
Epoch 16: loss 16.128969335115233
Epoch 17: loss 16.12895990883027
Epoch 18: loss 16.128946234831155
Epoch 19: loss 16.128934351499474
Epoch 20: loss 16.12892199210851
Epoch 21: loss 16.12890623962834
Epoch 22: loss 16.12889776338761
Epoch 23: loss 16.128886652744743
Epoch 24: loss 16.128872933110316
Epoch 25: loss 16.128859363864983
Epoch 26: loss 16.12884505719449
Epoch 27: loss 16.12883551785841
Epoch 28: loss 16.128819912137253
Epoch 29: loss 16.128810779370323
Epoch 30: loss 16.128796301567405
Epoch 31: loss 16.12878978297787
Epoch 32: loss 16.128775674406118
Epoch 33: loss 16.128760250189046
Epoch 34: loss 16.128746315861214
Epoch 35: loss 16.12873574247044
Epoch 36: loss 16.12872172620648
Epoch 37: loss 16.128710587041542
Epoch 38: loss 16.12869963975235
Epoch 39: loss 16.128686908537308
Epoch 40: loss 16.12867597369411
Epoch 41: loss 16.12865976071215
Epoch 42: loss 16.12864794635202
Epoch 43: loss 16.128634201825605
Epoch 44: loss 16.12862736690038
Epoch 45: loss 16.128613574145735
Epoch 46: loss 16.128594908784304
Epoch 47: loss 16.128588287515313
Epoch 48: loss 16.128574298217675
Epoch 49: loss 16.12856578100888
-----------Time: 0:05:15.501212, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017111778259277-------------


Epoch 0: loss 16.12917204769232
Epoch 1: loss 16.129156974037425
Epoch 2: loss 16.129149760546536
Epoch 3: loss 16.129132017744436
Epoch 4: loss 16.129121347897204
Epoch 5: loss 16.12910836776227
Epoch 6: loss 16.129099119869895
Epoch 7: loss 16.12908797759346
Epoch 8: loss 16.1290714669448
Epoch 9: loss 16.129056536937423
Epoch 10: loss 16.12904942975607
Epoch 11: loss 16.12903702576696
Epoch 12: loss 16.1290268874674
Epoch 13: loss 16.129009873793137
Epoch 14: loss 16.128999259434295
Epoch 15: loss 16.128985931330106
Epoch 16: loss 16.128970525782027
Epoch 17: loss 16.128961886706204
Epoch 18: loss 16.12894703293054
Epoch 19: loss 16.128933952709072
Epoch 20: loss 16.128926560825597
Epoch 21: loss 16.12891341215116
Epoch 22: loss 16.12889279795445
Epoch 23: loss 16.128886771500273
Epoch 24: loss 16.128872056186296
Epoch 25: loss 16.12886109282103
Epoch 26: loss 16.12885019739015
Epoch 27: loss 16.128836350702862
Epoch 28: loss 16.128824137552332
Epoch 29: loss 16.128812244886156
Epoch 30: loss 16.128798502434073
Epoch 31: loss 16.128791068026786
Epoch 32: loss 16.128771129025267
Epoch 33: loss 16.128757185881522
Epoch 34: loss 16.128750335398806
Epoch 35: loss 16.12873165292413
Epoch 36: loss 16.128721896301734
Epoch 37: loss 16.128717279875
Epoch 38: loss 16.128699472250013
Epoch 39: loss 16.12868730214188
Epoch 40: loss 16.128672307311614
Epoch 41: loss 16.128665013440347
Epoch 42: loss 16.128649679975318
Epoch 43: loss 16.128641593190494
Epoch 44: loss 16.12862552904188
Epoch 45: loss 16.128615857985697
Epoch 46: loss 16.128603495483233
Epoch 47: loss 16.1285879872557
Epoch 48: loss 16.12858130064524
Epoch 49: loss 16.12856504462088
-----------Time: 0:06:27.692909, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017112731933594-------------


Epoch 0: loss 16.129170675521443
Epoch 1: loss 16.129153140671168
Epoch 2: loss 16.129144530635998
Epoch 3: loss 16.12913228792623
Epoch 4: loss 16.129119873046875
Epoch 5: loss 16.129107016853304
Epoch 6: loss 16.129094095836845
Epoch 7: loss 16.12908392123647
Epoch 8: loss 16.12907029857708
Epoch 9: loss 16.1290578629544
Epoch 10: loss 16.129044975645844
Epoch 11: loss 16.129029085741152
Epoch 12: loss 16.129017612608703
Epoch 13: loss 16.12900927327391
Epoch 14: loss 16.12899422606675
Epoch 15: loss 16.128979994590804
Epoch 16: loss 16.128967947386865
Epoch 17: loss 16.12895300078483
Epoch 18: loss 16.12894786992366
Epoch 19: loss 16.128932488230404
Epoch 20: loss 16.128922558919836
Epoch 21: loss 16.128905566507477
Epoch 22: loss 16.128893925872685
Epoch 23: loss 16.128885729148248
Epoch 24: loss 16.12887247831294
Epoch 25: loss 16.12885961071054
Epoch 26: loss 16.12884882418211
Epoch 27: loss 16.128836033330007
Epoch 28: loss 16.128817625185793
Epoch 29: loss 16.128805151706548
Epoch 30: loss 16.128794064918505
Epoch 31: loss 16.12878011658893
Epoch 32: loss 16.12876566160367
Epoch 33: loss 16.128758573091307
Epoch 34: loss 16.128746525368786
Epoch 35: loss 16.12873002872187
Epoch 36: loss 16.12872179051078
Epoch 37: loss 16.128709797239484
Epoch 38: loss 16.128690447348365
Epoch 39: loss 16.128683231783146
Epoch 40: loss 16.128670628658128
Epoch 41: loss 16.128655987501535
Epoch 42: loss 16.12864867444269
Epoch 43: loss 16.128631733888643
Epoch 44: loss 16.128624951858892
Epoch 45: loss 16.128608600415244
Epoch 46: loss 16.12859882045661
Epoch 47: loss 16.128586760806673
Epoch 48: loss 16.128572994499766
Epoch 49: loss 16.12855820036116
-----------Time: 0:07:37.230367, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0171122550964355-------------


Epoch 0: loss 16.129262963088067
Epoch 1: loss 16.1291376028844
Epoch 2: loss 16.1290118200355
Epoch 3: loss 16.128887656203037
Epoch 4: loss 16.128763908274216
Epoch 5: loss 16.128641898117355
Epoch 6: loss 16.128520609309582
Epoch 7: loss 16.128393503555206
Epoch 8: loss 16.128273543875913
Epoch 9: loss 16.12814613060176
Epoch 10: loss 16.128025497801605
Epoch 11: loss 16.127895006736768
Epoch 12: loss 16.127775419400137
Epoch 13: loss 16.127647006816357
Epoch 14: loss 16.127523100201106
Epoch 15: loss 16.127402241298725
Epoch 16: loss 16.12727859501219
Epoch 17: loss 16.127151017865778
Epoch 18: loss 16.12702677520868
Epoch 19: loss 16.126901584581685
Epoch 20: loss 16.12678566933197
Epoch 21: loss 16.12665705657511
Epoch 22: loss 16.126534467679512
Epoch 23: loss 16.126410846284966
Epoch 24: loss 16.12628729282481
Epoch 25: loss 16.126162978603244
Epoch 26: loss 16.126037918659186
Epoch 27: loss 16.125914983350082
Epoch 28: loss 16.125792268957372
Epoch 29: loss 16.12566909302571
Epoch 30: loss 16.125545236194437
Epoch 31: loss 16.125416403558347
Epoch 32: loss 16.12529596781977
Epoch 33: loss 16.125169817295458
Epoch 34: loss 16.125049272135847
Epoch 35: loss 16.124925450049645
Epoch 36: loss 16.12479828102813
Epoch 37: loss 16.124671058592554
Epoch 38: loss 16.124553327783417
Epoch 39: loss 16.124430573978394
Epoch 40: loss 16.12430409985823
Epoch 41: loss 16.124179742075686
Epoch 42: loss 16.12405887798747
Epoch 43: loss 16.1239292991103
Epoch 44: loss 16.123808069939585
Epoch 45: loss 16.12368778873877
Epoch 46: loss 16.12356162524988
Epoch 47: loss 16.123437715004552
Epoch 48: loss 16.123314247110603
Epoch 49: loss 16.123188470484706
-----------Time: 0:03:28.433124, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016417026519775-------------


Epoch 0: loss 16.129122136662097
Epoch 1: loss 16.128997699536338
Epoch 2: loss 16.128873405539515
Epoch 3: loss 16.128749135397516
Epoch 4: loss 16.128625858342147
Epoch 5: loss 16.12850579027899
Epoch 6: loss 16.12837236610815
Epoch 7: loss 16.1282496408252
Epoch 8: loss 16.128129732485633
Epoch 9: loss 16.128004572455037
Epoch 10: loss 16.127880648207963
Epoch 11: loss 16.127754453085505
Epoch 12: loss 16.127633384156965
Epoch 13: loss 16.127508435189693
Epoch 14: loss 16.12738814932163
Epoch 15: loss 16.12725748193854
Epoch 16: loss 16.127136763572175
Epoch 17: loss 16.12701424883396
Epoch 18: loss 16.126889154144834
Epoch 19: loss 16.12676855816408
Epoch 20: loss 16.12663825949357
Epoch 21: loss 16.126509074220973
Epoch 22: loss 16.12639388550618
Epoch 23: loss 16.126268722882667
Epoch 24: loss 16.126141693878587
Epoch 25: loss 16.12602034024793
Epoch 26: loss 16.125898118509166
Epoch 27: loss 16.125775556061306
Epoch 28: loss 16.12564934434419
Epoch 29: loss 16.12552659105775
Epoch 30: loss 16.125402023249052
Epoch 31: loss 16.125274561228085
Epoch 32: loss 16.125155227997176
Epoch 33: loss 16.125029910317316
Epoch 34: loss 16.12490595184376
Epoch 35: loss 16.12477973442223
Epoch 36: loss 16.124655582517175
Epoch 37: loss 16.12453487452247
Epoch 38: loss 16.12441161561751
Epoch 39: loss 16.124284034322436
Epoch 40: loss 16.124161324078393
Epoch 41: loss 16.124039137084697
Epoch 42: loss 16.123913043604524
Epoch 43: loss 16.123790558425547
Epoch 44: loss 16.123667007558304
Epoch 45: loss 16.12353924268481
Epoch 46: loss 16.12341679795531
Epoch 47: loss 16.123293799897652
Epoch 48: loss 16.123164006845663
Epoch 49: loss 16.12304442262053
-----------Time: 0:04:21.837308, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016424179077148-------------


Epoch 0: loss 16.129110710202124
Epoch 1: loss 16.128987899871543
Epoch 2: loss 16.12886509005955
Epoch 3: loss 16.128741038759614
Epoch 4: loss 16.128620277869434
Epoch 5: loss 16.12849721032164
Epoch 6: loss 16.128367666189536
Epoch 7: loss 16.12824857150685
Epoch 8: loss 16.128125117096065
Epoch 9: loss 16.127995683940743
Epoch 10: loss 16.1278796499355
Epoch 11: loss 16.127751943143313
Epoch 12: loss 16.127625142315797
Epoch 13: loss 16.127497733190307
Epoch 14: loss 16.127378857868273
Epoch 15: loss 16.12725607398543
Epoch 16: loss 16.12712796529134
Epoch 17: loss 16.127004491174397
Epoch 18: loss 16.12687817885216
Epoch 19: loss 16.12675894363345
Epoch 20: loss 16.126635143846322
Epoch 21: loss 16.126516255559707
Epoch 22: loss 16.12638710399501
Epoch 23: loss 16.126265074650576
Epoch 24: loss 16.126136109775793
Epoch 25: loss 16.126013378269846
Epoch 26: loss 16.125887410805365
Epoch 27: loss 16.125768595120388
Epoch 28: loss 16.12564500691516
Epoch 29: loss 16.125517561488856
Epoch 30: loss 16.125393223931052
Epoch 31: loss 16.12527243400022
Epoch 32: loss 16.12514629332899
Epoch 33: loss 16.1250233144589
Epoch 34: loss 16.124896557710947
Epoch 35: loss 16.124778457152058
Epoch 36: loss 16.1246492983272
Epoch 37: loss 16.12452585584382
Epoch 38: loss 16.124401281293544
Epoch 39: loss 16.124277323338568
Epoch 40: loss 16.124158662191352
Epoch 41: loss 16.124034557477362
Epoch 42: loss 16.123910860369683
Epoch 43: loss 16.123780775873993
Epoch 44: loss 16.12365870971016
Epoch 45: loss 16.123533541382233
Epoch 46: loss 16.12341018135357
Epoch 47: loss 16.12328336444998
Epoch 48: loss 16.12316590019255
Epoch 49: loss 16.12303860100668
-----------Time: 0:05:55.418930, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016422271728516-------------


Epoch 0: loss 16.1291138834121
Epoch 1: loss 16.128995158997743
Epoch 2: loss 16.128865975799478
Epoch 3: loss 16.128745656742097
Epoch 4: loss 16.128617570865668
Epoch 5: loss 16.12849509916985
Epoch 6: loss 16.128367103526877
Epoch 7: loss 16.128246077122153
Epoch 8: loss 16.12811893714129
Epoch 9: loss 16.127995852998833
Epoch 10: loss 16.127867927883162
Epoch 11: loss 16.12774933155883
Epoch 12: loss 16.127628043788228
Epoch 13: loss 16.127503868028352
Epoch 14: loss 16.127376439196706
Epoch 15: loss 16.12725730147162
Epoch 16: loss 16.1271260703887
Epoch 17: loss 16.127007481324537
Epoch 18: loss 16.126879496053228
Epoch 19: loss 16.12676183421564
Epoch 20: loss 16.126635353353894
Epoch 21: loss 16.126508336277226
Epoch 22: loss 16.126388186277936
Epoch 23: loss 16.126262323567243
Epoch 24: loss 16.12613900554381
Epoch 25: loss 16.126013286480635
Epoch 26: loss 16.12589072973719
Epoch 27: loss 16.125768522000172
Epoch 28: loss 16.125641143471082
Epoch 29: loss 16.125515486119298
Epoch 30: loss 16.125395791435967
Epoch 31: loss 16.125272220343984
Epoch 32: loss 16.125144280189403
Epoch 33: loss 16.12502040987497
Epoch 34: loss 16.12489859729827
Epoch 35: loss 16.124772868382017
Epoch 36: loss 16.12465240723287
Epoch 37: loss 16.12452353466588
Epoch 38: loss 16.12440542632824
Epoch 39: loss 16.12427429481328
Epoch 40: loss 16.1241532087715
Epoch 41: loss 16.12403098962565
Epoch 42: loss 16.123905486811626
Epoch 43: loss 16.12378204951408
Epoch 44: loss 16.123660182486155
Epoch 45: loss 16.12353004665074
Epoch 46: loss 16.12340999362649
Epoch 47: loss 16.123287172924247
Epoch 48: loss 16.123165676164653
Epoch 49: loss 16.123034684148543
-----------Time: 0:06:32.475043, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016419410705566-------------


Epoch 0: loss 16.129114249531764
Epoch 1: loss 16.128991345856228
Epoch 2: loss 16.128868697842158
Epoch 3: loss 16.12873803149623
Epoch 4: loss 16.12862001546639
Epoch 5: loss 16.128491904697967
Epoch 6: loss 16.128372410706298
Epoch 7: loss 16.12824755508398
Epoch 8: loss 16.12811872659655
Epoch 9: loss 16.128000196132277
Epoch 10: loss 16.127874406023217
Epoch 11: loss 16.12775080277908
Epoch 12: loss 16.1276295694597
Epoch 13: loss 16.127503432937132
Epoch 14: loss 16.127376097969027
Epoch 15: loss 16.12725242108609
Epoch 16: loss 16.127129567713112
Epoch 17: loss 16.127006920217624
Epoch 18: loss 16.126881101586495
Epoch 19: loss 16.12676593465219
Epoch 20: loss 16.126630173227333
Epoch 21: loss 16.126515088747738
Epoch 22: loss 16.12638044901812
Epoch 23: loss 16.126260034541453
Epoch 24: loss 16.126135869153238
Epoch 25: loss 16.126011231854402
Epoch 26: loss 16.125888456268893
Epoch 27: loss 16.125766522343746
Epoch 28: loss 16.125641328605248
Epoch 29: loss 16.125517343683953
Epoch 30: loss 16.125393373282982
Epoch 31: loss 16.125271958459518
Epoch 32: loss 16.125145569386984
Epoch 33: loss 16.125025427166438
Epoch 34: loss 16.124899041205403
Epoch 35: loss 16.12477560494502
Epoch 36: loss 16.124651544310595
Epoch 37: loss 16.124527691109403
Epoch 38: loss 16.124406394522886
Epoch 39: loss 16.124278794040233
Epoch 40: loss 16.124151301422867
Epoch 41: loss 16.124029041827534
Epoch 42: loss 16.123912410933144
Epoch 43: loss 16.123779780194447
Epoch 44: loss 16.123662573154235
Epoch 45: loss 16.123537562994155
Epoch 46: loss 16.123412296654024
Epoch 47: loss 16.123286418904424
Epoch 48: loss 16.123164128194105
Epoch 49: loss 16.123040934630616
-----------Time: 0:06:37.154001, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016421318054199-------------


Epoch 0: loss 16.128687575435173
Epoch 1: loss 16.127447311874317
Epoch 2: loss 16.12620864399365
Epoch 3: loss 16.124973880525125
Epoch 4: loss 16.123729572534693
Epoch 5: loss 16.122494638452327
Epoch 6: loss 16.12125424420853
Epoch 7: loss 16.120011260679327
Epoch 8: loss 16.118779341639094
Epoch 9: loss 16.11753326891038
Epoch 10: loss 16.11629862212305
Epoch 11: loss 16.115053887338725
Epoch 12: loss 16.11381344279237
Epoch 13: loss 16.112575159181777
Epoch 14: loss 16.11133493192174
Epoch 15: loss 16.11010250830015
Epoch 16: loss 16.108862363494822
Epoch 17: loss 16.107621061212022
Epoch 18: loss 16.106379245531954
Epoch 19: loss 16.105140506086823
Epoch 20: loss 16.10390473345559
Epoch 21: loss 16.102664817863992
Epoch 22: loss 16.101427262862654
Epoch 23: loss 16.10018422073356
Epoch 24: loss 16.098945371348808
Epoch 25: loss 16.09770944521698
Epoch 26: loss 16.09646740499043
Epoch 27: loss 16.095233151289094
Epoch 28: loss 16.09399177173741
Epoch 29: loss 16.092752277235288
Epoch 30: loss 16.091514627333243
Epoch 31: loss 16.090275108976297
Epoch 32: loss 16.089036599263476
Epoch 33: loss 16.087806221452208
Epoch 34: loss 16.08656584172874
Epoch 35: loss 16.08532503884146
Epoch 36: loss 16.084087555923173
Epoch 37: loss 16.082852830829797
Epoch 38: loss 16.081615254047968
Epoch 39: loss 16.080373314945124
Epoch 40: loss 16.0791405978055
Epoch 41: loss 16.077900160000723
Epoch 42: loss 16.07666183023624
Epoch 43: loss 16.075423801768533
Epoch 44: loss 16.074190922830986
Epoch 45: loss 16.072946259092546
Epoch 46: loss 16.071712159410385
Epoch 47: loss 16.07047406767554
Epoch 48: loss 16.069234389595003
Epoch 49: loss 16.067997434075725
-----------Time: 0:04:11.734440, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009501934051514-------------


Epoch 0: loss 16.128620555311393
Epoch 1: loss 16.127377203069607
Epoch 2: loss 16.126142404856015
Epoch 3: loss 16.12489767318319
Epoch 4: loss 16.12366086286718
Epoch 5: loss 16.12241819308075
Epoch 6: loss 16.121179935917898
Epoch 7: loss 16.119932387301688
Epoch 8: loss 16.118694941721383
Epoch 9: loss 16.117456194497503
Epoch 10: loss 16.11622117640468
Epoch 11: loss 16.11498264024412
Epoch 12: loss 16.113740324131363
Epoch 13: loss 16.11250496325511
Epoch 14: loss 16.1112632041006
Epoch 15: loss 16.110023312363825
Epoch 16: loss 16.108781143528667
Epoch 17: loss 16.10754068912923
Epoch 18: loss 16.10630226135254
Epoch 19: loss 16.10506612882464
Epoch 20: loss 16.10383105005759
Epoch 21: loss 16.102587324954555
Epoch 22: loss 16.101353459153376
Epoch 23: loss 16.10010675738336
Epoch 24: loss 16.098875831948344
Epoch 25: loss 16.09763807210668
Epoch 26: loss 16.096395876305202
Epoch 27: loss 16.09515640721365
Epoch 28: loss 16.093917818157617
Epoch 29: loss 16.092679559439016
Epoch 30: loss 16.091439875135478
Epoch 31: loss 16.090195932746127
Epoch 32: loss 16.088966672481437
Epoch 33: loss 16.087728469769807
Epoch 34: loss 16.08649090802689
Epoch 35: loss 16.085253929689955
Epoch 36: loss 16.084014500529303
Epoch 37: loss 16.08277199720805
Epoch 38: loss 16.0815383113552
Epoch 39: loss 16.080298667501147
Epoch 40: loss 16.079063596512846
Epoch 41: loss 16.07782820348444
Epoch 42: loss 16.07658469618631
Epoch 43: loss 16.075349118023738
Epoch 44: loss 16.07410919932064
Epoch 45: loss 16.072874442075115
Epoch 46: loss 16.071634787849398
Epoch 47: loss 16.070398143479977
Epoch 48: loss 16.06916071605008
Epoch 49: loss 16.067922077728653
-----------Time: 0:05:17.653456, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.0095038414001465-------------


Epoch 0: loss 16.12854861590835
Epoch 1: loss 16.127307920367773
Epoch 2: loss 16.12607461515492
Epoch 3: loss 16.124830754701698
Epoch 4: loss 16.123591886129375
Epoch 5: loss 16.12234739248619
Epoch 6: loss 16.12111490300455
Epoch 7: loss 16.11987467418876
Epoch 8: loss 16.118634223419406
Epoch 9: loss 16.117394008086777
Epoch 10: loss 16.116151515137183
Epoch 11: loss 16.11491906558644
Epoch 12: loss 16.113676401504424
Epoch 13: loss 16.112428996017147
Epoch 14: loss 16.11119779936317
Epoch 15: loss 16.109958251446987
Epoch 16: loss 16.10871806774793
Epoch 17: loss 16.107480164777332
Epoch 18: loss 16.106238899832512
Epoch 19: loss 16.10500289280172
Epoch 20: loss 16.10376398274275
Epoch 21: loss 16.10252424606232
Epoch 22: loss 16.101286542746216
Epoch 23: loss 16.100048610734966
Epoch 24: loss 16.09880896325083
Epoch 25: loss 16.09756839268878
Epoch 26: loss 16.096331092830322
Epoch 27: loss 16.095092437395653
Epoch 28: loss 16.093855180061013
Epoch 29: loss 16.09261590491954
Epoch 30: loss 16.091376614220575
Epoch 31: loss 16.090137482208036
Epoch 32: loss 16.08890414432445
Epoch 33: loss 16.087661094416607
Epoch 34: loss 16.086421064218143
Epoch 35: loss 16.085190396518925
Epoch 36: loss 16.083952591560532
Epoch 37: loss 16.08271959957187
Epoch 38: loss 16.081473401864628
Epoch 39: loss 16.08023691825597
Epoch 40: loss 16.079003461616853
Epoch 41: loss 16.077768414483373
Epoch 42: loss 16.076518994300763
Epoch 43: loss 16.075285711905565
Epoch 44: loss 16.074049324753364
Epoch 45: loss 16.072811113225825
Epoch 46: loss 16.071570470062138
Epoch 47: loss 16.07033478455287
Epoch 48: loss 16.069097136206572
Epoch 49: loss 16.06786429149551
-----------Time: 0:05:28.297282, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009512424468994-------------


Epoch 0: loss 16.128563592588208
Epoch 1: loss 16.12732807665561
Epoch 2: loss 16.126082273590104
Epoch 3: loss 16.124845727751474
Epoch 4: loss 16.123605743188325
Epoch 5: loss 16.12236769657021
Epoch 6: loss 16.121130445458565
Epoch 7: loss 16.119883817327352
Epoch 8: loss 16.118648505716497
Epoch 9: loss 16.117409290212077
Epoch 10: loss 16.116170048259924
Epoch 11: loss 16.11493488496383
Epoch 12: loss 16.113691337216217
Epoch 13: loss 16.112455469165695
Epoch 14: loss 16.11121637760264
Epoch 15: loss 16.109974645933033
Epoch 16: loss 16.108733683840747
Epoch 17: loss 16.107498974823447
Epoch 18: loss 16.10625811748494
Epoch 19: loss 16.105020311489387
Epoch 20: loss 16.103773596236213
Epoch 21: loss 16.102536103464846
Epoch 22: loss 16.101304162644123
Epoch 23: loss 16.10006338101875
Epoch 24: loss 16.09882290069016
Epoch 25: loss 16.09758778302938
Epoch 26: loss 16.096346798119434
Epoch 27: loss 16.095103279931056
Epoch 28: loss 16.093866324411778
Epoch 29: loss 16.092629082116048
Epoch 30: loss 16.09138963013774
Epoch 31: loss 16.090151319042246
Epoch 32: loss 16.088913722035677
Epoch 33: loss 16.0876743825899
Epoch 34: loss 16.086433553773468
Epoch 35: loss 16.085200216927042
Epoch 36: loss 16.08396583202418
Epoch 37: loss 16.082727524040184
Epoch 38: loss 16.08148346030239
Epoch 39: loss 16.08024393312953
Epoch 40: loss 16.07900637086803
Epoch 41: loss 16.077771092965072
Epoch 42: loss 16.07653021384608
Epoch 43: loss 16.075294909495383
Epoch 44: loss 16.07405549485506
Epoch 45: loss 16.07281383941717
Epoch 46: loss 16.07157681181484
Epoch 47: loss 16.070339605301342
Epoch 48: loss 16.06910453223871
Epoch 49: loss 16.067862779307198
-----------Time: 0:05:35.930861, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.0095086097717285-------------


Epoch 0: loss 16.128557717560295
Epoch 1: loss 16.127319181399738
Epoch 2: loss 16.12607633788797
Epoch 3: loss 16.124837835953894
Epoch 4: loss 16.123596605235562
Epoch 5: loss 16.1223593178231
Epoch 6: loss 16.121118099550763
Epoch 7: loss 16.11987976823053
Epoch 8: loss 16.11864330795811
Epoch 9: loss 16.117397362800833
Epoch 10: loss 16.116158884203
Epoch 11: loss 16.114919948733455
Epoch 12: loss 16.113684717502977
Epoch 13: loss 16.112442954199803
Epoch 14: loss 16.111200130912778
Epoch 15: loss 16.10996099941882
Epoch 16: loss 16.10872359480658
Epoch 17: loss 16.107489137302085
Epoch 18: loss 16.106244605283752
Epoch 19: loss 16.10500396886165
Epoch 20: loss 16.103768438408725
Epoch 21: loss 16.10252570794807
Epoch 22: loss 16.101288286741173
Epoch 23: loss 16.10005014003652
Epoch 24: loss 16.09881136117907
Epoch 25: loss 16.097571223115324
Epoch 26: loss 16.096335617986433
Epoch 27: loss 16.09509413316238
Epoch 28: loss 16.093858874447
Epoch 29: loss 16.092621901295896
Epoch 30: loss 16.091372825971046
Epoch 31: loss 16.09014008653235
Epoch 32: loss 16.088905710963985
Epoch 33: loss 16.08766651724006
Epoch 34: loss 16.086423444514562
Epoch 35: loss 16.085179963145585
Epoch 36: loss 16.083946271588324
Epoch 37: loss 16.082706264207516
Epoch 38: loss 16.081466550344743
Epoch 39: loss 16.080228045299172
Epoch 40: loss 16.078988063328936
Epoch 41: loss 16.077750330453597
Epoch 42: loss 16.076517698880185
Epoch 43: loss 16.075271908779257
Epoch 44: loss 16.074038583341665
Epoch 45: loss 16.0727950708577
Epoch 46: loss 16.071557857602624
Epoch 47: loss 16.07032119404563
Epoch 48: loss 16.06908022832326
Epoch 49: loss 16.067846097526115
-----------Time: 0:07:16.578313, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009506702423096-------------


Epoch 0: loss 16.12294271654769
Epoch 1: loss 16.110555858311283
Epoch 2: loss 16.09817150068257
Epoch 3: loss 16.085794223787474
Epoch 4: loss 16.073420769886972
Epoch 5: loss 16.061045768015926
Epoch 6: loss 16.048684613350748
Epoch 7: loss 16.03632380665483
Epoch 8: loss 16.023973060471004
Epoch 9: loss 16.011626781880565
Epoch 10: loss 15.99928259525434
Epoch 11: loss 15.986949759893538
Epoch 12: loss 15.974616247263208
Epoch 13: loss 15.962295060315943
Epoch 14: loss 15.949983102073483
Epoch 15: loss 15.937666270187071
Epoch 16: loss 15.92535306319651
Epoch 17: loss 15.913047884390885
Epoch 18: loss 15.900749263068526
Epoch 19: loss 15.888441721598307
Epoch 20: loss 15.876151537104363
Epoch 21: loss 15.863867724959523
Epoch 22: loss 15.851590659062197
Epoch 23: loss 15.839319399221232
Epoch 24: loss 15.827052124691372
Epoch 25: loss 15.814785953706345
Epoch 26: loss 15.802526552823654
Epoch 27: loss 15.790279661721026
Epoch 28: loss 15.778034132417613
Epoch 29: loss 15.765781042172637
Epoch 30: loss 15.753544699568797
Epoch 31: loss 15.741314571146216
Epoch 32: loss 15.729087965977198
Epoch 33: loss 15.716871922271546
Epoch 34: loss 15.704657626190933
Epoch 35: loss 15.692444781105825
Epoch 36: loss 15.68023117629648
Epoch 37: loss 15.668031644276654
Epoch 38: loss 15.655835247091654
Epoch 39: loss 15.64364053737605
Epoch 40: loss 15.631453873477207
Epoch 41: loss 15.61926896498215
Epoch 42: loss 15.607093269634403
Epoch 43: loss 15.594914034438432
Epoch 44: loss 15.582755086213236
Epoch 45: loss 15.570592561839002
Epoch 46: loss 15.558436983885358
Epoch 47: loss 15.546282429096166
Epoch 48: loss 15.534134040778588
Epoch 49: loss 15.52199636898072
-----------Time: 0:05:03.825823, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9404962062835693-------------


Epoch 0: loss 16.12300012732667
Epoch 1: loss 16.110618863046138
Epoch 2: loss 16.09823124664124
Epoch 3: loss 16.08585615194382
Epoch 4: loss 16.073477846179856
Epoch 5: loss 16.06110493575444
Epoch 6: loss 16.048746125603447
Epoch 7: loss 16.03637752719649
Epoch 8: loss 16.0240178271569
Epoch 9: loss 16.01167089878184
Epoch 10: loss 15.999326602215998
Epoch 11: loss 15.986986858809752
Epoch 12: loss 15.974650186452612
Epoch 13: loss 15.962316344522018
Epoch 14: loss 15.949987432760931
Epoch 15: loss 15.937672765440368
Epoch 16: loss 15.925354259049135
Epoch 17: loss 15.913033079362032
Epoch 18: loss 15.900729780938718
Epoch 19: loss 15.888425160647087
Epoch 20: loss 15.876118212954514
Epoch 21: loss 15.863816171576628
Epoch 22: loss 15.85151717168861
Epoch 23: loss 15.839224663942387
Epoch 24: loss 15.826943247651457
Epoch 25: loss 15.814657208192212
Epoch 26: loss 15.802369964583015
Epoch 27: loss 15.790089388915286
Epoch 28: loss 15.77781852060456
Epoch 29: loss 15.765535462998125
Epoch 30: loss 15.753273387263818
Epoch 31: loss 15.741004355774423
Epoch 32: loss 15.728725469131843
Epoch 33: loss 15.71645599580965
Epoch 34: loss 15.704179646075582
Epoch 35: loss 15.691911455727972
Epoch 36: loss 15.679638758893244
Epoch 37: loss 15.66736715263877
Epoch 38: loss 15.655100079837736
Epoch 39: loss 15.64282459613744
Epoch 40: loss 15.630545309667292
Epoch 41: loss 15.618262733305972
Epoch 42: loss 15.605981274491228
Epoch 43: loss 15.593687999242022
Epoch 44: loss 15.581395638254818
Epoch 45: loss 15.569093221422769
Epoch 46: loss 15.556792530953851
Epoch 47: loss 15.5444741386509
Epoch 48: loss 15.532147431905145
Epoch 49: loss 15.51982674072332
-----------Time: 0:04:13.595735, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9401891231536865-------------


Epoch 0: loss 16.123030664610955
Epoch 1: loss 16.11065201035915
Epoch 2: loss 16.098264560419427
Epoch 3: loss 16.085888861054116
Epoch 4: loss 16.07350847888343
Epoch 5: loss 16.06113874529807
Epoch 6: loss 16.048773522867076
Epoch 7: loss 16.03640889006506
Epoch 8: loss 16.02404292243016
Epoch 9: loss 16.011681759467653
Epoch 10: loss 15.999325193744305
Epoch 11: loss 15.98697357374796
Epoch 12: loss 15.974626000255524
Epoch 13: loss 15.962275857702425
Epoch 14: loss 15.949922139518867
Epoch 15: loss 15.937575328343584
Epoch 16: loss 15.925226535662288
Epoch 17: loss 15.912874627852323
Epoch 18: loss 15.900513392806767
Epoch 19: loss 15.888147803737523
Epoch 20: loss 15.875774182853263
Epoch 21: loss 15.863386897822966
Epoch 22: loss 15.850991669655365
Epoch 23: loss 15.838569206022063
Epoch 24: loss 15.826132896220056
Epoch 25: loss 15.813672526755237
Epoch 26: loss 15.80118934689429
Epoch 27: loss 15.788667743137312
Epoch 28: loss 15.776114354903703
Epoch 29: loss 15.763515952620057
Epoch 30: loss 15.750859316047473
Epoch 31: loss 15.73814724968334
Epoch 32: loss 15.725364307011516
Epoch 33: loss 15.71251142926032
Epoch 34: loss 15.699559430573025
Epoch 35: loss 15.686504886226851
Epoch 36: loss 15.673348022842614
Epoch 37: loss 15.660055898466208
Epoch 38: loss 15.646621530894809
Epoch 39: loss 15.633042562131067
Epoch 40: loss 15.619295635192273
Epoch 41: loss 15.605364288176578
Epoch 42: loss 15.59122793385359
Epoch 43: loss 15.576872550773517
Epoch 44: loss 15.562285410832814
Epoch 45: loss 15.547437320914588
Epoch 46: loss 15.532326637110417
Epoch 47: loss 15.516929506672149
Epoch 48: loss 15.50124092216139
Epoch 49: loss 15.48522287247426
-----------Time: 0:05:12.037861, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9355628490448-------------


Epoch 0: loss 16.12302777091727
Epoch 1: loss 16.110640581824846
Epoch 2: loss 16.098255007081605
Epoch 3: loss 16.085876811775844
Epoch 4: loss 16.073500855193313
Epoch 5: loss 16.061122883396866
Epoch 6: loss 16.04875119371606
Epoch 7: loss 16.036381097641115
Epoch 8: loss 16.0240062083026
Epoch 9: loss 16.01163142838512
Epoch 10: loss 15.9992516700699
Epoch 11: loss 15.986861143895243
Epoch 12: loss 15.974465212010378
Epoch 13: loss 15.962051926779319
Epoch 14: loss 15.94962700765505
Epoch 15: loss 15.937175469660383
Epoch 16: loss 15.924700572090087
Epoch 17: loss 15.91217565718003
Epoch 18: loss 15.899613540155723
Epoch 19: loss 15.8869857632511
Epoch 20: loss 15.874283944607559
Epoch 21: loss 15.86147082247379
Epoch 22: loss 15.848553056493929
Epoch 23: loss 15.835497775241173
Epoch 24: loss 15.822271448169602
Epoch 25: loss 15.808860000415365
Epoch 26: loss 15.79522118093917
Epoch 27: loss 15.78132763603837
Epoch 28: loss 15.767141156510338
Epoch 29: loss 15.75262224654778
Epoch 30: loss 15.73775289420397
Epoch 31: loss 15.722477668650193
Epoch 32: loss 15.706765917995302
Epoch 33: loss 15.690573305458786
Epoch 34: loss 15.673872912947804
Epoch 35: loss 15.656620371011108
Epoch 36: loss 15.638776176581246
Epoch 37: loss 15.620310807759637
Epoch 38: loss 15.601194928819035
Epoch 39: loss 15.581389925543416
Epoch 40: loss 15.560865687442384
Epoch 41: loss 15.53960203592903
Epoch 42: loss 15.51756328522608
Epoch 43: loss 15.494737433764907
Epoch 44: loss 15.471087197495129
Epoch 45: loss 15.446598141137642
Epoch 46: loss 15.421245191199679
Epoch 47: loss 15.395020027015441
Epoch 48: loss 15.367919610725142
Epoch 49: loss 15.339909042204898
-----------Time: 0:06:15.699462, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.916328191757202-------------


Epoch 0: loss 16.123026300215603
Epoch 1: loss 16.110648308194417
Epoch 2: loss 16.09826250527461
Epoch 3: loss 16.085871737958815
Epoch 4: loss 16.073495851385005
Epoch 5: loss 16.06110966678804
Epoch 6: loss 16.04872536931497
Epoch 7: loss 16.036328922477086
Epoch 8: loss 16.023927610811167
Epoch 9: loss 16.011508370171818
Epoch 10: loss 15.999069739191865
Epoch 11: loss 15.986597293482454
Epoch 12: loss 15.974075483329404
Epoch 13: loss 15.96149715954613
Epoch 14: loss 15.94882990913329
Epoch 15: loss 15.936046614862123
Epoch 16: loss 15.923114667190294
Epoch 17: loss 15.909999288898113
Epoch 18: loss 15.896650724530286
Epoch 19: loss 15.883005649386183
Epoch 20: loss 15.86901143765307
Epoch 21: loss 15.854585108256586
Epoch 22: loss 15.839681614475966
Epoch 23: loss 15.824216501423688
Epoch 24: loss 15.808109039194626
Epoch 25: loss 15.79128970498297
Epoch 26: loss 15.773673183571844
Epoch 27: loss 15.755213039474944
Epoch 28: loss 15.73581881624256
Epoch 29: loss 15.715441370347454
Epoch 30: loss 15.694017591782405
Epoch 31: loss 15.67150282743121
Epoch 32: loss 15.647848663931635
Epoch 33: loss 15.623027450432915
Epoch 34: loss 15.596982185836717
Epoch 35: loss 15.569703237462004
Epoch 36: loss 15.541156533880685
Epoch 37: loss 15.511330485797693
Epoch 38: loss 15.480206648767998
Epoch 39: loss 15.447777025203072
Epoch 40: loss 15.414034089425
Epoch 41: loss 15.378967446954176
Epoch 42: loss 15.342576616545486
Epoch 43: loss 15.304844609416653
Epoch 44: loss 15.265783303194944
Epoch 45: loss 15.22538599830532
Epoch 46: loss 15.183645124990308
Epoch 47: loss 15.140575553101648
Epoch 48: loss 15.096185981352216
Epoch 49: loss 15.05047769479093
-----------Time: 0:07:58.080961, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.878195285797119-------------


Epoch 0: loss 16.067985891971723
Epoch 1: loss 15.94475313500907
Epoch 2: loss 15.822061257105666
Epoch 3: loss 15.699917262244833
Epoch 4: loss 15.578286934832375
Epoch 5: loss 15.457153305792174
Epoch 6: loss 15.336446293804423
Epoch 7: loss 15.216147722012975
Epoch 8: loss 15.096299080177129
Epoch 9: loss 14.97681858255657
Epoch 10: loss 14.857621327763734
Epoch 11: loss 14.738621253822082
Epoch 12: loss 14.619704559745188
Epoch 13: loss 14.50076284662675
Epoch 14: loss 14.381505559617375
Epoch 15: loss 14.26174614778221
Epoch 16: loss 14.14118040716473
Epoch 17: loss 14.019417941667516
Epoch 18: loss 13.895943204497046
Epoch 19: loss 13.770207691348201
Epoch 20: loss 13.641618440824077
Epoch 21: loss 13.509580621517113
Epoch 22: loss 13.373375790996873
Epoch 23: loss 13.232270901977142
Epoch 24: loss 13.085630384717966
Epoch 25: loss 12.932833557481542
Epoch 26: loss 12.773388011615
Epoch 27: loss 12.606820903568568
Epoch 28: loss 12.432611318176502
Epoch 29: loss 12.250535243098149
Epoch 30: loss 12.060406315644324
Epoch 31: loss 11.862127885927384
Epoch 32: loss 11.655694252126175
Epoch 33: loss 11.441153469262012
Epoch 34: loss 11.218595607440195
Epoch 35: loss 10.988159615815885
Epoch 36: loss 10.750177964754505
Epoch 37: loss 10.504899378118468
Epoch 38: loss 10.252635565836057
Epoch 39: loss 9.993804754285206
Epoch 40: loss 9.72888859774251
Epoch 41: loss 9.458436307341849
Epoch 42: loss 9.18276176017027
Epoch 43: loss 8.902414650162514
Epoch 44: loss 8.618116543175022
Epoch 45: loss 8.330368963514353
Epoch 46: loss 8.039836253725749
Epoch 47: loss 7.747320138092207
Epoch 48: loss 7.4534107286558005
Epoch 49: loss 7.1587853353914195
-----------Time: 0:03:24.130728, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.6550605297088623-------------


Epoch 0: loss 16.067766098823153
Epoch 1: loss 15.944539459066718
Epoch 2: loss 15.821692244782273
Epoch 3: loss 15.699129401762372
Epoch 4: loss 15.57651506447805
Epoch 5: loss 15.453137429399682
Epoch 6: loss 15.32764694624585
Epoch 7: loss 15.197607631071422
Epoch 8: loss 15.05929062248508
Epoch 9: loss 14.907959169509166
Epoch 10: loss 14.73853294793648
Epoch 11: loss 14.546534061691176
Epoch 12: loss 14.328709326469749
Epoch 13: loss 14.083003412841
Epoch 14: loss 13.808651183596638
Epoch 15: loss 13.50553312140875
Epoch 16: loss 13.17428968833023
Epoch 17: loss 12.816075450509317
Epoch 18: loss 12.432251480106688
Epoch 19: loss 12.024744335628839
Epoch 20: loss 11.595291051091936
Epoch 21: loss 11.145798770288465
Epoch 22: loss 10.678542943542713
Epoch 23: loss 10.195928768595643
Epoch 24: loss 9.700853363335813
Epoch 25: loss 9.195805729051333
Epoch 26: loss 8.683588427522379
Epoch 27: loss 8.167228746440113
Epoch 28: loss 7.649831272714355
Epoch 29: loss 7.134420242693322
Epoch 30: loss 6.624617106762835
Epoch 31: loss 6.12357840167238
Epoch 32: loss 5.63482319135391
Epoch 33: loss 5.161748997344473
Epoch 34: loss 4.707739896655018
Epoch 35: loss 4.27625474701633
Epoch 36: loss 3.8704774121735133
Epoch 37: loss 3.4934532797939433
Epoch 38: loss 3.147820413598812
Epoch 39: loss 2.8357841670351096
Epoch 40: loss 2.5589936163323026
Epoch 41: loss 2.317718687428282
Epoch 42: loss 2.1112413472491416
Epoch 43: loss 1.9378117231521481
Epoch 44: loss 1.7943267724383065
Epoch 45: loss 1.67700669226146
Epoch 46: loss 1.58170276559391
Epoch 47: loss 1.5041505148256518
Epoch 48: loss 1.4405833879458378
Epoch 49: loss 1.387787379539162
-----------Time: 0:04:13.647760, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.1783767938613892-------------


Epoch 0: loss 16.067766161053125
Epoch 1: loss 15.9444287934337
Epoch 2: loss 15.820855860479375
Epoch 3: loss 15.694244071022851
Epoch 4: loss 15.555130999525193
Epoch 5: loss 15.383650972637033
Epoch 6: loss 15.156823855758427
Epoch 7: loss 14.860855308416552
Epoch 8: loss 14.49254643158656
Epoch 9: loss 14.054743897986192
Epoch 10: loss 13.552357614526546
Epoch 11: loss 12.991774235943726
Epoch 12: loss 12.379908204921373
Epoch 13: loss 11.72321941672362
Epoch 14: loss 11.029164158176506
Epoch 15: loss 10.305347980397626
Epoch 16: loss 9.56004344749347
Epoch 17: loss 8.801759464446457
Epoch 18: loss 8.039078110129111
Epoch 19: loss 7.281253640712636
Epoch 20: loss 6.537770399909878
Epoch 21: loss 5.818171866760752
Epoch 22: loss 5.132557604739431
Epoch 23: loss 4.490335240319995
Epoch 24: loss 3.9007305659438813
Epoch 25: loss 3.372079531092434
Epoch 26: loss 2.910738923875063
Epoch 27: loss 2.520820029472903
Epoch 28: loss 2.2024540926465006
Epoch 29: loss 1.9512500268989053
Epoch 30: loss 1.7587054250939116
Epoch 31: loss 1.6136783985031111
Epoch 32: loss 1.5044953758266713
Epoch 33: loss 1.4210062012716504
Epoch 34: loss 1.35545491673105
Epoch 35: loss 1.3025236810401057
Epoch 36: loss 1.258817146951054
Epoch 37: loss 1.2221114456880475
Epoch 38: loss 1.190956918618937
Epoch 39: loss 1.164281477611826
Epoch 40: loss 1.1413425669973476
Epoch 41: loss 1.1215576973539645
Epoch 42: loss 1.1044409645517732
Epoch 43: loss 1.0895902998036442
Epoch 44: loss 1.0766872190275292
Epoch 45: loss 1.0654605567746995
Epoch 46: loss 1.0556753920821667
Epoch 47: loss 1.0471362024626698
Epoch 48: loss 1.0396693912843702
Epoch 49: loss 1.0331221757990436
-----------Time: 0:05:40.135674, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0248165130615234-------------


Epoch 0: loss 16.067684307897007
Epoch 1: loss 15.943746281551757
Epoch 2: loss 15.81347292619532
Epoch 3: loss 15.647775465927415
Epoch 4: loss 15.394324346752903
Epoch 5: loss 15.022819784038932
Epoch 6: loss 14.535415082084153
Epoch 7: loss 13.944342642779452
Epoch 8: loss 13.261940188094155
Epoch 9: loss 12.500352398476696
Epoch 10: loss 11.67251484390184
Epoch 11: loss 10.792154418961907
Epoch 12: loss 9.874096130922867
Epoch 13: loss 8.934064510401466
Epoch 14: loss 7.988347470727934
Epoch 15: loss 7.054083811452429
Epoch 16: loss 6.14856311119789
Epoch 17: loss 5.290087052957722
Epoch 18: loss 4.496093596240112
Epoch 19: loss 3.782979743904624
Epoch 20: loss 3.1651945146547185
Epoch 21: loss 2.6519883472417733
Epoch 22: loss 2.2456590993174914
Epoch 23: loss 1.9393151356773315
Epoch 24: loss 1.7175044484472974
Epoch 25: loss 1.5600477871264757
Epoch 26: loss 1.4471130500091296
Epoch 27: loss 1.3634804312854827
Epoch 28: loss 1.2990470912029979
Epoch 29: loss 1.2477735059296327
Epoch 30: loss 1.206048188519387
Epoch 31: loss 1.1716394162554011
Epoch 32: loss 1.1430348662852983
Epoch 33: loss 1.1191103734225927
Epoch 34: loss 1.0990425656190574
Epoch 35: loss 1.0821694039403906
Epoch 36: loss 1.0679269725372766
Epoch 37: loss 1.0558742153832548
Epoch 38: loss 1.0456586771519565
Epoch 39: loss 1.0369752131122945
Epoch 40: loss 1.0295866891506769
Epoch 41: loss 1.0232624509465507
Epoch 42: loss 1.0178586785854238
Epoch 43: loss 1.0132189752486427
Epoch 44: loss 1.009230607777202
Epoch 45: loss 1.005788716237917
Epoch 46: loss 1.0028196936265614
Epoch 47: loss 1.0002436919209747
Epoch 48: loss 0.9980076082767904
Epoch 49: loss 0.9960653528757495
-----------Time: 0:07:00.424861, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0071065425872803-------------


Epoch 0: loss 16.067760793199483
Epoch 1: loss 15.942820248233033
Epoch 2: loss 15.793970939839515
Epoch 3: loss 15.53546030564176
Epoch 4: loss 15.097373172600285
Epoch 5: loss 14.489614687385995
Epoch 6: loss 13.741909112665821
Epoch 7: loss 12.87855199758873
Epoch 8: loss 11.921454858494686
Epoch 9: loss 10.89245034405043
Epoch 10: loss 9.814426429897887
Epoch 11: loss 8.712571613680998
Epoch 12: loss 7.612019158239401
Epoch 13: loss 6.539619477693642
Epoch 14: loss 5.523018972843869
Epoch 15: loss 4.588749451245738
Epoch 16: loss 3.7623240388950103
Epoch 17: loss 3.064153745681323
Epoch 18: loss 2.507024425731657
Epoch 19: loss 2.08954049362444
Epoch 20: loss 1.7945380619908884
Epoch 21: loss 1.593804625080227
Epoch 22: loss 1.4570307986382365
Epoch 23: loss 1.3603613549046312
Epoch 24: loss 1.28850466625531
Epoch 25: loss 1.2328520737438502
Epoch 26: loss 1.1886441914396093
Epoch 27: loss 1.1530022463311054
Epoch 28: loss 1.1240371161663643
Epoch 29: loss 1.1003931001711955
Epoch 30: loss 1.0810059636037204
Epoch 31: loss 1.0650696262449852
Epoch 32: loss 1.0519229543150734
Epoch 33: loss 1.041048118995544
Epoch 34: loss 1.0320137834471164
Epoch 35: loss 1.0244800716136189
Epoch 36: loss 1.0181838510605614
Epoch 37: loss 1.01290873381805
Epoch 38: loss 1.008474359699019
Epoch 39: loss 1.004734673760649
Epoch 40: loss 1.001572595885942
Epoch 41: loss 0.9988857393293292
Epoch 42: loss 0.9966017175070808
Epoch 43: loss 0.9946465524659979
Epoch 44: loss 0.9929767803953937
Epoch 45: loss 0.991542485079213
Epoch 46: loss 0.9903154967270706
Epoch 47: loss 0.9892592747098664
Epoch 48: loss 0.9883434855464751
Epoch 49: loss 0.9875484867352646
-----------Time: 0:06:19.995512, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0028858184814453-------------


Epoch 0: loss 15.52519826645304
Epoch 1: loss 14.337325857410358
Epoch 2: loss 12.995739229481268
Epoch 3: loss 10.855092954609692
Epoch 4: loss 7.8207834294595555
Epoch 5: loss 4.799548995177211
Epoch 6: loss 2.645009658140874
Epoch 7: loss 1.6103195896480575
Epoch 8: loss 1.2611977442766804
Epoch 9: loss 1.1309008894244117
Epoch 10: loss 1.0666978244810017
Epoch 11: loss 1.0320994748312082
Epoch 12: loss 1.0126771002505512
Epoch 13: loss 1.0014464662021887
Epoch 14: loss 0.9947491192895476
Epoch 15: loss 0.9905924803864509
Epoch 16: loss 0.987921742851025
Epoch 17: loss 0.9861582467062049
Epoch 18: loss 0.9849145805271765
Epoch 19: loss 0.9840102165597103
Epoch 20: loss 0.9833387771723645
Epoch 21: loss 0.9827670498086163
Epoch 22: loss 0.9823061797915755
Epoch 23: loss 0.9818971308759531
Epoch 24: loss 0.9815162782956881
Epoch 25: loss 0.9811499732029445
Epoch 26: loss 0.9808100307681368
Epoch 27: loss 0.9804711601149458
Epoch 28: loss 0.9801201303735124
Epoch 29: loss 0.9797568532874754
Epoch 30: loss 0.9793946687263791
Epoch 31: loss 0.9790415504239317
Epoch 32: loss 0.9786562066547504
Epoch 33: loss 0.978257813617028
Epoch 34: loss 0.9778433276817421
Epoch 35: loss 0.9774239410065386
Epoch 36: loss 0.9769625014231994
Epoch 37: loss 0.9764877936968407
Epoch 38: loss 0.9759996605671379
Epoch 39: loss 0.9754718428334832
Epoch 40: loss 0.9749364225419725
Epoch 41: loss 0.9743542137387137
Epoch 42: loss 0.9737678928307828
Epoch 43: loss 0.9731333060326818
Epoch 44: loss 0.9724496852256086
Epoch 45: loss 0.9717623067329991
Epoch 46: loss 0.9710300792294264
Epoch 47: loss 0.9702652179721648
Epoch 48: loss 0.9694629958818113
Epoch 49: loss 0.9686131292934843
-----------Time: 0:03:58.095158, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9981243014335632-------------


Epoch 0: loss 15.518486262016026
Epoch 1: loss 13.235033010386331
Epoch 2: loss 7.6283215047744
Epoch 3: loss 3.0643805955790384
Epoch 4: loss 1.4817583955066758
Epoch 5: loss 1.1572751993067307
Epoch 6: loss 1.0580146384991143
Epoch 7: loss 1.0190177303153967
Epoch 8: loss 1.0025273929995775
Epoch 9: loss 0.9950424716354648
Epoch 10: loss 0.9914437914555847
Epoch 11: loss 0.9895613909804867
Epoch 12: loss 0.9884695783135942
Epoch 13: loss 0.9878092094437463
Epoch 14: loss 0.9873369946759832
Epoch 15: loss 0.987011586440005
Epoch 16: loss 0.9867110471481729
Epoch 17: loss 0.9864577551196618
Epoch 18: loss 0.9861927125167432
Epoch 19: loss 0.9859460376409683
Epoch 20: loss 0.98574816921342
Epoch 21: loss 0.985510863967406
Epoch 22: loss 0.9852387904148506
Epoch 23: loss 0.9849725723136956
Epoch 24: loss 0.9846965977846117
Epoch 25: loss 0.9843673902468035
Epoch 26: loss 0.9840535083952775
Epoch 27: loss 0.9837214612908957
Epoch 28: loss 0.9833277212076047
Epoch 29: loss 0.9829629557556663
Epoch 30: loss 0.9825082802785486
Epoch 31: loss 0.9820720903117609
Epoch 32: loss 0.9815714101288356
Epoch 33: loss 0.9810164582152416
Epoch 34: loss 0.9804298387330923
Epoch 35: loss 0.9798156257684364
Epoch 36: loss 0.9791639951210167
Epoch 37: loss 0.9784308728995434
Epoch 38: loss 0.9776651590268466
Epoch 39: loss 0.9767898051488523
Epoch 40: loss 0.97588876212272
Epoch 41: loss 0.9748698619800524
Epoch 42: loss 0.9738187209428038
Epoch 43: loss 0.9726452718551163
Epoch 44: loss 0.9713686177625547
Epoch 45: loss 0.9700497520430182
Epoch 46: loss 0.9685837982658979
Epoch 47: loss 0.9670441072877819
Epoch 48: loss 0.9653430752549371
Epoch 49: loss 0.9636096952401016
-----------Time: 0:04:55.130778, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9964811205863953-------------


Epoch 0: loss 15.201968150011803
Epoch 1: loss 8.169253555700273
Epoch 2: loss 2.2508296116076454
Epoch 3: loss 1.2129143727674894
Epoch 4: loss 1.0564723285467097
Epoch 5: loss 1.0133114904408353
Epoch 6: loss 0.9995421026633317
Epoch 7: loss 0.9946498199608612
Epoch 8: loss 0.9926709675996312
Epoch 9: loss 0.9916574522617915
Epoch 10: loss 0.9911779662459489
Epoch 11: loss 0.9908645271023309
Epoch 12: loss 0.9906480783609802
Epoch 13: loss 0.9903997514415137
Epoch 14: loss 0.9902268840347443
Epoch 15: loss 0.9900300856710064
Epoch 16: loss 0.9898616904924589
Epoch 17: loss 0.9896062937944463
Epoch 18: loss 0.9892862066099862
Epoch 19: loss 0.9891307225299958
Epoch 20: loss 0.9889029655656197
Epoch 21: loss 0.9885158832466039
Epoch 22: loss 0.9882385377458673
Epoch 23: loss 0.9878588529499152
Epoch 24: loss 0.9874499547799169
Epoch 25: loss 0.9870641325854166
Epoch 26: loss 0.9865798546672839
Epoch 27: loss 0.986047954485428
Epoch 28: loss 0.9854096124974718
Epoch 29: loss 0.9847570679769884
Epoch 30: loss 0.9840126399309367
Epoch 31: loss 0.9831289803612291
Epoch 32: loss 0.9822088910743813
Epoch 33: loss 0.9811568816697358
Epoch 34: loss 0.979980275999145
Epoch 35: loss 0.9786321763891186
Epoch 36: loss 0.9772593253588404
Epoch 37: loss 0.9755911979745302
Epoch 38: loss 0.9738591067359265
Epoch 39: loss 0.971841226880352
Epoch 40: loss 0.9697032402623535
Epoch 41: loss 0.9673119489105582
Epoch 42: loss 0.9646917572483024
Epoch 43: loss 0.9619265531248992
Epoch 44: loss 0.9589064476475759
Epoch 45: loss 0.9556383580990366
Epoch 46: loss 0.9522066717565286
Epoch 47: loss 0.9485125194884565
Epoch 48: loss 0.9446244138359311
Epoch 49: loss 0.9404272954058167
-----------Time: 0:06:14.878440, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9923839569091797-------------


Epoch 0: loss 14.295951800737905
Epoch 1: loss 4.738196198612274
Epoch 2: loss 1.3644439870083442
Epoch 3: loss 1.0703025968778774
Epoch 4: loss 1.0143882905484023
Epoch 5: loss 1.0002130749045408
Epoch 6: loss 0.9960573496152163
Epoch 7: loss 0.9945272549200862
Epoch 8: loss 0.9939105094872849
Epoch 9: loss 0.9935370918004781
Epoch 10: loss 0.9933208776531561
Epoch 11: loss 0.9930846762436768
Epoch 12: loss 0.9929440280158213
Epoch 13: loss 0.9927215910333859
Epoch 14: loss 0.9925617531241769
Epoch 15: loss 0.9922453853797498
Epoch 16: loss 0.9921349568688526
Epoch 17: loss 0.9918872533186809
Epoch 18: loss 0.9916562306420709
Epoch 19: loss 0.9913240653331543
Epoch 20: loss 0.9909329389410865
Epoch 21: loss 0.990592166222328
Epoch 22: loss 0.990183402138472
Epoch 23: loss 0.9896803357926577
Epoch 24: loss 0.9891486000664665
Epoch 25: loss 0.9885299153921719
Epoch 26: loss 0.9878364078955783
Epoch 27: loss 0.9870099744244462
Epoch 28: loss 0.9861082462528077
Epoch 29: loss 0.9849850679947799
Epoch 30: loss 0.9838003517817777
Epoch 31: loss 0.9823602916690044
Epoch 32: loss 0.9807768796305737
Epoch 33: loss 0.978981396891359
Epoch 34: loss 0.9768719208700751
Epoch 35: loss 0.9745118094890775
Epoch 36: loss 0.9719450436188126
Epoch 37: loss 0.9689702844088204
Epoch 38: loss 0.965734490108853
Epoch 39: loss 0.9622043008516508
Epoch 40: loss 0.9583166919369098
Epoch 41: loss 0.9541665093609664
Epoch 42: loss 0.9496997013047962
Epoch 43: loss 0.9449381774505109
Epoch 44: loss 0.9396975735013028
Epoch 45: loss 0.9342201155706098
Epoch 46: loss 0.9284000657640636
Epoch 47: loss 0.922177560566689
Epoch 48: loss 0.9155205311886703
Epoch 49: loss 0.9084763264707926
-----------Time: 0:05:23.473054, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9888088703155518-------------


Epoch 0: loss 13.310923976182549
Epoch 1: loss 3.1141616307632503
Epoch 2: loss 1.1771478525577128
Epoch 3: loss 1.0320298299314925
Epoch 4: loss 1.0049901631553904
Epoch 5: loss 0.998467025742575
Epoch 6: loss 0.9964670290954739
Epoch 7: loss 0.9958131285774248
Epoch 8: loss 0.9955210498715951
Epoch 9: loss 0.9952739356262909
Epoch 10: loss 0.9951049359743203
Epoch 11: loss 0.9948454455489241
Epoch 12: loss 0.994691406727615
Epoch 13: loss 0.9945741650719822
Epoch 14: loss 0.9942614004105054
Epoch 15: loss 0.9939465791781095
Epoch 16: loss 0.9937449913753511
Epoch 17: loss 0.9933997293475402
Epoch 18: loss 0.9931234299585572
Epoch 19: loss 0.9925571477821562
Epoch 20: loss 0.9922352388152744
Epoch 21: loss 0.991754437413405
Epoch 22: loss 0.9910017868109926
Epoch 23: loss 0.9903548812334664
Epoch 24: loss 0.9894128172795627
Epoch 25: loss 0.9884176865228173
Epoch 26: loss 0.9871740841619212
Epoch 27: loss 0.9857825612555126
Epoch 28: loss 0.9841704626262285
Epoch 29: loss 0.9823099775376561
Epoch 30: loss 0.9800865017181508
Epoch 31: loss 0.9775468934288876
Epoch 32: loss 0.9746618328242279
Epoch 33: loss 0.9714044639764757
Epoch 34: loss 0.9678563271092616
Epoch 35: loss 0.9639274683771346
Epoch 36: loss 0.9596669683967225
Epoch 37: loss 0.9550381717700035
Epoch 38: loss 0.9501120987114795
Epoch 39: loss 0.9447515455194632
Epoch 40: loss 0.9391114462193358
Epoch 41: loss 0.933058478963187
Epoch 42: loss 0.926526284140047
Epoch 43: loss 0.9195143019070503
Epoch 44: loss 0.9118793516006075
Epoch 45: loss 0.903672830315632
Epoch 46: loss 0.8949191712892334
Epoch 47: loss 0.8853241750700049
Epoch 48: loss 0.8748837037213539
Epoch 49: loss 0.8637493117274377
-----------Time: 0:07:04.512143, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9861860275268555-------------


Epoch 0: loss 7.205262795325658
Epoch 1: loss 1.0328562671299406
Epoch 2: loss 1.009232714034864
Epoch 3: loss 0.9994623513893306
Epoch 4: loss 0.9865070006457147
Epoch 5: loss 0.9735826292784724
Epoch 6: loss 0.9599812655360754
Epoch 7: loss 0.9457703504697209
Epoch 8: loss 0.9304485506762493
Epoch 9: loss 0.9141659130215191
Epoch 10: loss 0.8979483517438838
Epoch 11: loss 0.8811948391780573
Epoch 12: loss 0.8643631406813098
Epoch 13: loss 0.8479868480859728
Epoch 14: loss 0.8319477485599176
Epoch 15: loss 0.8170963460300978
Epoch 16: loss 0.8033179159460021
Epoch 17: loss 0.7909539478702867
Epoch 18: loss 0.78012460748938
Epoch 19: loss 0.7706751992613029
Epoch 20: loss 0.7626399174859305
Epoch 21: loss 0.7556333141264674
Epoch 22: loss 0.7495661743831997
Epoch 23: loss 0.7444774276410321
Epoch 24: loss 0.7398279474247014
Epoch 25: loss 0.7357336649951759
Epoch 26: loss 0.7320909689404901
Epoch 27: loss 0.7287213791317756
Epoch 28: loss 0.7258539166769429
Epoch 29: loss 0.7230223849326648
Epoch 30: loss 0.7203604117108792
Epoch 31: loss 0.7180991621040792
Epoch 32: loss 0.7157302297218264
Epoch 33: loss 0.7136233865469289
Epoch 34: loss 0.7115779431246103
Epoch 35: loss 0.7097851487201734
Epoch 36: loss 0.7081348831527838
Epoch 37: loss 0.7064225471428129
Epoch 38: loss 0.7047470915635168
Epoch 39: loss 0.703289287643889
Epoch 40: loss 0.7018715194413816
Epoch 41: loss 0.7003401745072262
Epoch 42: loss 0.6991312136917156
Epoch 43: loss 0.6979090180197379
Epoch 44: loss 0.6965147253285937
Epoch 45: loss 0.6954877821241403
Epoch 46: loss 0.6942221641281235
Epoch 47: loss 0.6932488350585597
Epoch 48: loss 0.6920610431940287
Epoch 49: loss 0.6911237288778926
-----------Time: 0:04:39.256464, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1214278936386108-------------


Epoch 0: loss 4.753624388418358
Epoch 1: loss 1.034158732885897
Epoch 2: loss 1.0263297933155326
Epoch 3: loss 1.0122782388998803
Epoch 4: loss 0.9942004995996891
Epoch 5: loss 0.9711027889552487
Epoch 6: loss 0.9407629899968266
Epoch 7: loss 0.9019731158727923
Epoch 8: loss 0.8565073797986713
Epoch 9: loss 0.8058231135241812
Epoch 10: loss 0.75324604296049
Epoch 11: loss 0.7020532023394607
Epoch 12: loss 0.6560384098312788
Epoch 13: loss 0.6162552991238802
Epoch 14: loss 0.5834662337838599
Epoch 15: loss 0.5569314493667568
Epoch 16: loss 0.5357070669426485
Epoch 17: loss 0.5184433117484579
Epoch 18: loss 0.5038215136190937
Epoch 19: loss 0.49168982460098204
Epoch 20: loss 0.4810150307666484
Epoch 21: loss 0.4721476075732235
Epoch 22: loss 0.4640784748161922
Epoch 23: loss 0.45717084683303666
Epoch 24: loss 0.4506894803163861
Epoch 25: loss 0.4450123903704701
Epoch 26: loss 0.4395886689116606
Epoch 27: loss 0.4350724361589773
Epoch 28: loss 0.43050336129578515
Epoch 29: loss 0.4265654549253836
Epoch 30: loss 0.4227568674113454
Epoch 31: loss 0.41929933386629464
Epoch 32: loss 0.4161028465931671
Epoch 33: loss 0.4130231796755228
Epoch 34: loss 0.41010848982558423
Epoch 35: loss 0.4072727575582159
Epoch 36: loss 0.40474187138289325
Epoch 37: loss 0.4022110493174061
Epoch 38: loss 0.40007702516044463
Epoch 39: loss 0.39794729286202146
Epoch 40: loss 0.39577380127398587
Epoch 41: loss 0.39378589809815995
Epoch 42: loss 0.3919357409809127
Epoch 43: loss 0.3902062593172788
Epoch 44: loss 0.3884447717258242
Epoch 45: loss 0.38668088301359926
Epoch 46: loss 0.38496870737545125
Epoch 47: loss 0.38361856709231884
Epoch 48: loss 0.38215236690719856
Epoch 49: loss 0.3806200808581611
-----------Time: 0:05:02.625365, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3773229122161865-------------


Epoch 0: loss 3.67001076079115
Epoch 1: loss 1.052799130925152
Epoch 2: loss 1.038139002585035
Epoch 3: loss 1.0174696436714
Epoch 4: loss 0.987920459681973
Epoch 5: loss 0.9376063740337718
Epoch 6: loss 0.8629717462214521
Epoch 7: loss 0.7663132477999381
Epoch 8: loss 0.65938642489774
Epoch 9: loss 0.5588828042319703
Epoch 10: loss 0.4738723113687224
Epoch 11: loss 0.4064882424229056
Epoch 12: loss 0.35547048333159215
Epoch 13: loss 0.3166993232023593
Epoch 14: loss 0.2869780111355001
Epoch 15: loss 0.26351840096430135
Epoch 16: loss 0.24473063206951406
Epoch 17: loss 0.2292672531429097
Epoch 18: loss 0.21632886533344356
Epoch 19: loss 0.20510665601592404
Epoch 20: loss 0.19569212917791495
Epoch 21: loss 0.18727452554266502
Epoch 22: loss 0.1798828821051957
Epoch 23: loss 0.17335831380412267
Epoch 24: loss 0.16757509502510845
Epoch 25: loss 0.1623709102512184
Epoch 26: loss 0.15757294132892305
Epoch 27: loss 0.15316338944104524
Epoch 28: loss 0.14923733166941097
Epoch 29: loss 0.14573605566033856
Epoch 30: loss 0.14222890920957318
Epoch 31: loss 0.13921111828610586
Epoch 32: loss 0.1362921493882521
Epoch 33: loss 0.1337009260168602
Epoch 34: loss 0.13110708155633316
Epoch 35: loss 0.12873935151709495
Epoch 36: loss 0.12657663403985422
Epoch 37: loss 0.12446622547246373
Epoch 38: loss 0.12249548106478116
Epoch 39: loss 0.12053494411052214
Epoch 40: loss 0.11897686724269954
Epoch 41: loss 0.11719804082132669
Epoch 42: loss 0.11559163350378968
Epoch 43: loss 0.11421219053282694
Epoch 44: loss 0.11267481087957665
Epoch 45: loss 0.11133845386669129
Epoch 46: loss 0.11005863809452684
Epoch 47: loss 0.10879521430462454
Epoch 48: loss 0.10757030763416721
Epoch 49: loss 0.10640425640986366
-----------Time: 0:04:52.215626, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5910346508026123-------------


Epoch 0: loss 3.2702086198388267
Epoch 1: loss 1.0614704935041182
Epoch 2: loss 1.0363898592646061
Epoch 3: loss 1.0003884243472494
Epoch 4: loss 0.9408097017258649
Epoch 5: loss 0.8473873903013689
Epoch 6: loss 0.7194261657212855
Epoch 7: loss 0.5753974536471291
Epoch 8: loss 0.44425667277038455
Epoch 9: loss 0.34138187407928683
Epoch 10: loss 0.2663631017779189
Epoch 11: loss 0.21385431948435443
Epoch 12: loss 0.17656211880739128
Epoch 13: loss 0.14962491305983538
Epoch 14: loss 0.1294750502376598
Epoch 15: loss 0.1141720786690712
Epoch 16: loss 0.10180266674905772
Epoch 17: loss 0.09219638834737318
Epoch 18: loss 0.08413558267968514
Epoch 19: loss 0.07742297384622565
Epoch 20: loss 0.07192959062767262
Epoch 21: loss 0.06708519793378392
Epoch 22: loss 0.06301276760946609
Epoch 23: loss 0.05954845686802208
Epoch 24: loss 0.05640937902880078
Epoch 25: loss 0.05351293371821176
Epoch 26: loss 0.05118910408598564
Epoch 27: loss 0.049077270020765736
Epoch 28: loss 0.04704425676891407
Epoch 29: loss 0.04542265177409146
Epoch 30: loss 0.04377649849267548
Epoch 31: loss 0.042351559846742506
Epoch 32: loss 0.0410974120671591
Epoch 33: loss 0.039882535979864776
Epoch 34: loss 0.038862903236273255
Epoch 35: loss 0.03776444889738743
Epoch 36: loss 0.03682534195089742
Epoch 37: loss 0.035998582971637307
Epoch 38: loss 0.03521183041157834
Epoch 39: loss 0.03437791450547744
Epoch 40: loss 0.033772626828935046
Epoch 41: loss 0.03300695939778536
Epoch 42: loss 0.032446899899504895
Epoch 43: loss 0.031859345050440524
Epoch 44: loss 0.03134049392746414
Epoch 45: loss 0.030788727394341128
Epoch 46: loss 0.03025365876598064
Epoch 47: loss 0.029836317159685186
Epoch 48: loss 0.029431263087170482
Epoch 49: loss 0.02899663965863466
-----------Time: 0:06:01.457069, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.468568205833435-------------


Epoch 0: loss 3.0383181349617945
Epoch 1: loss 1.0708726577424303
Epoch 2: loss 1.0346514390109998
Epoch 3: loss 0.9746248065420569
Epoch 4: loss 0.873921305313909
Epoch 5: loss 0.7287095583865927
Epoch 6: loss 0.5600340334600311
Epoch 7: loss 0.40403467877005805
Epoch 8: loss 0.2837230596385487
Epoch 9: loss 0.20134178858985713
Epoch 10: loss 0.14742071423000977
Epoch 11: loss 0.11264147931155853
Epoch 12: loss 0.08990140552792036
Epoch 13: loss 0.07403037476881552
Epoch 14: loss 0.06284065531438106
Epoch 15: loss 0.05503868031689756
Epoch 16: loss 0.04915095785305577
Epoch 17: loss 0.04469721576947437
Epoch 18: loss 0.04112016901580486
Epoch 19: loss 0.038755288679819186
Epoch 20: loss 0.03637378839163141
Epoch 21: loss 0.034753614748955226
Epoch 22: loss 0.03350500364713432
Epoch 23: loss 0.03220524184118152
Epoch 24: loss 0.031117398617966398
Epoch 25: loss 0.030483228368699843
Epoch 26: loss 0.029738929775979545
Epoch 27: loss 0.02892345686348448
Epoch 28: loss 0.02840133692183982
Epoch 29: loss 0.027922490885143956
Epoch 30: loss 0.02746320032737967
Epoch 31: loss 0.027033595015791657
Epoch 32: loss 0.026590957229854686
Epoch 33: loss 0.026186816461177674
Epoch 34: loss 0.025846671933929127
Epoch 35: loss 0.025586453438121187
Epoch 36: loss 0.02518432793805559
Epoch 37: loss 0.02485638538141527
Epoch 38: loss 0.024663531279688688
Epoch 39: loss 0.02438018884147199
Epoch 40: loss 0.024137304664314473
Epoch 41: loss 0.023829718558457055
Epoch 42: loss 0.023684551434965156
Epoch 43: loss 0.023485079925262196
Epoch 44: loss 0.023153063687716184
Epoch 45: loss 0.022972739957447218
Epoch 46: loss 0.022828703667196713
Epoch 47: loss 0.02260486881059755
Epoch 48: loss 0.02241196535535809
Epoch 49: loss 0.022268710802792854
-----------Time: 0:07:40.020882, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2636840343475342-------------


Epoch 0: loss 16.129426563609353
Epoch 1: loss 16.129428776922012
Epoch 2: loss 16.12942305902478
Epoch 3: loss 16.1294275561774
Epoch 4: loss 16.129423000943472
Epoch 5: loss 16.129423278904014
Epoch 6: loss 16.129421262134347
Epoch 7: loss 16.129423473891258
Epoch 8: loss 16.129417086503246
Epoch 9: loss 16.1294176585004
Epoch 10: loss 16.129419111051657
Epoch 11: loss 16.12942117397522
Epoch 12: loss 16.129412943542878
Epoch 13: loss 16.129412152185072
Epoch 14: loss 16.129412762038793
Epoch 15: loss 16.12940817568988
Epoch 16: loss 16.129410106374753
Epoch 17: loss 16.129411151838276
Epoch 18: loss 16.129407665922695
Epoch 19: loss 16.12940701043366
Epoch 20: loss 16.12940311846751
Epoch 21: loss 16.129404851053643
Epoch 22: loss 16.12940748908586
Epoch 23: loss 16.129400356493935
Epoch 24: loss 16.129397385012787
Epoch 25: loss 16.12940081751431
Epoch 26: loss 16.12939457014373
Epoch 27: loss 16.129396695297267
Epoch 28: loss 16.12939367610647
Epoch 29: loss 16.129394206616983
Epoch 30: loss 16.129390331245492
Epoch 31: loss 16.12939206020154
Epoch 32: loss 16.129391386043512
Epoch 33: loss 16.12938634645297
Epoch 34: loss 16.129387769963575
Epoch 35: loss 16.129382909802782
Epoch 36: loss 16.129388421303943
Epoch 37: loss 16.12938186589501
Epoch 38: loss 16.129383971860968
Epoch 39: loss 16.129377025440377
Epoch 40: loss 16.12937978378387
Epoch 41: loss 16.129381705652833
Epoch 42: loss 16.12937475612074
Epoch 43: loss 16.129372702531676
Epoch 44: loss 16.129383489578686
Epoch 45: loss 16.12936924617533
Epoch 46: loss 16.129374172714755
Epoch 47: loss 16.12937435421884
Epoch 48: loss 16.12936836406548
Epoch 49: loss 16.12936752655378
-----------Time: 0:04:14.753114, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017201900482178-------------


Epoch 0: loss 16.129199294048266
Epoch 1: loss 16.12919699413223
Epoch 2: loss 16.129194865867195
Epoch 3: loss 16.129195617812687
Epoch 4: loss 16.129190757133316
Epoch 5: loss 16.129190988421374
Epoch 6: loss 16.129190264997955
Epoch 7: loss 16.129189121522224
Epoch 8: loss 16.12918333309769
Epoch 9: loss 16.129185630939393
Epoch 10: loss 16.129182985647013
Epoch 11: loss 16.129183444593057
Epoch 12: loss 16.12918314744494
Epoch 13: loss 16.129178947959016
Epoch 14: loss 16.129180662394738
Epoch 15: loss 16.12917871563379
Epoch 16: loss 16.129179088495036
Epoch 17: loss 16.129177889012333
Epoch 18: loss 16.129175726520813
Epoch 19: loss 16.129174921679848
Epoch 20: loss 16.12917324613786
Epoch 21: loss 16.129170949851904
Epoch 22: loss 16.12917314449557
Epoch 23: loss 16.129167660997897
Epoch 24: loss 16.129168698682676
Epoch 25: loss 16.12916118908084
Epoch 26: loss 16.129173401712787
Epoch 27: loss 16.129163541892353
Epoch 28: loss 16.12916235744856
Epoch 29: loss 16.129159944481405
Epoch 30: loss 16.12916337905726
Epoch 31: loss 16.12916004716086
Epoch 32: loss 16.129159378707246
Epoch 33: loss 16.129154757613264
Epoch 34: loss 16.129156211201686
Epoch 35: loss 16.12915308103411
Epoch 36: loss 16.129148658038872
Epoch 37: loss 16.12915097299382
Epoch 38: loss 16.129154362452944
Epoch 39: loss 16.12915106841311
Epoch 40: loss 16.12914614498518
Epoch 41: loss 16.129143289666644
Epoch 42: loss 16.12914802847899
Epoch 43: loss 16.12914473651349
Epoch 44: loss 16.129143553106857
Epoch 45: loss 16.12914534273713
Epoch 46: loss 16.129140677563583
Epoch 47: loss 16.129141536337194
Epoch 48: loss 16.129134168308543
Epoch 49: loss 16.12914286183559
-----------Time: 0:03:54.928393, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017186164855957-------------


Epoch 0: loss 16.129177804483287
Epoch 1: loss 16.12916793221686
Epoch 2: loss 16.129170900067926
Epoch 3: loss 16.129169651319827
Epoch 4: loss 16.129167498162804
Epoch 5: loss 16.129170285028373
Epoch 6: loss 16.12916695624347
Epoch 7: loss 16.129161874129114
Epoch 8: loss 16.12916323489116
Epoch 9: loss 16.129159553988334
Epoch 10: loss 16.129157247330717
Epoch 11: loss 16.129158611722847
Epoch 12: loss 16.129157575075233
Epoch 13: loss 16.129155639723113
Epoch 14: loss 16.129153070143865
Epoch 15: loss 16.129153017248388
Epoch 16: loss 16.129153178527734
Epoch 17: loss 16.129151630557185
Epoch 18: loss 16.129156994262164
Epoch 19: loss 16.129144068059876
Epoch 20: loss 16.12914049968958
Epoch 21: loss 16.129146226402728
Epoch 22: loss 16.12914630885744
Epoch 23: loss 16.12914409243328
Epoch 24: loss 16.1291431314988
Epoch 25: loss 16.129148989932055
Epoch 26: loss 16.129141695542202
Epoch 27: loss 16.1291385539658
Epoch 28: loss 16.12914006563553
Epoch 29: loss 16.129139068918814
Epoch 30: loss 16.129138882747483
Epoch 31: loss 16.129133287754442
Epoch 32: loss 16.129131123707175
Epoch 33: loss 16.129132116275226
Epoch 34: loss 16.129131267873277
Epoch 35: loss 16.12913386130735
Epoch 36: loss 16.12912698696981
Epoch 37: loss 16.12912796035028
Epoch 38: loss 16.129126132863444
Epoch 39: loss 16.129120221016134
Epoch 40: loss 16.12912349327548
Epoch 41: loss 16.12912780788685
Epoch 42: loss 16.12912682517188
Epoch 43: loss 16.129122930094233
Epoch 44: loss 16.129119426028247
Epoch 45: loss 16.129117377106425
Epoch 46: loss 16.12911612680258
Epoch 47: loss 16.12912039577864
Epoch 48: loss 16.129112402857356
Epoch 49: loss 16.129110384013355
-----------Time: 0:05:24.042689, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129173838359755
Epoch 1: loss 16.12917595210446
Epoch 2: loss 16.129173886069402
Epoch 3: loss 16.129176582182925
Epoch 4: loss 16.12917266376904
Epoch 5: loss 16.129172959361405
Epoch 6: loss 16.129171627640012
Epoch 7: loss 16.129165444055158
Epoch 8: loss 16.129165731868774
Epoch 9: loss 16.129169589608438
Epoch 10: loss 16.12916628727127
Epoch 11: loss 16.12916118596934
Epoch 12: loss 16.12916241812278
Epoch 13: loss 16.129160875338066
Epoch 14: loss 16.129160414317692
Epoch 15: loss 16.129155858046598
Epoch 16: loss 16.129159145863436
Epoch 17: loss 16.129154412755504
Epoch 18: loss 16.129150854756872
Epoch 19: loss 16.12915285337613
Epoch 20: loss 16.12915392684314
Epoch 21: loss 16.129150732889844
Epoch 22: loss 16.12915180168961
Epoch 23: loss 16.129150091921137
Epoch 24: loss 16.129150475672628
Epoch 25: loss 16.12914450729976
Epoch 26: loss 16.12914563729233
Epoch 27: loss 16.129143880332794
Epoch 28: loss 16.12914470332417
Epoch 29: loss 16.129140027778963
Epoch 30: loss 16.129143135647464
Epoch 31: loss 16.129135448171628
Epoch 32: loss 16.12913573183658
Epoch 33: loss 16.129137915590004
Epoch 34: loss 16.129133218264307
Epoch 35: loss 16.12913095827917
Epoch 36: loss 16.129133974358464
Epoch 37: loss 16.129128615839317
Epoch 38: loss 16.129127854040746
Epoch 39: loss 16.129134013252195
Epoch 40: loss 16.129127184031386
Epoch 41: loss 16.12912515948297
Epoch 42: loss 16.129127306416997
Epoch 43: loss 16.12912352853913
Epoch 44: loss 16.129125875127645
Epoch 45: loss 16.129119585751837
Epoch 46: loss 16.129119415138
Epoch 47: loss 16.129117406665664
Epoch 48: loss 16.12911654011331
Epoch 49: loss 16.129118798024113
-----------Time: 0:06:45.450235, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.0171799659729-------------


Epoch 0: loss 16.1291691519243
Epoch 1: loss 16.12917104164111
Epoch 2: loss 16.129168167653585
Epoch 3: loss 16.1291695693837
Epoch 4: loss 16.12916658286364
Epoch 5: loss 16.129172314762616
Epoch 6: loss 16.12916352633486
Epoch 7: loss 16.129163637830228
Epoch 8: loss 16.129162450793515
Epoch 9: loss 16.129163667908045
Epoch 10: loss 16.129165649932645
Epoch 11: loss 16.129159527540597
Epoch 12: loss 16.129154795988413
Epoch 13: loss 16.12915792304449
Epoch 14: loss 16.129156841798732
Epoch 15: loss 16.12915533064759
Epoch 16: loss 16.129152318716958
Epoch 17: loss 16.129150880167444
Epoch 18: loss 16.12915311577918
Epoch 19: loss 16.129151021222047
Epoch 20: loss 16.129146257517714
Epoch 21: loss 16.12914550194214
Epoch 22: loss 16.129142537202572
Epoch 23: loss 16.129147140664728
Epoch 24: loss 16.129141665982967
Epoch 25: loss 16.129142161229826
Epoch 26: loss 16.129137349815846
Epoch 27: loss 16.1291392441999
Epoch 28: loss 16.129138571597622
Epoch 29: loss 16.12913419475628
Epoch 30: loss 16.129134235724344
Epoch 31: loss 16.129130763291922
Epoch 32: loss 16.129136864940648
Epoch 33: loss 16.129129858883
Epoch 34: loss 16.129127618085437
Epoch 35: loss 16.129129045744705
Epoch 36: loss 16.129124697944015
Epoch 37: loss 16.129122742885738
Epoch 38: loss 16.12912616086693
Epoch 39: loss 16.12911758142817
Epoch 40: loss 16.12912444435688
Epoch 41: loss 16.129127008231713
Epoch 42: loss 16.129116973648777
Epoch 43: loss 16.129119571231513
Epoch 44: loss 16.12911726405531
Epoch 45: loss 16.129118182465977
Epoch 46: loss 16.129117808567564
Epoch 47: loss 16.12911358522682
Epoch 48: loss 16.12911841064254
Epoch 49: loss 16.12911231469823
-----------Time: 0:06:56.290966, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017181396484375-------------


Epoch 0: loss 16.12926664865814
Epoch 1: loss 16.12925174821
Epoch 2: loss 16.129237078012753
Epoch 3: loss 16.129232231335124
Epoch 4: loss 16.129212894927164
Epoch 5: loss 16.129200683850968
Epoch 6: loss 16.129188722731822
Epoch 7: loss 16.129176195319936
Epoch 8: loss 16.129164144485912
Epoch 9: loss 16.129154560033104
Epoch 10: loss 16.129142348956908
Epoch 11: loss 16.129129469945678
Epoch 12: loss 16.129116304676582
Epoch 13: loss 16.12910136066746
Epoch 14: loss 16.129092797304768
Epoch 15: loss 16.12908218190876
Epoch 16: loss 16.129061721212647
Epoch 17: loss 16.129052370640814
Epoch 18: loss 16.12904240347368
Epoch 19: loss 16.12902910700306
Epoch 20: loss 16.12901846101065
Epoch 21: loss 16.129002688824322
Epoch 22: loss 16.128996532205786
Epoch 23: loss 16.128981858378456
Epoch 24: loss 16.12896640978798
Epoch 25: loss 16.128956336829894
Epoch 26: loss 16.128945647276506
Epoch 27: loss 16.128928675088886
Epoch 28: loss 16.12891733315796
Epoch 29: loss 16.12890640401918
Epoch 30: loss 16.128894836504607
Epoch 31: loss 16.12887731721182
Epoch 32: loss 16.12887217390466
Epoch 33: loss 16.12885731753608
Epoch 34: loss 16.128842863587984
Epoch 35: loss 16.12883608518832
Epoch 36: loss 16.128818611012264
Epoch 37: loss 16.128804325085092
Epoch 38: loss 16.128800497941832
Epoch 39: loss 16.128780442259117
Epoch 40: loss 16.12877315409226
Epoch 41: loss 16.128751366860584
Epoch 42: loss 16.128747740408983
Epoch 43: loss 16.128734745235143
Epoch 44: loss 16.12872042611865
Epoch 45: loss 16.128706921177628
Epoch 46: loss 16.128694029720403
Epoch 47: loss 16.128684133599155
Epoch 48: loss 16.128667851645638
Epoch 49: loss 16.128657541176494
-----------Time: 0:03:36.946719, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017159938812256-------------


Epoch 0: loss 16.129153753636388
Epoch 1: loss 16.129152010160013
Epoch 2: loss 16.12913937125276
Epoch 3: loss 16.129128385069837
Epoch 4: loss 16.12911561133098
Epoch 5: loss 16.12910023430497
Epoch 6: loss 16.129088080791497
Epoch 7: loss 16.129078256753296
Epoch 8: loss 16.12905675629807
Epoch 9: loss 16.129054732786823
Epoch 10: loss 16.12903726224085
Epoch 11: loss 16.129025893862188
Epoch 12: loss 16.129013103528667
Epoch 13: loss 16.12900534085828
Epoch 14: loss 16.128990513530354
Epoch 15: loss 16.12897918974984
Epoch 16: loss 16.128966909183504
Epoch 17: loss 16.128953644346453
Epoch 18: loss 16.128941759977604
Epoch 19: loss 16.12893218952654
Epoch 20: loss 16.128914701867323
Epoch 21: loss 16.128900360451762
Epoch 22: loss 16.128891884211033
Epoch 23: loss 16.128882407623507
Epoch 24: loss 16.12886581711305
Epoch 25: loss 16.12885033792617
Epoch 26: loss 16.128840664277067
Epoch 27: loss 16.128829596158017
Epoch 28: loss 16.12881772993958
Epoch 29: loss 16.128804976944043
Epoch 30: loss 16.128790480472134
Epoch 31: loss 16.128778901548735
Epoch 32: loss 16.128766867827956
Epoch 33: loss 16.128759859695975
Epoch 34: loss 16.12874643409816
Epoch 35: loss 16.12873342544116
Epoch 36: loss 16.128719724994305
Epoch 37: loss 16.128707371826337
Epoch 38: loss 16.128690236803628
Epoch 39: loss 16.128681576984484
Epoch 40: loss 16.128669172995373
Epoch 41: loss 16.128658606346175
Epoch 42: loss 16.12864508636624
Epoch 43: loss 16.128630360680603
Epoch 44: loss 16.128619105353053
Epoch 45: loss 16.12860690049985
Epoch 46: loss 16.128594448801095
Epoch 47: loss 16.12858460661249
Epoch 48: loss 16.12857058931136
Epoch 49: loss 16.12855813190819
-----------Time: 0:04:33.971313, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017119884490967-------------


Epoch 0: loss 16.129176225397753
Epoch 1: loss 16.12915345596969
Epoch 2: loss 16.129146114388778
Epoch 3: loss 16.129135424316804
Epoch 4: loss 16.12911908843065
Epoch 5: loss 16.12910899680357
Epoch 6: loss 16.12910028771903
Epoch 7: loss 16.129087336106167
Epoch 8: loss 16.129074600223877
Epoch 9: loss 16.129060374452344
Epoch 10: loss 16.129046071411928
Epoch 11: loss 16.12903467191828
Epoch 12: loss 16.129023092476295
Epoch 13: loss 16.129012600503064
Epoch 14: loss 16.128996927366106
Epoch 15: loss 16.12898174636451
Epoch 16: loss 16.128977791649806
Epoch 17: loss 16.128958887221568
Epoch 18: loss 16.128947923856302
Epoch 19: loss 16.12893881235128
Epoch 20: loss 16.128921135409232
Epoch 21: loss 16.12891048371241
Epoch 22: loss 16.128903491137923
Epoch 23: loss 16.12888625343576
Epoch 24: loss 16.128875494392233
Epoch 25: loss 16.12886258945185
Epoch 26: loss 16.12885238010641
Epoch 27: loss 16.12883965822586
Epoch 28: loss 16.12882470125216
Epoch 29: loss 16.128808441079137
Epoch 30: loss 16.128799449366806
Epoch 31: loss 16.128786068885724
Epoch 32: loss 16.12877469532123
Epoch 33: loss 16.128760042237225
Epoch 34: loss 16.128750521051554
Epoch 35: loss 16.12873248628717
Epoch 36: loss 16.128728494753066
Epoch 37: loss 16.128715251177923
Epoch 38: loss 16.128704742610033
Epoch 39: loss 16.128693823842916
Epoch 40: loss 16.12867653220811
Epoch 41: loss 16.12866679529187
Epoch 42: loss 16.128648938920072
Epoch 43: loss 16.128643781092585
Epoch 44: loss 16.128625947538442
Epoch 45: loss 16.128612764118934
Epoch 46: loss 16.128601691332637
Epoch 47: loss 16.128591623041796
Epoch 48: loss 16.12857771308737
Epoch 49: loss 16.128571619217393
-----------Time: 0:06:10.237790, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017110824584961-------------


Epoch 0: loss 16.12917551701324
Epoch 1: loss 16.129160694352564
Epoch 2: loss 16.129153605321623
Epoch 3: loss 16.129139905911934
Epoch 4: loss 16.129122652133695
Epoch 5: loss 16.129112866470646
Epoch 6: loss 16.12910244917338
Epoch 7: loss 16.12908912781077
Epoch 8: loss 16.129076848800185
Epoch 9: loss 16.12906369338417
Epoch 10: loss 16.129049025779835
Epoch 11: loss 16.12903665601721
Epoch 12: loss 16.129027105272304
Epoch 13: loss 16.129008578891142
Epoch 14: loss 16.12899712546485
Epoch 15: loss 16.128981755180423
Epoch 16: loss 16.128978483958242
Epoch 17: loss 16.128963992672162
Epoch 18: loss 16.128951688251007
Epoch 19: loss 16.128935388665667
Epoch 20: loss 16.128933077340804
Epoch 21: loss 16.12891415165066
Epoch 22: loss 16.128898075574632
Epoch 23: loss 16.128892288187263
Epoch 24: loss 16.128879775814287
Epoch 25: loss 16.128864773205276
Epoch 26: loss 16.128848997388864
Epoch 27: loss 16.128838386660103
Epoch 28: loss 16.12883351094182
Epoch 29: loss 16.128811298472005
Epoch 30: loss 16.128796574342115
Epoch 31: loss 16.128791509341
Epoch 32: loss 16.12877831710558
Epoch 33: loss 16.128762042930816
Epoch 34: loss 16.12875123669623
Epoch 35: loss 16.12873867194636
Epoch 36: loss 16.12872992293092
Epoch 37: loss 16.12870966759371
Epoch 38: loss 16.12870158132747
Epoch 39: loss 16.128693295406737
Epoch 40: loss 16.128680425730007
Epoch 41: loss 16.12866460531545
Epoch 42: loss 16.128652442986063
Epoch 43: loss 16.12863974081167
Epoch 44: loss 16.12862485229094
Epoch 45: loss 16.12861660422677
Epoch 46: loss 16.128601797642172
Epoch 47: loss 16.128591011632324
Epoch 48: loss 16.128575885600537
Epoch 49: loss 16.128566965971256
-----------Time: 0:05:53.948869, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.0171122550964355-------------


Epoch 0: loss 16.129168889002674
Epoch 1: loss 16.12916004871661
Epoch 2: loss 16.129148356742093
Epoch 3: loss 16.129129251622196
Epoch 4: loss 16.12911931142138
Epoch 5: loss 16.129107912964898
Epoch 6: loss 16.129093188316425
Epoch 7: loss 16.129084558056515
Epoch 8: loss 16.129076073518455
Epoch 9: loss 16.129058596749484
Epoch 10: loss 16.129054019216483
Epoch 11: loss 16.129030354195407
Epoch 12: loss 16.12902219792045
Epoch 13: loss 16.129010094709535
Epoch 14: loss 16.128994463059225
Epoch 15: loss 16.1289844435152
Epoch 16: loss 16.128968432780646
Epoch 17: loss 16.128959786963243
Epoch 18: loss 16.128947726794724
Epoch 19: loss 16.128937060577574
Epoch 20: loss 16.128919855027565
Epoch 21: loss 16.128907741963573
Epoch 22: loss 16.12889899554105
Epoch 23: loss 16.128889324484863
Epoch 24: loss 16.1288708779655
Epoch 25: loss 16.12885963300961
Epoch 26: loss 16.128846735847976
Epoch 27: loss 16.128835391324134
Epoch 28: loss 16.12882360807899
Epoch 29: loss 16.128813247307285
Epoch 30: loss 16.12880211177243
Epoch 31: loss 16.128787971048528
Epoch 32: loss 16.12876945037178
Epoch 33: loss 16.128762777763065
Epoch 34: loss 16.128749203850486
Epoch 35: loss 16.128738037719227
Epoch 36: loss 16.128727877639182
Epoch 37: loss 16.128710258259858
Epoch 38: loss 16.128702811925162
Epoch 39: loss 16.128687500240623
Epoch 40: loss 16.128671567812116
Epoch 41: loss 16.128659205828235
Epoch 42: loss 16.12864774980903
Epoch 43: loss 16.128635049190386
Epoch 44: loss 16.12862572091763
Epoch 45: loss 16.128612897913374
Epoch 46: loss 16.128592898237635
Epoch 47: loss 16.128587729001318
Epoch 48: loss 16.128574489574838
Epoch 49: loss 16.128562963028326
-----------Time: 0:06:49.552081, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017112731933594-------------


Epoch 0: loss 16.12925762997949
Epoch 1: loss 16.129134739268533
Epoch 2: loss 16.129011296266572
Epoch 3: loss 16.128882680398217
Epoch 4: loss 16.128762581738652
Epoch 5: loss 16.12863647270099
Epoch 6: loss 16.12851484110979
Epoch 7: loss 16.128384171652367
Epoch 8: loss 16.128261674027392
Epoch 9: loss 16.128143335957446
Epoch 10: loss 16.128016975406982
Epoch 11: loss 16.127890962825774
Epoch 12: loss 16.127769953015708
Epoch 13: loss 16.12764517569944
Epoch 14: loss 16.127517117826493
Epoch 15: loss 16.127393452352386
Epoch 16: loss 16.127275205034483
Epoch 17: loss 16.127143989509054
Epoch 18: loss 16.127024587825176
Epoch 19: loss 16.126900115435767
Epoch 20: loss 16.126779623171632
Epoch 21: loss 16.126649695806623
Epoch 22: loss 16.1265292154699
Epoch 23: loss 16.126403889492714
Epoch 24: loss 16.126281856518197
Epoch 25: loss 16.126157616454016
Epoch 26: loss 16.126031841383867
Epoch 27: loss 16.125911610485613
Epoch 28: loss 16.125784279666174
Epoch 29: loss 16.125663212293382
Epoch 30: loss 16.125534699104477
Epoch 31: loss 16.125411394045624
Epoch 32: loss 16.12528967792538
Epoch 33: loss 16.125166311155137
Epoch 34: loss 16.125039292522718
Epoch 35: loss 16.12492187338202
Epoch 36: loss 16.124794312311685
Epoch 37: loss 16.124673106477207
Epoch 38: loss 16.124543989657578
Epoch 39: loss 16.12442535547668
Epoch 40: loss 16.12429866562595
Epoch 41: loss 16.12417517958159
Epoch 42: loss 16.124050723786844
Epoch 43: loss 16.123925815787633
Epoch 44: loss 16.12380332594141
Epoch 45: loss 16.12367817835681
Epoch 46: loss 16.123554667939047
Epoch 47: loss 16.123431725369777
Epoch 48: loss 16.123307709333492
Epoch 49: loss 16.123184125276932
-----------Time: 0:04:08.689891, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016458511352539-------------


Epoch 0: loss 16.129111683582597
Epoch 1: loss 16.12898452078408
Epoch 2: loss 16.12886471564255
Epoch 3: loss 16.128737390527522
Epoch 4: loss 16.128613981752046
Epoch 5: loss 16.12849533512516
Epoch 6: loss 16.128366402402527
Epoch 7: loss 16.128240159051842
Epoch 8: loss 16.128118555982713
Epoch 9: loss 16.127993685321485
Epoch 10: loss 16.12787202935688
Epoch 11: loss 16.127747555930306
Epoch 12: loss 16.12762178293449
Epoch 13: loss 16.127496807000895
Epoch 14: loss 16.127373608770156
Epoch 15: loss 16.12725123093788
Epoch 16: loss 16.127121705993353
Epoch 17: loss 16.127006821686837
Epoch 18: loss 16.12687921186969
Epoch 19: loss 16.126754767483767
Epoch 20: loss 16.12663114764497
Epoch 21: loss 16.126508536968885
Epoch 22: loss 16.126377729568357
Epoch 23: loss 16.126257085359374
Epoch 24: loss 16.126133193783037
Epoch 25: loss 16.126017218896266
Epoch 26: loss 16.125878145281163
Epoch 27: loss 16.125763435218566
Epoch 28: loss 16.125636934132082
Epoch 29: loss 16.125508599335763
Epoch 30: loss 16.12538899125581
Epoch 31: loss 16.12526768118613
Epoch 32: loss 16.12513854932759
Epoch 33: loss 16.125015819895975
Epoch 34: loss 16.12489504448547
Epoch 35: loss 16.124766273560766
Epoch 36: loss 16.124644582851094
Epoch 37: loss 16.12452244979004
Epoch 38: loss 16.124394370136603
Epoch 39: loss 16.124273795417757
Epoch 40: loss 16.124151384914743
Epoch 41: loss 16.124023683308387
Epoch 42: loss 16.123906258463276
Epoch 43: loss 16.123781503964665
Epoch 44: loss 16.123653709531933
Epoch 45: loss 16.12353389713024
Epoch 46: loss 16.123406609353193
Epoch 47: loss 16.123283298589925
Epoch 48: loss 16.123163799412424
Epoch 49: loss 16.12303358423379
-----------Time: 0:05:20.947710, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016426086425781-------------


Epoch 0: loss 16.129097376912103
Epoch 1: loss 16.12897692457887
Epoch 2: loss 16.128847507499625
Epoch 3: loss 16.12872086795145
Epoch 4: loss 16.128599244657583
Epoch 5: loss 16.12847999180705
Epoch 6: loss 16.128349467552894
Epoch 7: loss 16.128229899922424
Epoch 8: loss 16.128106252598723
Epoch 9: loss 16.12797805367118
Epoch 10: loss 16.127854950341145
Epoch 11: loss 16.127738402420054
Epoch 12: loss 16.12760895526299
Epoch 13: loss 16.127483530236432
Epoch 14: loss 16.127361926130135
Epoch 15: loss 16.1272397863275
Epoch 16: loss 16.127114549028025
Epoch 17: loss 16.126990359784987
Epoch 18: loss 16.12686766665419
Epoch 19: loss 16.126740278790606
Epoch 20: loss 16.126617656187108
Epoch 21: loss 16.126496126238195
Epoch 22: loss 16.12637179542197
Epoch 23: loss 16.12624551265897
Epoch 24: loss 16.12612069126314
Epoch 25: loss 16.125997810405256
Epoch 26: loss 16.125874225311527
Epoch 27: loss 16.12575424800041
Epoch 28: loss 16.12562668796724
Epoch 29: loss 16.125500230960316
Epoch 30: loss 16.125378540250644
Epoch 31: loss 16.125255548934565
Epoch 32: loss 16.125128448366016
Epoch 33: loss 16.125003853072414
Epoch 34: loss 16.124885110507652
Epoch 35: loss 16.12475980838529
Epoch 36: loss 16.124635777310097
Epoch 37: loss 16.124509132057508
Epoch 38: loss 16.124387513430886
Epoch 39: loss 16.124262491343398
Epoch 40: loss 16.12413732975705
Epoch 41: loss 16.124015134466024
Epoch 42: loss 16.123891991723678
Epoch 43: loss 16.12376674923837
Epoch 44: loss 16.12364144763459
Epoch 45: loss 16.123517948625658
Epoch 46: loss 16.12339128159258
Epoch 47: loss 16.12326856979279
Epoch 48: loss 16.123144015985833
Epoch 49: loss 16.123022558119974
-----------Time: 0:05:08.229629, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016423225402832-------------


Epoch 0: loss 16.129112190238285
Epoch 1: loss 16.128985422081502
Epoch 2: loss 16.1288611980934
Epoch 3: loss 16.12873611896177
Epoch 4: loss 16.128622867154842
Epoch 5: loss 16.12849151316773
Epoch 6: loss 16.128367182351507
Epoch 7: loss 16.128240350927587
Epoch 8: loss 16.1281207407733
Epoch 9: loss 16.127995175729307
Epoch 10: loss 16.12786812805624
Epoch 11: loss 16.127750131732554
Epoch 12: loss 16.12762353418961
Epoch 13: loss 16.127500254541328
Epoch 14: loss 16.12737789485946
Epoch 15: loss 16.1272574586023
Epoch 16: loss 16.127131570481037
Epoch 17: loss 16.127009540618015
Epoch 18: loss 16.126876805644116
Epoch 19: loss 16.126755875695846
Epoch 20: loss 16.126630698033427
Epoch 21: loss 16.1265069257312
Epoch 22: loss 16.126385706932147
Epoch 23: loss 16.126260201006627
Epoch 24: loss 16.126134373041
Epoch 25: loss 16.12601558743384
Epoch 26: loss 16.125892702427294
Epoch 27: loss 16.125767786130755
Epoch 28: loss 16.125640337074366
Epoch 29: loss 16.125520129512353
Epoch 30: loss 16.1253943207343
Epoch 31: loss 16.125263523186856
Epoch 32: loss 16.125147521852345
Epoch 33: loss 16.1250205286305
Epoch 34: loss 16.12489787594918
Epoch 35: loss 16.124773785755515
Epoch 36: loss 16.12465345062206
Epoch 37: loss 16.124524473819868
Epoch 38: loss 16.12440617412507
Epoch 39: loss 16.124277492915244
Epoch 40: loss 16.1241534234649
Epoch 41: loss 16.124027086769257
Epoch 42: loss 16.12390718983852
Epoch 43: loss 16.123778138878944
Epoch 44: loss 16.123654272194592
Epoch 45: loss 16.123531982521442
Epoch 46: loss 16.123406555939134
Epoch 47: loss 16.123287836710613
Epoch 48: loss 16.12315744210223
Epoch 49: loss 16.123035519585912
-----------Time: 0:05:47.845250, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016419410705566-------------


Epoch 0: loss 16.12910976015789
Epoch 1: loss 16.128988666855946
Epoch 2: loss 16.12886700933559
Epoch 3: loss 16.128741990359597
Epoch 4: loss 16.128618200425546
Epoch 5: loss 16.128497067711287
Epoch 6: loss 16.12836951338253
Epoch 7: loss 16.12824462353373
Epoch 8: loss 16.128120441550855
Epoch 9: loss 16.127997532170905
Epoch 10: loss 16.1278670400689
Epoch 11: loss 16.127744819885883
Epoch 12: loss 16.127626141625427
Epoch 13: loss 16.127499559639976
Epoch 14: loss 16.127373507646453
Epoch 15: loss 16.127248139664037
Epoch 16: loss 16.127130312917025
Epoch 17: loss 16.127004689791722
Epoch 18: loss 16.12688126494017
Epoch 19: loss 16.126755296438528
Epoch 20: loss 16.12663175179428
Epoch 21: loss 16.12650775235266
Epoch 22: loss 16.126384859048784
Epoch 23: loss 16.12626300083677
Epoch 24: loss 16.126136891280524
Epoch 25: loss 16.126017257271414
Epoch 26: loss 16.125883553584288
Epoch 27: loss 16.125761029511576
Epoch 28: loss 16.125639799303695
Epoch 29: loss 16.12551707505791
Epoch 30: loss 16.12538963741035
Epoch 31: loss 16.125268351714077
Epoch 32: loss 16.125145992032206
Epoch 33: loss 16.12502116441338
Epoch 34: loss 16.124898742501536
Epoch 35: loss 16.124773481347237
Epoch 36: loss 16.12464396003279
Epoch 37: loss 16.124521523600624
Epoch 38: loss 16.124395810760447
Epoch 39: loss 16.124280066124573
Epoch 40: loss 16.12414721343231
Epoch 41: loss 16.12403191374073
Epoch 42: loss 16.123902980499516
Epoch 43: loss 16.123780186763597
Epoch 44: loss 16.12365279163985
Epoch 45: loss 16.12353365235902
Epoch 46: loss 16.12340761488582
Epoch 47: loss 16.123284845004722
Epoch 48: loss 16.123158652993762
Epoch 49: loss 16.12303269019653
-----------Time: 0:07:17.269550, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016420364379883-------------


Epoch 0: loss 16.128489138575727
Epoch 1: loss 16.127250769398923
Epoch 2: loss 16.12601050117082
Epoch 3: loss 16.124768774168462
Epoch 4: loss 16.123530609313402
Epoch 5: loss 16.12229534385644
Epoch 6: loss 16.121052185046146
Epoch 7: loss 16.11980938353961
Epoch 8: loss 16.11857506294105
Epoch 9: loss 16.117334506380747
Epoch 10: loss 16.116096050600568
Epoch 11: loss 16.114854171653363
Epoch 12: loss 16.11361193021657
Epoch 13: loss 16.112377176082543
Epoch 14: loss 16.111136548476352
Epoch 15: loss 16.109898332800146
Epoch 16: loss 16.108662898285093
Epoch 17: loss 16.107415640594002
Epoch 18: loss 16.10618144341822
Epoch 19: loss 16.10494423016314
Epoch 20: loss 16.103704181814273
Epoch 21: loss 16.102460955588178
Epoch 22: loss 16.10122305261758
Epoch 23: loss 16.099985182836303
Epoch 24: loss 16.098745258428792
Epoch 25: loss 16.097507725726526
Epoch 26: loss 16.096272106077308
Epoch 27: loss 16.09503324995098
Epoch 28: loss 16.093796030991488
Epoch 29: loss 16.09255690520195
Epoch 30: loss 16.0913176108729
Epoch 31: loss 16.09008039865499
Epoch 32: loss 16.08883987476542
Epoch 33: loss 16.08760099841435
Epoch 34: loss 16.086369791388705
Epoch 35: loss 16.08512532419326
Epoch 36: loss 16.083893374556624
Epoch 37: loss 16.082651555765057
Epoch 38: loss 16.081408723143536
Epoch 39: loss 16.080178724935095
Epoch 40: loss 16.078943176850345
Epoch 41: loss 16.077699397296087
Epoch 42: loss 16.076460512129103
Epoch 43: loss 16.075226476232665
Epoch 44: loss 16.07398855718599
Epoch 45: loss 16.07274917054915
Epoch 46: loss 16.071511556429343
Epoch 47: loss 16.070276698578695
Epoch 48: loss 16.06903547926919
Epoch 49: loss 16.067802578032566
-----------Time: 0:05:18.410213, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009533405303955-------------


Epoch 0: loss 16.12855066171867
Epoch 1: loss 16.127314829968967
Epoch 2: loss 16.126077867189526
Epoch 3: loss 16.124833295240293
Epoch 4: loss 16.123596605235562
Epoch 5: loss 16.122354241931742
Epoch 6: loss 16.121118874832494
Epoch 7: loss 16.119878448436545
Epoch 8: loss 16.118639607349124
Epoch 9: loss 16.117393807395118
Epoch 10: loss 16.116159149717546
Epoch 11: loss 16.114922085295817
Epoch 12: loss 16.11368012233815
Epoch 13: loss 16.112446475379034
Epoch 14: loss 16.111205521065493
Epoch 15: loss 16.109966479286413
Epoch 16: loss 16.108727778735012
Epoch 17: loss 16.10748522562978
Epoch 18: loss 16.106248404942107
Epoch 19: loss 16.105011517357216
Epoch 20: loss 16.103771173934565
Epoch 21: loss 16.102528827743985
Epoch 22: loss 16.1012921916719
Epoch 23: loss 16.100055299419758
Epoch 24: loss 16.098815581926903
Epoch 25: loss 16.097578620184628
Epoch 26: loss 16.096341097854026
Epoch 27: loss 16.095104836717514
Epoch 28: loss 16.093859435035327
Epoch 29: loss 16.09262858479486
Epoch 30: loss 16.091390531953746
Epoch 31: loss 16.090149648167504
Epoch 32: loss 16.088910385472026
Epoch 33: loss 16.087672298923014
Epoch 34: loss 16.08643572093223
Epoch 35: loss 16.085198965067445
Epoch 36: loss 16.08395833279401
Epoch 37: loss 16.0827226514334
Epoch 38: loss 16.081483801530066
Epoch 39: loss 16.080249272461103
Epoch 40: loss 16.07901137675067
Epoch 41: loss 16.07777603609916
Epoch 42: loss 16.076534251015495
Epoch 43: loss 16.075297005608263
Epoch 44: loss 16.074057840406407
Epoch 45: loss 16.07282269940939
Epoch 46: loss 16.071582151664995
Epoch 47: loss 16.070355299181625
Epoch 48: loss 16.069108827143925
Epoch 49: loss 16.0678724192484
-----------Time: 0:03:47.719883, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.00950813293457-------------


Epoch 0: loss 16.12855353311328
Epoch 1: loss 16.127314298939872
Epoch 2: loss 16.126072349465367
Epoch 3: loss 16.124834571473297
Epoch 4: loss 16.123595027187196
Epoch 5: loss 16.122354729918438
Epoch 6: loss 16.12111542366198
Epoch 7: loss 16.11987946382225
Epoch 8: loss 16.11863795877346
Epoch 9: loss 16.117400573348792
Epoch 10: loss 16.116161847905403
Epoch 11: loss 16.11492090655644
Epoch 12: loss 16.113678764687602
Epoch 13: loss 16.112441058778582
Epoch 14: loss 16.111202405418243
Epoch 15: loss 16.109964190779205
Epoch 16: loss 16.108724397054633
Epoch 17: loss 16.10748804672183
Epoch 18: loss 16.106251467693884
Epoch 19: loss 16.105010143112008
Epoch 20: loss 16.103768767708992
Epoch 21: loss 16.102532117116574
Epoch 22: loss 16.10129322157793
Epoch 23: loss 16.100049374089284
Epoch 24: loss 16.098814033437773
Epoch 25: loss 16.09757621188472
Epoch 26: loss 16.096332490411772
Epoch 27: loss 16.09509537309457
Epoch 28: loss 16.09385779994282
Epoch 29: loss 16.092625369579654
Epoch 30: loss 16.091383386397244
Epoch 31: loss 16.09014788083631
Epoch 32: loss 16.08890494812825
Epoch 33: loss 16.087664675232897
Epoch 34: loss 16.086430596812644
Epoch 35: loss 16.08518963057169
Epoch 36: loss 16.083953495450874
Epoch 37: loss 16.08271821132492
Epoch 38: loss 16.08147544767495
Epoch 39: loss 16.08024012102518
Epoch 40: loss 16.07900426490207
Epoch 41: loss 16.077766713530814
Epoch 42: loss 16.076523816604986
Epoch 43: loss 16.075284492716705
Epoch 44: loss 16.074049439360227
Epoch 45: loss 16.072813187039632
Epoch 46: loss 16.071577010950747
Epoch 47: loss 16.07033792405494
Epoch 48: loss 16.069100602415997
Epoch 49: loss 16.06786670705558
-----------Time: 0:05:14.870661, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009511470794678-------------


Epoch 0: loss 16.128561639085678
Epoch 1: loss 16.1273205447547
Epoch 2: loss 16.126083331499622
Epoch 3: loss 16.124849899752494
Epoch 4: loss 16.123602625985324
Epoch 5: loss 16.12236305940015
Epoch 6: loss 16.121122713384583
Epoch 7: loss 16.119885562359478
Epoch 8: loss 16.118644643309583
Epoch 9: loss 16.11740462140845
Epoch 10: loss 16.116163158883474
Epoch 11: loss 16.11492615772888
Epoch 12: loss 16.113685937210423
Epoch 13: loss 16.112445837521825
Epoch 14: loss 16.11121115235935
Epoch 15: loss 16.109964365541707
Epoch 16: loss 16.108732173208182
Epoch 17: loss 16.10749331189602
Epoch 18: loss 16.106250526465562
Epoch 19: loss 16.10501685046579
Epoch 20: loss 16.10377575561623
Epoch 21: loss 16.102538732162568
Epoch 22: loss 16.101295671883065
Epoch 23: loss 16.100053597948616
Epoch 24: loss 16.09881649878182
Epoch 25: loss 16.097577894168293
Epoch 26: loss 16.096339278145937
Epoch 27: loss 16.095101934207918
Epoch 28: loss 16.09386298992246
Epoch 29: loss 16.09262055246126
Epoch 30: loss 16.09138503964016
Epoch 31: loss 16.090147826385085
Epoch 32: loss 16.088913583055405
Epoch 33: loss 16.08767125034799
Epoch 34: loss 16.086427906403532
Epoch 35: loss 16.085185600662438
Epoch 36: loss 16.083951509277608
Epoch 37: loss 16.08271590259297
Epoch 38: loss 16.081473595814707
Epoch 39: loss 16.080235835454463
Epoch 40: loss 16.07899824778239
Epoch 41: loss 16.077760837984318
Epoch 42: loss 16.076524235101548
Epoch 43: loss 16.075279323480387
Epoch 44: loss 16.074048404268364
Epoch 45: loss 16.072807272599402
Epoch 46: loss 16.07156948216134
Epoch 47: loss 16.070330818429337
Epoch 48: loss 16.069097259110766
Epoch 49: loss 16.067852894594775
-----------Time: 0:06:22.315106, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.00950813293457-------------


Epoch 0: loss 16.128562941766422
Epoch 1: loss 16.12732416550189
Epoch 2: loss 16.126086156221756
Epoch 3: loss 16.124835768363084
Epoch 4: loss 16.12359991120281
Epoch 5: loss 16.122360788524766
Epoch 6: loss 16.12111921450442
Epoch 7: loss 16.119882026141333
Epoch 8: loss 16.118644417725935
Epoch 9: loss 16.11739741932639
Epoch 10: loss 16.116167377556973
Epoch 11: loss 16.11492458901501
Epoch 12: loss 16.113690571268982
Epoch 13: loss 16.112445520667553
Epoch 14: loss 16.11120507145395
Epoch 15: loss 16.109969993205482
Epoch 16: loss 16.108732423165236
Epoch 17: loss 16.107490027709265
Epoch 18: loss 16.10625330399663
Epoch 19: loss 16.10501407604622
Epoch 20: loss 16.103774946108015
Epoch 21: loss 16.1025372645724
Epoch 22: loss 16.10129205113588
Epoch 23: loss 16.10005901299332
Epoch 24: loss 16.09881797103923
Epoch 25: loss 16.097578321999347
Epoch 26: loss 16.096337380650382
Epoch 27: loss 16.095099805424304
Epoch 28: loss 16.09386265284345
Epoch 29: loss 16.09262630821506
Epoch 30: loss 16.091381278875538
Epoch 31: loss 16.090149949982866
Epoch 32: loss 16.088907363169735
Epoch 33: loss 16.08766588819876
Epoch 34: loss 16.086433523177064
Epoch 35: loss 16.085192389433768
Epoch 36: loss 16.083955169955694
Epoch 37: loss 16.082717383147713
Epoch 38: loss 16.081474314052297
Epoch 39: loss 16.08024031030801
Epoch 40: loss 16.07900270448553
Epoch 41: loss 16.07776288586897
Epoch 42: loss 16.076520671917084
Epoch 43: loss 16.075281789343016
Epoch 44: loss 16.074047815157968
Epoch 45: loss 16.07280473413514
Epoch 46: loss 16.071564840842615
Epoch 47: loss 16.070326190075193
Epoch 48: loss 16.069087386325755
Epoch 49: loss 16.067853920870725
-----------Time: 0:07:49.327303, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009507179260254-------------


Epoch 0: loss 16.123101785171286
Epoch 1: loss 16.110718314838255
Epoch 2: loss 16.09833430154872
Epoch 3: loss 16.08595456449541
Epoch 4: loss 16.073580271008872
Epoch 5: loss 16.0612034566899
Epoch 6: loss 16.048843639969114
Epoch 7: loss 16.036488079259808
Epoch 8: loss 16.02413134551554
Epoch 9: loss 16.01178794194979
Epoch 10: loss 15.999448833289257
Epoch 11: loss 15.98711470458221
Epoch 12: loss 15.97477330274722
Epoch 13: loss 15.962447992027162
Epoch 14: loss 15.950124199718415
Epoch 15: loss 15.937810307161
Epoch 16: loss 15.925496754275514
Epoch 17: loss 15.913189247031779
Epoch 18: loss 15.900891829859374
Epoch 19: loss 15.888596866622187
Epoch 20: loss 15.876304140033898
Epoch 21: loss 15.864003904362358
Epoch 22: loss 15.851728858346197
Epoch 23: loss 15.839453143357842
Epoch 24: loss 15.827184820252958
Epoch 25: loss 15.814911315465764
Epoch 26: loss 15.802650170588118
Epoch 27: loss 15.790397777318825
Epoch 28: loss 15.77815285216472
Epoch 29: loss 15.765909619147262
Epoch 30: loss 15.753668242138712
Epoch 31: loss 15.741427160203942
Epoch 32: loss 15.729198642500462
Epoch 33: loss 15.716974496452494
Epoch 34: loss 15.704752778929171
Epoch 35: loss 15.692540806619418
Epoch 36: loss 15.680329328000772
Epoch 37: loss 15.668123805827586
Epoch 38: loss 15.655924894033145
Epoch 39: loss 15.6437263903636
Epoch 40: loss 15.631542583857625
Epoch 41: loss 15.619354338798917
Epoch 42: loss 15.607174289427483
Epoch 43: loss 15.595001024678714
Epoch 44: loss 15.582828006256916
Epoch 45: loss 15.570662800808067
Epoch 46: loss 15.558495505987917
Epoch 47: loss 15.546334165538894
Epoch 48: loss 15.534175744194126
Epoch 49: loss 15.522027419662269
-----------Time: 0:03:25.772096, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9405174255371094-------------


Epoch 0: loss 16.123013941343224
Epoch 1: loss 16.110621837638785
Epoch 2: loss 16.098241002226473
Epoch 3: loss 16.085863289721573
Epoch 4: loss 16.073486469179603
Epoch 5: loss 16.061121169479726
Epoch 6: loss 16.048754410487014
Epoch 7: loss 16.036389525653618
Epoch 8: loss 16.02402966174177
Epoch 9: loss 16.011682684101313
Epoch 10: loss 15.999329712158834
Epoch 11: loss 15.986988454489945
Epoch 12: loss 15.974651339262838
Epoch 13: loss 15.962314046680312
Epoch 14: loss 15.949991191969804
Epoch 15: loss 15.937667706143667
Epoch 16: loss 15.925349821014985
Epoch 17: loss 15.913041065023151
Epoch 18: loss 15.900728340314872
Epoch 19: loss 15.88841935666506
Epoch 20: loss 15.876119969914049
Epoch 21: loss 15.863817023608657
Epoch 22: loss 15.851521982065421
Epoch 23: loss 15.839238554190656
Epoch 24: loss 15.826938818951804
Epoch 25: loss 15.814653289778327
Epoch 26: loss 15.802372697515938
Epoch 27: loss 15.79009571459191
Epoch 28: loss 15.777812160701451
Epoch 29: loss 15.765537097053466
Epoch 30: loss 15.753266706876355
Epoch 31: loss 15.740995380656754
Epoch 32: loss 15.728725212951792
Epoch 33: loss 15.716458086736699
Epoch 34: loss 15.704194237966782
Epoch 35: loss 15.691926890835287
Epoch 36: loss 15.679665111211929
Epoch 37: loss 15.667388812299006
Epoch 38: loss 15.655125776667385
Epoch 39: loss 15.642855133940307
Epoch 40: loss 15.63058232353588
Epoch 41: loss 15.618320540801022
Epoch 42: loss 15.606038609557077
Epoch 43: loss 15.593759112023607
Epoch 44: loss 15.58148128743921
Epoch 45: loss 15.56919445195491
Epoch 46: loss 15.556904647063794
Epoch 47: loss 15.544602713551193
Epoch 48: loss 15.532293479425743
Epoch 49: loss 15.51997665428091
-----------Time: 0:04:19.450694, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.940234661102295-------------


Epoch 0: loss 16.123045249760573
Epoch 1: loss 16.11065507570384
Epoch 2: loss 16.09827918498136
Epoch 3: loss 16.085898084054502
Epoch 4: loss 16.073520169302196
Epoch 5: loss 16.0611448775432
Epoch 6: loss 16.048776982334918
Epoch 7: loss 16.03641436941407
Epoch 8: loss 16.02404386676998
Epoch 9: loss 16.01169024037563
Epoch 10: loss 15.999341413467848
Epoch 11: loss 15.986978557331527
Epoch 12: loss 15.974627708468248
Epoch 13: loss 15.962292927902247
Epoch 14: loss 15.949949596938133
Epoch 15: loss 15.937600130617392
Epoch 16: loss 15.925255506307032
Epoch 17: loss 15.912910180353743
Epoch 18: loss 15.90056092302199
Epoch 19: loss 15.88820645289294
Epoch 20: loss 15.875843870568495
Epoch 21: loss 15.863464791078552
Epoch 22: loss 15.85108159663173
Epoch 23: loss 15.838685497244523
Epoch 24: loss 15.826264900510372
Epoch 25: loss 15.813833340410955
Epoch 26: loss 15.801365456677054
Epoch 27: loss 15.788884762903477
Epoch 28: loss 15.77636515517284
Epoch 29: loss 15.763813099697792
Epoch 30: loss 15.751216658176839
Epoch 31: loss 15.738569070879308
Epoch 32: loss 15.725847858265082
Epoch 33: loss 15.713055588876248
Epoch 34: loss 15.700191718719134
Epoch 35: loss 15.687239167222256
Epoch 36: loss 15.67419285278984
Epoch 37: loss 15.66102341220974
Epoch 38: loss 15.647734399850503
Epoch 39: loss 15.634307089676476
Epoch 40: loss 15.620719086676592
Epoch 41: loss 15.606973896991176
Epoch 42: loss 15.593041678237293
Epoch 43: loss 15.578912470507971
Epoch 44: loss 15.564566572312755
Epoch 45: loss 15.549986183286816
Epoch 46: loss 15.535148275229126
Epoch 47: loss 15.520050295673679
Epoch 48: loss 15.504665273113572
Epoch 49: loss 15.488983087918239
-----------Time: 0:05:48.082029, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9360690116882324-------------


Epoch 0: loss 16.12302025094377
Epoch 1: loss 16.110638634545314
Epoch 2: loss 16.098252421944863
Epoch 3: loss 16.08587512689936
Epoch 4: loss 16.073493096153005
Epoch 5: loss 16.061124583830843
Epoch 6: loss 16.04875161221262
Epoch 7: loss 16.03637432131578
Epoch 8: loss 16.024007154716752
Epoch 9: loss 16.01163240643284
Epoch 10: loss 15.999264538190882
Epoch 11: loss 15.986880825679545
Epoch 12: loss 15.974492160699626
Epoch 13: loss 15.962087855253484
Epoch 14: loss 15.949676408918089
Epoch 15: loss 15.937237883210507
Epoch 16: loss 15.924772803973996
Epoch 17: loss 15.912268699285063
Epoch 18: loss 15.899722494983102
Epoch 19: loss 15.887103948857616
Epoch 20: loss 15.87442050153889
Epoch 21: loss 15.861643610731813
Epoch 22: loss 15.848758576161321
Epoch 23: loss 15.835743696680531
Epoch 24: loss 15.822571790756383
Epoch 25: loss 15.809200263010412
Epoch 26: loss 15.795620327607777
Epoch 27: loss 15.781782098410245
Epoch 28: loss 15.767663726474748
Epoch 29: loss 15.753232295776334
Epoch 30: loss 15.738453455370623
Epoch 31: loss 15.723271825250041
Epoch 32: loss 15.707669268489338
Epoch 33: loss 15.69159828028386
Epoch 34: loss 15.675017492222747
Epoch 35: loss 15.657890493549557
Epoch 36: loss 15.640206561521579
Epoch 37: loss 15.621913555201271
Epoch 38: loss 15.602969766505845
Epoch 39: loss 15.58335936776058
Epoch 40: loss 15.563050206294845
Epoch 41: loss 15.542013087435997
Epoch 42: loss 15.52022647702091
Epoch 43: loss 15.497673703640164
Epoch 44: loss 15.47431660994944
Epoch 45: loss 15.450139898265945
Epoch 46: loss 15.425139552682168
Epoch 47: loss 15.399277015507643
Epoch 48: loss 15.372551601694095
Epoch 49: loss 15.344938715363794
-----------Time: 0:06:44.061589, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.9170005321502686-------------


Epoch 0: loss 16.123029901256633
Epoch 1: loss 16.110644529797966
Epoch 2: loss 16.098264714957192
Epoch 3: loss 16.08587756009125
Epoch 4: loss 16.073498686478796
Epoch 5: loss 16.06110649098515
Epoch 6: loss 16.048720864902183
Epoch 7: loss 16.036329296894085
Epoch 8: loss 16.02392798937683
Epoch 9: loss 16.01151954719332
Epoch 10: loss 15.99908687836324
Epoch 11: loss 15.986622513215655
Epoch 12: loss 15.974122281823876
Epoch 13: loss 15.9615655207258
Epoch 14: loss 15.948925753142795
Epoch 15: loss 15.936178946896957
Epoch 16: loss 15.923295881386487
Epoch 17: loss 15.910235010845108
Epoch 18: loss 15.896947468668952
Epoch 19: loss 15.883393473830024
Epoch 20: loss 15.869505713205093
Epoch 21: loss 15.855214899425082
Epoch 22: loss 15.840459303469551
Epoch 23: loss 15.825170588532242
Epoch 24: loss 15.809272418662605
Epoch 25: loss 15.792702340379625
Epoch 26: loss 15.775373894472107
Epoch 27: loss 15.757241426958474
Epoch 28: loss 15.738228101455498
Epoch 29: loss 15.71827646154888
Epoch 30: loss 15.697331636479655
Epoch 31: loss 15.675356241592317
Epoch 32: loss 15.652288979327569
Epoch 33: loss 15.628111406277547
Epoch 34: loss 15.602780924470869
Epoch 35: loss 15.576270728865806
Epoch 36: loss 15.548549281312695
Epoch 37: loss 15.51959256311679
Epoch 38: loss 15.489401060190454
Epoch 39: loss 15.457931611381062
Epoch 40: loss 15.425195801835011
Epoch 41: loss 15.391174726605481
Epoch 42: loss 15.355846434069951
Epoch 43: loss 15.319227798922416
Epoch 44: loss 15.281303491846513
Epoch 45: loss 15.242065728391363
Epoch 46: loss 15.20152468419451
Epoch 47: loss 15.159661271767877
Epoch 48: loss 15.116478898721002
Epoch 49: loss 15.072003350042662
-----------Time: 0:06:36.904525, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.8810434341430664-------------


Epoch 0: loss 16.06794606945708
Epoch 1: loss 15.944689786972203
Epoch 2: loss 15.821986332219732
Epoch 3: loss 15.699825523339127
Epoch 4: loss 15.578159311013483
Epoch 5: loss 15.456972026773093
Epoch 6: loss 15.336232175510691
Epoch 7: loss 15.215909211126082
Epoch 8: loss 15.096010155234406
Epoch 9: loss 14.976450164010824
Epoch 10: loss 14.857131806062965
Epoch 11: loss 14.737998912099783
Epoch 12: loss 14.618907463297758
Epoch 13: loss 14.499670748479863
Epoch 14: loss 14.38003465622388
Epoch 15: loss 14.259813923236791
Epoch 16: loss 14.138746786402775
Epoch 17: loss 14.016404245780045
Epoch 18: loss 13.892285120882633
Epoch 19: loss 13.765928765235225
Epoch 20: loss 13.636797691312026
Epoch 21: loss 13.504323617085742
Epoch 22: loss 13.368001158954398
Epoch 23: loss 13.227207642783933
Epoch 24: loss 13.081454010486862
Epoch 25: loss 12.930209548788397
Epoch 26: loss 12.772926334715072
Epoch 27: loss 12.609177786478082
Epoch 28: loss 12.438593941191217
Epoch 29: loss 12.26086835254464
Epoch 30: loss 12.075674535140452
Epoch 31: loss 11.882991493104786
Epoch 32: loss 11.682681662417936
Epoch 33: loss 11.474733500457315
Epoch 34: loss 11.25920781212828
Epoch 35: loss 11.036231820384986
Epoch 36: loss 10.806032379922042
Epoch 37: loss 10.568750361763568
Epoch 38: loss 10.324779695632213
Epoch 39: loss 10.074380095203393
Epoch 40: loss 9.818016862791476
Epoch 41: loss 9.556020460289545
Epoch 42: loss 9.288936264428061
Epoch 43: loss 9.017064698174183
Epoch 44: loss 8.741007178940807
Epoch 45: loss 8.461299735998058
Epoch 46: loss 8.17844512139798
Epoch 47: loss 7.893020832130221
Epoch 48: loss 7.605771194404075
Epoch 49: loss 7.31739825331173
-----------Time: 0:04:32.675192, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.6854660511016846-------------


Epoch 0: loss 16.067849645671668
Epoch 1: loss 15.944613482136646
Epoch 2: loss 15.821788335637336
Epoch 3: loss 15.699200485503304
Epoch 4: loss 15.576448551009957
Epoch 5: loss 15.452770350354077
Epoch 6: loss 15.326663745363618
Epoch 7: loss 15.195512259246344
Epoch 8: loss 15.055438169777037
Epoch 9: loss 14.901430913583944
Epoch 10: loss 14.728379748968276
Epoch 11: loss 14.531820257310313
Epoch 12: loss 14.308601022090777
Epoch 13: loss 14.056813174191216
Epoch 14: loss 13.775598000676299
Epoch 15: loss 13.464887138810607
Epoch 16: loss 13.12544123539139
Epoch 17: loss 12.758509743271993
Epoch 18: loss 12.365884761696215
Epoch 19: loss 11.949118876599824
Epoch 20: loss 11.510379845710991
Epoch 21: loss 11.052244338346215
Epoch 22: loss 10.57708804694254
Epoch 23: loss 10.087112343265934
Epoch 24: loss 9.585287945629657
Epoch 25: loss 9.074587253593375
Epoch 26: loss 8.557747977268702
Epoch 27: loss 8.037667464276511
Epoch 28: loss 7.517688808005552
Epoch 29: loss 7.00108107073661
Epoch 30: loss 6.4910489002474625
Epoch 31: loss 5.99080303481508
Epoch 32: loss 5.504026333361362
Epoch 33: loss 5.034036422696303
Epoch 34: loss 4.584482228905044
Epoch 35: loss 4.158546178220341
Epoch 36: loss 3.759570619993848
Epoch 37: loss 3.390378893556123
Epoch 38: loss 3.053488100268648
Epoch 39: loss 2.7510069022041224
Epoch 40: loss 2.4839944227549484
Epoch 41: loss 2.252584228507846
Epoch 42: loss 2.0558353058341536
Epoch 43: loss 1.8914580270607488
Epoch 44: loss 1.7561701543957335
Epoch 45: loss 1.6458610622956742
Epoch 46: loss 1.5562672855544182
Epoch 47: loss 1.4832907226048067
Epoch 48: loss 1.423288868392791
Epoch 49: loss 1.373229177637292
-----------Time: 0:05:15.877169, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.1727665662765503-------------


Epoch 0: loss 16.067762888275197
Epoch 1: loss 15.944359561034426
Epoch 2: loss 15.82048402706393
Epoch 3: loss 15.69262528225026
Epoch 4: loss 15.549685218400317
Epoch 5: loss 15.370233566363005
Epoch 6: loss 15.1322485595504
Epoch 7: loss 14.823976238763093
Epoch 8: loss 14.44380903321806
Epoch 9: loss 13.995262398546062
Epoch 10: loss 13.483840180065659
Epoch 11: loss 12.915845045905971
Epoch 12: loss 12.29780134758008
Epoch 13: loss 11.636456028542614
Epoch 14: loss 10.939379152751215
Epoch 15: loss 10.213903238878359
Epoch 16: loss 9.467514397464024
Epoch 17: loss 8.709159314340713
Epoch 18: loss 7.947337798543311
Epoch 19: loss 7.191462475557311
Epoch 20: loss 6.451011390125966
Epoch 21: loss 5.735445544277604
Epoch 22: loss 5.054860327929631
Epoch 23: loss 4.41868526965396
Epoch 24: loss 3.835949888690909
Epoch 25: loss 3.3147666127607835
Epoch 26: loss 2.861647464582102
Epoch 27: loss 2.480404460086584
Epoch 28: loss 2.170250670880063
Epoch 29: loss 1.9262436171988548
Epoch 30: loss 1.7396740743555148
Epoch 31: loss 1.5992292356335513
Epoch 32: loss 1.4934472710364146
Epoch 33: loss 1.4123245166526532
Epoch 34: loss 1.3484568619870698
Epoch 35: loss 1.2968122294702369
Epoch 36: loss 1.2540586898351507
Epoch 37: loss 1.2180956285760869
Epoch 38: loss 1.1875198954729491
Epoch 39: loss 1.1613351205498579
Epoch 40: loss 1.1388173390497909
Epoch 41: loss 1.1193739140143921
Epoch 42: loss 1.1025509291721984
Epoch 43: loss 1.0879723385276192
Epoch 44: loss 1.0752740631030915
Epoch 45: loss 1.0642313689171716
Epoch 46: loss 1.0546053293089688
Epoch 47: loss 1.04620387843927
Epoch 48: loss 1.0388560266907025
Epoch 49: loss 1.032424109341205
-----------Time: 0:05:24.097525, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0244686603546143-------------


Epoch 0: loss 16.06778084628928
Epoch 1: loss 15.944052929071837
Epoch 2: loss 15.815648132851098
Epoch 3: loss 15.657665932029404
Epoch 4: loss 15.41971223726423
Epoch 5: loss 15.067019106837444
Epoch 6: loss 14.59615694329167
Epoch 7: loss 14.01859687475357
Epoch 8: loss 13.347484976523203
Epoch 9: loss 12.595377741072605
Epoch 10: loss 11.775399994240823
Epoch 11: loss 10.90115424172265
Epoch 12: loss 9.987200205970419
Epoch 13: loss 9.049120556342336
Epoch 14: loss 8.103213460890089
Epoch 15: loss 7.166934998933876
Epoch 16: loss 6.257550206114378
Epoch 17: loss 5.39239040725318
Epoch 18: loss 4.589535982470074
Epoch 19: loss 3.865929719739274
Epoch 20: loss 3.2357559679901553
Epoch 21: loss 2.709328973727618
Epoch 22: loss 2.28989531716424
Epoch 23: loss 1.9718109454195936
Epoch 24: loss 1.740506074945845
Epoch 25: loss 1.5762015017171862
Epoch 26: loss 1.4587942756603047
Epoch 27: loss 1.372258254145072
Epoch 28: loss 1.3059306087670215
Epoch 29: loss 1.2533200214970948
Epoch 30: loss 1.210597675024264
Epoch 31: loss 1.1754327851381556
Epoch 32: loss 1.1461996987437735
Epoch 33: loss 1.121778668496452
Epoch 34: loss 1.1012863943990383
Epoch 35: loss 1.084062263912452
Epoch 36: loss 1.0695369716180674
Epoch 37: loss 1.0572438528837231
Epoch 38: loss 1.046833505346569
Epoch 39: loss 1.037994187120642
Epoch 40: loss 1.0304530299896129
Epoch 41: loss 1.0240146009218052
Epoch 42: loss 1.0185125430619477
Epoch 43: loss 1.0137874440954975
Epoch 44: loss 1.0097247711532202
Epoch 45: loss 1.0062177576339395
Epoch 46: loss 1.0031898987234902
Epoch 47: loss 1.0005717511537477
Epoch 48: loss 0.9982985171239748
Epoch 49: loss 0.9963206002866528
-----------Time: 0:05:35.202111, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0072304010391235-------------


Epoch 0: loss 16.067717055382452
Epoch 1: loss 15.942753124392635
Epoch 2: loss 15.794150524647616
Epoch 3: loss 15.53454751900968
Epoch 4: loss 15.092014162355042
Epoch 5: loss 14.480256070255779
Epoch 6: loss 13.729886746263945
Epoch 7: loss 12.864778999921873
Epoch 8: loss 11.906210887425615
Epoch 9: loss 10.876135620751933
Epoch 10: loss 9.797730557357182
Epoch 11: loss 8.695662115759795
Epoch 12: loss 7.595207327925167
Epoch 13: loss 6.5234140701563605
Epoch 14: loss 5.507436901671268
Epoch 15: loss 4.574691784647641
Epoch 16: loss 3.7499976919162785
Epoch 17: loss 3.0540383379377705
Epoch 18: loss 2.498855780686549
Epoch 19: loss 2.0833651058177316
Epoch 20: loss 1.7901120487926705
Epoch 21: loss 1.59073341145601
Epoch 22: loss 1.4550292491264614
Epoch 23: loss 1.3589548798214164
Epoch 24: loss 1.2874426041173184
Epoch 25: loss 1.2320536675175744
Epoch 26: loss 1.1879949540207735
Epoch 27: loss 1.1525018937954636
Epoch 28: loss 1.1236632978741146
Epoch 29: loss 1.100118623439743
Epoch 30: loss 1.0808108157279246
Epoch 31: loss 1.0649262433988105
Epoch 32: loss 1.05183089599459
Epoch 33: loss 1.0409846981949882
Epoch 34: loss 1.0319703377544265
Epoch 35: loss 1.024456035287047
Epoch 36: loss 1.018173775444736
Epoch 37: loss 1.0129047088353385
Epoch 38: loss 1.0084764200296656
Epoch 39: loss 1.0047407463038986
Epoch 40: loss 1.0015749664265154
Epoch 41: loss 0.9988984006977133
Epoch 42: loss 0.9966119240651382
Epoch 43: loss 0.9946609527731538
Epoch 44: loss 0.9929929080380248
Epoch 45: loss 0.9915618507871167
Epoch 46: loss 0.9903316226843066
Epoch 47: loss 0.9892645327535383
Epoch 48: loss 0.9883519623381992
Epoch 49: loss 0.9875557802500058
-----------Time: 0:07:02.804526, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0028774738311768-------------


Epoch 0: loss 15.525683867665592
Epoch 1: loss 14.343402793577276
Epoch 2: loss 13.042268872844453
Epoch 3: loss 10.945832167973396
Epoch 4: loss 7.897292498339124
Epoch 5: loss 4.846304536060252
Epoch 6: loss 2.667802944787499
Epoch 7: loss 1.6171043065658661
Epoch 8: loss 1.2620334369517852
Epoch 9: loss 1.1309033379145068
Epoch 10: loss 1.0666071692194998
Epoch 11: loss 1.0319551837839724
Epoch 12: loss 1.0125994776305198
Epoch 13: loss 1.001400030100067
Epoch 14: loss 0.9947359351569897
Epoch 15: loss 0.99064624157206
Epoch 16: loss 0.98801390560118
Epoch 17: loss 0.9862835047165942
Epoch 18: loss 0.9850799296782548
Epoch 19: loss 0.9842010743920087
Epoch 20: loss 0.9835413045828727
Epoch 21: loss 0.9829993550060494
Epoch 22: loss 0.9825600959024331
Epoch 23: loss 0.9821676137396795
Epoch 24: loss 0.9818329919026298
Epoch 25: loss 0.981508116705253
Epoch 26: loss 0.9812042082438591
Epoch 27: loss 0.9808914943062915
Epoch 28: loss 0.9805813819704787
Epoch 29: loss 0.980271719505503
Epoch 30: loss 0.9800039292548648
Epoch 31: loss 0.9796831414322286
Epoch 32: loss 0.9793552694727766
Epoch 33: loss 0.9790183414642807
Epoch 34: loss 0.978696548238664
Epoch 35: loss 0.9783355717168936
Epoch 36: loss 0.977996088357773
Epoch 37: loss 0.9776240529218013
Epoch 38: loss 0.9772326361880217
Epoch 39: loss 0.9768338368404423
Epoch 40: loss 0.9764265022633062
Epoch 41: loss 0.975997466798578
Epoch 42: loss 0.9755474672556053
Epoch 43: loss 0.9750725889802303
Epoch 44: loss 0.9746055096760595
Epoch 45: loss 0.9740951546123976
Epoch 46: loss 0.973565203395468
Epoch 47: loss 0.9730111180990529
Epoch 48: loss 0.9724433388380722
Epoch 49: loss 0.9718436915761689
-----------Time: 0:04:45.337666, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.99935382604599-------------


Epoch 0: loss 15.513211407435854
Epoch 1: loss 13.073427589254706
Epoch 2: loss 7.404990645839573
Epoch 3: loss 2.969096198815765
Epoch 4: loss 1.4623523978450623
Epoch 5: loss 1.1528312654324107
Epoch 6: loss 1.056461760212119
Epoch 7: loss 1.018351069314769
Epoch 8: loss 1.0021907757844142
Epoch 9: loss 0.9948930155005774
Epoch 10: loss 0.9913058787535688
Epoch 11: loss 0.9893673336849451
Epoch 12: loss 0.98827188241216
Epoch 13: loss 0.9876069513723343
Epoch 14: loss 0.9870783458461315
Epoch 15: loss 0.9866804484766161
Epoch 16: loss 0.9863738871385638
Epoch 17: loss 0.9860215786684461
Epoch 18: loss 0.9857302442265957
Epoch 19: loss 0.985427749228257
Epoch 20: loss 0.9850526769696661
Epoch 21: loss 0.984694049078333
Epoch 22: loss 0.984260418189486
Epoch 23: loss 0.9838486631127918
Epoch 24: loss 0.9834055227063933
Epoch 25: loss 0.9828955207329979
Epoch 26: loss 0.9822848662855574
Epoch 27: loss 0.9816977703551872
Epoch 28: loss 0.9810098019094815
Epoch 29: loss 0.9802739176171963
Epoch 30: loss 0.9794443595856671
Epoch 31: loss 0.978579909435882
Epoch 32: loss 0.9776465617618592
Epoch 33: loss 0.9766169975411964
Epoch 34: loss 0.9755515132733439
Epoch 35: loss 0.9743417926687725
Epoch 36: loss 0.973116020620874
Epoch 37: loss 0.9717984253651036
Epoch 38: loss 0.9704511231738241
Epoch 39: loss 0.9690004386870741
Epoch 40: loss 0.9674883359083992
Epoch 41: loss 0.9659823115070337
Epoch 42: loss 0.9643445487188347
Epoch 43: loss 0.9627559770054633
Epoch 44: loss 0.9610267719410891
Epoch 45: loss 0.9592749592465768
Epoch 46: loss 0.9575177833656697
Epoch 47: loss 0.9556924252810849
Epoch 48: loss 0.9538371308852046
Epoch 49: loss 0.9519480435727665
-----------Time: 0:04:28.149760, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9934525489807129-------------


Epoch 0: loss 15.178358246542436
Epoch 1: loss 8.073402084430448
Epoch 2: loss 2.2308944832572086
Epoch 3: loss 1.2107875147807072
Epoch 4: loss 1.0559764140452166
Epoch 5: loss 1.0131754145925624
Epoch 6: loss 0.999594343298476
Epoch 7: loss 0.994585203729031
Epoch 8: loss 0.9926344208771798
Epoch 9: loss 0.991639238359359
Epoch 10: loss 0.9910917962744807
Epoch 11: loss 0.9907629079569805
Epoch 12: loss 0.99048785559829
Epoch 13: loss 0.9902026968653141
Epoch 14: loss 0.9899505184291821
Epoch 15: loss 0.9897312501839415
Epoch 16: loss 0.989475284827151
Epoch 17: loss 0.9891008618345981
Epoch 18: loss 0.9888075951521264
Epoch 19: loss 0.988341184722917
Epoch 20: loss 0.9879158675638212
Epoch 21: loss 0.9875076097791256
Epoch 22: loss 0.9869204995228973
Epoch 23: loss 0.986335878951709
Epoch 24: loss 0.9856413756783337
Epoch 25: loss 0.9848842479211601
Epoch 26: loss 0.9840425767932267
Epoch 27: loss 0.9830764428696729
Epoch 28: loss 0.9820849819958631
Epoch 29: loss 0.9809347312692847
Epoch 30: loss 0.9797676763760129
Epoch 31: loss 0.9784108638893073
Epoch 32: loss 0.9770191856996724
Epoch 33: loss 0.975551805462508
Epoch 34: loss 0.9739363557816589
Epoch 35: loss 0.9722206768229839
Epoch 36: loss 0.9703383332947922
Epoch 37: loss 0.9684852790806591
Epoch 38: loss 0.9664782977350515
Epoch 39: loss 0.9643275341370496
Epoch 40: loss 0.9620925418509939
Epoch 41: loss 0.9596779322805711
Epoch 42: loss 0.9571838070025193
Epoch 43: loss 0.954493707648563
Epoch 44: loss 0.9517203887692608
Epoch 45: loss 0.9486715731703762
Epoch 46: loss 0.9455198007086297
Epoch 47: loss 0.9421370860478877
Epoch 48: loss 0.9385259828533797
Epoch 49: loss 0.934828621236574
-----------Time: 0:05:03.198302, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9914987683296204-------------


Epoch 0: loss 14.311346871882693
Epoch 1: loss 4.7738606093563805
Epoch 2: loss 1.3676059608747286
Epoch 3: loss 1.0706903397097025
Epoch 4: loss 1.014540119265784
Epoch 5: loss 1.0003142802529907
Epoch 6: loss 0.9960732976660384
Epoch 7: loss 0.9945501128962145
Epoch 8: loss 0.993781330696975
Epoch 9: loss 0.9934797121688942
Epoch 10: loss 0.9932371540974507
Epoch 11: loss 0.9930570555355576
Epoch 12: loss 0.992875182187836
Epoch 13: loss 0.9926964269506341
Epoch 14: loss 0.9924921475395423
Epoch 15: loss 0.9921628369333434
Epoch 16: loss 0.992004066462618
Epoch 17: loss 0.9917912818349659
Epoch 18: loss 0.9914906413870185
Epoch 19: loss 0.9911412862800527
Epoch 20: loss 0.990780062264499
Epoch 21: loss 0.9903463162502042
Epoch 22: loss 0.9898774718512265
Epoch 23: loss 0.9893554428338097
Epoch 24: loss 0.9887681376551597
Epoch 25: loss 0.9880037269493754
Epoch 26: loss 0.9872533939596491
Epoch 27: loss 0.9863210201198604
Epoch 28: loss 0.9852690877572163
Epoch 29: loss 0.9839842980033746
Epoch 30: loss 0.9826086969269301
Epoch 31: loss 0.9809773363387734
Epoch 32: loss 0.9791274534547504
Epoch 33: loss 0.9771017585194843
Epoch 34: loss 0.9747541576964237
Epoch 35: loss 0.9721929829622366
Epoch 36: loss 0.9692566624278929
Epoch 37: loss 0.9661919953966996
Epoch 38: loss 0.9627210270976035
Epoch 39: loss 0.9590476258997687
Epoch 40: loss 0.9551443033337658
Epoch 41: loss 0.9508703834599551
Epoch 42: loss 0.9463353454256395
Epoch 43: loss 0.9416078620789297
Epoch 44: loss 0.9366044174594164
Epoch 45: loss 0.9311727369006656
Epoch 46: loss 0.9254624048544701
Epoch 47: loss 0.9193824015278217
Epoch 48: loss 0.9128643473637629
Epoch 49: loss 0.9059117998423428
-----------Time: 0:06:10.216733, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9885407090187073-------------


Epoch 0: loss 13.29225780757761
Epoch 1: loss 3.1038902518281217
Epoch 2: loss 1.1777719555178565
Epoch 3: loss 1.0322489719665717
Epoch 4: loss 1.004946587752103
Epoch 5: loss 0.9984684252687094
Epoch 6: loss 0.9965911165665776
Epoch 7: loss 0.9958639778541183
Epoch 8: loss 0.9955768459448677
Epoch 9: loss 0.99526448898694
Epoch 10: loss 0.9951169479470203
Epoch 11: loss 0.9948747655566196
Epoch 12: loss 0.9947180896106137
Epoch 13: loss 0.9945239650289153
Epoch 14: loss 0.9943677867930373
Epoch 15: loss 0.9939968590181505
Epoch 16: loss 0.9937849220469829
Epoch 17: loss 0.9935870399742168
Epoch 18: loss 0.9931936359651845
Epoch 19: loss 0.9928240149872921
Epoch 20: loss 0.9923169254089841
Epoch 21: loss 0.9919256111926006
Epoch 22: loss 0.9912292037469139
Epoch 23: loss 0.9905857976653643
Epoch 24: loss 0.9898541238464824
Epoch 25: loss 0.9888962191884578
Epoch 26: loss 0.9878156290811453
Epoch 27: loss 0.9865591118295534
Epoch 28: loss 0.9849971733772606
Epoch 29: loss 0.9833387761676098
Epoch 30: loss 0.9812723596890768
Epoch 31: loss 0.978991787319536
Epoch 32: loss 0.9763666518101425
Epoch 33: loss 0.9733974677897977
Epoch 34: loss 0.9701673746951707
Epoch 35: loss 0.9662812115901057
Epoch 36: loss 0.9622706321674304
Epoch 37: loss 0.9578453451476063
Epoch 38: loss 0.9530722324844285
Epoch 39: loss 0.9477161146364114
Epoch 40: loss 0.9420968674654024
Epoch 41: loss 0.9360479176725104
Epoch 42: loss 0.9295777879440635
Epoch 43: loss 0.9225339586803484
Epoch 44: loss 0.9148748029568327
Epoch 45: loss 0.9066295217988023
Epoch 46: loss 0.8977008221135183
Epoch 47: loss 0.8881301246563205
Epoch 48: loss 0.877655957593031
Epoch 49: loss 0.8664840462740638
-----------Time: 0:07:51.616255, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9864747524261475-------------


Epoch 0: loss 7.120473230773175
Epoch 1: loss 1.032056399342027
Epoch 2: loss 1.0087121444075182
Epoch 3: loss 0.997308972511945
Epoch 4: loss 0.9839161526515083
Epoch 5: loss 0.9703874447022915
Epoch 6: loss 0.9560997191170365
Epoch 7: loss 0.9412044919723399
Epoch 8: loss 0.9253710432179665
Epoch 9: loss 0.9093323947619718
Epoch 10: loss 0.8922918518643589
Epoch 11: loss 0.8759484142308134
Epoch 12: loss 0.8594948440352622
Epoch 13: loss 0.8438607581352786
Epoch 14: loss 0.8286549350112077
Epoch 15: loss 0.8142394585866135
Epoch 16: loss 0.8008228875878973
Epoch 17: loss 0.7888605251073708
Epoch 18: loss 0.7782512369397024
Epoch 19: loss 0.7687661597883008
Epoch 20: loss 0.7610407064893182
Epoch 21: loss 0.7541090528753673
Epoch 22: loss 0.7482865477075001
Epoch 23: loss 0.7430215187861
Epoch 24: loss 0.7385965751006981
Epoch 25: loss 0.7344779270767453
Epoch 26: loss 0.7307506274308375
Epoch 27: loss 0.7275799345295996
Epoch 28: loss 0.7245503870153505
Epoch 29: loss 0.7217147785272853
Epoch 30: loss 0.719203938278833
Epoch 31: loss 0.7168571159786475
Epoch 32: loss 0.7148002918289044
Epoch 33: loss 0.7126277816107638
Epoch 34: loss 0.7108350406852088
Epoch 35: loss 0.7087649001966293
Epoch 36: loss 0.7072053234041742
Epoch 37: loss 0.7055425568726349
Epoch 38: loss 0.7041155485889068
Epoch 39: loss 0.7024571114483574
Epoch 40: loss 0.7012615803793243
Epoch 41: loss 0.6998178226070082
Epoch 42: loss 0.698607000629172
Epoch 43: loss 0.6972405380240726
Epoch 44: loss 0.6962089825415495
Epoch 45: loss 0.6948102193399899
Epoch 46: loss 0.6939121647786031
Epoch 47: loss 0.6928578434656858
Epoch 48: loss 0.6918788871861075
Epoch 49: loss 0.6909397518654243
-----------Time: 0:03:45.105696, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1179094314575195-------------


Epoch 0: loss 4.773277248553182
Epoch 1: loss 1.0326294113981653
Epoch 2: loss 1.018242412283733
Epoch 3: loss 1.0021313181255356
Epoch 4: loss 0.9846313902059931
Epoch 5: loss 0.9627161436978081
Epoch 6: loss 0.9343328880446965
Epoch 7: loss 0.8975615679342634
Epoch 8: loss 0.8534605744871645
Epoch 9: loss 0.8041554132902862
Epoch 10: loss 0.7532410676094292
Epoch 11: loss 0.7029911666346866
Epoch 12: loss 0.6573532624955148
Epoch 13: loss 0.6185132642198089
Epoch 14: loss 0.5860599782591348
Epoch 15: loss 0.559598450494759
Epoch 16: loss 0.5385266635131422
Epoch 17: loss 0.5210236890475474
Epoch 18: loss 0.5066131574424341
Epoch 19: loss 0.49440183929284676
Epoch 20: loss 0.483899263793713
Epoch 21: loss 0.47490955522165407
Epoch 22: loss 0.46685456915223256
Epoch 23: loss 0.45974292672153655
Epoch 24: loss 0.4535307492524013
Epoch 25: loss 0.4477578247125023
Epoch 26: loss 0.4424473834854549
Epoch 27: loss 0.43766990919810655
Epoch 28: loss 0.4330783142318798
Epoch 29: loss 0.4291529834043597
Epoch 30: loss 0.4251971975690065
Epoch 31: loss 0.4217545183830251
Epoch 32: loss 0.418485434872745
Epoch 33: loss 0.41540659537063856
Epoch 34: loss 0.41237739012123387
Epoch 35: loss 0.4098056564031573
Epoch 36: loss 0.40730788036492155
Epoch 37: loss 0.4047476839863651
Epoch 38: loss 0.4024356243369241
Epoch 39: loss 0.4000701305819569
Epoch 40: loss 0.3981016766059133
Epoch 41: loss 0.39602146254342946
Epoch 42: loss 0.3940810730043736
Epoch 43: loss 0.39236720391691476
Epoch 44: loss 0.39035280868046435
Epoch 45: loss 0.3887807819718832
Epoch 46: loss 0.38712091360680756
Epoch 47: loss 0.38556393783270115
Epoch 48: loss 0.3840408726286927
Epoch 49: loss 0.38252089628387625
-----------Time: 0:04:09.577482, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3749057054519653-------------


Epoch 0: loss 3.678582613140166
Epoch 1: loss 1.0517396129234255
Epoch 2: loss 1.0361019987461033
Epoch 3: loss 1.0136838281783414
Epoch 4: loss 0.9809547990090566
Epoch 5: loss 0.928433309792824
Epoch 6: loss 0.8555277315023608
Epoch 7: loss 0.7640612132203132
Epoch 8: loss 0.6616696059995529
Epoch 9: loss 0.5624866528216751
Epoch 10: loss 0.4774411535412154
Epoch 11: loss 0.4102362442625937
Epoch 12: loss 0.35852044556632257
Epoch 13: loss 0.3195347126538887
Epoch 14: loss 0.28933990859771436
Epoch 15: loss 0.2656296604171663
Epoch 16: loss 0.2466298687986345
Epoch 17: loss 0.2309457894218169
Epoch 18: loss 0.21796906184022227
Epoch 19: loss 0.20684337602191932
Epoch 20: loss 0.19726357488300958
Epoch 21: loss 0.1889016868312635
Epoch 22: loss 0.18154995676986935
Epoch 23: loss 0.1749746323506945
Epoch 24: loss 0.16919740214093085
Epoch 25: loss 0.163902893702967
Epoch 26: loss 0.15912464202714394
Epoch 27: loss 0.1546378245677489
Epoch 28: loss 0.15077435869020636
Epoch 29: loss 0.14709858596730063
Epoch 30: loss 0.14364775177040717
Epoch 31: loss 0.14043847300910378
Epoch 32: loss 0.13757271395389511
Epoch 33: loss 0.13492634509603507
Epoch 34: loss 0.13230815535900448
Epoch 35: loss 0.12990415964909646
Epoch 36: loss 0.12764186898672822
Epoch 37: loss 0.1256529996581801
Epoch 38: loss 0.12365835214711325
Epoch 39: loss 0.12168923992706161
Epoch 40: loss 0.1200401926167054
Epoch 41: loss 0.11827744137599068
Epoch 42: loss 0.1167750095805764
Epoch 43: loss 0.11522181021366643
Epoch 44: loss 0.11375799448578303
Epoch 45: loss 0.11233396732560832
Epoch 46: loss 0.11109907966048384
Epoch 47: loss 0.10986555491681978
Epoch 48: loss 0.1085433606109002
Epoch 49: loss 0.10751078885135734
-----------Time: 0:05:35.190061, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5841968059539795-------------


Epoch 0: loss 3.2484927191690236
Epoch 1: loss 1.0640673426067524
Epoch 2: loss 1.042112068127782
Epoch 3: loss 1.009870396514766
Epoch 4: loss 0.951070129255551
Epoch 5: loss 0.8551584161319702
Epoch 6: loss 0.7263697699530738
Epoch 7: loss 0.5827320201232292
Epoch 8: loss 0.45140657093033576
Epoch 9: loss 0.34687424386695004
Epoch 10: loss 0.270844303035036
Epoch 11: loss 0.21732957528264968
Epoch 12: loss 0.1793610374845844
Epoch 13: loss 0.15198637550780797
Epoch 14: loss 0.13155934719805357
Epoch 15: loss 0.11590428120452079
Epoch 16: loss 0.10358411744804147
Epoch 17: loss 0.09360696537427243
Epoch 18: loss 0.08547366342227247
Epoch 19: loss 0.07876199038011539
Epoch 20: loss 0.07309176903086359
Epoch 21: loss 0.06815212575466559
Epoch 22: loss 0.06415908262660285
Epoch 23: loss 0.06040542300480614
Epoch 24: loss 0.05734319300423698
Epoch 25: loss 0.05446594376613291
Epoch 26: loss 0.052037480242181695
Epoch 27: loss 0.04989488550288124
Epoch 28: loss 0.047868749539025
Epoch 29: loss 0.046125080185279436
Epoch 30: loss 0.0445341766255342
Epoch 31: loss 0.04305228567365688
Epoch 32: loss 0.041864146065734666
Epoch 33: loss 0.04060561463739186
Epoch 34: loss 0.03956086987901992
Epoch 35: loss 0.03841541563487364
Epoch 36: loss 0.03753449738657688
Epoch 37: loss 0.03675822853077813
Epoch 38: loss 0.03580933613900162
Epoch 39: loss 0.03512603348772315
Epoch 40: loss 0.03437646543136137
Epoch 41: loss 0.03370557946232494
Epoch 42: loss 0.0331905590316616
Epoch 43: loss 0.03255636026741468
Epoch 44: loss 0.031962507204347616
Epoch 45: loss 0.03149252643584881
Epoch 46: loss 0.030991705275298202
Epoch 47: loss 0.0305004695480028
Epoch 48: loss 0.030058962914357373
Epoch 49: loss 0.029745505105799278
-----------Time: 0:06:54.811956, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.460922360420227-------------


Epoch 0: loss 3.025381342589434
Epoch 1: loss 1.0706772332349634
Epoch 2: loss 1.038018736682423
Epoch 3: loss 0.9827001697670966
Epoch 4: loss 0.8866245636413641
Epoch 5: loss 0.7433616797388605
Epoch 6: loss 0.5706657900344814
Epoch 7: loss 0.40967890853594024
Epoch 8: loss 0.28689621798916964
Epoch 9: loss 0.20289509112644352
Epoch 10: loss 0.1481699297217781
Epoch 11: loss 0.11338354662149869
Epoch 12: loss 0.09016441293729266
Epoch 13: loss 0.07417036481140878
Epoch 14: loss 0.0632021644696099
Epoch 15: loss 0.055220034739175784
Epoch 16: loss 0.04947614790532432
Epoch 17: loss 0.04485998095062325
Epoch 18: loss 0.041484726974745686
Epoch 19: loss 0.038841440448210314
Epoch 20: loss 0.036851073827446644
Epoch 21: loss 0.035173120290089586
Epoch 22: loss 0.03380542205744881
Epoch 23: loss 0.032561543802242035
Epoch 24: loss 0.0316746695946584
Epoch 25: loss 0.030851180007343968
Epoch 26: loss 0.030159137653852364
Epoch 27: loss 0.029553826829462095
Epoch 28: loss 0.029008432280813113
Epoch 29: loss 0.02849294553455644
Epoch 30: loss 0.02803752099454014
Epoch 31: loss 0.02754237661528127
Epoch 32: loss 0.027225468334067835
Epoch 33: loss 0.026876101146749404
Epoch 34: loss 0.026478087634877252
Epoch 35: loss 0.026211663594448157
Epoch 36: loss 0.025890580721325107
Epoch 37: loss 0.02570200288012956
Epoch 38: loss 0.025374302442037196
Epoch 39: loss 0.025126474286070528
Epoch 40: loss 0.024858802859836004
Epoch 41: loss 0.024662165515136954
Epoch 42: loss 0.024480800459879517
Epoch 43: loss 0.024243970410194392
Epoch 44: loss 0.0240355489736037
Epoch 45: loss 0.023907644929422055
Epoch 46: loss 0.0236947261335864
Epoch 47: loss 0.02353173196761423
Epoch 48: loss 0.023331735477605028
Epoch 49: loss 0.023233514895957406
-----------Time: 0:06:38.087077, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2617427110671997-------------


Epoch 0: loss 16.129268827225737
Epoch 1: loss 16.12926636706752
Epoch 2: loss 16.129272754974117
Epoch 3: loss 16.129262907081095
Epoch 4: loss 16.129261900511302
Epoch 5: loss 16.12926037328408
Epoch 6: loss 16.129257097913236
Epoch 7: loss 16.129260371209746
Epoch 8: loss 16.129259597483767
Epoch 9: loss 16.129265843298594
Epoch 10: loss 16.129252702921484
Epoch 11: loss 16.129257071465496
Epoch 12: loss 16.129253058150905
Epoch 13: loss 16.1292551568567
Epoch 14: loss 16.129251767397577
Epoch 15: loss 16.129253578808335
Epoch 16: loss 16.129250882694812
Epoch 17: loss 16.129249405251567
Epoch 18: loss 16.12924112918391
Epoch 19: loss 16.129240607489315
Epoch 20: loss 16.12924850032406
Epoch 21: loss 16.129240679053783
Epoch 22: loss 16.129243436360113
Epoch 23: loss 16.129237399534276
Epoch 24: loss 16.12923907455768
Epoch 25: loss 16.129238116216115
Epoch 26: loss 16.129237911375792
Epoch 27: loss 16.129237225290353
Epoch 28: loss 16.129235512928965
Epoch 29: loss 16.129232256745695
Epoch 30: loss 16.129236166343667
Epoch 31: loss 16.12923362321216
Epoch 32: loss 16.129227216636572
Epoch 33: loss 16.129226266592337
Epoch 34: loss 16.129225568060903
Epoch 35: loss 16.129227425106976
Epoch 36: loss 16.129223818880117
Epoch 37: loss 16.12922479226059
Epoch 38: loss 16.12922055854818
Epoch 39: loss 16.129220894590027
Epoch 40: loss 16.129221215592967
Epoch 41: loss 16.129221446362443
Epoch 42: loss 16.129214784643974
Epoch 43: loss 16.12921422872289
Epoch 44: loss 16.129213602793094
Epoch 45: loss 16.12921395698535
Epoch 46: loss 16.129213024054355
Epoch 47: loss 16.129208397774544
Epoch 48: loss 16.129212461391695
Epoch 49: loss 16.12920962111207
-----------Time: 0:03:52.499467, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017178535461426-------------


Epoch 0: loss 16.129165050969167
Epoch 1: loss 16.12916082710984
Epoch 2: loss 16.12915914482627
Epoch 3: loss 16.12916164284105
Epoch 4: loss 16.12915601258436
Epoch 5: loss 16.12915738527382
Epoch 6: loss 16.129157953640895
Epoch 7: loss 16.12915997507781
Epoch 8: loss 16.12915672408037
Epoch 9: loss 16.12915122969245
Epoch 10: loss 16.129151500911412
Epoch 11: loss 16.129146903153668
Epoch 12: loss 16.12915142364253
Epoch 13: loss 16.129150967807988
Epoch 14: loss 16.129146530811006
Epoch 15: loss 16.12914641931564
Epoch 16: loss 16.12914292406556
Epoch 17: loss 16.129141623977734
Epoch 18: loss 16.129143050081254
Epoch 19: loss 16.129140251288277
Epoch 20: loss 16.129143298482557
Epoch 21: loss 16.12914161982907
Epoch 22: loss 16.129137236246148
Epoch 23: loss 16.129136659581743
Epoch 24: loss 16.129131756897138
Epoch 25: loss 16.129133460442613
Epoch 26: loss 16.129133965023968
Epoch 27: loss 16.129127823444342
Epoch 28: loss 16.129130499333126
Epoch 29: loss 16.129125559829124
Epoch 30: loss 16.129126180054506
Epoch 31: loss 16.129128612209236
Epoch 32: loss 16.12912539232678
Epoch 33: loss 16.129124339084512
Epoch 34: loss 16.12912255412149
Epoch 35: loss 16.129122372098823
Epoch 36: loss 16.129119562934182
Epoch 37: loss 16.129120474084687
Epoch 38: loss 16.129117867686038
Epoch 39: loss 16.129115781426236
Epoch 40: loss 16.129115045038237
Epoch 41: loss 16.129115849879206
Epoch 42: loss 16.12911748704604
Epoch 43: loss 16.129108716250116
Epoch 44: loss 16.129110075974996
Epoch 45: loss 16.12910945678678
Epoch 46: loss 16.129108944426676
Epoch 47: loss 16.12910995825663
Epoch 48: loss 16.12910874892085
Epoch 49: loss 16.12910403240758
-----------Time: 0:04:46.729203, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.0171942710876465-------------


Epoch 0: loss 16.12917576593313
Epoch 1: loss 16.129170644406457
Epoch 2: loss 16.12916759098918
Epoch 3: loss 16.129166955724884
Epoch 4: loss 16.129167857540892
Epoch 5: loss 16.129165659785723
Epoch 6: loss 16.129161097810215
Epoch 7: loss 16.12916712893164
Epoch 8: loss 16.129161109737627
Epoch 9: loss 16.129159273953462
Epoch 10: loss 16.129158283459745
Epoch 11: loss 16.12915634344038
Epoch 12: loss 16.129157432464883
Epoch 13: loss 16.129157791324385
Epoch 14: loss 16.129156988039167
Epoch 15: loss 16.12915175138705
Epoch 16: loss 16.12915104870695
Epoch 17: loss 16.129151038853873
Epoch 18: loss 16.129152093651893
Epoch 19: loss 16.129153708001073
Epoch 20: loss 16.129150841792296
Epoch 21: loss 16.129148128046946
Epoch 22: loss 16.12914234117816
Epoch 23: loss 16.129141104876055
Epoch 24: loss 16.129143533919283
Epoch 25: loss 16.129140689490995
Epoch 26: loss 16.12913826770793
Epoch 27: loss 16.129137264768218
Epoch 28: loss 16.129135982312217
Epoch 29: loss 16.129135850073528
Epoch 30: loss 16.129135054048472
Epoch 31: loss 16.129133015498315
Epoch 32: loss 16.129132651971567
Epoch 33: loss 16.129133696916508
Epoch 34: loss 16.129129519729656
Epoch 35: loss 16.129128213418834
Epoch 36: loss 16.129124907451587
Epoch 37: loss 16.129124128539775
Epoch 38: loss 16.12913198144362
Epoch 39: loss 16.129122608572715
Epoch 40: loss 16.12912152991987
Epoch 41: loss 16.129123735972367
Epoch 42: loss 16.12911557140008
Epoch 43: loss 16.129114443481843
Epoch 44: loss 16.129116067165523
Epoch 45: loss 16.12911542723398
Epoch 46: loss 16.129115493612616
Epoch 47: loss 16.12911233492297
Epoch 48: loss 16.12911314546835
Epoch 49: loss 16.12911113958893
-----------Time: 0:06:23.144890, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129171301969826
Epoch 1: loss 16.12917189522889
Epoch 2: loss 16.129169048207682
Epoch 3: loss 16.129166645612194
Epoch 4: loss 16.129168362122247
Epoch 5: loss 16.129164957105626
Epoch 6: loss 16.12916005545819
Epoch 7: loss 16.12916343402707
Epoch 8: loss 16.129161025727164
Epoch 9: loss 16.129163133248873
Epoch 10: loss 16.129153120446425
Epoch 11: loss 16.129157314227935
Epoch 12: loss 16.129153426929037
Epoch 13: loss 16.12915348604751
Epoch 14: loss 16.129157543960247
Epoch 15: loss 16.12915605458959
Epoch 16: loss 16.12915538821031
Epoch 17: loss 16.129148302809448
Epoch 18: loss 16.12914919373521
Epoch 19: loss 16.129141388541008
Epoch 20: loss 16.12914436520799
Epoch 21: loss 16.129146648529368
Epoch 22: loss 16.1291444139548
Epoch 23: loss 16.129144772814303
Epoch 24: loss 16.129138456472177
Epoch 25: loss 16.129141639016645
Epoch 26: loss 16.129142499346006
Epoch 27: loss 16.129134659406734
Epoch 28: loss 16.1291345562087
Epoch 29: loss 16.12913250728688
Epoch 30: loss 16.12913125127862
Epoch 31: loss 16.129133056984966
Epoch 32: loss 16.129131749118393
Epoch 33: loss 16.129131300544014
Epoch 34: loss 16.129128364326515
Epoch 35: loss 16.129127415319445
Epoch 36: loss 16.12912473061475
Epoch 37: loss 16.129124968125808
Epoch 38: loss 16.12912155792336
Epoch 39: loss 16.129123988003755
Epoch 40: loss 16.1291209185104
Epoch 41: loss 16.129119004420186
Epoch 42: loss 16.129121451613823
Epoch 43: loss 16.129120869245007
Epoch 44: loss 16.129117306579126
Epoch 45: loss 16.129113011673912
Epoch 46: loss 16.1291118614566
Epoch 47: loss 16.12911704158316
Epoch 48: loss 16.129115647631796
Epoch 49: loss 16.129109515386666
-----------Time: 0:05:19.116761, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129180091434748
Epoch 1: loss 16.129175147263496
Epoch 2: loss 16.129175885207243
Epoch 3: loss 16.1291736008487
Epoch 4: loss 16.12917492530993
Epoch 5: loss 16.12917263369122
Epoch 6: loss 16.12917224319815
Epoch 7: loss 16.129171862558156
Epoch 8: loss 16.12917318287072
Epoch 9: loss 16.129168744836573
Epoch 10: loss 16.129166212076722
Epoch 11: loss 16.129161668251623
Epoch 12: loss 16.1291668110402
Epoch 13: loss 16.129159738085335
Epoch 14: loss 16.12916766618373
Epoch 15: loss 16.129156831945654
Epoch 16: loss 16.129154280516815
Epoch 17: loss 16.129156907658786
Epoch 18: loss 16.129151243175613
Epoch 19: loss 16.129150690366032
Epoch 20: loss 16.129151811024105
Epoch 21: loss 16.129153053030624
Epoch 22: loss 16.12915100047872
Epoch 23: loss 16.129152534447524
Epoch 24: loss 16.129148676707864
Epoch 25: loss 16.129144965208635
Epoch 26: loss 16.1291439752335
Epoch 27: loss 16.129141557599098
Epoch 28: loss 16.12914844023397
Epoch 29: loss 16.12913839320504
Epoch 30: loss 16.12914130660488
Epoch 31: loss 16.129134639181995
Epoch 32: loss 16.129135189917246
Epoch 33: loss 16.129133306423434
Epoch 34: loss 16.12913326545537
Epoch 35: loss 16.129134307288812
Epoch 36: loss 16.12913013528779
Epoch 37: loss 16.129133722845662
Epoch 38: loss 16.12913434255246
Epoch 39: loss 16.129132638488404
Epoch 40: loss 16.129131246611372
Epoch 41: loss 16.12912633407369
Epoch 42: loss 16.129128394404333
Epoch 43: loss 16.129121510732297
Epoch 44: loss 16.129127714541895
Epoch 45: loss 16.12912494323382
Epoch 46: loss 16.129119736140936
Epoch 47: loss 16.12911823328712
Epoch 48: loss 16.129119981949327
Epoch 49: loss 16.12911937313277
-----------Time: 0:06:56.501572, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.0171799659729-------------


Epoch 0: loss 16.129349677960725
Epoch 1: loss 16.129336824878653
Epoch 2: loss 16.129326212594144
Epoch 3: loss 16.12930822605799
Epoch 4: loss 16.12929927220223
Epoch 5: loss 16.129283325771976
Epoch 6: loss 16.12927231832715
Epoch 7: loss 16.12925939056911
Epoch 8: loss 16.129249557196413
Epoch 9: loss 16.129237369975037
Epoch 10: loss 16.12922302233648
Epoch 11: loss 16.12921372310437
Epoch 12: loss 16.129198272958146
Epoch 13: loss 16.12919154330529
Epoch 14: loss 16.12917360136728
Epoch 15: loss 16.12916249331733
Epoch 16: loss 16.12914808189305
Epoch 17: loss 16.129137955520903
Epoch 18: loss 16.129126535802513
Epoch 19: loss 16.129115187130004
Epoch 20: loss 16.129097009236688
Epoch 21: loss 16.12908936584041
Epoch 22: loss 16.129076310510932
Epoch 23: loss 16.129066388460526
Epoch 24: loss 16.12905509372066
Epoch 25: loss 16.129039355242234
Epoch 26: loss 16.129026566983047
Epoch 27: loss 16.129017919609897
Epoch 28: loss 16.128997220365555
Epoch 29: loss 16.12899168397241
Epoch 30: loss 16.1289761995997
Epoch 31: loss 16.12896396829876
Epoch 32: loss 16.12895046180198
Epoch 33: loss 16.128938555134063
Epoch 34: loss 16.12892888304071
Epoch 35: loss 16.128916826502273
Epoch 36: loss 16.128900190356504
Epoch 37: loss 16.12889242976045
Epoch 38: loss 16.128877312025992
Epoch 39: loss 16.12886673085647
Epoch 40: loss 16.1288533498568
Epoch 41: loss 16.128846426772448
Epoch 42: loss 16.128834524771776
Epoch 43: loss 16.128816028468435
Epoch 44: loss 16.128805140816304
Epoch 45: loss 16.128793589377803
Epoch 46: loss 16.12877855461664
Epoch 47: loss 16.128771447435287
Epoch 48: loss 16.128753396594828
Epoch 49: loss 16.128743757172213
-----------Time: 0:04:45.351948, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017102241516113-------------


Epoch 0: loss 16.129159487091115
Epoch 1: loss 16.129147606352348
Epoch 2: loss 16.129140594590286
Epoch 3: loss 16.129126274436633
Epoch 4: loss 16.129115543915177
Epoch 5: loss 16.129101302067568
Epoch 6: loss 16.129085009742393
Epoch 7: loss 16.129075862455135
Epoch 8: loss 16.1290643919156
Epoch 9: loss 16.12904725170706
Epoch 10: loss 16.12903778082395
Epoch 11: loss 16.1290228222945
Epoch 12: loss 16.129014405172242
Epoch 13: loss 16.1290035455236
Epoch 14: loss 16.12898995190486
Epoch 15: loss 16.1289745764346
Epoch 16: loss 16.12896738213129
Epoch 17: loss 16.12894773249914
Epoch 18: loss 16.12893805781287
Epoch 19: loss 16.128926287013723
Epoch 20: loss 16.12891866747227
Epoch 21: loss 16.128901099951257
Epoch 22: loss 16.128891076258565
Epoch 23: loss 16.12887848039371
Epoch 24: loss 16.128864965599604
Epoch 25: loss 16.128848980794203
Epoch 26: loss 16.12884040654127
Epoch 27: loss 16.128828927185822
Epoch 28: loss 16.128814674966552
Epoch 29: loss 16.128798272701758
Epoch 30: loss 16.128790356012193
Epoch 31: loss 16.12877996256975
Epoch 32: loss 16.128765982088023
Epoch 33: loss 16.128753399187744
Epoch 34: loss 16.12874345224535
Epoch 35: loss 16.128727886973678
Epoch 36: loss 16.128717322917396
Epoch 37: loss 16.12870143560562
Epoch 38: loss 16.128695399816948
Epoch 39: loss 16.128677013971807
Epoch 40: loss 16.128666287080435
Epoch 41: loss 16.128653779893288
Epoch 42: loss 16.12863567563877
Epoch 43: loss 16.128628199744835
Epoch 44: loss 16.128614930240534
Epoch 45: loss 16.128603521412387
Epoch 46: loss 16.128594232551944
Epoch 47: loss 16.128582011622665
Epoch 48: loss 16.128569056379725
Epoch 49: loss 16.12855278790937
-----------Time: 0:05:14.281129, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017115592956543-------------


Epoch 0: loss 16.1291760890104
Epoch 1: loss 16.129160385795622
Epoch 2: loss 16.12914890384726
Epoch 3: loss 16.12914149744346
Epoch 4: loss 16.129125690512062
Epoch 5: loss 16.12911372628142
Epoch 6: loss 16.129102024972408
Epoch 7: loss 16.12909262669093
Epoch 8: loss 16.129076735230488
Epoch 9: loss 16.129070250867436
Epoch 10: loss 16.129050529152234
Epoch 11: loss 16.12904233813221
Epoch 12: loss 16.129025190144922
Epoch 13: loss 16.129019449948615
Epoch 14: loss 16.129000335494222
Epoch 15: loss 16.128991926150714
Epoch 16: loss 16.128975809625206
Epoch 17: loss 16.128965937877364
Epoch 18: loss 16.12895474581695
Epoch 19: loss 16.128936353748813
Epoch 20: loss 16.12892758191572
Epoch 21: loss 16.128922043448235
Epoch 22: loss 16.128905125711846
Epoch 23: loss 16.128890833043094
Epoch 24: loss 16.128882312204215
Epoch 25: loss 16.12886951927778
Epoch 26: loss 16.128857045798537
Epoch 27: loss 16.128842226249358
Epoch 28: loss 16.128827514046883
Epoch 29: loss 16.12881747531528
Epoch 30: loss 16.128806922149245
Epoch 31: loss 16.128789721785065
Epoch 32: loss 16.128779729207356
Epoch 33: loss 16.12876297793614
Epoch 34: loss 16.128754735057804
Epoch 35: loss 16.12874275060242
Epoch 36: loss 16.128733032873754
Epoch 37: loss 16.128719887310815
Epoch 38: loss 16.128703209159813
Epoch 39: loss 16.128695069479516
Epoch 40: loss 16.12867747654793
Epoch 41: loss 16.12866923626251
Epoch 42: loss 16.128653006167305
Epoch 43: loss 16.128640686188657
Epoch 44: loss 16.128631682548917
Epoch 45: loss 16.12861635738122
Epoch 46: loss 16.128605976384772
Epoch 47: loss 16.128594929009044
Epoch 48: loss 16.12858193176087
Epoch 49: loss 16.128572944197206
-----------Time: 0:04:38.680883, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017114639282227-------------


Epoch 0: loss 16.129174654090967
Epoch 1: loss 16.12916106669523
Epoch 2: loss 16.12914474325507
Epoch 3: loss 16.129132412386173
Epoch 4: loss 16.12911967131805
Epoch 5: loss 16.12910446957313
Epoch 6: loss 16.129092312948156
Epoch 7: loss 16.12908415408028
Epoch 8: loss 16.129069740063084
Epoch 9: loss 16.129060217840248
Epoch 10: loss 16.1290447070198
Epoch 11: loss 16.129034891797513
Epoch 12: loss 16.12902416179464
Epoch 13: loss 16.129006997212695
Epoch 14: loss 16.128994109385555
Epoch 15: loss 16.128986694165842
Epoch 16: loss 16.128970880492865
Epoch 17: loss 16.1289628839415
Epoch 18: loss 16.128947475281922
Epoch 19: loss 16.128930663855066
Epoch 20: loss 16.128919612330673
Epoch 21: loss 16.128907522602923
Epoch 22: loss 16.128894924663733
Epoch 23: loss 16.128883900624245
Epoch 24: loss 16.12886911841305
Epoch 25: loss 16.128860546234446
Epoch 26: loss 16.12884918615311
Epoch 27: loss 16.128835758480967
Epoch 28: loss 16.12882348413763
Epoch 29: loss 16.128813101585433
Epoch 30: loss 16.12879341305955
Epoch 31: loss 16.128784224285646
Epoch 32: loss 16.128771242076382
Epoch 33: loss 16.12875663307194
Epoch 34: loss 16.12874292847642
Epoch 35: loss 16.12873766796948
Epoch 36: loss 16.128722703217033
Epoch 37: loss 16.128710485399257
Epoch 38: loss 16.128699522552573
Epoch 39: loss 16.128684058923188
Epoch 40: loss 16.12867334603356
Epoch 41: loss 16.128660775579274
Epoch 42: loss 16.128647995098834
Epoch 43: loss 16.128635360340244
Epoch 44: loss 16.128623634657828
Epoch 45: loss 16.12861049324355
Epoch 46: loss 16.128601697555634
Epoch 47: loss 16.12858448785696
Epoch 48: loss 16.128574793464534
Epoch 49: loss 16.12855933190948
-----------Time: 0:05:51.096174, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017111301422119-------------


Epoch 0: loss 16.129168588224477
Epoch 1: loss 16.12915631751122
Epoch 2: loss 16.12914230280301
Epoch 3: loss 16.12913056311885
Epoch 4: loss 16.129113850741362
Epoch 5: loss 16.129110032932598
Epoch 6: loss 16.12908997621272
Epoch 7: loss 16.12908066349745
Epoch 8: loss 16.129067149740514
Epoch 9: loss 16.129054454826285
Epoch 10: loss 16.129046958189026
Epoch 11: loss 16.129029618844577
Epoch 12: loss 16.129018831797563
Epoch 13: loss 16.129004239387783
Epoch 14: loss 16.128995655281766
Epoch 15: loss 16.128978941867118
Epoch 16: loss 16.128969232435782
Epoch 17: loss 16.12895946336739
Epoch 18: loss 16.128942632234374
Epoch 19: loss 16.128933558067335
Epoch 20: loss 16.128919143531554
Epoch 21: loss 16.128908156311468
Epoch 22: loss 16.12889786710423
Epoch 23: loss 16.128886351447964
Epoch 24: loss 16.128872012625315
Epoch 25: loss 16.128855900767057
Epoch 26: loss 16.128841913543752
Epoch 27: loss 16.128832290715792
Epoch 28: loss 16.128820809804594
Epoch 29: loss 16.12881157539538
Epoch 30: loss 16.128796186441956
Epoch 31: loss 16.128782707430087
Epoch 32: loss 16.128772392812277
Epoch 33: loss 16.128757090462233
Epoch 34: loss 16.128746771177173
Epoch 35: loss 16.128730921721964
Epoch 36: loss 16.128722707884283
Epoch 37: loss 16.128707887297935
Epoch 38: loss 16.128693187022872
Epoch 39: loss 16.12868166877369
Epoch 40: loss 16.12867129711174
Epoch 41: loss 16.128654797353324
Epoch 42: loss 16.12865079544756
Epoch 43: loss 16.128631864571584
Epoch 44: loss 16.128620557385727
Epoch 45: loss 16.128608665756715
Epoch 46: loss 16.12859612019442
Epoch 47: loss 16.128584231158325
Epoch 48: loss 16.128571775310903
Epoch 49: loss 16.12855952948964
-----------Time: 0:07:24.391755, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017111778259277-------------


Epoch 0: loss 16.12934280777185
Epoch 1: loss 16.129221918791647
Epoch 2: loss 16.129094687021578
Epoch 3: loss 16.128973843676686
Epoch 4: loss 16.128850109749607
Epoch 5: loss 16.12872487400588
Epoch 6: loss 16.12860301320095
Epoch 7: loss 16.128483515579198
Epoch 8: loss 16.128357981131607
Epoch 9: loss 16.128233309087708
Epoch 10: loss 16.128102191574488
Epoch 11: loss 16.127986266990277
Epoch 12: loss 16.1278614284812
Epoch 13: loss 16.12773421849162
Epoch 14: loss 16.127611614557114
Epoch 15: loss 16.127486932660133
Epoch 16: loss 16.127362501238792
Epoch 17: loss 16.127240211565642
Epoch 18: loss 16.127115146435752
Epoch 19: loss 16.126994564975327
Epoch 20: loss 16.12686735706008
Epoch 21: loss 16.126741491237887
Epoch 22: loss 16.12661603457776
Epoch 23: loss 16.126493655189734
Epoch 24: loss 16.126372353417384
Epoch 25: loss 16.126248287597125
Epoch 26: loss 16.126124263263513
Epoch 27: loss 16.126001479380673
Epoch 28: loss 16.1258718361992
Epoch 29: loss 16.12575595517597
Epoch 30: loss 16.125633046833183
Epoch 31: loss 16.125501592240948
Epoch 32: loss 16.12537611224458
Epoch 33: loss 16.125254317299706
Epoch 34: loss 16.125129698151284
Epoch 35: loss 16.125002835093795
Epoch 36: loss 16.124884965822968
Epoch 37: loss 16.12476092385753
Epoch 38: loss 16.124631975577408
Epoch 39: loss 16.124515960241155
Epoch 40: loss 16.124392661405295
Epoch 41: loss 16.124266838625502
Epoch 42: loss 16.124143753964464
Epoch 43: loss 16.12401066531689
Epoch 44: loss 16.12389167538799
Epoch 45: loss 16.12376696859902
Epoch 46: loss 16.123645925081053
Epoch 47: loss 16.12352022105679
Epoch 48: loss 16.123395364397307
Epoch 49: loss 16.123273612494827
-----------Time: 0:04:43.144804, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016418933868408-------------


Epoch 0: loss 16.12908689116187
Epoch 1: loss 16.128967864932154
Epoch 2: loss 16.128843191851086
Epoch 3: loss 16.12871644806771
Epoch 4: loss 16.128592033241027
Epoch 5: loss 16.12847056967075
Epoch 6: loss 16.12834843557253
Epoch 7: loss 16.12822373033931
Epoch 8: loss 16.12809510046921
Epoch 9: loss 16.127971994546265
Epoch 10: loss 16.127845337366267
Epoch 11: loss 16.127728393247686
Epoch 12: loss 16.127603024746687
Epoch 13: loss 16.127477381915227
Epoch 14: loss 16.127349897595188
Epoch 15: loss 16.127231819335375
Epoch 16: loss 16.12710679517355
Epoch 17: loss 16.126980046722927
Epoch 18: loss 16.126857202165862
Epoch 19: loss 16.12673451059081
Epoch 20: loss 16.12661068020728
Epoch 21: loss 16.12648570531085
Epoch 22: loss 16.12635968391373
Epoch 23: loss 16.126236203573786
Epoch 24: loss 16.126111812601923
Epoch 25: loss 16.125990990000357
Epoch 26: loss 16.12586570758415
Epoch 27: loss 16.12574064193568
Epoch 28: loss 16.12561234395876
Epoch 29: loss 16.125494346597907
Epoch 30: loss 16.12536856478618
Epoch 31: loss 16.125247373472032
Epoch 32: loss 16.12511822968608
Epoch 33: loss 16.124999205012113
Epoch 34: loss 16.12487739087966
Epoch 35: loss 16.12475198192918
Epoch 36: loss 16.124627133567024
Epoch 37: loss 16.124501666535238
Epoch 38: loss 16.124380132956244
Epoch 39: loss 16.124254635846633
Epoch 40: loss 16.12412759387798
Epoch 41: loss 16.12400772028348
Epoch 42: loss 16.12388091064005
Epoch 43: loss 16.123764558224785
Epoch 44: loss 16.123632073207936
Epoch 45: loss 16.12351000859985
Epoch 46: loss 16.123386589971297
Epoch 47: loss 16.123265021647235
Epoch 48: loss 16.123131565324243
Epoch 49: loss 16.123011120769757
-----------Time: 0:03:49.635732, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016414165496826-------------


Epoch 0: loss 16.129111306572685
Epoch 1: loss 16.128994560034265
Epoch 2: loss 16.128870454801692
Epoch 3: loss 16.128744863309958
Epoch 4: loss 16.12862072592523
Epoch 5: loss 16.12849595483196
Epoch 6: loss 16.128372890395664
Epoch 7: loss 16.12824862129083
Epoch 8: loss 16.128128629459383
Epoch 9: loss 16.127997811168612
Epoch 10: loss 16.127879317523735
Epoch 11: loss 16.12775015973604
Epoch 12: loss 16.127626805930372
Epoch 13: loss 16.127504955497105
Epoch 14: loss 16.127383567639964
Epoch 15: loss 16.127255552290833
Epoch 16: loss 16.127134978609153
Epoch 17: loss 16.12700925176723
Epoch 18: loss 16.126882068743974
Epoch 19: loss 16.126759411395405
Epoch 20: loss 16.126635144883487
Epoch 21: loss 16.126515258842996
Epoch 22: loss 16.126389027938302
Epoch 23: loss 16.126262992539438
Epoch 24: loss 16.126141670023767
Epoch 25: loss 16.12601667853268
Epoch 26: loss 16.125897469243128
Epoch 27: loss 16.12576552199695
Epoch 28: loss 16.12564195972088
Epoch 29: loss 16.125523532973222
Epoch 30: loss 16.125397793166723
Epoch 31: loss 16.12527094099948
Epoch 32: loss 16.125155047530257
Epoch 33: loss 16.125024987407972
Epoch 34: loss 16.124907121767226
Epoch 35: loss 16.124775326984484
Epoch 36: loss 16.124654893320237
Epoch 37: loss 16.124531326376918
Epoch 38: loss 16.124409210429107
Epoch 39: loss 16.124281811675278
Epoch 40: loss 16.124157070659827
Epoch 41: loss 16.124040312712577
Epoch 42: loss 16.123911015944614
Epoch 43: loss 16.123786815811332
Epoch 44: loss 16.123662439359798
Epoch 45: loss 16.123540885037478
Epoch 46: loss 16.123411054647505
Epoch 47: loss 16.12329152850368
Epoch 48: loss 16.123164846431695
Epoch 49: loss 16.123046154688076
-----------Time: 0:05:17.154419, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016421318054199-------------


Epoch 0: loss 16.129117029655752
Epoch 1: loss 16.12899722606997
Epoch 2: loss 16.128870621266866
Epoch 3: loss 16.128740117756035
Epoch 4: loss 16.128619780029663
Epoch 5: loss 16.128499375406072
Epoch 6: loss 16.128374240786048
Epoch 7: loss 16.12824631463321
Epoch 8: loss 16.128126025135067
Epoch 9: loss 16.127999850755934
Epoch 10: loss 16.12787490904882
Epoch 11: loss 16.127752996904167
Epoch 12: loss 16.127630327109607
Epoch 13: loss 16.12750532265394
Epoch 14: loss 16.12738135380872
Epoch 15: loss 16.127259699399865
Epoch 16: loss 16.12713105034219
Epoch 17: loss 16.127002397654433
Epoch 18: loss 16.126886368316434
Epoch 19: loss 16.12676103611625
Epoch 20: loss 16.126635891643147
Epoch 21: loss 16.12650908044397
Epoch 22: loss 16.12638442499473
Epoch 23: loss 16.126265315791716
Epoch 24: loss 16.12613716405523
Epoch 25: loss 16.126016589336384
Epoch 26: loss 16.125893155150337
Epoch 27: loss 16.125765520959785
Epoch 28: loss 16.12564054865627
Epoch 29: loss 16.125520487853272
Epoch 30: loss 16.12539769359877
Epoch 31: loss 16.125271166064547
Epoch 32: loss 16.12514612012223
Epoch 33: loss 16.125021423704926
Epoch 34: loss 16.124894566351852
Epoch 35: loss 16.124776672707622
Epoch 36: loss 16.124648051134848
Epoch 37: loss 16.12452978825945
Epoch 38: loss 16.12440204724078
Epoch 39: loss 16.1242814367397
Epoch 40: loss 16.124152681891072
Epoch 41: loss 16.124037412277314
Epoch 42: loss 16.12390724843841
Epoch 43: loss 16.123782238796913
Epoch 44: loss 16.1236542566371
Epoch 45: loss 16.123535606898713
Epoch 46: loss 16.12341375698403
Epoch 47: loss 16.123289649677123
Epoch 48: loss 16.123167882735732
Epoch 49: loss 16.123039462373203
-----------Time: 0:06:32.914545, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.01641845703125-------------


Epoch 0: loss 16.12911536189251
Epoch 1: loss 16.128991483799332
Epoch 2: loss 16.128871156963204
Epoch 3: loss 16.128742885952608
Epoch 4: loss 16.128624361711328
Epoch 5: loss 16.128494976265653
Epoch 6: loss 16.128371896271865
Epoch 7: loss 16.128250897870625
Epoch 8: loss 16.12812553144396
Epoch 9: loss 16.128002199937367
Epoch 10: loss 16.12787356747435
Epoch 11: loss 16.127750066391084
Epoch 12: loss 16.127626390545313
Epoch 13: loss 16.12750234754271
Epoch 14: loss 16.127379243694097
Epoch 15: loss 16.127257501644696
Epoch 16: loss 16.127133420266944
Epoch 17: loss 16.127009957040244
Epoch 18: loss 16.126884960363324
Epoch 19: loss 16.126757118739533
Epoch 20: loss 16.126639538838074
Epoch 21: loss 16.126513186584937
Epoch 22: loss 16.126385661815416
Epoch 23: loss 16.12626489781374
Epoch 24: loss 16.12613462247947
Epoch 25: loss 16.126009512751434
Epoch 26: loss 16.12589073284869
Epoch 27: loss 16.12576869624409
Epoch 28: loss 16.125644257562584
Epoch 29: loss 16.125525609898528
Epoch 30: loss 16.125399664733123
Epoch 31: loss 16.125271035900187
Epoch 32: loss 16.12514599410654
Epoch 33: loss 16.12502751342624
Epoch 34: loss 16.12490093766379
Epoch 35: loss 16.124778476339635
Epoch 36: loss 16.12465194517533
Epoch 37: loss 16.124528578923666
Epoch 38: loss 16.124403379480757
Epoch 39: loss 16.124278846417127
Epoch 40: loss 16.124153783880153
Epoch 41: loss 16.124030712702275
Epoch 42: loss 16.123906832534765
Epoch 43: loss 16.123782833611724
Epoch 44: loss 16.123663637805336
Epoch 45: loss 16.123536278463824
Epoch 46: loss 16.123407719639605
Epoch 47: loss 16.123287246044463
Epoch 48: loss 16.123160725770404
Epoch 49: loss 16.123039875683933
-----------Time: 0:07:21.301028, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016421318054199-------------


Epoch 0: loss 16.128607867138744
Epoch 1: loss 16.127373055441993
Epoch 2: loss 16.126130309942432
Epoch 3: loss 16.124894684070217
Epoch 4: loss 16.123656280148346
Epoch 5: loss 16.122414973198296
Epoch 6: loss 16.121178301343974
Epoch 7: loss 16.119935348411175
Epoch 8: loss 16.11869210818334
Epoch 9: loss 16.117452346092335
Epoch 10: loss 16.11621461892141
Epoch 11: loss 16.114976177661557
Epoch 12: loss 16.113741482646002
Epoch 13: loss 16.112494712941604
Epoch 14: loss 16.1112602341752
Epoch 15: loss 16.11002701452856
Epoch 16: loss 16.10878310947719
Epoch 17: loss 16.107545669082715
Epoch 18: loss 16.106303245104677
Epoch 19: loss 16.105069228914395
Epoch 20: loss 16.103827798022984
Epoch 21: loss 16.102591821070014
Epoch 22: loss 16.101349579633222
Epoch 23: loss 16.100110930421547
Epoch 24: loss 16.09887527343435
Epoch 25: loss 16.09764252880982
Epoch 26: loss 16.096398875271255
Epoch 27: loss 16.095159199265048
Epoch 28: loss 16.093921393269493
Epoch 29: loss 16.092684854691026
Epoch 30: loss 16.091445325443836
Epoch 31: loss 16.090212803291458
Epoch 32: loss 16.088975829103187
Epoch 33: loss 16.08773677072945
Epoch 34: loss 16.086498751077656
Epoch 35: loss 16.085258769107423
Epoch 36: loss 16.084026740646156
Epoch 37: loss 16.082785190999218
Epoch 38: loss 16.081545820438453
Epoch 39: loss 16.080310330435008
Epoch 40: loss 16.07907146289985
Epoch 41: loss 16.07783230184666
Epoch 42: loss 16.076594293085112
Epoch 43: loss 16.07535812373781
Epoch 44: loss 16.074120561994892
Epoch 45: loss 16.072884282189392
Epoch 46: loss 16.071643672215025
Epoch 47: loss 16.070409221970692
Epoch 48: loss 16.069170801454163
Epoch 49: loss 16.067935803586078
-----------Time: 0:03:27.739612, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009551048278809-------------


Epoch 0: loss 16.128553661203306
Epoch 1: loss 16.12731649513929
Epoch 2: loss 16.126072993545574
Epoch 3: loss 16.124837159721537
Epoch 4: loss 16.123599665913005
Epoch 5: loss 16.122358014105195
Epoch 6: loss 16.121119544841857
Epoch 7: loss 16.11987541628117
Epoch 8: loss 16.11863979922487
Epoch 9: loss 16.117392941879928
Epoch 10: loss 16.116160592415724
Epoch 11: loss 16.1149221594532
Epoch 12: loss 16.11368497057153
Epoch 13: loss 16.112440158518325
Epoch 14: loss 16.11120424535107
Epoch 15: loss 16.10996692371213
Epoch 16: loss 16.10872610889744
Epoch 17: loss 16.10748471482543
Epoch 18: loss 16.106242417381665
Epoch 19: loss 16.105007868606542
Epoch 20: loss 16.103766456384125
Epoch 21: loss 16.102529406482724
Epoch 22: loss 16.101291684497628
Epoch 23: loss 16.100047792410837
Epoch 24: loss 16.098810121765467
Epoch 25: loss 16.09757460583287
Epoch 26: loss 16.096339058785286
Epoch 27: loss 16.095098327462473
Epoch 28: loss 16.093858333046246
Epoch 29: loss 16.09262237579943
Epoch 30: loss 16.091383315869944
Epoch 31: loss 16.090140974346614
Epoch 32: loss 16.088907691951416
Epoch 33: loss 16.087669168755436
Epoch 34: loss 16.086426366211732
Epoch 35: loss 16.085197878635856
Epoch 36: loss 16.083956288539436
Epoch 37: loss 16.082715152203228
Epoch 38: loss 16.081476096940985
Epoch 39: loss 16.08024006553679
Epoch 40: loss 16.07900053680818
Epoch 41: loss 16.077763910070587
Epoch 42: loss 16.07652138808034
Epoch 43: loss 16.07528588303799
Epoch 44: loss 16.074049028642413
Epoch 45: loss 16.072813179779466
Epoch 46: loss 16.071573285449777
Epoch 47: loss 16.07033635689682
Epoch 48: loss 16.0690998966244
Epoch 49: loss 16.06786241318753
-----------Time: 0:04:18.904373, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.009512424468994-------------


Epoch 0: loss 16.128559222488445
Epoch 1: loss 16.127316340601528
Epoch 2: loss 16.126078905392887
Epoch 3: loss 16.12483593482826
Epoch 4: loss 16.123600088039645
Epoch 5: loss 16.12236295205345
Epoch 6: loss 16.12111933948295
Epoch 7: loss 16.119883350602564
Epoch 8: loss 16.118640137859632
Epoch 9: loss 16.117401237135155
Epoch 10: loss 16.116164246352227
Epoch 11: loss 16.114925881842677
Epoch 12: loss 16.11368377160741
Epoch 13: loss 16.112445476069407
Epoch 14: loss 16.11120497707182
Epoch 15: loss 16.109964122326236
Epoch 16: loss 16.108724077607445
Epoch 17: loss 16.107493046900057
Epoch 18: loss 16.106247142192267
Epoch 19: loss 16.105009455989403
Epoch 20: loss 16.103769035816452
Epoch 21: loss 16.102528761883935
Epoch 22: loss 16.10129298043679
Epoch 23: loss 16.100050098549872
Epoch 24: loss 16.09881713871336
Epoch 25: loss 16.09757745492841
Epoch 26: loss 16.09633925221678
Epoch 27: loss 16.095089980348938
Epoch 28: loss 16.093859586461594
Epoch 29: loss 16.09262191477906
Epoch 30: loss 16.09138647611534
Epoch 31: loss 16.09014703087861
Epoch 32: loss 16.0889093861624
Epoch 33: loss 16.087668567717625
Epoch 34: loss 16.08643260528498
Epoch 35: loss 16.085189134287667
Epoch 36: loss 16.083956284390773
Epoch 37: loss 16.08271493543549
Epoch 38: loss 16.0814799837213
Epoch 39: loss 16.080240274007195
Epoch 40: loss 16.079002400595833
Epoch 41: loss 16.077766101084176
Epoch 42: loss 16.076530468988963
Epoch 43: loss 16.075287213722216
Epoch 44: loss 16.074054242476876
Epoch 45: loss 16.072814799833065
Epoch 46: loss 16.07157743618889
Epoch 47: loss 16.07033482707668
Epoch 48: loss 16.069099962484454
Epoch 49: loss 16.067865171531025
-----------Time: 0:05:54.413373, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009507179260254-------------


Epoch 0: loss 16.128561410909118
Epoch 1: loss 16.127314817522972
Epoch 2: loss 16.126072602533917
Epoch 3: loss 16.124830693508894
Epoch 4: loss 16.123593728136537
Epoch 5: loss 16.122355077887697
Epoch 6: loss 16.12111925754682
Epoch 7: loss 16.119877948522436
Epoch 8: loss 16.118641423427132
Epoch 9: loss 16.117396959861768
Epoch 10: loss 16.116155593793245
Epoch 11: loss 16.114916844495035
Epoch 12: loss 16.113679341352004
Epoch 13: loss 16.11243873137764
Epoch 14: loss 16.111202187094758
Epoch 15: loss 16.109959280315852
Epoch 16: loss 16.10872017889972
Epoch 17: loss 16.107480294941688
Epoch 18: loss 16.106239756013206
Epoch 19: loss 16.105003199802916
Epoch 20: loss 16.103763001064944
Epoch 21: loss 16.102528569489603
Epoch 22: loss 16.10128594741282
Epoch 23: loss 16.100048922403406
Epoch 24: loss 16.098808549421516
Epoch 25: loss 16.09756795448606
Epoch 26: loss 16.096331844775815
Epoch 27: loss 16.09508751656064
Epoch 28: loss 16.093851909876
Epoch 29: loss 16.09261200258173
Epoch 30: loss 16.09137715302841
Epoch 31: loss 16.090138911423054
Epoch 32: loss 16.088898778545136
Epoch 33: loss 16.08766195007872
Epoch 34: loss 16.086423945984418
Epoch 35: loss 16.08518348639915
Epoch 36: loss 16.083946295443145
Epoch 37: loss 16.082706625141352
Epoch 38: loss 16.081468740321164
Epoch 39: loss 16.080231841327446
Epoch 40: loss 16.078995447433662
Epoch 41: loss 16.077754120258874
Epoch 42: loss 16.076515661367196
Epoch 43: loss 16.075278052433216
Epoch 44: loss 16.07404314842867
Epoch 45: loss 16.072797930843485
Epoch 46: loss 16.071560130552342
Epoch 47: loss 16.070322815654976
Epoch 48: loss 16.06908692841688
Epoch 49: loss 16.067844649123522
-----------Time: 0:06:31.655252, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.009510040283203-------------


Epoch 0: loss 16.128562291463215
Epoch 1: loss 16.127319838963103
Epoch 2: loss 16.126082862700503
Epoch 3: loss 16.12484516975606
Epoch 4: loss 16.123597148192065
Epoch 5: loss 16.122365415841745
Epoch 6: loss 16.121129451334767
Epoch 7: loss 16.119882958553745
Epoch 8: loss 16.118647862673456
Epoch 9: loss 16.117406679146182
Epoch 10: loss 16.1161637993336
Epoch 11: loss 16.11492871382497
Epoch 12: loss 16.11368293824437
Epoch 13: loss 16.11244638566416
Epoch 14: loss 16.111212679586572
Epoch 15: loss 16.109969936679924
Epoch 16: loss 16.1087370110699
Epoch 17: loss 16.107496162547307
Epoch 18: loss 16.10625453096424
Epoch 19: loss 16.105013832830746
Epoch 20: loss 16.103779211453993
Epoch 21: loss 16.102540777454305
Epoch 22: loss 16.101296395306488
Epoch 23: loss 16.100062342296805
Epoch 24: loss 16.098827702769643
Epoch 25: loss 16.097584956232915
Epoch 26: loss 16.096344493536147
Epoch 27: loss 16.095103481141297
Epoch 28: loss 16.0938675601953
Epoch 29: loss 16.092623578912217
Epoch 30: loss 16.091393463503998
Epoch 31: loss 16.0901487401285
Epoch 32: loss 16.088910072247835
Epoch 33: loss 16.087673648276233
Epoch 34: loss 16.086437392844136
Epoch 35: loss 16.085195854606027
Epoch 36: loss 16.083956908764822
Epoch 37: loss 16.082725321099186
Epoch 38: loss 16.081478411895933
Epoch 39: loss 16.08024203407823
Epoch 40: loss 16.079001985210773
Epoch 41: loss 16.077763149309185
Epoch 42: loss 16.076529833206084
Epoch 43: loss 16.075290446050662
Epoch 44: loss 16.074049413431073
Epoch 45: loss 16.072814695079277
Epoch 46: loss 16.071576796257347
Epoch 47: loss 16.070336327856168
Epoch 48: loss 16.069099169570897
Epoch 49: loss 16.067860466945167
-----------Time: 0:06:36.750074, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009507656097412-------------


Epoch 0: loss 16.12314529429317
Epoch 1: loss 16.110757272874874
Epoch 2: loss 16.09837524835152
Epoch 3: loss 16.08599250818349
Epoch 4: loss 16.07361555229133
Epoch 5: loss 16.061247892001195
Epoch 6: loss 16.048888351166617
Epoch 7: loss 16.0365322386849
Epoch 8: loss 16.02418105481694
Epoch 9: loss 16.011835101896683
Epoch 10: loss 15.999491381995247
Epoch 11: loss 15.987153888202478
Epoch 12: loss 15.974826341870688
Epoch 13: loss 15.962490000888145
Epoch 14: loss 15.95016573511303
Epoch 15: loss 15.93785208421534
Epoch 16: loss 15.925543731162573
Epoch 17: loss 15.913235980184782
Epoch 18: loss 15.900936643217749
Epoch 19: loss 15.88864322328386
Epoch 20: loss 15.876352223577287
Epoch 21: loss 15.864072410745294
Epoch 22: loss 15.85179486619577
Epoch 23: loss 15.839522992352418
Epoch 24: loss 15.827260338397958
Epoch 25: loss 15.814998130943543
Epoch 26: loss 15.802739071807114
Epoch 27: loss 15.79049427733595
Epoch 28: loss 15.77824600109787
Epoch 29: loss 15.76600272607518
Epoch 30: loss 15.753768125910549
Epoch 31: loss 15.741542085478525
Epoch 32: loss 15.729314040722828
Epoch 33: loss 15.717093481714146
Epoch 34: loss 15.704874422447782
Epoch 35: loss 15.69266072118198
Epoch 36: loss 15.680454447063303
Epoch 37: loss 15.668249244337305
Epoch 38: loss 15.65605394136264
Epoch 39: loss 15.643860770283087
Epoch 40: loss 15.631674632227504
Epoch 41: loss 15.619487237126494
Epoch 42: loss 15.607309218007886
Epoch 43: loss 15.595140727335114
Epoch 44: loss 15.582969194135307
Epoch 45: loss 15.570807343400517
Epoch 46: loss 15.558648838045288
Epoch 47: loss 15.546496347735406
Epoch 48: loss 15.534346895803894
Epoch 49: loss 15.522206999803121
-----------Time: 0:04:12.397231, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.940521001815796-------------


Epoch 0: loss 16.12303460687966
Epoch 1: loss 16.11065334467346
Epoch 2: loss 16.09826167139299
Epoch 3: loss 16.085874028021774
Epoch 4: loss 16.073493445159432
Epoch 5: loss 16.061117886330138
Epoch 6: loss 16.048750256636403
Epoch 7: loss 16.03638179513538
Epoch 8: loss 16.02401810667319
Epoch 9: loss 16.01166730292664
Epoch 10: loss 15.999307305738938
Epoch 11: loss 15.986961614184564
Epoch 12: loss 15.974624438283234
Epoch 13: loss 15.96228797284075
Epoch 14: loss 15.949962395050397
Epoch 15: loss 15.937632229873964
Epoch 16: loss 15.925308195905494
Epoch 17: loss 15.912983084321347
Epoch 18: loss 15.900665352174677
Epoch 19: loss 15.888345064450503
Epoch 20: loss 15.876032167054053
Epoch 21: loss 15.863716272765881
Epoch 22: loss 15.851407474250234
Epoch 23: loss 15.839108602970823
Epoch 24: loss 15.82680039460274
Epoch 25: loss 15.814504009929282
Epoch 26: loss 15.802203062243148
Epoch 27: loss 15.789910821567222
Epoch 28: loss 15.777615460576799
Epoch 29: loss 15.765308759211198
Epoch 30: loss 15.753005978852396
Epoch 31: loss 15.740713553560889
Epoch 32: loss 15.7284100886724
Epoch 33: loss 15.716113959141826
Epoch 34: loss 15.703813970834421
Epoch 35: loss 15.691513277254007
Epoch 36: loss 15.679211269584021
Epoch 37: loss 15.666907635118859
Epoch 38: loss 15.654590650250434
Epoch 39: loss 15.64227909857712
Epoch 40: loss 15.629951656999117
Epoch 41: loss 15.617623923458828
Epoch 42: loss 15.605287195094713
Epoch 43: loss 15.592949397930832
Epoch 44: loss 15.58059474474195
Epoch 45: loss 15.568227072005818
Epoch 46: loss 15.555845136678757
Epoch 47: loss 15.543451653024695
Epoch 48: loss 15.531049178176888
Epoch 49: loss 15.518631157244982
-----------Time: 0:05:22.328262, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.940031051635742-------------


Epoch 0: loss 16.123030887601686
Epoch 1: loss 16.110642717350043
Epoch 2: loss 16.09826333500757
Epoch 3: loss 16.08588047452826
Epoch 4: loss 16.073511536449374
Epoch 5: loss 16.061142417903564
Epoch 6: loss 16.048779261507633
Epoch 7: loss 16.03640821279554
Epoch 8: loss 16.02405139244789
Epoch 9: loss 16.011690382467396
Epoch 10: loss 15.999330653387156
Epoch 11: loss 15.986980023365945
Epoch 12: loss 15.974626854361887
Epoch 13: loss 15.962270875156022
Epoch 14: loss 15.949921726726721
Epoch 15: loss 15.937568712260425
Epoch 16: loss 15.925212520435492
Epoch 17: loss 15.91285177182088
Epoch 18: loss 15.900483271944712
Epoch 19: loss 15.888107148897006
Epoch 20: loss 15.875716936983707
Epoch 21: loss 15.863316084782412
Epoch 22: loss 15.850907718312033
Epoch 23: loss 15.838481217509317
Epoch 24: loss 15.826033490063251
Epoch 25: loss 15.81355984227822
Epoch 26: loss 15.801063624719617
Epoch 27: loss 15.788525923624707
Epoch 28: loss 15.775950045479323
Epoch 29: loss 15.763315828291212
Epoch 30: loss 15.750645203976738
Epoch 31: loss 15.737892928753553
Epoch 32: loss 15.725072534010422
Epoch 33: loss 15.712173500289211
Epoch 34: loss 15.699174163781022
Epoch 35: loss 15.68606732032946
Epoch 36: loss 15.67284640104243
Epoch 37: loss 15.65950148438811
Epoch 38: loss 15.646006153224926
Epoch 39: loss 15.63235780945156
Epoch 40: loss 15.618526816562571
Epoch 41: loss 15.6045091970256
Epoch 42: loss 15.590284015640997
Epoch 43: loss 15.575837549662836
Epoch 44: loss 15.561148258704995
Epoch 45: loss 15.546211636798935
Epoch 46: loss 15.530993893070024
Epoch 47: loss 15.515500514127432
Epoch 48: loss 15.499698460004847
Epoch 49: loss 15.48357398928735
-----------Time: 0:05:25.966832, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9353513717651367-------------


Epoch 0: loss 16.123037637479282
Epoch 1: loss 16.110652396184975
Epoch 2: loss 16.098260833881287
Epoch 3: loss 16.08587493450503
Epoch 4: loss 16.07350011102657
Epoch 5: loss 16.0611254809796
Epoch 6: loss 16.048747517480482
Epoch 7: loss 16.036375917514555
Epoch 8: loss 16.02400099498672
Epoch 9: loss 16.011624166666003
Epoch 10: loss 15.99923679347658
Epoch 11: loss 15.986848629447932
Epoch 12: loss 15.974447453132202
Epoch 13: loss 15.96203012087865
Epoch 14: loss 15.9495904252476
Epoch 15: loss 15.937127218614654
Epoch 16: loss 15.924634024395509
Epoch 17: loss 15.912088994684266
Epoch 18: loss 15.899491155581865
Epoch 19: loss 15.886810925034093
Epoch 20: loss 15.87404999673334
Epoch 21: loss 15.861177481795991
Epoch 22: loss 15.848169779505271
Epoch 23: loss 15.835006694161029
Epoch 24: loss 15.8216661621268
Epoch 25: loss 15.808100783870296
Epoch 26: loss 15.794281925307207
Epoch 27: loss 15.780171923302904
Epoch 28: loss 15.765741499692867
Epoch 29: loss 15.750940127888732
Epoch 30: loss 15.73573886317491
Epoch 31: loss 15.720087709473553
Epoch 32: loss 15.703951898899462
Epoch 33: loss 15.687287165199434
Epoch 34: loss 15.670058822942986
Epoch 35: loss 15.652229148839854
Epoch 36: loss 15.6337591485526
Epoch 37: loss 15.614612425845106
Epoch 38: loss 15.594773018729628
Epoch 39: loss 15.574189732682775
Epoch 40: loss 15.552832674500474
Epoch 41: loss 15.530690645899314
Epoch 42: loss 15.507728511318685
Epoch 43: loss 15.483937009901634
Epoch 44: loss 15.459295430476411
Epoch 45: loss 15.433773075516552
Epoch 46: loss 15.407352748287963
Epoch 47: loss 15.380044371359629
Epoch 48: loss 15.35181864271221
Epoch 49: loss 15.32267156562577
-----------Time: 0:05:39.168375, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.9140594005584717-------------


Epoch 0: loss 16.12302779321634
Epoch 1: loss 16.110642415534677
Epoch 2: loss 16.098266917379608
Epoch 3: loss 16.085878252399688
Epoch 4: loss 16.073503418549564
Epoch 5: loss 16.061122012695844
Epoch 6: loss 16.048740098630688
Epoch 7: loss 16.036355377475225
Epoch 8: loss 16.023960702117208
Epoch 9: loss 16.011549259930476
Epoch 10: loss 15.999119542875388
Epoch 11: loss 15.986660083005324
Epoch 12: loss 15.974163613933918
Epoch 13: loss 15.961613776438776
Epoch 14: loss 15.948998427378088
Epoch 15: loss 15.936268960995283
Epoch 16: loss 15.92341299508174
Epoch 17: loss 15.910398287251956
Epoch 18: loss 15.897173051279223
Epoch 19: loss 15.88369007898842
Epoch 20: loss 15.869891919151087
Epoch 21: loss 15.85573091968497
Epoch 22: loss 15.841128303150304
Epoch 23: loss 15.826044913930307
Epoch 24: loss 15.810387003091186
Epoch 25: loss 15.794089315766028
Epoch 26: loss 15.77709202042995
Epoch 27: loss 15.759330647869431
Epoch 28: loss 15.740727343722618
Epoch 29: loss 15.721240088238282
Epoch 30: loss 15.700784880289117
Epoch 31: loss 15.679325906007817
Epoch 32: loss 15.656799049335955
Epoch 33: loss 15.633166549126695
Epoch 34: loss 15.608400515462472
Epoch 35: loss 15.5824560032648
Epoch 36: loss 15.555323436896785
Epoch 37: loss 15.526961151836618
Epoch 38: loss 15.497363471155131
Epoch 39: loss 15.466494115815985
Epoch 40: loss 15.434352078212221
Epoch 41: loss 15.400928544505515
Epoch 42: loss 15.366188495816454
Epoch 43: loss 15.330145559471674
Epoch 44: loss 15.292786248680558
Epoch 45: loss 15.254112091707496
Epoch 46: loss 15.214112725187865
Epoch 47: loss 15.172777455938194
Epoch 48: loss 15.13012094793273
Epoch 49: loss 15.086163466361763
-----------Time: 0:07:14.066017, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.8828914165496826-------------


Epoch 0: loss 16.067907049190485
Epoch 1: loss 15.944749181850115
Epoch 2: loss 15.822078805957688
Epoch 3: loss 15.699941441700341
Epoch 4: loss 15.57828043646758
Epoch 5: loss 15.457149203799872
Epoch 6: loss 15.336446680667414
Epoch 7: loss 15.216168406736987
Epoch 8: loss 15.09636698137359
Epoch 9: loss 14.97695732064841
Epoch 10: loss 14.8578363255744
Epoch 11: loss 14.738988773144737
Epoch 12: loss 14.620335942963791
Epoch 13: loss 14.501736182502718
Epoch 14: loss 14.383068039081484
Epoch 15: loss 14.26411016069073
Epoch 16: loss 14.144680424058095
Epoch 17: loss 14.024561565683353
Epoch 18: loss 13.903493212253391
Epoch 19: loss 13.781172604065604
Epoch 20: loss 13.657204110963374
Epoch 21: loss 13.531208980595567
Epoch 22: loss 13.402747197278236
Epoch 23: loss 13.271310407484012
Epoch 24: loss 13.13631279433532
Epoch 25: loss 12.997211529937628
Epoch 26: loss 12.853451270911926
Epoch 27: loss 12.704314124007274
Epoch 28: loss 12.549157293546841
Epoch 29: loss 12.38743956650386
Epoch 30: loss 12.218668901382289
Epoch 31: loss 12.042378631475634
Epoch 32: loss 11.858267868129115
Epoch 33: loss 11.666206206362684
Epoch 34: loss 11.466012936042922
Epoch 35: loss 11.257713968692881
Epoch 36: loss 11.041375420027418
Epoch 37: loss 10.817132907305288
Epoch 38: loss 10.585270636363028
Epoch 39: loss 10.346073680626692
Epoch 40: loss 10.10000446531162
Epoch 41: loss 9.847400969172899
Epoch 42: loss 9.588570373352615
Epoch 43: loss 9.324084069302316
Epoch 44: loss 9.054436612090317
Epoch 45: loss 8.780158813262906
Epoch 46: loss 8.50177740104306
Epoch 47: loss 8.21990506917384
Epoch 48: loss 7.935154621597215
Epoch 49: loss 7.648283570534901
-----------Time: 0:05:08.077888, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.7463037967681885-------------


Epoch 0: loss 16.067762387842507
Epoch 1: loss 15.944462166848943
Epoch 2: loss 15.821571013537225
Epoch 3: loss 15.698919932047525
Epoch 4: loss 15.576128750666774
Epoch 5: loss 15.452475402068677
Epoch 6: loss 15.326530094571966
Epoch 7: loss 15.19592667870576
Epoch 8: loss 15.057252806642252
Epoch 9: loss 14.906141926763887
Epoch 10: loss 14.737709055781819
Epoch 11: loss 14.547427242252086
Epoch 12: loss 14.331778585424626
Epoch 13: loss 14.088564764876415
Epoch 14: loss 13.816837025570312
Epoch 15: loss 13.516691432950806
Epoch 16: loss 13.188830015774718
Epoch 17: loss 12.834387101966314
Epoch 18: loss 12.454653557906532
Epoch 19: loss 12.051215582531778
Epoch 20: loss 11.626161421298203
Epoch 21: loss 11.181346381987613
Epoch 22: loss 10.718735262905534
Epoch 23: loss 10.24102128751256
Epoch 24: loss 9.750320352634184
Epoch 25: loss 9.249634615165375
Epoch 26: loss 8.74153690483338
Epoch 27: loss 8.228792707060522
Epoch 28: loss 7.714493881690236
Epoch 29: loss 7.201870084651596
Epoch 30: loss 6.694003896780672
Epoch 31: loss 6.194242462896665
Epoch 32: loss 5.705932253401457
Epoch 33: loss 5.232407392529835
Epoch 34: loss 4.777076613844705
Epoch 35: loss 4.343466647103016
Epoch 36: loss 3.934859448117105
Epoch 37: loss 3.554019150752098
Epoch 38: loss 3.203920086548988
Epoch 39: loss 2.8869184991339747
Epoch 40: loss 2.6046364076634085
Epoch 41: loss 2.3575671980600115
Epoch 42: loss 2.1453611811196565
Epoch 43: loss 1.9664841229789862
Epoch 44: loss 1.8180756119037855
Epoch 45: loss 1.6965222971928127
Epoch 46: loss 1.5977051380602414
Epoch 47: loss 1.5173435300637226
Epoch 48: loss 1.4515721204813696
Epoch 49: loss 1.3970735861984136
-----------Time: 0:04:10.121560, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.1820882558822632-------------


Epoch 0: loss 16.067745887046925
Epoch 1: loss 15.944334073712348
Epoch 2: loss 15.820535126168032
Epoch 3: loss 15.693191263842932
Epoch 4: loss 15.55208777603473
Epoch 5: loss 15.377278175997047
Epoch 6: loss 15.147171632435867
Epoch 7: loss 14.848584093692317
Epoch 8: loss 14.477280029724188
Epoch 9: loss 14.035532652651117
Epoch 10: loss 13.529076007865836
Epoch 11: loss 12.964676639189728
Epoch 12: loss 12.349264280766233
Epoch 13: loss 11.689700853182869
Epoch 14: loss 10.993454391247168
Epoch 15: loss 10.268072287500909
Epoch 16: loss 9.52136111090921
Epoch 17: loss 8.76217416734265
Epoch 18: loss 7.9995547935585405
Epoch 19: loss 7.24248176012563
Epoch 20: loss 6.500046953291528
Epoch 21: loss 5.782171414817657
Epoch 22: loss 5.098528188878651
Epoch 23: loss 4.459130232589539
Epoch 24: loss 3.8728293222600576
Epoch 25: loss 3.347782120230147
Epoch 26: loss 2.890419692822032
Epoch 27: loss 2.5044691060275213
Epoch 28: loss 2.1898102746702137
Epoch 29: loss 1.9418983159342689
Epoch 30: loss 1.752048294592189
Epoch 31: loss 1.6089371345560468
Epoch 32: loss 1.5011257036280672
Epoch 33: loss 1.418537868566653
Epoch 34: loss 1.3535909660488707
Epoch 35: loss 1.3011248588821303
Epoch 36: loss 1.2577530944911341
Epoch 37: loss 1.221306060175976
Epoch 38: loss 1.1903392832975404
Epoch 39: loss 1.163848731555389
Epoch 40: loss 1.1410649079483057
Epoch 41: loss 1.1213884500461535
Epoch 42: loss 1.1043677883928662
Epoch 43: loss 1.089593697398042
Epoch 44: loss 1.0767568188429268
Epoch 45: loss 1.0655913368199688
Epoch 46: loss 1.0558488847380945
Epoch 47: loss 1.047341298836609
Epoch 48: loss 1.0398942561432225
Epoch 49: loss 1.033375058827548
-----------Time: 0:05:10.993331, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.024953842163086-------------


Epoch 0: loss 16.067704708437482
Epoch 1: loss 15.943756178710174
Epoch 2: loss 15.813820201589596
Epoch 3: loss 15.649275079118356
Epoch 4: loss 15.3970118419446
Epoch 5: loss 15.024252500528872
Epoch 6: loss 14.533064023899993
Epoch 7: loss 13.936847049947534
Epoch 8: loss 13.248124801963487
Epoch 9: loss 12.48012897435448
Epoch 10: loss 11.646672491537222
Epoch 11: loss 10.761663958584245
Epoch 12: loss 9.8401425456534
Epoch 13: loss 8.897943172071082
Epoch 14: loss 7.950904073243299
Epoch 15: loss 7.016204954814237
Epoch 16: loss 6.111765468425243
Epoch 17: loss 5.2550141514482025
Epoch 18: loss 4.463724089550414
Epoch 19: loss 3.754423680416976
Epoch 20: loss 3.1410417332475506
Epoch 21: loss 2.6326515668108774
Epoch 22: loss 2.230907791720064
Epoch 23: loss 1.9287447565725928
Epoch 24: loss 1.7102474335939615
Epoch 25: loss 1.554989388871932
Epoch 26: loss 1.4436071125173646
Epoch 27: loss 1.3609728003533526
Epoch 28: loss 1.297222718847647
Epoch 29: loss 1.2464082723988859
Epoch 30: loss 1.2050313127954866
Epoch 31: loss 1.1708825228005535
Epoch 32: loss 1.1424771167131271
Epoch 33: loss 1.1187387653963277
Epoch 34: loss 1.098807046632523
Epoch 35: loss 1.0820408897791434
Epoch 36: loss 1.0678831859734346
Epoch 37: loss 1.055911779727801
Epoch 38: loss 1.0457550892368357
Epoch 39: loss 1.0371110729382957
Epoch 40: loss 1.0297470497916483
Epoch 41: loss 1.0234569638173434
Epoch 42: loss 1.0180737574571108
Epoch 43: loss 1.0134420794206447
Epoch 44: loss 1.0094613005377275
Epoch 45: loss 1.00602334138166
Epoch 46: loss 1.0030516551006352
Epoch 47: loss 1.0004789622145025
Epoch 48: loss 0.9982392902021631
Epoch 49: loss 0.9962967272489339
-----------Time: 0:06:15.674280, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0072122812271118-------------


Epoch 0: loss 16.06772167077202
Epoch 1: loss 15.942764328899042
Epoch 2: loss 15.793632410867763
Epoch 3: loss 15.534159971489213
Epoch 4: loss 15.099153509342262
Epoch 5: loss 14.498803328534324
Epoch 6: loss 13.757923094587134
Epoch 7: loss 12.899761854921625
Epoch 8: loss 11.9470980272402
Epoch 9: loss 10.921926359432815
Epoch 10: loss 9.847335389154381
Epoch 11: loss 8.7473841755853
Epoch 12: loss 7.6477026957542495
Epoch 13: loss 6.575377163775529
Epoch 14: loss 5.557511745295232
Epoch 15: loss 4.62096487587726
Epoch 16: loss 3.7912009935653876
Epoch 17: loss 3.088777699633874
Epoch 18: loss 2.5265794823907393
Epoch 19: loss 2.1039568870984193
Epoch 20: loss 1.8047295032862154
Epoch 21: loss 1.6009063577444544
Epoch 22: loss 1.4622145081551714
Epoch 23: loss 1.3642985215324497
Epoch 24: loss 1.2916289895951196
Epoch 25: loss 1.2354618092475216
Epoch 26: loss 1.190851656913498
Epoch 27: loss 1.1549022431409897
Epoch 28: loss 1.1257216636093497
Epoch 29: loss 1.1018686130877875
Epoch 30: loss 1.0823078460833377
Epoch 31: loss 1.0662267361405493
Epoch 32: loss 1.0529559239412922
Epoch 33: loss 1.041972479235549
Epoch 34: loss 1.032832865451846
Epoch 35: loss 1.0252307492018136
Epoch 36: loss 1.0188686985759516
Epoch 37: loss 1.0135235858715508
Epoch 38: loss 1.0090359754378282
Epoch 39: loss 1.0052468407860653
Epoch 40: loss 1.0020266975184509
Epoch 41: loss 0.9992984016347412
Epoch 42: loss 0.9969785588470862
Epoch 43: loss 0.9950008404002104
Epoch 44: loss 0.9933042307143759
Epoch 45: loss 0.9918507974779172
Epoch 46: loss 0.9905940593747485
Epoch 47: loss 0.9895103508652132
Epoch 48: loss 0.988580243816967
Epoch 49: loss 0.9877687100809251
-----------Time: 0:08:00.412783, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0029851198196411-------------


Epoch 0: loss 15.525508187788216
Epoch 1: loss 14.33184522801991
Epoch 2: loss 12.912706779098304
Epoch 3: loss 10.583281552292458
Epoch 4: loss 7.4860985407432565
Epoch 5: loss 4.52190057994102
Epoch 6: loss 2.4866034077716432
Epoch 7: loss 1.5526033577029636
Epoch 8: loss 1.2415994958685688
Epoch 9: loss 1.1217780593881923
Epoch 10: loss 1.061885772417783
Epoch 11: loss 1.0294669430303858
Epoch 12: loss 1.0112410735150534
Epoch 13: loss 1.0006697878622373
Epoch 14: loss 0.9943136118105276
Epoch 15: loss 0.9903793540773604
Epoch 16: loss 0.9878334664922489
Epoch 17: loss 0.9861407718495093
Epoch 18: loss 0.9849694772153007
Epoch 19: loss 0.9840865479726516
Epoch 20: loss 0.9834199495909329
Epoch 21: loss 0.9828670573273451
Epoch 22: loss 0.9824294474152134
Epoch 23: loss 0.9820170894856683
Epoch 24: loss 0.9816470743522104
Epoch 25: loss 0.9812971885402674
Epoch 26: loss 0.980977795965455
Epoch 27: loss 0.9806657826660637
Epoch 28: loss 0.9803158125519558
Epoch 29: loss 0.9799748586856392
Epoch 30: loss 0.9796405946709268
Epoch 31: loss 0.9792817574664909
Epoch 32: loss 0.978946699695525
Epoch 33: loss 0.9785603021654894
Epoch 34: loss 0.9781939021396274
Epoch 35: loss 0.977803680545419
Epoch 36: loss 0.9773904520120875
Epoch 37: loss 0.9769685064534119
Epoch 38: loss 0.9764989621617212
Epoch 39: loss 0.9760342091889146
Epoch 40: loss 0.9755399023922583
Epoch 41: loss 0.9750314229140145
Epoch 42: loss 0.9744907367352625
Epoch 43: loss 0.9739303664385851
Epoch 44: loss 0.9733434820577073
Epoch 45: loss 0.9727266907951247
Epoch 46: loss 0.972069445545742
Epoch 47: loss 0.9714028579570341
Epoch 48: loss 0.9706832554562057
Epoch 49: loss 0.9699423400975882
-----------Time: 0:03:24.675469, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9986788630485535-------------


Epoch 0: loss 15.51071127061289
Epoch 1: loss 12.92350787660102
Epoch 2: loss 7.137622627893046
Epoch 3: loss 2.8293596487968364
Epoch 4: loss 1.4337708574718209
Epoch 5: loss 1.1459847012248099
Epoch 6: loss 1.0540797709770473
Epoch 7: loss 1.017604158357669
Epoch 8: loss 1.0020648398238592
Epoch 9: loss 0.9948985424295854
Epoch 10: loss 0.9914324011991956
Epoch 11: loss 0.9895998969422922
Epoch 12: loss 0.9885311755166876
Epoch 13: loss 0.9878586542677659
Epoch 14: loss 0.9873925492515896
Epoch 15: loss 0.9870843940159755
Epoch 16: loss 0.9867498649728889
Epoch 17: loss 0.9864916656573484
Epoch 18: loss 0.9862743153268712
Epoch 19: loss 0.9860075957558607
Epoch 20: loss 0.985794659539887
Epoch 21: loss 0.9855043806739318
Epoch 22: loss 0.9852296080908223
Epoch 23: loss 0.9849602506496519
Epoch 24: loss 0.9846637084018672
Epoch 25: loss 0.9843702374300664
Epoch 26: loss 0.9840261713845241
Epoch 27: loss 0.9836930316255558
Epoch 28: loss 0.9833104494410148
Epoch 29: loss 0.9828850380942639
Epoch 30: loss 0.9824486868157213
Epoch 31: loss 0.9819265237769852
Epoch 32: loss 0.9814230108338896
Epoch 33: loss 0.9808699772888192
Epoch 34: loss 0.980261836219184
Epoch 35: loss 0.979585373673898
Epoch 36: loss 0.9788415759331899
Epoch 37: loss 0.9780619335018985
Epoch 38: loss 0.9771988009097072
Epoch 39: loss 0.9763153404759091
Epoch 40: loss 0.9752904939677418
Epoch 41: loss 0.9742159493725348
Epoch 42: loss 0.9730498470840019
Epoch 43: loss 0.9718001214235357
Epoch 44: loss 0.9704264652281757
Epoch 45: loss 0.9690136410672746
Epoch 46: loss 0.9674843561720627
Epoch 47: loss 0.9658449817572423
Epoch 48: loss 0.9640833519605789
Epoch 49: loss 0.9622998627843644
-----------Time: 0:04:10.181633, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9957457184791565-------------


Epoch 0: loss 15.17957213444837
Epoch 1: loss 8.045537680760228
Epoch 2: loss 2.217178387574491
Epoch 3: loss 1.2097807501631628
Epoch 4: loss 1.0561003200770591
Epoch 5: loss 1.0134340381738995
Epoch 6: loss 0.9997263872279882
Epoch 7: loss 0.9947985330309927
Epoch 8: loss 0.9927654156482628
Epoch 9: loss 0.9918302646015181
Epoch 10: loss 0.9913656200774018
Epoch 11: loss 0.990963854402093
Epoch 12: loss 0.990722692485994
Epoch 13: loss 0.9905687839910996
Epoch 14: loss 0.9903734296544082
Epoch 15: loss 0.990078342388738
Epoch 16: loss 0.9899613315674065
Epoch 17: loss 0.9897250238159806
Epoch 18: loss 0.9895299834813548
Epoch 19: loss 0.9893185797108458
Epoch 20: loss 0.9890712717423431
Epoch 21: loss 0.9887668401278381
Epoch 22: loss 0.9884726457974391
Epoch 23: loss 0.98814310348183
Epoch 24: loss 0.9877943925523058
Epoch 25: loss 0.9874701833452719
Epoch 26: loss 0.9869453362500687
Epoch 27: loss 0.9865461489050981
Epoch 28: loss 0.9859843785312916
Epoch 29: loss 0.9853357401083448
Epoch 30: loss 0.9846560737890935
Epoch 31: loss 0.9840084984290852
Epoch 32: loss 0.9831476496444441
Epoch 33: loss 0.9822226888203893
Epoch 34: loss 0.9811979539696454
Epoch 35: loss 0.9800837987982235
Epoch 36: loss 0.9787259646704043
Epoch 37: loss 0.9773432491848558
Epoch 38: loss 0.9757374605450571
Epoch 39: loss 0.974029894656886
Epoch 40: loss 0.9720626678591257
Epoch 41: loss 0.9698777511561935
Epoch 42: loss 0.9675863031915766
Epoch 43: loss 0.9650874634097619
Epoch 44: loss 0.962332571700191
Epoch 45: loss 0.959399037318362
Epoch 46: loss 0.9561888694179778
Epoch 47: loss 0.9528464166089982
Epoch 48: loss 0.949247024997931
Epoch 49: loss 0.9455212666782236
-----------Time: 0:05:39.558392, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.992953360080719-------------


Epoch 0: loss 14.275885591823293
Epoch 1: loss 4.720626851020136
Epoch 2: loss 1.3640450451800588
Epoch 3: loss 1.0706482261597559
Epoch 4: loss 1.014587140491697
Epoch 5: loss 1.0004022196299878
Epoch 6: loss 0.9962437326587886
Epoch 7: loss 0.9945874940512814
Epoch 8: loss 0.9939326507491609
Epoch 9: loss 0.9935699995605112
Epoch 10: loss 0.993358629238612
Epoch 11: loss 0.9930943578659938
Epoch 12: loss 0.9929559817127458
Epoch 13: loss 0.992770943614437
Epoch 14: loss 0.9925419628587736
Epoch 15: loss 0.9923238768533497
Epoch 16: loss 0.9921572870246219
Epoch 17: loss 0.9917629957523211
Epoch 18: loss 0.9916595696365788
Epoch 19: loss 0.991257730452116
Epoch 20: loss 0.9909142794137159
Epoch 21: loss 0.99058034042096
Epoch 22: loss 0.9901351445456313
Epoch 23: loss 0.9895834478105521
Epoch 24: loss 0.9889966478290734
Epoch 25: loss 0.9883040402362112
Epoch 26: loss 0.9875849265647751
Epoch 27: loss 0.9866927669643384
Epoch 28: loss 0.9857008563817488
Epoch 29: loss 0.9845646088573192
Epoch 30: loss 0.9831614214937087
Epoch 31: loss 0.9816659005068125
Epoch 32: loss 0.9798865453778174
Epoch 33: loss 0.9780508781853676
Epoch 34: loss 0.975801009792164
Epoch 35: loss 0.9734219032957089
Epoch 36: loss 0.9707516256786676
Epoch 37: loss 0.9677570784072502
Epoch 38: loss 0.9646069986655571
Epoch 39: loss 0.9611444714731856
Epoch 40: loss 0.9574650932486256
Epoch 41: loss 0.9535919095978521
Epoch 42: loss 0.9493662726302197
Epoch 43: loss 0.9449285536826207
Epoch 44: loss 0.9402795453769088
Epoch 45: loss 0.9353024103902098
Epoch 46: loss 0.9299873449414758
Epoch 47: loss 0.924294926376042
Epoch 48: loss 0.9182653971636275
Epoch 49: loss 0.9118155737880004
-----------Time: 0:06:59.842906, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9892341494560242-------------


Epoch 0: loss 13.2659074089461
Epoch 1: loss 3.08391359663969
Epoch 2: loss 1.1763749149521128
Epoch 3: loss 1.0321719810779877
Epoch 4: loss 1.0050686491190448
Epoch 5: loss 0.9986615956250969
Epoch 6: loss 0.9966882775502726
Epoch 7: loss 0.995918482363775
Epoch 8: loss 0.9955453634780668
Epoch 9: loss 0.9953282606737843
Epoch 10: loss 0.9951336627553636
Epoch 11: loss 0.9949421189024501
Epoch 12: loss 0.9947655455712199
Epoch 13: loss 0.9946324555947769
Epoch 14: loss 0.994474445269638
Epoch 15: loss 0.9942903059406603
Epoch 16: loss 0.9939621258780255
Epoch 17: loss 0.9937225456079625
Epoch 18: loss 0.9933480599640893
Epoch 19: loss 0.9930927318486913
Epoch 20: loss 0.992664428558474
Epoch 21: loss 0.992304549650241
Epoch 22: loss 0.9917780736207055
Epoch 23: loss 0.9910893304766748
Epoch 24: loss 0.9904509197116352
Epoch 25: loss 0.9896481081867685
Epoch 26: loss 0.9887145545380215
Epoch 27: loss 0.9876191746028239
Epoch 28: loss 0.9862465966713694
Epoch 29: loss 0.9847630292193406
Epoch 30: loss 0.9830573435838356
Epoch 31: loss 0.9809591344609865
Epoch 32: loss 0.9786226414609953
Epoch 33: loss 0.9759676670691059
Epoch 34: loss 0.9728433519942142
Epoch 35: loss 0.9694421101744374
Epoch 36: loss 0.9656071196178044
Epoch 37: loss 0.9613122057823982
Epoch 38: loss 0.9566680261983762
Epoch 39: loss 0.951663029751355
Epoch 40: loss 0.9461590864530006
Epoch 41: loss 0.9402763280225487
Epoch 42: loss 0.9339902012357251
Epoch 43: loss 0.9270931723780318
Epoch 44: loss 0.9197708261667742
Epoch 45: loss 0.9118791855243705
Epoch 46: loss 0.9033201143364339
Epoch 47: loss 0.894191148846353
Epoch 48: loss 0.8842882959333174
Epoch 49: loss 0.8736347573292262
-----------Time: 0:06:22.767732, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9867227077484131-------------


Epoch 0: loss 7.043537061554119
Epoch 1: loss 1.031530184548727
Epoch 2: loss 1.0144398846076583
Epoch 3: loss 1.0101932347825586
Epoch 4: loss 1.0039349900373238
Epoch 5: loss 0.9948249199002771
Epoch 6: loss 0.9830199960718471
Epoch 7: loss 0.9700703910344836
Epoch 8: loss 0.9550151086942601
Epoch 9: loss 0.9374909860534212
Epoch 10: loss 0.917864033357808
Epoch 11: loss 0.896579745381082
Epoch 12: loss 0.8750138591332822
Epoch 13: loss 0.8542510890455075
Epoch 14: loss 0.8353822804327048
Epoch 15: loss 0.818589450387348
Epoch 16: loss 0.8037200164963462
Epoch 17: loss 0.7911373846164017
Epoch 18: loss 0.7804768545398121
Epoch 19: loss 0.7714402011194068
Epoch 20: loss 0.763631314930545
Epoch 21: loss 0.7569411973255753
Epoch 22: loss 0.7512386637320008
Epoch 23: loss 0.7462189659403354
Epoch 24: loss 0.7417729449311049
Epoch 25: loss 0.7378146467227013
Epoch 26: loss 0.7343791999072727
Epoch 27: loss 0.7310349853742765
Epoch 28: loss 0.7280838212414628
Epoch 29: loss 0.725542234979031
Epoch 30: loss 0.7226896701849563
Epoch 31: loss 0.7205433731755853
Epoch 32: loss 0.718305439907549
Epoch 33: loss 0.7162551778759109
Epoch 34: loss 0.7141780351218223
Epoch 35: loss 0.7125072736919024
Epoch 36: loss 0.7107704008902591
Epoch 37: loss 0.7091847249773159
Epoch 38: loss 0.7076329003539404
Epoch 39: loss 0.7061087552330427
Epoch 40: loss 0.7046363288582247
Epoch 41: loss 0.7033318339838419
Epoch 42: loss 0.7019572857607312
Epoch 43: loss 0.7007698919735504
Epoch 44: loss 0.6995032896744031
Epoch 45: loss 0.6983504955894342
Epoch 46: loss 0.6974405036016674
Epoch 47: loss 0.6961681285975354
Epoch 48: loss 0.695332883167941
Epoch 49: loss 0.694246748564618
-----------Time: 0:03:52.734646, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.1116219758987427-------------


Epoch 0: loss 4.6082696994405
Epoch 1: loss 1.0345487403376974
Epoch 2: loss 1.024341647102497
Epoch 3: loss 1.009636986216752
Epoch 4: loss 0.9952220243886478
Epoch 5: loss 0.9773753868814005
Epoch 6: loss 0.9529591856474718
Epoch 7: loss 0.9215642459500154
Epoch 8: loss 0.8820945259473323
Epoch 9: loss 0.8340438511982244
Epoch 10: loss 0.7799927907640873
Epoch 11: loss 0.7250701900796439
Epoch 12: loss 0.6743118800567245
Epoch 13: loss 0.6308839158593085
Epoch 14: loss 0.5956342624152984
Epoch 15: loss 0.5673820751396582
Epoch 16: loss 0.5449310718119437
Epoch 17: loss 0.5267792508095227
Epoch 18: loss 0.5117717980690272
Epoch 19: loss 0.4992031843977301
Epoch 20: loss 0.48823629993015555
Epoch 21: loss 0.47896195864664465
Epoch 22: loss 0.47085500805516683
Epoch 23: loss 0.4635875624376124
Epoch 24: loss 0.45702894052258647
Epoch 25: loss 0.451507859268287
Epoch 26: loss 0.44585979485589566
Epoch 27: loss 0.4412653805478362
Epoch 28: loss 0.43675153622684304
Epoch 29: loss 0.43272711484895054
Epoch 30: loss 0.4289256327174292
Epoch 31: loss 0.42563136263085555
Epoch 32: loss 0.4222810261271582
Epoch 33: loss 0.4193049300423519
Epoch 34: loss 0.4163906076409494
Epoch 35: loss 0.4138325433357699
Epoch 36: loss 0.4111552479119324
Epoch 37: loss 0.4088438144191702
Epoch 38: loss 0.4066676286460914
Epoch 39: loss 0.40446728827708306
Epoch 40: loss 0.4023415111471221
Epoch 41: loss 0.4004436784655326
Epoch 42: loss 0.39870442682598645
Epoch 43: loss 0.3969273913840096
Epoch 44: loss 0.3952521438213585
Epoch 45: loss 0.3937170299062786
Epoch 46: loss 0.3919934909456511
Epoch 47: loss 0.3905952301538788
Epoch 48: loss 0.38926231669886985
Epoch 49: loss 0.3879015007992683
-----------Time: 0:04:51.804059, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3410887718200684-------------


Epoch 0: loss 3.589052845272699
Epoch 1: loss 1.0553819927396562
Epoch 2: loss 1.043919472768295
Epoch 3: loss 1.023502319354347
Epoch 4: loss 0.9946493109067281
Epoch 5: loss 0.9498091261953423
Epoch 6: loss 0.8828024430855776
Epoch 7: loss 0.793778449896563
Epoch 8: loss 0.6903593494491514
Epoch 9: loss 0.5872171839791319
Epoch 10: loss 0.4975526708017684
Epoch 11: loss 0.4263567881554349
Epoch 12: loss 0.3717567394971199
Epoch 13: loss 0.33032254415274054
Epoch 14: loss 0.2988162377627405
Epoch 15: loss 0.27400370474546226
Epoch 16: loss 0.25408157458313135
Epoch 17: loss 0.23788178482413486
Epoch 18: loss 0.22463643479567627
Epoch 19: loss 0.21307359662569367
Epoch 20: loss 0.20334964006928533
Epoch 21: loss 0.19476627428418855
Epoch 22: loss 0.18738465078373587
Epoch 23: loss 0.18074456092259883
Epoch 24: loss 0.1749277335995238
Epoch 25: loss 0.16957970012572876
Epoch 26: loss 0.16491450912654887
Epoch 27: loss 0.16053539502387335
Epoch 28: loss 0.15675381075564515
Epoch 29: loss 0.15294790884166906
Epoch 30: loss 0.14974246830428795
Epoch 31: loss 0.14663993135345832
Epoch 32: loss 0.1437310458704335
Epoch 33: loss 0.14126010623605048
Epoch 34: loss 0.1387664215145972
Epoch 35: loss 0.1365838860844775
Epoch 36: loss 0.13432577694804207
Epoch 37: loss 0.13229558416330794
Epoch 38: loss 0.13047285900208663
Epoch 39: loss 0.12873317990421665
Epoch 40: loss 0.12689334580744654
Epoch 41: loss 0.12532970714400551
Epoch 42: loss 0.1240100682818935
Epoch 43: loss 0.12240881170589292
Epoch 44: loss 0.12112049540467337
Epoch 45: loss 0.119886934562764
Epoch 46: loss 0.1186578405971369
Epoch 47: loss 0.11746792038249347
Epoch 48: loss 0.11629580553213885
Epoch 49: loss 0.11533616200016529
-----------Time: 0:06:19.503897, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5167465209960938-------------


Epoch 0: loss 3.225502705852774
Epoch 1: loss 1.0628174475964935
Epoch 2: loss 1.0361006190881838
Epoch 3: loss 0.997743455043884
Epoch 4: loss 0.9411041038784402
Epoch 5: loss 0.8575570119859862
Epoch 6: loss 0.7398882283666589
Epoch 7: loss 0.6027014346437781
Epoch 8: loss 0.4719661312656859
Epoch 9: loss 0.3648995193630019
Epoch 10: loss 0.2854764697796758
Epoch 11: loss 0.2287199296920179
Epoch 12: loss 0.18826756296175987
Epoch 13: loss 0.15944058260495017
Epoch 14: loss 0.13794036271570168
Epoch 15: loss 0.12158786501004165
Epoch 16: loss 0.10877683537932697
Epoch 17: loss 0.0987182625018915
Epoch 18: loss 0.09038468941138646
Epoch 19: loss 0.08353595558377697
Epoch 20: loss 0.0778506069062779
Epoch 21: loss 0.07316100285421447
Epoch 22: loss 0.06907997843913244
Epoch 23: loss 0.06554052665983613
Epoch 24: loss 0.06250455507997457
Epoch 25: loss 0.0597375453778458
Epoch 26: loss 0.057499041749430845
Epoch 27: loss 0.055241578595349165
Epoch 28: loss 0.05345438280795631
Epoch 29: loss 0.05175103190445653
Epoch 30: loss 0.050225489266933855
Epoch 31: loss 0.04887240060998604
Epoch 32: loss 0.047752639617063934
Epoch 33: loss 0.04651626394155105
Epoch 34: loss 0.04545750046911578
Epoch 35: loss 0.044640418012434534
Epoch 36: loss 0.0438087701558451
Epoch 37: loss 0.04295059143605603
Epoch 38: loss 0.042139176250946396
Epoch 39: loss 0.04162696576583574
Epoch 40: loss 0.040862133194727375
Epoch 41: loss 0.0403063589450764
Epoch 42: loss 0.039747506724452764
Epoch 43: loss 0.03931956297134076
Epoch 44: loss 0.03876716634456135
Epoch 45: loss 0.03833221922293832
Epoch 46: loss 0.037887704031478134
Epoch 47: loss 0.03749321458143116
Epoch 48: loss 0.03705298039270134
Epoch 49: loss 0.03677837278687837
-----------Time: 0:05:20.801599, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4117376804351807-------------


Epoch 0: loss 3.0113900052327836
Epoch 1: loss 1.070838529043317
Epoch 2: loss 1.038083516494809
Epoch 3: loss 0.9830185408628525
Epoch 4: loss 0.8921608728776489
Epoch 5: loss 0.7562993819652659
Epoch 6: loss 0.5901707084753255
Epoch 7: loss 0.4297208948554132
Epoch 8: loss 0.30411278989212615
Epoch 9: loss 0.21585451700947997
Epoch 10: loss 0.15851585572264387
Epoch 11: loss 0.1212665562585296
Epoch 12: loss 0.09670485876998933
Epoch 13: loss 0.08023551625846326
Epoch 14: loss 0.06852009586783257
Epoch 15: loss 0.06032664175669482
Epoch 16: loss 0.05412151783607523
Epoch 17: loss 0.04956060012700183
Epoch 18: loss 0.04626545738459217
Epoch 19: loss 0.043458400690212014
Epoch 20: loss 0.04142311817645832
Epoch 21: loss 0.03986216155187211
Epoch 22: loss 0.03859542466116671
Epoch 23: loss 0.03752284254341945
Epoch 24: loss 0.036590632793654106
Epoch 25: loss 0.03600333305911372
Epoch 26: loss 0.03538703701199073
Epoch 27: loss 0.03482347967882011
Epoch 28: loss 0.03451857187639863
Epoch 29: loss 0.03406558248714269
Epoch 30: loss 0.033778536644454814
Epoch 31: loss 0.03349132402159391
Epoch 32: loss 0.03312841596415083
Epoch 33: loss 0.03293725544644511
Epoch 34: loss 0.03281141520498595
Epoch 35: loss 0.032579211509052734
Epoch 36: loss 0.032352979097954926
Epoch 37: loss 0.032186308984055736
Epoch 38: loss 0.031999244852484794
Epoch 39: loss 0.03192512121693702
Epoch 40: loss 0.03184726989466966
Epoch 41: loss 0.03162949443442819
Epoch 42: loss 0.0314874902039426
Epoch 43: loss 0.031379402226699056
Epoch 44: loss 0.031283243433200124
Epoch 45: loss 0.031152172127557293
Epoch 46: loss 0.03103582012756257
Epoch 47: loss 0.030943566838065654
Epoch 48: loss 0.03089545672843481
Epoch 49: loss 0.030723899932068737
-----------Time: 0:07:05.959977, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2418566942214966-------------


Epoch 0: loss 16.12926034683634
Epoch 1: loss 16.129258031881392
Epoch 2: loss 16.12925996671493
Epoch 3: loss 16.129260467666203
Epoch 4: loss 16.12925940094077
Epoch 5: loss 16.12925458952679
Epoch 6: loss 16.129251737319755
Epoch 7: loss 16.129252263163018
Epoch 8: loss 16.12925322513466
Epoch 9: loss 16.12925614112742
Epoch 10: loss 16.129250946480532
Epoch 11: loss 16.12925092418146
Epoch 12: loss 16.129249420290474
Epoch 13: loss 16.129241720368643
Epoch 14: loss 16.129246854341307
Epoch 15: loss 16.12924554180749
Epoch 16: loss 16.129244307061132
Epoch 17: loss 16.129241213194373
Epoch 18: loss 16.129244675255134
Epoch 19: loss 16.129240324861527
Epoch 20: loss 16.12924051933019
Epoch 21: loss 16.129237089940165
Epoch 22: loss 16.129236609213635
Epoch 23: loss 16.129235660206565
Epoch 24: loss 16.129232664870592
Epoch 25: loss 16.129228181719714
Epoch 26: loss 16.129229691833697
Epoch 27: loss 16.129227439627304
Epoch 28: loss 16.129231883365865
Epoch 29: loss 16.12922253020112
Epoch 30: loss 16.129229820960887
Epoch 31: loss 16.129225678519102
Epoch 32: loss 16.129219469523676
Epoch 33: loss 16.12921916252248
Epoch 34: loss 16.129222862612885
Epoch 35: loss 16.129220492688127
Epoch 36: loss 16.129222414557088
Epoch 37: loss 16.12921537375437
Epoch 38: loss 16.129212273664614
Epoch 39: loss 16.129213780667097
Epoch 40: loss 16.129212228029303
Epoch 41: loss 16.12921112811455
Epoch 42: loss 16.129214115671775
Epoch 43: loss 16.129210140732333
Epoch 44: loss 16.12921463166196
Epoch 45: loss 16.129206258619266
Epoch 46: loss 16.12920512136653
Epoch 47: loss 16.12920556631083
Epoch 48: loss 16.129202496298895
Epoch 49: loss 16.12920286708581
-----------Time: 0:04:47.848003, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017197608947754-------------


Epoch 0: loss 16.12920422058769
Epoch 1: loss 16.1291978969854
Epoch 2: loss 16.12919912706451
Epoch 3: loss 16.12919892118702
Epoch 4: loss 16.12919840571542
Epoch 5: loss 16.129193275372835
Epoch 6: loss 16.12919828851564
Epoch 7: loss 16.12918933673421
Epoch 8: loss 16.12919232999585
Epoch 9: loss 16.129191448404583
Epoch 10: loss 16.129189225757425
Epoch 11: loss 16.129186763524878
Epoch 12: loss 16.1291850630909
Epoch 13: loss 16.12918851114992
Epoch 14: loss 16.129183554532673
Epoch 15: loss 16.129182261186426
Epoch 16: loss 16.129180600164766
Epoch 17: loss 16.129181660148618
Epoch 18: loss 16.12917793724056
Epoch 19: loss 16.129177703878167
Epoch 20: loss 16.12917614605454
Epoch 21: loss 16.129168931007904
Epoch 22: loss 16.129176561958186
Epoch 23: loss 16.129176226953504
Epoch 24: loss 16.129170167310008
Epoch 25: loss 16.129172912170343
Epoch 26: loss 16.12917084043087
Epoch 27: loss 16.129168612597883
Epoch 28: loss 16.129172276387465
Epoch 29: loss 16.129168138094347
Epoch 30: loss 16.129168468950365
Epoch 31: loss 16.129161501267866
Epoch 32: loss 16.12916185390437
Epoch 33: loss 16.129163342237863
Epoch 34: loss 16.129161019504167
Epoch 35: loss 16.129161101440296
Epoch 36: loss 16.12915427844248
Epoch 37: loss 16.129157216734313
Epoch 38: loss 16.12915348967759
Epoch 39: loss 16.129152636089813
Epoch 40: loss 16.12915418924619
Epoch 41: loss 16.12915297057591
Epoch 42: loss 16.129150406182493
Epoch 43: loss 16.129150378697588
Epoch 44: loss 16.129145158640128
Epoch 45: loss 16.12914769139998
Epoch 46: loss 16.129145279988574
Epoch 47: loss 16.129145719228458
Epoch 48: loss 16.12913692198479
Epoch 49: loss 16.129137481535953
-----------Time: 0:05:07.521046, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017197608947754-------------


Epoch 0: loss 16.129166896087828
Epoch 1: loss 16.129164997036526
Epoch 2: loss 16.129159381300163
Epoch 3: loss 16.12915894724611
Epoch 4: loss 16.129161571276583
Epoch 5: loss 16.12915572943799
Epoch 6: loss 16.129161094698716
Epoch 7: loss 16.129154800655662
Epoch 8: loss 16.129151290885257
Epoch 9: loss 16.129155289679524
Epoch 10: loss 16.129154289332728
Epoch 11: loss 16.129156790459007
Epoch 12: loss 16.129148575584157
Epoch 13: loss 16.12915374741339
Epoch 14: loss 16.1291466905346
Epoch 15: loss 16.12914965216267
Epoch 16: loss 16.129143756391432
Epoch 17: loss 16.129145846281315
Epoch 18: loss 16.12914401879448
Epoch 19: loss 16.129137247654974
Epoch 20: loss 16.129143607558085
Epoch 21: loss 16.12913666891624
Epoch 22: loss 16.129136165890635
Epoch 23: loss 16.129137867361777
Epoch 24: loss 16.129140423457866
Epoch 25: loss 16.12913710660037
Epoch 26: loss 16.129132494840885
Epoch 27: loss 16.129130647647894
Epoch 28: loss 16.12912962240911
Epoch 29: loss 16.1291299356333
Epoch 30: loss 16.129127718690558
Epoch 31: loss 16.129127399761952
Epoch 32: loss 16.12912142257317
Epoch 33: loss 16.129126035369822
Epoch 34: loss 16.129123470457824
Epoch 35: loss 16.129122862678432
Epoch 36: loss 16.1291229342429
Epoch 37: loss 16.129118680824334
Epoch 38: loss 16.129119101395226
Epoch 39: loss 16.12911453319672
Epoch 40: loss 16.12911567459812
Epoch 41: loss 16.129116101392007
Epoch 42: loss 16.12911004797151
Epoch 43: loss 16.12911739421967
Epoch 44: loss 16.129115367596924
Epoch 45: loss 16.12910798712228
Epoch 46: loss 16.12911050484322
Epoch 47: loss 16.129102754618824
Epoch 48: loss 16.12910411693662
Epoch 49: loss 16.12910284588945
-----------Time: 0:04:48.870832, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017184257507324-------------


Epoch 0: loss 16.12916596834267
Epoch 1: loss 16.129169779409853
Epoch 2: loss 16.129170949333318
Epoch 3: loss 16.129167113374148
Epoch 4: loss 16.12916772322787
Epoch 5: loss 16.12917250974986
Epoch 6: loss 16.129165554513357
Epoch 7: loss 16.12916270749215
Epoch 8: loss 16.129163502998622
Epoch 9: loss 16.129165736536024
Epoch 10: loss 16.12915895243194
Epoch 11: loss 16.129156671184894
Epoch 12: loss 16.129156117338145
Epoch 13: loss 16.12915547377652
Epoch 14: loss 16.129152889158362
Epoch 15: loss 16.129152609642073
Epoch 16: loss 16.129150642656384
Epoch 17: loss 16.129151060115777
Epoch 18: loss 16.129148869620774
Epoch 19: loss 16.129144588717306
Epoch 20: loss 16.129141702802368
Epoch 21: loss 16.129144503669675
Epoch 22: loss 16.129146686385933
Epoch 23: loss 16.129145735823116
Epoch 24: loss 16.12914639805373
Epoch 25: loss 16.129139209454834
Epoch 26: loss 16.129134502276056
Epoch 27: loss 16.129138937198707
Epoch 28: loss 16.129134631921833
Epoch 29: loss 16.129135734429497
Epoch 30: loss 16.12913437885328
Epoch 31: loss 16.129131833128852
Epoch 32: loss 16.129131240388375
Epoch 33: loss 16.12913262967249
Epoch 34: loss 16.129132864590634
Epoch 35: loss 16.12912989518382
Epoch 36: loss 16.12912631903478
Epoch 37: loss 16.129124951012567
Epoch 38: loss 16.129124989387716
Epoch 39: loss 16.129123586101855
Epoch 40: loss 16.129123114709817
Epoch 41: loss 16.12912150087922
Epoch 42: loss 16.129113872521852
Epoch 43: loss 16.129116268894347
Epoch 44: loss 16.12912046993602
Epoch 45: loss 16.129112434490924
Epoch 46: loss 16.129108062316828
Epoch 47: loss 16.12911521824499
Epoch 48: loss 16.12911502844358
Epoch 49: loss 16.12911535203943
-----------Time: 0:06:00.803223, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.01718282699585-------------


Epoch 0: loss 16.12916999669617
Epoch 1: loss 16.12916902227853
Epoch 2: loss 16.129168193064157
Epoch 3: loss 16.129167634031578
Epoch 4: loss 16.129168188915493
Epoch 5: loss 16.12917040170957
Epoch 6: loss 16.129168060825467
Epoch 7: loss 16.12916405373387
Epoch 8: loss 16.129160644050003
Epoch 9: loss 16.129160873263736
Epoch 10: loss 16.12916048951224
Epoch 11: loss 16.129156629179665
Epoch 12: loss 16.129156948108267
Epoch 13: loss 16.12915451543496
Epoch 14: loss 16.12915309296152
Epoch 15: loss 16.129149937901957
Epoch 16: loss 16.12915050160178
Epoch 17: loss 16.129151155016487
Epoch 18: loss 16.12914852061435
Epoch 19: loss 16.129151844732007
Epoch 20: loss 16.12914852735593
Epoch 21: loss 16.129143518880376
Epoch 22: loss 16.12914439528581
Epoch 23: loss 16.1291453282168
Epoch 24: loss 16.129140030371875
Epoch 25: loss 16.129137128899448
Epoch 26: loss 16.129141702802368
Epoch 27: loss 16.129138002711965
Epoch 28: loss 16.12914000236839
Epoch 29: loss 16.129134841947987
Epoch 30: loss 16.129137045407568
Epoch 31: loss 16.129135625008463
Epoch 32: loss 16.129136714551553
Epoch 33: loss 16.12913061653291
Epoch 34: loss 16.129135869779684
Epoch 35: loss 16.129129525952653
Epoch 36: loss 16.129126985414057
Epoch 37: loss 16.12912500183371
Epoch 38: loss 16.129119482553804
Epoch 39: loss 16.12912995948812
Epoch 40: loss 16.129123643664578
Epoch 41: loss 16.129124425169305
Epoch 42: loss 16.129121349971538
Epoch 43: loss 16.129119277194896
Epoch 44: loss 16.129120428449372
Epoch 45: loss 16.129112100004825
Epoch 46: loss 16.12911416966997
Epoch 47: loss 16.12911040683101
Epoch 48: loss 16.12910947078852
Epoch 49: loss 16.129115335963355
-----------Time: 0:07:53.418359, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129267716939324
Epoch 1: loss 16.129249961172647
Epoch 2: loss 16.12923517896145
Epoch 3: loss 16.12922595129381
Epoch 4: loss 16.1292165234531
Epoch 5: loss 16.1292036895586
Epoch 6: loss 16.129194095252714
Epoch 7: loss 16.129177963688296
Epoch 8: loss 16.12916998010151
Epoch 9: loss 16.129158782855267
Epoch 10: loss 16.1291449662458
Epoch 11: loss 16.129135177989834
Epoch 12: loss 16.129119106581058
Epoch 13: loss 16.129111414956554
Epoch 14: loss 16.129096283738935
Epoch 15: loss 16.129078320020433
Epoch 16: loss 16.12906530151035
Epoch 17: loss 16.129057042555942
Epoch 18: loss 16.129043576508646
Epoch 19: loss 16.129034939507157
Epoch 20: loss 16.129020015722777
Epoch 21: loss 16.129002904554888
Epoch 22: loss 16.128993817941854
Epoch 23: loss 16.128980118013583
Epoch 24: loss 16.12897404073826
Epoch 25: loss 16.128953771917892
Epoch 26: loss 16.128944808727635
Epoch 27: loss 16.128929609057046
Epoch 28: loss 16.128921289428412
Epoch 29: loss 16.12891050912298
Epoch 30: loss 16.128891389482757
Epoch 31: loss 16.12888284686339
Epoch 32: loss 16.12886865324401
Epoch 33: loss 16.128859595153045
Epoch 34: loss 16.12884709626323
Epoch 35: loss 16.128836623996158
Epoch 36: loss 16.128823017931424
Epoch 37: loss 16.12880727789725
Epoch 38: loss 16.128797627584387
Epoch 39: loss 16.128785517113307
Epoch 40: loss 16.12877617069014
Epoch 41: loss 16.128757230998254
Epoch 42: loss 16.12874726953553
Epoch 43: loss 16.128732163728483
Epoch 44: loss 16.128720557320175
Epoch 45: loss 16.1287086278346
Epoch 46: loss 16.128695883654977
Epoch 47: loss 16.12868275883536
Epoch 48: loss 16.128673955368697
Epoch 49: loss 16.128665995636734
-----------Time: 0:04:10.300539, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017133712768555-------------


Epoch 0: loss 16.129154056488918
Epoch 1: loss 16.129147480336655
Epoch 2: loss 16.1291327551696
Epoch 3: loss 16.129114812194427
Epoch 4: loss 16.129105148398402
Epoch 5: loss 16.12909374527467
Epoch 6: loss 16.129083713803233
Epoch 7: loss 16.129071066598648
Epoch 8: loss 16.12905977289595
Epoch 9: loss 16.129043195868654
Epoch 10: loss 16.129033202772362
Epoch 11: loss 16.129022286079575
Epoch 12: loss 16.12900363160839
Epoch 13: loss 16.128997535145498
Epoch 14: loss 16.128981998914476
Epoch 15: loss 16.128968848165705
Epoch 16: loss 16.128958710384733
Epoch 17: loss 16.128943198527118
Epoch 18: loss 16.128934959278865
Epoch 19: loss 16.128916334366917
Epoch 20: loss 16.12890746400303
Epoch 21: loss 16.12889697358555
Epoch 22: loss 16.128877585837866
Epoch 23: loss 16.128866944512705
Epoch 24: loss 16.128859078125696
Epoch 25: loss 16.128846200151635
Epoch 26: loss 16.12883278077682
Epoch 27: loss 16.1288214201769
Epoch 28: loss 16.12880780892634
Epoch 29: loss 16.128798677715157
Epoch 30: loss 16.128784373118997
Epoch 31: loss 16.128768588486672
Epoch 32: loss 16.128758888389832
Epoch 33: loss 16.12874641854067
Epoch 34: loss 16.128734379634057
Epoch 35: loss 16.12871897719748
Epoch 36: loss 16.128708109770084
Epoch 37: loss 16.128692139485015
Epoch 38: loss 16.128681257537295
Epoch 39: loss 16.128670116298025
Epoch 40: loss 16.12866016365122
Epoch 41: loss 16.128651521463897
Epoch 42: loss 16.128631106403095
Epoch 43: loss 16.128627651083917
Epoch 44: loss 16.128612482528315
Epoch 45: loss 16.12859422062453
Epoch 46: loss 16.12858650929387
Epoch 47: loss 16.12857232708332
Epoch 48: loss 16.128566073489747
Epoch 49: loss 16.128551051174576
-----------Time: 0:03:58.438327, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017107009887695-------------


Epoch 0: loss 16.129173486760415
Epoch 1: loss 16.12915718717508
Epoch 2: loss 16.12915052286369
Epoch 3: loss 16.12913281013941
Epoch 4: loss 16.129121259738078
Epoch 5: loss 16.129105458511095
Epoch 6: loss 16.129100189706826
Epoch 7: loss 16.129092858497575
Epoch 8: loss 16.12907087420432
Epoch 9: loss 16.129056141258516
Epoch 10: loss 16.12904562957913
Epoch 11: loss 16.12903965135318
Epoch 12: loss 16.129024429902103
Epoch 13: loss 16.12900891960024
Epoch 14: loss 16.128998513711803
Epoch 15: loss 16.12898419252098
Epoch 16: loss 16.128972554479105
Epoch 17: loss 16.128964836925448
Epoch 18: loss 16.128950668198055
Epoch 19: loss 16.128934912087804
Epoch 20: loss 16.128925214583877
Epoch 21: loss 16.128912199185294
Epoch 22: loss 16.128902816461313
Epoch 23: loss 16.128884116873397
Epoch 24: loss 16.12887660882731
Epoch 25: loss 16.12885902160014
Epoch 26: loss 16.128851111652153
Epoch 27: loss 16.128832468589795
Epoch 28: loss 16.12882128741963
Epoch 29: loss 16.128811096224595
Epoch 30: loss 16.1288044874016
Epoch 31: loss 16.128793967424883
Epoch 32: loss 16.128776113646
Epoch 33: loss 16.12876407110931
Epoch 34: loss 16.12875508925006
Epoch 35: loss 16.128740290444206
Epoch 36: loss 16.128725271759116
Epoch 37: loss 16.128715527582713
Epoch 38: loss 16.128701332926166
Epoch 39: loss 16.128683224522984
Epoch 40: loss 16.128672116473034
Epoch 41: loss 16.128667477228642
Epoch 42: loss 16.128654260101236
Epoch 43: loss 16.128638529401552
Epoch 44: loss 16.12862912489708
Epoch 45: loss 16.128615722116923
Epoch 46: loss 16.128602697383844
Epoch 47: loss 16.12859585571704
Epoch 48: loss 16.12857610547977
Epoch 49: loss 16.1285678973465
-----------Time: 0:05:28.271789, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017110824584961-------------


Epoch 0: loss 16.12916710196532
Epoch 1: loss 16.129155249748624
Epoch 2: loss 16.12914347272648
Epoch 3: loss 16.129131168305324
Epoch 4: loss 16.129119485665303
Epoch 5: loss 16.12910225159322
Epoch 6: loss 16.129092248125268
Epoch 7: loss 16.12908039383424
Epoch 8: loss 16.129070686477238
Epoch 9: loss 16.12905678585731
Epoch 10: loss 16.129040625770823
Epoch 11: loss 16.12903208055854
Epoch 12: loss 16.12902151339076
Epoch 13: loss 16.129008278631527
Epoch 14: loss 16.128991160722062
Epoch 15: loss 16.12897854411388
Epoch 16: loss 16.128967904863053
Epoch 17: loss 16.128956014271207
Epoch 18: loss 16.1289467243736
Epoch 19: loss 16.128930131270227
Epoch 20: loss 16.12892110948008
Epoch 21: loss 16.128902842390467
Epoch 22: loss 16.128900047746153
Epoch 23: loss 16.12888059517558
Epoch 24: loss 16.128867942785167
Epoch 25: loss 16.128858392040257
Epoch 26: loss 16.128843313718114
Epoch 27: loss 16.12883306703469
Epoch 28: loss 16.128819054919393
Epoch 29: loss 16.128803363113445
Epoch 30: loss 16.12879576327815
Epoch 31: loss 16.128780055914707
Epoch 32: loss 16.12877526628122
Epoch 33: loss 16.128754825291264
Epoch 34: loss 16.128746639457066
Epoch 35: loss 16.128732028896877
Epoch 36: loss 16.128720750751672
Epoch 37: loss 16.128713298712558
Epoch 38: loss 16.12869626896222
Epoch 39: loss 16.128680750363024
Epoch 40: loss 16.12867652702228
Epoch 41: loss 16.12865762207546
Epoch 42: loss 16.12864472180232
Epoch 43: loss 16.12862992870088
Epoch 44: loss 16.128623021174022
Epoch 45: loss 16.12860782980076
Epoch 46: loss 16.128595583460914
Epoch 47: loss 16.12858913228718
Epoch 48: loss 16.128579101334324
Epoch 49: loss 16.128566039263262
-----------Time: 0:06:47.994073, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017111301422119-------------


Epoch 0: loss 16.129165308186383
Epoch 1: loss 16.129152397023006
Epoch 2: loss 16.129141909198438
Epoch 3: loss 16.129126760867578
Epoch 4: loss 16.129117473562882
Epoch 5: loss 16.12910454684201
Epoch 6: loss 16.129086873011463
Epoch 7: loss 16.12908414370862
Epoch 8: loss 16.12906725812438
Epoch 9: loss 16.129057510317896
Epoch 10: loss 16.12904319483149
Epoch 11: loss 16.12903108436041
Epoch 12: loss 16.12901627985014
Epoch 13: loss 16.12900293515129
Epoch 14: loss 16.128990593910736
Epoch 15: loss 16.128983154836202
Epoch 16: loss 16.128967605640604
Epoch 17: loss 16.128958592147786
Epoch 18: loss 16.12894237190566
Epoch 19: loss 16.128926123141465
Epoch 20: loss 16.128920304120527
Epoch 21: loss 16.12890581490878
Epoch 22: loss 16.12889480072237
Epoch 23: loss 16.128880120153465
Epoch 24: loss 16.128871513229793
Epoch 25: loss 16.12884999306841
Epoch 26: loss 16.128846085544772
Epoch 27: loss 16.12883354309397
Epoch 28: loss 16.12881983434979
Epoch 29: loss 16.128807236929184
Epoch 30: loss 16.128795831212535
Epoch 31: loss 16.12878263586562
Epoch 32: loss 16.128771490477686
Epoch 33: loss 16.12875868043801
Epoch 34: loss 16.12874291810476
Epoch 35: loss 16.12873537583219
Epoch 36: loss 16.12871789284022
Epoch 37: loss 16.128709562839926
Epoch 38: loss 16.128699480547343
Epoch 39: loss 16.128682743796453
Epoch 40: loss 16.1286725194121
Epoch 41: loss 16.128660196840535
Epoch 42: loss 16.12864439146489
Epoch 43: loss 16.128633304158264
Epoch 44: loss 16.12862472212658
Epoch 45: loss 16.12860961217087
Epoch 46: loss 16.128600612679794
Epoch 47: loss 16.128581926056455
Epoch 48: loss 16.128573210230336
Epoch 49: loss 16.128558815919295
-----------Time: 0:06:45.766206, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0171122550964355-------------


Epoch 0: loss 16.129353656011666
Epoch 1: loss 16.129235316385973
Epoch 2: loss 16.12910833872162
Epoch 3: loss 16.128981227262827
Epoch 4: loss 16.12885954070182
Epoch 5: loss 16.128737293552483
Epoch 6: loss 16.128612119520145
Epoch 7: loss 16.12849048533603
Epoch 8: loss 16.128369515975447
Epoch 9: loss 16.128239063285758
Epoch 10: loss 16.128114784327845
Epoch 11: loss 16.127988566906314
Epoch 12: loss 16.12786620826161
Epoch 13: loss 16.127742184965165
Epoch 14: loss 16.127622069710945
Epoch 15: loss 16.127498810287403
Epoch 16: loss 16.127376590104387
Epoch 17: loss 16.12724934381399
Epoch 18: loss 16.12712251705732
Epoch 19: loss 16.12699829877363
Epoch 20: loss 16.126874817915102
Epoch 21: loss 16.126753591855884
Epoch 22: loss 16.126629123615142
Epoch 23: loss 16.1265065974681
Epoch 24: loss 16.12637737174602
Epoch 25: loss 16.12626105200149
Epoch 26: loss 16.12612957614735
Epoch 27: loss 16.12600746901545
Epoch 28: loss 16.125886327485276
Epoch 29: loss 16.125761623289222
Epoch 30: loss 16.125637710450977
Epoch 31: loss 16.125512685251987
Epoch 32: loss 16.12539004242375
Epoch 33: loss 16.12526617418365
Epoch 34: loss 16.125141867740833
Epoch 35: loss 16.12502371117497
Epoch 36: loss 16.124893540075902
Epoch 37: loss 16.124767959992997
Epoch 38: loss 16.1246490742993
Epoch 39: loss 16.124526623346807
Epoch 40: loss 16.124399282155704
Epoch 41: loss 16.12427792126488
Epoch 42: loss 16.12414722691547
Epoch 43: loss 16.124027889535892
Epoch 44: loss 16.12390408767443
Epoch 45: loss 16.1237774652395
Epoch 46: loss 16.12364953390083
Epoch 47: loss 16.123531927551635
Epoch 48: loss 16.123406301314834
Epoch 49: loss 16.123282935581756
-----------Time: 0:03:40.599231, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016432285308838-------------


Epoch 0: loss 16.12911807926794
Epoch 1: loss 16.12899156521688
Epoch 2: loss 16.12886671944764
Epoch 3: loss 16.12874031274328
Epoch 4: loss 16.12862421598948
Epoch 5: loss 16.128492888450104
Epoch 6: loss 16.128371327386205
Epoch 7: loss 16.12824728853227
Epoch 8: loss 16.128124533171494
Epoch 9: loss 16.12800148947852
Epoch 10: loss 16.12787467983509
Epoch 11: loss 16.127751536574163
Epoch 12: loss 16.127627484237067
Epoch 13: loss 16.1275023293923
Epoch 14: loss 16.127382235918567
Epoch 15: loss 16.12726312930847
Epoch 16: loss 16.12713181058501
Epoch 17: loss 16.127006483052075
Epoch 18: loss 16.12688594359688
Epoch 19: loss 16.126760140523242
Epoch 20: loss 16.126635327943323
Epoch 21: loss 16.126513577596594
Epoch 22: loss 16.126393448340625
Epoch 23: loss 16.126266922362152
Epoch 24: loss 16.12613832879287
Epoch 25: loss 16.126018012328405
Epoch 26: loss 16.125889009078474
Epoch 27: loss 16.125769354326042
Epoch 28: loss 16.125644480034733
Epoch 29: loss 16.125523307389575
Epoch 30: loss 16.12540095652362
Epoch 31: loss 16.125273163128053
Epoch 32: loss 16.125142283644475
Epoch 33: loss 16.125024648254627
Epoch 34: loss 16.12490091951338
Epoch 35: loss 16.124778935285676
Epoch 36: loss 16.124650407576443
Epoch 37: loss 16.124527892319648
Epoch 38: loss 16.124406998153614
Epoch 39: loss 16.12428115929774
Epoch 40: loss 16.12415683055585
Epoch 41: loss 16.124033699740917
Epoch 42: loss 16.123908746624977
Epoch 43: loss 16.12378401494402
Epoch 44: loss 16.123660583869473
Epoch 45: loss 16.123537352449418
Epoch 46: loss 16.123410097343108
Epoch 47: loss 16.123295123321714
Epoch 48: loss 16.12316736622697
Epoch 49: loss 16.12304513722804
-----------Time: 0:04:42.512590, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016421794891357-------------


Epoch 0: loss 16.129113824293626
Epoch 1: loss 16.12899047567379
Epoch 2: loss 16.128866087294842
Epoch 3: loss 16.12874276097408
Epoch 4: loss 16.128618677003413
Epoch 5: loss 16.12849997333238
Epoch 6: loss 16.128371866194044
Epoch 7: loss 16.1282540311497
Epoch 8: loss 16.1281256590154
Epoch 9: loss 16.127998119725554
Epoch 10: loss 16.12787709539516
Epoch 11: loss 16.127753015054576
Epoch 12: loss 16.127628034453732
Epoch 13: loss 16.127505803899055
Epoch 14: loss 16.127379287255078
Epoch 15: loss 16.12725336750024
Epoch 16: loss 16.127131824586755
Epoch 17: loss 16.12700749325195
Epoch 18: loss 16.12688397868552
Epoch 19: loss 16.12675539030207
Epoch 20: loss 16.12663497115815
Epoch 21: loss 16.126516219777475
Epoch 22: loss 16.12638974565731
Epoch 23: loss 16.126263301096383
Epoch 24: loss 16.126142009695695
Epoch 25: loss 16.126018929701903
Epoch 26: loss 16.12589467719173
Epoch 27: loss 16.125775145343493
Epoch 28: loss 16.125645411409977
Epoch 29: loss 16.125516704789575
Epoch 30: loss 16.12539633128097
Epoch 31: loss 16.125277681024002
Epoch 32: loss 16.125145651323113
Epoch 33: loss 16.12502263978229
Epoch 34: loss 16.124900591768863
Epoch 35: loss 16.12477750658924
Epoch 36: loss 16.124648763668024
Epoch 37: loss 16.12452984167351
Epoch 38: loss 16.1244002456831
Epoch 39: loss 16.124284981255173
Epoch 40: loss 16.124158861327263
Epoch 41: loss 16.124033068106705
Epoch 42: loss 16.12390468352641
Epoch 43: loss 16.123786705353133
Epoch 44: loss 16.123661101933987
Epoch 45: loss 16.123535875006173
Epoch 46: loss 16.123415137970817
Epoch 47: loss 16.123288260393004
Epoch 48: loss 16.123164012550077
Epoch 49: loss 16.12304043938376
-----------Time: 0:06:13.521354, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016417026519775-------------


Epoch 0: loss 16.12910593975621
Epoch 1: loss 16.128985217759762
Epoch 2: loss 16.12885368278715
Epoch 3: loss 16.128735912565695
Epoch 4: loss 16.12860734596273
Epoch 5: loss 16.128487593198095
Epoch 6: loss 16.128363266011956
Epoch 7: loss 16.128234506496078
Epoch 8: loss 16.12811203168876
Epoch 9: loss 16.12799246716979
Epoch 10: loss 16.127865603075136
Epoch 11: loss 16.12774247381595
Epoch 12: loss 16.12761778777031
Epoch 13: loss 16.12749512834741
Epoch 14: loss 16.127364787153084
Epoch 15: loss 16.1272464226354
Epoch 16: loss 16.1271252022806
Epoch 17: loss 16.126994711215758
Epoch 18: loss 16.12687377763741
Epoch 19: loss 16.126753881225255
Epoch 20: loss 16.126626570630552
Epoch 21: loss 16.126504053818007
Epoch 22: loss 16.12637918004528
Epoch 23: loss 16.12624918267155
Epoch 24: loss 16.126125331544696
Epoch 25: loss 16.126003820264774
Epoch 26: loss 16.12588555687079
Epoch 27: loss 16.125758568316197
Epoch 28: loss 16.125631240608254
Epoch 29: loss 16.12551101334008
Epoch 30: loss 16.12538784155708
Epoch 31: loss 16.125265280664973
Epoch 32: loss 16.125138288998876
Epoch 33: loss 16.125016082299023
Epoch 34: loss 16.124885015088363
Epoch 35: loss 16.124767291020802
Epoch 36: loss 16.124641119753168
Epoch 37: loss 16.124517846846462
Epoch 38: loss 16.124401085787717
Epoch 39: loss 16.124268516241823
Epoch 40: loss 16.12414484973055
Epoch 41: loss 16.124023471207902
Epoch 42: loss 16.123900332095637
Epoch 43: loss 16.123771436192406
Epoch 44: loss 16.123647517649747
Epoch 45: loss 16.123530308016623
Epoch 46: loss 16.123404441157263
Epoch 47: loss 16.123277548021957
Epoch 48: loss 16.123157057832156
Epoch 49: loss 16.123028571090988
-----------Time: 0:05:45.516471, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016420841217041-------------


Epoch 0: loss 16.12911831003742
Epoch 1: loss 16.128991959858613
Epoch 2: loss 16.128867647192802
Epoch 3: loss 16.128742696151193
Epoch 4: loss 16.12861868322641
Epoch 5: loss 16.12850303193549
Epoch 6: loss 16.128373568702354
Epoch 7: loss 16.12824899467066
Epoch 8: loss 16.128128791775893
Epoch 9: loss 16.128004840562497
Epoch 10: loss 16.127875778194092
Epoch 11: loss 16.127751662071272
Epoch 12: loss 16.127631252261853
Epoch 13: loss 16.127498045895916
Epoch 14: loss 16.12738100013505
Epoch 15: loss 16.127257826277717
Epoch 16: loss 16.12713364585059
Epoch 17: loss 16.1270111917866
Epoch 18: loss 16.12688486235112
Epoch 19: loss 16.126764194287315
Epoch 20: loss 16.12663386242749
Epoch 21: loss 16.126513785029832
Epoch 22: loss 16.126385738047134
Epoch 23: loss 16.126263441113817
Epoch 24: loss 16.126142247725337
Epoch 25: loss 16.126016363752736
Epoch 26: loss 16.125897270107217
Epoch 27: loss 16.125776079311652
Epoch 28: loss 16.125639576831546
Epoch 29: loss 16.125517470736813
Epoch 30: loss 16.1253954284278
Epoch 31: loss 16.12527403279191
Epoch 32: loss 16.125146882957967
Epoch 33: loss 16.12502727747093
Epoch 34: loss 16.12490082409409
Epoch 35: loss 16.124777614454526
Epoch 36: loss 16.124655863589215
Epoch 37: loss 16.12452663216272
Epoch 38: loss 16.124408441888956
Epoch 39: loss 16.124281586610213
Epoch 40: loss 16.12416048604811
Epoch 41: loss 16.124033237164795
Epoch 42: loss 16.12390960902867
Epoch 43: loss 16.12378517345866
Epoch 44: loss 16.123658402190383
Epoch 45: loss 16.123541518746023
Epoch 46: loss 16.123414269344128
Epoch 47: loss 16.123286747167523
Epoch 48: loss 16.123161263541075
Epoch 49: loss 16.123040101786163
-----------Time: 0:06:54.285329, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016420841217041-------------


Epoch 0: loss 16.12877936931069
Epoch 1: loss 16.12754828518924
Epoch 2: loss 16.12630699846393
Epoch 3: loss 16.125063654519476
Epoch 4: loss 16.123823334951645
Epoch 5: loss 16.122588703203228
Epoch 6: loss 16.12134543289757
Epoch 7: loss 16.120107580229536
Epoch 8: loss 16.118867198431733
Epoch 9: loss 16.11763256046032
Epoch 10: loss 16.116388972263223
Epoch 11: loss 16.115154555208207
Epoch 12: loss 16.113911057763154
Epoch 13: loss 16.11267642445899
Epoch 14: loss 16.111436424856926
Epoch 15: loss 16.110193829746464
Epoch 16: loss 16.108957620468264
Epoch 17: loss 16.107718630028913
Epoch 18: loss 16.10648191824369
Epoch 19: loss 16.105239871794144
Epoch 20: loss 16.10400439838536
Epoch 21: loss 16.102763065506156
Epoch 22: loss 16.10152551776498
Epoch 23: loss 16.10028886457965
Epoch 24: loss 16.099049677597304
Epoch 25: loss 16.097810087157306
Epoch 26: loss 16.09656968772768
Epoch 27: loss 16.095331989078822
Epoch 28: loss 16.094091150409312
Epoch 29: loss 16.092855157380264
Epoch 30: loss 16.091619887774634
Epoch 31: loss 16.090383268297206
Epoch 32: loss 16.089145514678542
Epoch 33: loss 16.087908649392723
Epoch 34: loss 16.086663536042735
Epoch 35: loss 16.085432716917254
Epoch 36: loss 16.084193750851306
Epoch 37: loss 16.082949637329534
Epoch 38: loss 16.08171347316806
Epoch 39: loss 16.080480949459936
Epoch 40: loss 16.07924461675896
Epoch 41: loss 16.07800122821636
Epoch 42: loss 16.07676761755806
Epoch 43: loss 16.07552929557232
Epoch 44: loss 16.074292518445628
Epoch 45: loss 16.073051297580374
Epoch 46: loss 16.071816129617034
Epoch 47: loss 16.070577515669008
Epoch 48: loss 16.069342352372917
Epoch 49: loss 16.068102180082686
-----------Time: 0:04:25.211094, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009533882141113-------------


Epoch 0: loss 16.128567627164706
Epoch 1: loss 16.127330415983963
Epoch 2: loss 16.12608281499086
Epoch 3: loss 16.124851958008808
Epoch 4: loss 16.12360493368011
Epoch 5: loss 16.122371151370807
Epoch 6: loss 16.121127190312464
Epoch 7: loss 16.11989095043786
Epoch 8: loss 16.118650696730082
Epoch 9: loss 16.11740810161962
Epoch 10: loss 16.11617714403245
Epoch 11: loss 16.114933319880045
Epoch 12: loss 16.113696653730134
Epoch 13: loss 16.112452435454575
Epoch 14: loss 16.111214501368995
Epoch 15: loss 16.109978022427583
Epoch 16: loss 16.10874429353234
Epoch 17: loss 16.107496494959072
Epoch 18: loss 16.10626032561177
Epoch 19: loss 16.105026337424977
Epoch 20: loss 16.10378147921788
Epoch 21: loss 16.10254270917634
Epoch 22: loss 16.10130539168606
Epoch 23: loss 16.100064737632135
Epoch 24: loss 16.09882932334182
Epoch 25: loss 16.09758978735305
Epoch 26: loss 16.096348392762458
Epoch 27: loss 16.095109189704036
Epoch 28: loss 16.093875737213583
Epoch 29: loss 16.092638808660627
Epoch 30: loss 16.091399697391413
Epoch 31: loss 16.090156123714642
Epoch 32: loss 16.088917471391472
Epoch 33: loss 16.087687696692345
Epoch 34: loss 16.08644285611707
Epoch 35: loss 16.08520665721053
Epoch 36: loss 16.083968861068055
Epoch 37: loss 16.082730974173533
Epoch 38: loss 16.081495609667197
Epoch 39: loss 16.080256388976952
Epoch 40: loss 16.079023106063172
Epoch 41: loss 16.07778394034273
Epoch 42: loss 16.076540270209506
Epoch 43: loss 16.075306427744565
Epoch 44: loss 16.074069334800765
Epoch 45: loss 16.07283239328323
Epoch 46: loss 16.07159722635706
Epoch 47: loss 16.070358917854477
Epoch 48: loss 16.069118045477065
Epoch 49: loss 16.067886212003042
-----------Time: 0:05:29.810754, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.009512424468994-------------


Epoch 0: loss 16.128563479537092
Epoch 1: loss 16.12732039073552
Epoch 2: loss 16.12608083815209
Epoch 3: loss 16.124844809340807
Epoch 4: loss 16.123606427718013
Epoch 5: loss 16.122364636929934
Epoch 6: loss 16.12112218287407
Epoch 7: loss 16.11988444636865
Epoch 8: loss 16.11864575567033
Epoch 9: loss 16.11740458770055
Epoch 10: loss 16.11616084963294
Epoch 11: loss 16.114927208378237
Epoch 12: loss 16.113685073250984
Epoch 13: loss 16.112450996386478
Epoch 14: loss 16.111212273536005
Epoch 15: loss 16.10996714203561
Epoch 16: loss 16.1087289849593
Epoch 17: loss 16.107488769108087
Epoch 18: loss 16.106251737357095
Epoch 19: loss 16.105013603617017
Epoch 20: loss 16.10377993591458
Epoch 21: loss 16.102537325246622
Epoch 22: loss 16.101291345862865
Epoch 23: loss 16.100059872804096
Epoch 24: loss 16.098819367583516
Epoch 25: loss 16.097576346197744
Epoch 26: loss 16.096341812980118
Epoch 27: loss 16.0951049046519
Epoch 28: loss 16.093863621038093
Epoch 29: loss 16.092624365602777
Epoch 30: loss 16.091384465050087
Epoch 31: loss 16.09015092803059
Epoch 32: loss 16.08891174727124
Epoch 33: loss 16.087670581375793
Epoch 34: loss 16.086436269074564
Epoch 35: loss 16.085198129630076
Epoch 36: loss 16.083952001931554
Epoch 37: loss 16.082721303117346
Epoch 38: loss 16.081480598242273
Epoch 39: loss 16.080244784124393
Epoch 40: loss 16.079009273377626
Epoch 41: loss 16.077768300913675
Epoch 42: loss 16.076531407105787
Epoch 43: loss 16.07529519834617
Epoch 44: loss 16.07405719269612
Epoch 45: loss 16.07281915333817
Epoch 46: loss 16.07157933835169
Epoch 47: loss 16.070337305903887
Epoch 48: loss 16.069102884181625
Epoch 49: loss 16.067867074212412
-----------Time: 0:04:52.614202, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009510040283203-------------


Epoch 0: loss 16.128570614721934
Epoch 1: loss 16.127328467667265
Epoch 2: loss 16.12608851266335
Epoch 3: loss 16.124848417123417
Epoch 4: loss 16.12360636911812
Epoch 5: loss 16.122369967445593
Epoch 6: loss 16.121129581499126
Epoch 7: loss 16.119890191232205
Epoch 8: loss 16.118650576418805
Epoch 9: loss 16.117408210003486
Epoch 10: loss 16.116173687676103
Epoch 11: loss 16.114934741316315
Epoch 12: loss 16.113690938425815
Epoch 13: loss 16.112455040297476
Epoch 14: loss 16.111219646750488
Epoch 15: loss 16.109980104020135
Epoch 16: loss 16.10873823855609
Epoch 17: loss 16.1074988902944
Epoch 18: loss 16.10625657366306
Epoch 19: loss 16.105019032144885
Epoch 20: loss 16.103784543006817
Epoch 21: loss 16.102541780394013
Epoch 22: loss 16.101304341036705
Epoch 23: loss 16.100062984821264
Epoch 24: loss 16.09882627459179
Epoch 25: loss 16.097584677235208
Epoch 26: loss 16.096345648939288
Epoch 27: loss 16.095112104141045
Epoch 28: loss 16.09387161136646
Epoch 29: loss 16.092632838213426
Epoch 30: loss 16.09139361337451
Epoch 31: loss 16.09015670556488
Epoch 32: loss 16.088916263092855
Epoch 33: loss 16.08768469409621
Epoch 34: loss 16.08643961341696
Epoch 35: loss 16.085201171119945
Epoch 36: loss 16.08396137220954
Epoch 37: loss 16.082722536826534
Epoch 38: loss 16.08148579911216
Epoch 39: loss 16.08024119760369
Epoch 40: loss 16.07901654287549
Epoch 41: loss 16.0777734115501
Epoch 42: loss 16.076529187051545
Epoch 43: loss 16.075293093417375
Epoch 44: loss 16.074059664781746
Epoch 45: loss 16.072817001218315
Epoch 46: loss 16.071582750628473
Epoch 47: loss 16.070344884477276
Epoch 48: loss 16.069104363699203
Epoch 49: loss 16.067872635497547
-----------Time: 0:05:49.415238, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.009507656097412-------------


Epoch 0: loss 16.12856536043799
Epoch 1: loss 16.127322921421037
Epoch 2: loss 16.126085101423737
Epoch 3: loss 16.12484309542367
Epoch 4: loss 16.123606956672774
Epoch 5: loss 16.12236374755992
Epoch 6: loss 16.121124173714584
Epoch 7: loss 16.11988621110693
Epoch 8: loss 16.118649888777615
Epoch 9: loss 16.117410078458388
Epoch 10: loss 16.116166362171267
Epoch 11: loss 16.114931479947213
Epoch 12: loss 16.113689707828126
Epoch 13: loss 16.11245099016348
Epoch 14: loss 16.111209836714032
Epoch 15: loss 16.109968335813903
Epoch 16: loss 16.10873623267667
Epoch 17: loss 16.107497937138668
Epoch 18: loss 16.106258936327652
Epoch 19: loss 16.105017847182506
Epoch 20: loss 16.103779523641016
Epoch 21: loss 16.102539162586538
Epoch 22: loss 16.10129919980388
Epoch 23: loss 16.100059942812813
Epoch 24: loss 16.09881927838722
Epoch 25: loss 16.09758401811609
Epoch 26: loss 16.09634378878172
Epoch 27: loss 16.09510101372292
Epoch 28: loss 16.093872203069772
Epoch 29: loss 16.09262951409577
Epoch 30: loss 16.09139090066633
Epoch 31: loss 16.090158377476786
Epoch 32: loss 16.088916311839665
Epoch 33: loss 16.08767374369552
Epoch 34: loss 16.086435550836974
Epoch 35: loss 16.085199802060565
Epoch 36: loss 16.083963478694084
Epoch 37: loss 16.082717633104764
Epoch 38: loss 16.081482890898148
Epoch 39: loss 16.080243865195147
Epoch 40: loss 16.079011641746632
Epoch 41: loss 16.07776697541528
Epoch 42: loss 16.076526650661616
Epoch 43: loss 16.07529306385814
Epoch 44: loss 16.07405282311494
Epoch 45: loss 16.072813264827097
Epoch 46: loss 16.071570793657994
Epoch 47: loss 16.0703365477354
Epoch 48: loss 16.069099346407736
Epoch 49: loss 16.067860327964894
-----------Time: 0:07:18.283899, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009507656097412-------------


Epoch 0: loss 16.12304461812636
Epoch 1: loss 16.11066057216609
Epoch 2: loss 16.098274430611525
Epoch 3: loss 16.08589561767329
Epoch 4: loss 16.07351765884142
Epoch 5: loss 16.061147305030676
Epoch 6: loss 16.048791746395707
Epoch 7: loss 16.036424371326273
Epoch 8: loss 16.024070628250726
Epoch 9: loss 16.011709209626755
Epoch 10: loss 15.99936316647304
Epoch 11: loss 15.987022077343656
Epoch 12: loss 15.974683856497387
Epoch 13: loss 15.962354518979577
Epoch 14: loss 15.95003587827532
Epoch 15: loss 15.937710527105782
Epoch 16: loss 15.925397392198274
Epoch 17: loss 15.913088018573971
Epoch 18: loss 15.900781248236818
Epoch 19: loss 15.88849042692283
Epoch 20: loss 15.87620210829087
Epoch 21: loss 15.86392168456799
Epoch 22: loss 15.851634185815907
Epoch 23: loss 15.83935573322738
Epoch 24: loss 15.827089553426438
Epoch 25: loss 15.814824910705799
Epoch 26: loss 15.80256154732966
Epoch 27: loss 15.790311319663383
Epoch 28: loss 15.778058001760428
Epoch 29: loss 15.765813781879336
Epoch 30: loss 15.753582669186034
Epoch 31: loss 15.741351442923035
Epoch 32: loss 15.729119433599559
Epoch 33: loss 15.716893605786604
Epoch 34: loss 15.704683440220363
Epoch 35: loss 15.69246371353755
Epoch 36: loss 15.680248900948172
Epoch 37: loss 15.668042347831788
Epoch 38: loss 15.655843592649441
Epoch 39: loss 15.643656636269732
Epoch 40: loss 15.631466509272963
Epoch 41: loss 15.619279055053479
Epoch 42: loss 15.607102759705088
Epoch 43: loss 15.594929758396532
Epoch 44: loss 15.582759622778175
Epoch 45: loss 15.570589099259658
Epoch 46: loss 15.558427852155592
Epoch 47: loss 15.546269318796877
Epoch 48: loss 15.53412025113544
Epoch 49: loss 15.521978175529908
-----------Time: 0:05:14.060393, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.940502166748047-------------


Epoch 0: loss 16.12300845784555
Epoch 1: loss 16.110621695547017
Epoch 2: loss 16.098237906803963
Epoch 3: loss 16.08585276352186
Epoch 4: loss 16.07348026433284
Epoch 5: loss 16.061109404903576
Epoch 6: loss 16.048743148936467
Epoch 7: loss 16.036375803426274
Epoch 8: loss 16.024020464670535
Epoch 9: loss 16.011673843296666
Epoch 10: loss 15.99931703487643
Epoch 11: loss 15.986982534345302
Epoch 12: loss 15.974641708656133
Epoch 13: loss 15.962315210899366
Epoch 14: loss 15.94998699248388
Epoch 15: loss 15.937661019533207
Epoch 16: loss 15.925340256786917
Epoch 17: loss 15.913025947807276
Epoch 18: loss 15.900711915232424
Epoch 19: loss 15.888409137466539
Epoch 20: loss 15.87610847344536
Epoch 21: loss 15.863805330596973
Epoch 22: loss 15.85151015059205
Epoch 23: loss 15.839211469632637
Epoch 24: loss 15.826918304323048
Epoch 25: loss 15.814637344385243
Epoch 26: loss 15.802357912711827
Epoch 27: loss 15.790078875680148
Epoch 28: loss 15.777797949968829
Epoch 29: loss 15.765527878720324
Epoch 30: loss 15.75324794350272
Epoch 31: loss 15.74098276508574
Epoch 32: loss 15.728703400309543
Epoch 33: loss 15.71643083312059
Epoch 34: loss 15.704163986421785
Epoch 35: loss 15.691896086999293
Epoch 36: loss 15.679623892671586
Epoch 37: loss 15.667355948132364
Epoch 38: loss 15.655072484993948
Epoch 39: loss 15.642796838977144
Epoch 40: loss 15.630516063654921
Epoch 41: loss 15.61822649731203
Epoch 42: loss 15.605931836408788
Epoch 43: loss 15.593639433416353
Epoch 44: loss 15.581338342601285
Epoch 45: loss 15.569043468041807
Epoch 46: loss 15.556734935559287
Epoch 47: loss 15.54442075622542
Epoch 48: loss 15.532094567025595
Epoch 49: loss 15.519757201841436
-----------Time: 0:03:48.004395, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9401907920837402-------------


Epoch 0: loss 16.12304138579791
Epoch 1: loss 16.1106658243757
Epoch 2: loss 16.098275818339893
Epoch 3: loss 16.085899860720197
Epoch 4: loss 16.073535179689955
Epoch 5: loss 16.061163910580046
Epoch 6: loss 16.04880111408078
Epoch 7: loss 16.036439655525907
Epoch 8: loss 16.024086604240214
Epoch 9: loss 16.011730979745188
Epoch 10: loss 15.999385411613593
Epoch 11: loss 15.987041712974063
Epoch 12: loss 15.974697922026742
Epoch 13: loss 15.962352566514214
Epoch 14: loss 15.950009919042623
Epoch 15: loss 15.937665492312425
Epoch 16: loss 15.925325768612337
Epoch 17: loss 15.912986563495348
Epoch 18: loss 15.900641064853886
Epoch 19: loss 15.888287784354462
Epoch 20: loss 15.87592619874673
Epoch 21: loss 15.863561165079984
Epoch 22: loss 15.851176865532581
Epoch 23: loss 15.838802662279504
Epoch 24: loss 15.826396813011481
Epoch 25: loss 15.813970405035139
Epoch 26: loss 15.801529572669937
Epoch 27: loss 15.789062281676516
Epoch 28: loss 15.776560287102207
Epoch 29: loss 15.764021800353904
Epoch 30: loss 15.751440308546488
Epoch 31: loss 15.73881813078357
Epoch 32: loss 15.72612797144381
Epoch 33: loss 15.713378609620468
Epoch 34: loss 15.700555613145937
Epoch 35: loss 15.687647087798293
Epoch 36: loss 15.674635767871884
Epoch 37: loss 15.661525136689377
Epoch 38: loss 15.64829633597643
Epoch 39: loss 15.634930003395931
Epoch 40: loss 15.621413856825795
Epoch 41: loss 15.607742906459459
Epoch 42: loss 15.593890570764506
Epoch 43: loss 15.579847523542512
Epoch 44: loss 15.565597706349276
Epoch 45: loss 15.551126687535778
Epoch 46: loss 15.53641048678242
Epoch 47: loss 15.521432485575259
Epoch 48: loss 15.50618352262529
Epoch 49: loss 15.490641656163078
-----------Time: 0:05:15.209775, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.936279773712158-------------


Epoch 0: loss 16.12303357697363
Epoch 1: loss 16.110654620906462
Epoch 2: loss 16.098274989644104
Epoch 3: loss 16.08589404688509
Epoch 4: loss 16.07351698364623
Epoch 5: loss 16.06114269015969
Epoch 6: loss 16.048774786654082
Epoch 7: loss 16.036405000691833
Epoch 8: loss 16.024035448091972
Epoch 9: loss 16.011662743025465
Epoch 10: loss 15.999301434341106
Epoch 11: loss 15.986927686922572
Epoch 12: loss 15.974553645986004
Epoch 13: loss 15.962164627332413
Epoch 14: loss 15.949759952655105
Epoch 15: loss 15.937350788603924
Epoch 16: loss 15.92491199167738
Epoch 17: loss 15.91243634008726
Epoch 18: loss 15.899929534098971
Epoch 19: loss 15.887374118724033
Epoch 20: loss 15.8747566631788
Epoch 21: loss 15.8620716564807
Epoch 22: loss 15.849280373436915
Epoch 23: loss 15.83637836875314
Epoch 24: loss 15.823342372050034
Epoch 25: loss 15.810141715878958
Epoch 26: loss 15.796744032875878
Epoch 27: loss 15.783126984555285
Epoch 28: loss 15.769237140782051
Epoch 29: loss 15.755058724702604
Epoch 30: loss 15.74054252900398
Epoch 31: loss 15.725643686913743
Epoch 32: loss 15.710327309716844
Epoch 33: loss 15.694548646284874
Epoch 34: loss 15.678264954480353
Epoch 35: loss 15.66144740212022
Epoch 36: loss 15.644042571515348
Epoch 37: loss 15.626005859852096
Epoch 38: loss 15.60731208473525
Epoch 39: loss 15.587921831775061
Epoch 40: loss 15.567804136374775
Epoch 41: loss 15.54692731466288
Epoch 42: loss 15.525260216908457
Epoch 43: loss 15.50277378897488
Epoch 44: loss 15.479466319537927
Epoch 45: loss 15.455305956705166
Epoch 46: loss 15.430273087663842
Epoch 47: loss 15.404360537298222
Epoch 48: loss 15.377546767815097
Epoch 49: loss 15.349820524924082
-----------Time: 0:06:27.294054, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.917603015899658-------------


Epoch 0: loss 16.123019821038383
Epoch 1: loss 16.110640160216786
Epoch 2: loss 16.098256255829703
Epoch 3: loss 16.085869846686258
Epoch 4: loss 16.07348626745078
Epoch 5: loss 16.061105624432795
Epoch 6: loss 16.04870813264997
Epoch 7: loss 16.036317343553684
Epoch 8: loss 16.023911695495904
Epoch 9: loss 16.011491335235647
Epoch 10: loss 15.999044730003403
Epoch 11: loss 15.986561773133188
Epoch 12: loss 15.974027927703608
Epoch 13: loss 15.961420690838315
Epoch 14: loss 15.948721862863495
Epoch 15: loss 15.935898711851202
Epoch 16: loss 15.922931394737788
Epoch 17: loss 15.909745350682652
Epoch 18: loss 15.896314502734734
Epoch 19: loss 15.882573682111913
Epoch 20: loss 15.868459410094385
Epoch 21: loss 15.85390920519764
Epoch 22: loss 15.838847909173348
Epoch 23: loss 15.823217124897479
Epoch 24: loss 15.806936261963754
Epoch 25: loss 15.789952444602381
Epoch 26: loss 15.772173611348968
Epoch 27: loss 15.753558573740989
Epoch 28: loss 15.73402464318496
Epoch 29: loss 15.713545559151397
Epoch 30: loss 15.692043613518885
Epoch 31: loss 15.669474432168933
Epoch 32: loss 15.645809555261144
Epoch 33: loss 15.62100479640411
Epoch 34: loss 15.595008534799133
Epoch 35: loss 15.567796289175345
Epoch 36: loss 15.539345440493776
Epoch 37: loss 15.509644175431468
Epoch 38: loss 15.478661898622828
Epoch 39: loss 15.446385532439306
Epoch 40: loss 15.412804035728168
Epoch 41: loss 15.37791561782198
Epoch 42: loss 15.341706083545612
Epoch 43: loss 15.304159411792849
Epoch 44: loss 15.265290969736618
Epoch 45: loss 15.225076182242514
Epoch 46: loss 15.183535833083916
Epoch 47: loss 15.140658903925752
Epoch 48: loss 15.09644704178794
Epoch 49: loss 15.050919312896646
-----------Time: 0:07:38.418643, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.878261089324951-------------


Epoch 0: loss 16.06781818167939
Epoch 1: loss 15.944560842840746
Epoch 2: loss 15.82186654004278
Epoch 3: loss 15.6997685803221
Epoch 4: loss 15.578160588283653
Epoch 5: loss 15.457008150234502
Epoch 6: loss 15.336361764759522
Epoch 7: loss 15.216146120109785
Epoch 8: loss 15.096357288536915
Epoch 9: loss 14.97691838802236
Epoch 10: loss 14.857811946464402
Epoch 11: loss 14.738951914851079
Epoch 12: loss 14.6201801631942
Epoch 13: loss 14.501371135265689
Epoch 14: loss 14.382402916846553
Epoch 15: loss 14.262983660259735
Epoch 16: loss 14.142819788353544
Epoch 17: loss 14.021580574238929
Epoch 18: loss 13.89883025461335
Epoch 19: loss 13.774099468731116
Epoch 20: loss 13.646869196847705
Epoch 21: loss 13.516611994836174
Epoch 22: loss 13.382678363295986
Epoch 23: loss 13.244513405866764
Epoch 24: loss 13.10157351107491
Epoch 25: loss 12.953286230596529
Epoch 26: loss 12.799204457642398
Epoch 27: loss 12.638801116798156
Epoch 28: loss 12.471769239540784
Epoch 29: loss 12.29777675740676
Epoch 30: loss 12.11653763810988
Epoch 31: loss 11.927852051876497
Epoch 32: loss 11.731693797814192
Epoch 33: loss 11.52801833948796
Epoch 34: loss 11.316877512908487
Epoch 35: loss 11.098318872664402
Epoch 36: loss 10.87245423616178
Epoch 37: loss 10.639490210536772
Epoch 38: loss 10.399608218455976
Epoch 39: loss 10.153165300233393
Epoch 40: loss 9.900456652037146
Epoch 41: loss 9.641771512552731
Epoch 42: loss 9.377481359966168
Epoch 43: loss 9.108171958778136
Epoch 44: loss 8.834320120736267
Epoch 45: loss 8.556474666740662
Epoch 46: loss 8.275124273460932
Epoch 47: loss 7.990917088610767
Epoch 48: loss 7.704446578168429
Epoch 49: loss 7.416440515689321
-----------Time: 0:03:27.807542, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.703667402267456-------------


Epoch 0: loss 16.06770339071783
Epoch 1: loss 15.944422071559588
Epoch 2: loss 15.821663809833868
Epoch 3: loss 15.699248621423585
Epoch 4: loss 15.57696825009834
Epoch 5: loss 15.454363324060072
Epoch 6: loss 15.330444916231986
Epoch 7: loss 15.203407150173135
Epoch 8: loss 15.070150830163588
Epoch 9: loss 14.92622287225438
Epoch 10: loss 14.766172187622635
Epoch 11: loss 14.584552020725834
Epoch 12: loss 14.376959550763162
Epoch 13: loss 14.140767260308237
Epoch 14: loss 13.87472030168775
Epoch 15: loss 13.578979881643406
Epoch 16: loss 13.253983294853638
Epoch 17: loss 12.900946829226951
Epoch 18: loss 12.521308140757293
Epoch 19: loss 12.116912940327664
Epoch 20: loss 11.689845115740447
Epoch 21: loss 11.24243177130794
Epoch 22: loss 10.776868554675884
Epoch 23: loss 10.295494835684
Epoch 24: loss 9.801220490402214
Epoch 25: loss 9.296793243300337
Epoch 26: loss 8.78499694464322
Epoch 27: loss 8.268828901277411
Epoch 28: loss 7.7512229001022925
Epoch 29: loss 7.2352507540944995
Epoch 30: loss 6.724427606957058
Epoch 31: loss 6.222096983280566
Epoch 32: loss 5.7315395208984175
Epoch 33: loss 5.255935718469998
Epoch 34: loss 4.798894521527604
Epoch 35: loss 4.363828748383556
Epoch 36: loss 3.953865038045662
Epoch 37: loss 3.572070368610691
Epoch 38: loss 3.2212760710340276
Epoch 39: loss 2.903693596522013
Epoch 40: loss 2.62077221421848
Epoch 41: loss 2.3731181609235166
Epoch 42: loss 2.160452352781021
Epoch 43: loss 1.9810286917849815
Epoch 44: loss 1.8320009304557954
Epoch 45: loss 1.7098337050168784
Epoch 46: loss 1.6103141967727803
Epoch 47: loss 1.5292337375726954
Epoch 48: loss 1.4627731459629543
Epoch 49: loss 1.4076918348661902
-----------Time: 0:04:17.033917, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.1867341995239258-------------


Epoch 0: loss 16.067757001838455
Epoch 1: loss 15.94442570630852
Epoch 2: loss 15.82092035873355
Epoch 3: loss 15.694505099825006
Epoch 4: loss 15.55602542831562
Epoch 5: loss 15.38674227552222
Epoch 6: loss 15.166312838974953
Epoch 7: loss 14.882097979094944
Epoch 8: loss 14.529197834444279
Epoch 9: loss 14.10863436636165
Epoch 10: loss 13.625091761723363
Epoch 11: loss 13.084159075273904
Epoch 12: loss 12.491723344272865
Epoch 13: loss 11.853779070399389
Epoch 14: loss 11.17734986378357
Epoch 15: loss 10.469587177216455
Epoch 16: loss 9.738260201749236
Epoch 17: loss 8.991466662234233
Epoch 18: loss 8.23746871559305
Epoch 19: loss 7.485255109673936
Epoch 20: loss 6.744234685602235
Epoch 21: loss 6.024005948492468
Epoch 22: loss 5.333977240074452
Epoch 23: loss 4.683787909067474
Epoch 24: loss 4.082609760819602
Epoch 25: loss 3.539110317769551
Epoch 26: loss 3.060222398722671
Epoch 27: loss 2.650751723745324
Epoch 28: loss 2.3119285124161113
Epoch 29: loss 2.0408499373762163
Epoch 30: loss 1.8305349259871773
Epoch 31: loss 1.6708554163648097
Epoch 32: loss 1.55034292593412
Epoch 33: loss 1.458480646780096
Epoch 34: loss 1.3868905830279583
Epoch 35: loss 1.3295657658071347
Epoch 36: loss 1.2825045704906437
Epoch 37: loss 1.2431855022161793
Epoch 38: loss 1.2098695472117804
Epoch 39: loss 1.1814051026102639
Epoch 40: loss 1.1569343467909983
Epoch 41: loss 1.1358087715602168
Epoch 42: loss 1.1175134288609967
Epoch 43: loss 1.101624048585928
Epoch 44: loss 1.087789934312863
Epoch 45: loss 1.0757345379598118
Epoch 46: loss 1.0652088062992686
Epoch 47: loss 1.05599705209675
Epoch 48: loss 1.0479104873795688
Epoch 49: loss 1.0408260709180204
-----------Time: 0:05:52.608444, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0285608768463135-------------


Epoch 0: loss 16.067756802183965
Epoch 1: loss 15.943886296395181
Epoch 2: loss 15.814718537903662
Epoch 3: loss 15.653502899385652
Epoch 4: loss 15.408567821156272
Epoch 5: loss 15.046239101478365
Epoch 6: loss 14.564807409563423
Epoch 7: loss 13.97610810400676
Epoch 8: loss 13.2943062800438
Epoch 9: loss 12.533111145990318
Epoch 10: loss 11.705977043682884
Epoch 11: loss 10.82681778424975
Epoch 12: loss 9.910753611055906
Epoch 13: loss 8.97298180039257
Epoch 14: loss 8.029190027435037
Epoch 15: loss 7.09661821699324
Epoch 16: loss 6.19259752794217
Epoch 17: loss 5.335033060222167
Epoch 18: loss 4.5407250287936005
Epoch 19: loss 3.8265688817354566
Epoch 20: loss 3.2064409913285523
Epoch 21: loss 2.689884173345021
Epoch 22: loss 2.2795316738042577
Epoch 23: loss 1.9686187452567279
Epoch 24: loss 1.7423116307727924
Epoch 25: loss 1.5808754781460619
Epoch 26: loss 1.464933556470617
Epoch 27: loss 1.3790370631049418
Epoch 28: loss 1.3128821305958975
Epoch 29: loss 1.2602576430301553
Epoch 30: loss 1.2174539111307485
Epoch 31: loss 1.1821591771770392
Epoch 32: loss 1.1527732678766547
Epoch 33: loss 1.1281836269795085
Epoch 34: loss 1.1075012124122778
Epoch 35: loss 1.0900553803589113
Epoch 36: loss 1.075292593308802
Epoch 37: loss 1.0627803303678118
Epoch 38: loss 1.0521401837832776
Epoch 39: loss 1.0430621164463472
Epoch 40: loss 1.0353055710764019
Epoch 41: loss 1.02865284003921
Epoch 42: loss 1.0229304819861593
Epoch 43: loss 1.0180095005385443
Epoch 44: loss 1.013754845216522
Epoch 45: loss 1.0100665356945642
Epoch 46: loss 1.0068635270931332
Epoch 47: loss 1.0040694188332675
Epoch 48: loss 1.0016361280522184
Epoch 49: loss 0.9995009887471284
-----------Time: 0:06:37.082111, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.008820652961731-------------


Epoch 0: loss 16.067703927451337
Epoch 1: loss 15.942228574996541
Epoch 2: loss 15.788875857275942
Epoch 3: loss 15.517790319350441
Epoch 4: loss 15.064916893864146
Epoch 5: loss 14.445651425169238
Epoch 6: loss 13.689181069565441
Epoch 7: loss 12.819716804633003
Epoch 8: loss 11.858935517938063
Epoch 9: loss 10.828430423145388
Epoch 10: loss 9.751517627212001
Epoch 11: loss 8.652862727998585
Epoch 12: loss 7.557187108127496
Epoch 13: loss 6.491606742937711
Epoch 14: loss 5.482855906753581
Epoch 15: loss 4.557752713617778
Epoch 16: loss 3.740585327278083
Epoch 17: loss 3.0520726715241393
Epoch 18: loss 2.5030818259087253
Epoch 19: loss 2.091721508216443
Epoch 20: loss 1.8007069228459078
Epoch 21: loss 1.6018973999920587
Epoch 22: loss 1.465900916583905
Epoch 23: loss 1.3692880693241718
Epoch 24: loss 1.297142443610249
Epoch 25: loss 1.2410957259351367
Epoch 26: loss 1.1965332156034059
Epoch 27: loss 1.160550606905474
Epoch 28: loss 1.1312679639972378
Epoch 29: loss 1.1072962639965787
Epoch 30: loss 1.0875824145676973
Epoch 31: loss 1.0713072788981137
Epoch 32: loss 1.0578264391182428
Epoch 33: loss 1.0466238146180884
Epoch 34: loss 1.0372835339055624
Epoch 35: loss 1.0294641200584715
Epoch 36: loss 1.0228763143610475
Epoch 37: loss 1.017337484551617
Epoch 38: loss 1.012647045313372
Epoch 39: loss 1.0086664568476862
Epoch 40: loss 1.0052794928846314
Epoch 41: loss 1.0023802226493383
Epoch 42: loss 0.9998989587831004
Epoch 43: loss 0.9977631067827774
Epoch 44: loss 0.9959241114860646
Epoch 45: loss 0.9943329974190576
Epoch 46: loss 0.9929591765087412
Epoch 47: loss 0.9917562969875698
Epoch 48: loss 0.9907053300645962
Epoch 49: loss 0.989790903867231
-----------Time: 0:06:34.086920, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0039963722229004-------------


Epoch 0: loss 15.525167946973658
Epoch 1: loss 14.333558670135217
Epoch 2: loss 12.95616040118302
Epoch 3: loss 10.745248503111963
Epoch 4: loss 7.674914742242129
Epoch 5: loss 4.674254284662161
Epoch 6: loss 2.5786701727976546
Epoch 7: loss 1.59256585231613
Epoch 8: loss 1.2599125293495215
Epoch 9: loss 1.1329733151466448
Epoch 10: loss 1.069726869034469
Epoch 11: loss 1.035269180026114
Epoch 12: loss 1.0157410294028195
Epoch 13: loss 1.0042569143348183
Epoch 14: loss 0.9972406879853398
Epoch 15: loss 0.9928412274733518
Epoch 16: loss 0.9899617021274411
Epoch 17: loss 0.9880223113794311
Epoch 18: loss 0.9866437765163465
Epoch 19: loss 0.9856366000789997
Epoch 20: loss 0.9848871199640001
Epoch 21: loss 0.9842844624244499
Epoch 22: loss 0.9837857953162865
Epoch 23: loss 0.9833693927951064
Epoch 24: loss 0.9829946132118511
Epoch 25: loss 0.9826674474977553
Epoch 26: loss 0.9823480986783918
Epoch 27: loss 0.9820813058251074
Epoch 28: loss 0.981786362109721
Epoch 29: loss 0.981507758688447
Epoch 30: loss 0.9812430421122394
Epoch 31: loss 0.9809895013907216
Epoch 32: loss 0.9806808620577859
Epoch 33: loss 0.9804092644996912
Epoch 34: loss 0.9801548884056261
Epoch 35: loss 0.9798812245207679
Epoch 36: loss 0.9795734470405288
Epoch 37: loss 0.9792820982728204
Epoch 38: loss 0.9789590646288459
Epoch 39: loss 0.9786674247091396
Epoch 40: loss 0.978327568650829
Epoch 41: loss 0.978011605401218
Epoch 42: loss 0.9776307957661159
Epoch 43: loss 0.9772888258658653
Epoch 44: loss 0.9769217961180657
Epoch 45: loss 0.9765266854520593
Epoch 46: loss 0.9761068574626398
Epoch 47: loss 0.9756954480215801
Epoch 48: loss 0.9752591153147948
Epoch 49: loss 0.9747938472269151
-----------Time: 0:04:03.666666, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.999409019947052-------------


Epoch 0: loss 15.513653110589198
Epoch 1: loss 13.108955813724751
Epoch 2: loss 7.494578943361466
Epoch 3: loss 3.046760934406548
Epoch 4: loss 1.4939446329357444
Epoch 5: loss 1.1665431270137827
Epoch 6: loss 1.0645056865732587
Epoch 7: loss 1.0234260641407098
Epoch 8: loss 1.0054958702883945
Epoch 9: loss 0.9969733207688635
Epoch 10: loss 0.9925357031070259
Epoch 11: loss 0.989969725029309
Epoch 12: loss 0.9883184045342533
Epoch 13: loss 0.987075390765173
Epoch 14: loss 0.9860323146674347
Epoch 15: loss 0.9851039833449488
Epoch 16: loss 0.9841959745810044
Epoch 17: loss 0.9833319115535016
Epoch 18: loss 0.9824225369065012
Epoch 19: loss 0.9814967933784949
Epoch 20: loss 0.9805532409290441
Epoch 21: loss 0.9795801019850083
Epoch 22: loss 0.978539370401454
Epoch 23: loss 0.9775479027212411
Epoch 24: loss 0.9765134891327987
Epoch 25: loss 0.9754437524835463
Epoch 26: loss 0.9744104752079049
Epoch 27: loss 0.9733475011415362
Epoch 28: loss 0.9722892375923228
Epoch 29: loss 0.971200582188974
Epoch 30: loss 0.9701937550497548
Epoch 31: loss 0.9690725816988051
Epoch 32: loss 0.9680382001328689
Epoch 33: loss 0.9669881575518552
Epoch 34: loss 0.965904724669236
Epoch 35: loss 0.9648375194185516
Epoch 36: loss 0.9637825020698829
Epoch 37: loss 0.962750488192313
Epoch 38: loss 0.9616668770143427
Epoch 39: loss 0.9605330003221895
Epoch 40: loss 0.9595010471836648
Epoch 41: loss 0.9583755758808254
Epoch 42: loss 0.9572690352270822
Epoch 43: loss 0.9560842032728148
Epoch 44: loss 0.9549560172522826
Epoch 45: loss 0.9537104351031773
Epoch 46: loss 0.9524956455757074
Epoch 47: loss 0.9512332713817889
Epoch 48: loss 0.9499208266285206
Epoch 49: loss 0.9485941977524252
-----------Time: 0:05:04.293577, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9925181269645691-------------


Epoch 0: loss 15.151549993746302
Epoch 1: loss 7.9320196279564135
Epoch 2: loss 2.2045350621873236
Epoch 3: loss 1.2155262242522558
Epoch 4: loss 1.0604794133675364
Epoch 5: loss 1.0165220954419478
Epoch 6: loss 1.0019075455323323
Epoch 7: loss 0.9964722082172807
Epoch 8: loss 0.9940554740119071
Epoch 9: loss 0.9929158221190879
Epoch 10: loss 0.9922478226878968
Epoch 11: loss 0.9918538373472121
Epoch 12: loss 0.9916052939193024
Epoch 13: loss 0.9913481541338532
Epoch 14: loss 0.99116797856237
Epoch 15: loss 0.9909860380581373
Epoch 16: loss 0.9908253207940522
Epoch 17: loss 0.9906846279680502
Epoch 18: loss 0.9904788615523376
Epoch 19: loss 0.990327547658326
Epoch 20: loss 0.9900653679906317
Epoch 21: loss 0.9899617901245105
Epoch 22: loss 0.9897777033668941
Epoch 23: loss 0.9894060304205077
Epoch 24: loss 0.9892634330656686
Epoch 25: loss 0.9889620400888237
Epoch 26: loss 0.9886528007876555
Epoch 27: loss 0.9883222113879496
Epoch 28: loss 0.9879851408015132
Epoch 29: loss 0.9875762928368411
Epoch 30: loss 0.9871586988709944
Epoch 31: loss 0.986688291786675
Epoch 32: loss 0.9861819698734087
Epoch 33: loss 0.9855034929893145
Epoch 34: loss 0.9847968066108168
Epoch 35: loss 0.984144607725968
Epoch 36: loss 0.9833401412079683
Epoch 37: loss 0.9824258510414319
Epoch 38: loss 0.9814205336920784
Epoch 39: loss 0.9803243035398922
Epoch 40: loss 0.9790501246768667
Epoch 41: loss 0.9777607388830885
Epoch 42: loss 0.9762509132676179
Epoch 43: loss 0.9745775919818827
Epoch 44: loss 0.9727953303242715
Epoch 45: loss 0.9708573282445624
Epoch 46: loss 0.9688064517567766
Epoch 47: loss 0.9665559752600682
Epoch 48: loss 0.9640644816940022
Epoch 49: loss 0.9614163028474344
-----------Time: 0:05:44.608943, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9950735569000244-------------


Epoch 0: loss 14.166798725408208
Epoch 1: loss 4.601735155954511
Epoch 2: loss 1.3643483477095146
Epoch 3: loss 1.0745748459741822
Epoch 4: loss 1.0176606711451939
Epoch 5: loss 1.0026104846617785
Epoch 6: loss 0.9976332951266199
Epoch 7: loss 0.9958016214129587
Epoch 8: loss 0.994948876818086
Epoch 9: loss 0.9944706036060513
Epoch 10: loss 0.9941106072387467
Epoch 11: loss 0.9939569703193383
Epoch 12: loss 0.9937985618196148
Epoch 13: loss 0.9935173024425953
Epoch 14: loss 0.9934494889190886
Epoch 15: loss 0.9933015989051558
Epoch 16: loss 0.9930803897330267
Epoch 17: loss 0.9929404075927486
Epoch 18: loss 0.9927817542893916
Epoch 19: loss 0.9925367420882618
Epoch 20: loss 0.9923469611585886
Epoch 21: loss 0.9921046768666092
Epoch 22: loss 0.9917511663856942
Epoch 23: loss 0.9915103303666605
Epoch 24: loss 0.9910321805774032
Epoch 25: loss 0.9907651987329912
Epoch 26: loss 0.9901673013973392
Epoch 27: loss 0.9896719040204287
Epoch 28: loss 0.9891431755900318
Epoch 29: loss 0.9883512864947773
Epoch 30: loss 0.9876444895608455
Epoch 31: loss 0.9867011590973718
Epoch 32: loss 0.9856981419557589
Epoch 33: loss 0.9845592287197393
Epoch 34: loss 0.9832285208958786
Epoch 35: loss 0.981792187872239
Epoch 36: loss 0.9801848290276437
Epoch 37: loss 0.9784275363358937
Epoch 38: loss 0.9764413892931106
Epoch 39: loss 0.9743208393187157
Epoch 40: loss 0.9720947044967893
Epoch 41: loss 0.9695367662721254
Epoch 42: loss 0.9669109834413804
Epoch 43: loss 0.96407021181035
Epoch 44: loss 0.9610513585491502
Epoch 45: loss 0.9578329858214651
Epoch 46: loss 0.9544796724887307
Epoch 47: loss 0.9508762878526352
Epoch 48: loss 0.9471829950064016
Epoch 49: loss 0.9430909516242226
-----------Time: 0:05:32.954406, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9921354651451111-------------


Epoch 0: loss 13.175330977001678
Epoch 1: loss 3.0564361235056965
Epoch 2: loss 1.1829059574511986
Epoch 3: loss 1.0361473453699603
Epoch 4: loss 1.007555398006294
Epoch 5: loss 1.000205555644095
Epoch 6: loss 0.9978108058940334
Epoch 7: loss 0.9969337439653729
Epoch 8: loss 0.9964136622923103
Epoch 9: loss 0.9960803841688893
Epoch 10: loss 0.9959988101401233
Epoch 11: loss 0.9958115230765662
Epoch 12: loss 0.995637052567385
Epoch 13: loss 0.9953770112122706
Epoch 14: loss 0.9953509491380627
Epoch 15: loss 0.9953120898591105
Epoch 16: loss 0.9949930770167713
Epoch 17: loss 0.9947250290951566
Epoch 18: loss 0.9946501958363726
Epoch 19: loss 0.9943624027336726
Epoch 20: loss 0.9940189429117714
Epoch 21: loss 0.9937698125450296
Epoch 22: loss 0.993433200061937
Epoch 23: loss 0.992987116048231
Epoch 24: loss 0.9924561857140018
Epoch 25: loss 0.9920169303377016
Epoch 26: loss 0.9912970815747247
Epoch 27: loss 0.9905558418747911
Epoch 28: loss 0.9896798257661812
Epoch 29: loss 0.9886471035041
Epoch 30: loss 0.9873925275683338
Epoch 31: loss 0.9860521509246764
Epoch 32: loss 0.9844485273693099
Epoch 33: loss 0.9827302779238661
Epoch 34: loss 0.9807150539268029
Epoch 35: loss 0.9785024789832998
Epoch 36: loss 0.9758139681466057
Epoch 37: loss 0.9730924592515967
Epoch 38: loss 0.9700532150112979
Epoch 39: loss 0.9667335357466361
Epoch 40: loss 0.9630937928282223
Epoch 41: loss 0.9593488981763976
Epoch 42: loss 0.9551202455508184
Epoch 43: loss 0.950816147868046
Epoch 44: loss 0.9461331212241861
Epoch 45: loss 0.9411626966379465
Epoch 46: loss 0.9358009861305138
Epoch 47: loss 0.9301227386909182
Epoch 48: loss 0.9240249814450449
Epoch 49: loss 0.9174166614429792
-----------Time: 0:07:08.700389, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9890667796134949-------------


Epoch 0: loss 7.0703093711288805
Epoch 1: loss 1.0341958781504774
Epoch 2: loss 1.0156453735417423
Epoch 3: loss 1.0114800200765712
Epoch 4: loss 1.0057266045031565
Epoch 5: loss 0.9971605292022585
Epoch 6: loss 0.9866078753650286
Epoch 7: loss 0.97440079789631
Epoch 8: loss 0.9619691973798232
Epoch 9: loss 0.9477123973355337
Epoch 10: loss 0.9324732000507564
Epoch 11: loss 0.9156205590617857
Epoch 12: loss 0.8988027212216583
Epoch 13: loss 0.8816291091881607
Epoch 14: loss 0.8644072393997653
Epoch 15: loss 0.8482368487323867
Epoch 16: loss 0.8326013282370865
Epoch 17: loss 0.8185705270308266
Epoch 18: loss 0.8065818606218998
Epoch 19: loss 0.7955328053336483
Epoch 20: loss 0.7862713227718016
Epoch 21: loss 0.7784631317759935
Epoch 22: loss 0.7715934535094484
Epoch 23: loss 0.7656576396525716
Epoch 24: loss 0.7603752664925937
Epoch 25: loss 0.7561158603011686
Epoch 26: loss 0.7518739848113566
Epoch 27: loss 0.7484175411821254
Epoch 28: loss 0.7453644787766092
Epoch 29: loss 0.7423811418456575
Epoch 30: loss 0.7398460009877225
Epoch 31: loss 0.737292241246367
Epoch 32: loss 0.7351648685982721
Epoch 33: loss 0.7332880334273313
Epoch 34: loss 0.7313679583115446
Epoch 35: loss 0.7296259965891421
Epoch 36: loss 0.7278453353437929
Epoch 37: loss 0.7262556033655636
Epoch 38: loss 0.7249909079288775
Epoch 39: loss 0.7235507200177822
Epoch 40: loss 0.7222922232048611
Epoch 41: loss 0.7210942510679534
Epoch 42: loss 0.7197955890541948
Epoch 43: loss 0.7187799571842652
Epoch 44: loss 0.7177630299261694
Epoch 45: loss 0.7166327809172522
Epoch 46: loss 0.7158591442616365
Epoch 47: loss 0.714945567049625
Epoch 48: loss 0.714051757415006
Epoch 49: loss 0.7131923363220439
-----------Time: 0:04:55.459714, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0642390251159668-------------


Epoch 0: loss 4.51702033212614
Epoch 1: loss 1.0379432624549305
Epoch 2: loss 1.0332499123773475
Epoch 3: loss 1.0236944135535728
Epoch 4: loss 1.0110964612380002
Epoch 5: loss 0.9974895878743062
Epoch 6: loss 0.9798879436399056
Epoch 7: loss 0.9567905545818086
Epoch 8: loss 0.9261391076981988
Epoch 9: loss 0.8886710312004774
Epoch 10: loss 0.8454582759904887
Epoch 11: loss 0.7988549283887459
Epoch 12: loss 0.7519299781730864
Epoch 13: loss 0.7076280066797175
Epoch 14: loss 0.6680760948537419
Epoch 15: loss 0.63448504567989
Epoch 16: loss 0.6063771796103338
Epoch 17: loss 0.5836189138040133
Epoch 18: loss 0.5651314518029046
Epoch 19: loss 0.5496126972766854
Epoch 20: loss 0.5371016526332428
Epoch 21: loss 0.5262759176527048
Epoch 22: loss 0.5172820506438152
Epoch 23: loss 0.5094664668083969
Epoch 24: loss 0.5024678177033642
Epoch 25: loss 0.4966869446458604
Epoch 26: loss 0.4912692573389455
Epoch 27: loss 0.48666701529452566
Epoch 28: loss 0.4823499284178229
Epoch 29: loss 0.47827552310016375
Epoch 30: loss 0.474906924400983
Epoch 31: loss 0.47155355243594704
Epoch 32: loss 0.46871808895668043
Epoch 33: loss 0.46587722451548136
Epoch 34: loss 0.4635274253052301
Epoch 35: loss 0.4610844742772111
Epoch 36: loss 0.4590129652316315
Epoch 37: loss 0.45691053171466395
Epoch 38: loss 0.4550436661903336
Epoch 39: loss 0.45321933585641955
Epoch 40: loss 0.4515963354436129
Epoch 41: loss 0.4499888827840941
Epoch 42: loss 0.44853709360190613
Epoch 43: loss 0.44703768883275236
Epoch 44: loss 0.4455535548773066
Epoch 45: loss 0.44450075238084197
Epoch 46: loss 0.4432440346968829
Epoch 47: loss 0.4420073697948145
Epoch 48: loss 0.4408258503386999
Epoch 49: loss 0.4398432333648302
-----------Time: 0:04:24.400925, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.1909536123275757-------------


Epoch 0: loss 3.537432573099639
Epoch 1: loss 1.0524505047839643
Epoch 2: loss 1.0351695817342719
Epoch 3: loss 1.016213800513012
Epoch 4: loss 0.9921631804946456
Epoch 5: loss 0.9594739964631928
Epoch 6: loss 0.9155509643998077
Epoch 7: loss 0.8598559043730258
Epoch 8: loss 0.7892474302978474
Epoch 9: loss 0.7084639470754854
Epoch 10: loss 0.6260921246594485
Epoch 11: loss 0.5501723560449414
Epoch 12: loss 0.4853040883889854
Epoch 13: loss 0.4329948639532611
Epoch 14: loss 0.39167838074902983
Epoch 15: loss 0.3592094885648237
Epoch 16: loss 0.3335727907721279
Epoch 17: loss 0.31307166427487326
Epoch 18: loss 0.29705660190188155
Epoch 19: loss 0.2832253303149915
Epoch 20: loss 0.2722527994579307
Epoch 21: loss 0.262766119682186
Epoch 22: loss 0.2546146922765574
Epoch 23: loss 0.24761593198406753
Epoch 24: loss 0.24176855312377749
Epoch 25: loss 0.23658124621210053
Epoch 26: loss 0.2316729307839117
Epoch 27: loss 0.22756002458494082
Epoch 28: loss 0.22388488708176388
Epoch 29: loss 0.22033135391176234
Epoch 30: loss 0.2174276075102571
Epoch 31: loss 0.21444491786014264
Epoch 32: loss 0.2119966210911882
Epoch 33: loss 0.2095658844615403
Epoch 34: loss 0.20733367207259054
Epoch 35: loss 0.20529748065890405
Epoch 36: loss 0.20358346530670313
Epoch 37: loss 0.20157965325333618
Epoch 38: loss 0.20023321360349655
Epoch 39: loss 0.19846208677984178
Epoch 40: loss 0.1970552590601207
Epoch 41: loss 0.19561686299137604
Epoch 42: loss 0.19430741233839297
Epoch 43: loss 0.19299118596778866
Epoch 44: loss 0.1919944043584723
Epoch 45: loss 0.1907109027447877
Epoch 46: loss 0.18980072040461277
Epoch 47: loss 0.18874102875047433
Epoch 48: loss 0.18776372139903758
Epoch 49: loss 0.18661775317624446
-----------Time: 0:04:53.911380, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.2732847929000854-------------


Epoch 0: loss 3.160707296723578
Epoch 1: loss 1.0658030133975984
Epoch 2: loss 1.039430544939036
Epoch 3: loss 1.0130102073387843
Epoch 4: loss 0.9794416250969937
Epoch 5: loss 0.9256606863334557
Epoch 6: loss 0.8469522404826291
Epoch 7: loss 0.7439991124237666
Epoch 8: loss 0.6297701120117295
Epoch 9: loss 0.5198127802556595
Epoch 10: loss 0.42633852960623886
Epoch 11: loss 0.3533787476005212
Epoch 12: loss 0.29802267304446917
Epoch 13: loss 0.2562878098077525
Epoch 14: loss 0.22585549895500218
Epoch 15: loss 0.20287493507359067
Epoch 16: loss 0.18507517350390917
Epoch 17: loss 0.1717034182174495
Epoch 18: loss 0.16072653984329893
Epoch 19: loss 0.1522181924227919
Epoch 20: loss 0.14516523798857778
Epoch 21: loss 0.13928216890254183
Epoch 22: loss 0.1344050793559607
Epoch 23: loss 0.13022077405271223
Epoch 24: loss 0.12680138656064308
Epoch 25: loss 0.12336705150229572
Epoch 26: loss 0.1208873866262677
Epoch 27: loss 0.11818776024852906
Epoch 28: loss 0.11617640941897574
Epoch 29: loss 0.1140805729150837
Epoch 30: loss 0.11225628740619614
Epoch 31: loss 0.11080845720486643
Epoch 32: loss 0.10918065412309326
Epoch 33: loss 0.10781894428751726
Epoch 34: loss 0.10661799236807958
Epoch 35: loss 0.10560078947496453
Epoch 36: loss 0.10448081575459667
Epoch 37: loss 0.10326158819297787
Epoch 38: loss 0.10258985953552364
Epoch 39: loss 0.10158962158404206
Epoch 40: loss 0.10068692293680512
Epoch 41: loss 0.09988779804551323
Epoch 42: loss 0.09915406074194626
Epoch 43: loss 0.09847454496169622
Epoch 44: loss 0.09778098869398277
Epoch 45: loss 0.09727914245265408
Epoch 46: loss 0.09659074761820202
Epoch 47: loss 0.09612248283914732
Epoch 48: loss 0.09544901218814264
Epoch 49: loss 0.09495236725222811
-----------Time: 0:06:07.303711, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.2280631065368652-------------


Epoch 0: loss 2.992816475534517
Epoch 1: loss 1.0780503956956538
Epoch 2: loss 1.0526772570584118
Epoch 3: loss 1.016837767377504
Epoch 4: loss 0.9564609788500011
Epoch 5: loss 0.8629074951693569
Epoch 6: loss 0.7369540402784238
Epoch 7: loss 0.5978584193288794
Epoch 8: loss 0.4685952621448811
Epoch 9: loss 0.36356472952849456
Epoch 10: loss 0.2854075591207004
Epoch 11: loss 0.22860419791538214
Epoch 12: loss 0.1888422851712565
Epoch 13: loss 0.1612410953692796
Epoch 14: loss 0.1417425923726039
Epoch 15: loss 0.12766059625806725
Epoch 16: loss 0.11745926079427892
Epoch 17: loss 0.11007150076633565
Epoch 18: loss 0.10436933275358777
Epoch 19: loss 0.09972355213743503
Epoch 20: loss 0.09622290630406047
Epoch 21: loss 0.09335134922647813
Epoch 22: loss 0.09093907967720297
Epoch 23: loss 0.08903695000593853
Epoch 24: loss 0.0870914781276722
Epoch 25: loss 0.0858325674692336
Epoch 26: loss 0.08436607018258988
Epoch 27: loss 0.08352390897134648
Epoch 28: loss 0.08215589373047874
Epoch 29: loss 0.08146580750079113
Epoch 30: loss 0.08058607992131209
Epoch 31: loss 0.07974496576783124
Epoch 32: loss 0.07909566978759387
Epoch 33: loss 0.07846305190568842
Epoch 34: loss 0.07774574174472274
Epoch 35: loss 0.0772674638289774
Epoch 36: loss 0.07646195091157261
Epoch 37: loss 0.0760710399543189
Epoch 38: loss 0.07561241898760646
Epoch 39: loss 0.07528262086467875
Epoch 40: loss 0.07462358713158128
Epoch 41: loss 0.07428668734129841
Epoch 42: loss 0.07380624532359265
Epoch 43: loss 0.07349770259296459
Epoch 44: loss 0.07302150331628005
Epoch 45: loss 0.0731142069283486
Epoch 46: loss 0.0724301121221443
Epoch 47: loss 0.07224993681197843
Epoch 48: loss 0.07181296702180627
Epoch 49: loss 0.0716836997097669
-----------Time: 0:07:46.278906, Loss: regression, n_iter: 50, l2: 1e-05, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.1574715375900269-------------


Epoch 0: loss 16.129351270010837
Epoch 1: loss 16.12935101797945
Epoch 2: loss 16.129348856006516
Epoch 3: loss 16.129349028694687
Epoch 4: loss 16.129344457384683
Epoch 5: loss 16.129344461533346
Epoch 6: loss 16.12934603595163
Epoch 7: loss 16.129342688497736
Epoch 8: loss 16.129342724798555
Epoch 9: loss 16.129335635249028
Epoch 10: loss 16.12934074692262
Epoch 11: loss 16.12933958529648
Epoch 12: loss 16.129335534125325
Epoch 13: loss 16.129333609663448
Epoch 14: loss 16.129334197736682
Epoch 15: loss 16.129332758150003
Epoch 16: loss 16.129330376297837
Epoch 17: loss 16.129328442501464
Epoch 18: loss 16.12933402660426
Epoch 19: loss 16.129321977325986
Epoch 20: loss 16.129328037488065
Epoch 21: loss 16.129326443363624
Epoch 22: loss 16.129323253040408
Epoch 23: loss 16.129323956239087
Epoch 24: loss 16.12932551043263
Epoch 25: loss 16.129319504203195
Epoch 26: loss 16.129317362973584
Epoch 27: loss 16.129316366775456
Epoch 28: loss 16.12932080895827
Epoch 29: loss 16.12931374274498
Epoch 30: loss 16.12931297576058
Epoch 31: loss 16.12931129503276
Epoch 32: loss 16.12931127688235
Epoch 33: loss 16.12931098025282
Epoch 34: loss 16.129312010677435
Epoch 35: loss 16.129310344469943
Epoch 36: loss 16.129305605139013
Epoch 37: loss 16.129304314385685
Epoch 38: loss 16.129306620006137
Epoch 39: loss 16.129304907126166
Epoch 40: loss 16.12930643020472
Epoch 41: loss 16.129301717321532
Epoch 42: loss 16.129294484124486
Epoch 43: loss 16.12929722846624
Epoch 44: loss 16.129299650249305
Epoch 45: loss 16.129298533221313
Epoch 46: loss 16.1292933546505
Epoch 47: loss 16.12929488810072
Epoch 48: loss 16.129290684984714
Epoch 49: loss 16.129289196651225
-----------Time: 0:03:55.480740, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017207145690918-------------


Epoch 0: loss 16.129135798215216
Epoch 1: loss 16.129139832273133
Epoch 2: loss 16.12913779735306
Epoch 3: loss 16.129137281362876
Epoch 4: loss 16.129135304005526
Epoch 5: loss 16.12913188291283
Epoch 6: loss 16.129136609797765
Epoch 7: loss 16.12913002742251
Epoch 8: loss 16.129126402526655
Epoch 9: loss 16.12912434893759
Epoch 10: loss 16.129126156718268
Epoch 11: loss 16.129123982817923
Epoch 12: loss 16.12912724522419
Epoch 13: loss 16.129122635539037
Epoch 14: loss 16.129120624992368
Epoch 15: loss 16.12912189137229
Epoch 16: loss 16.129120892581245
Epoch 17: loss 16.129115467683462
Epoch 18: loss 16.129120114188016
Epoch 19: loss 16.12911764210239
Epoch 20: loss 16.129116347200394
Epoch 21: loss 16.129112272174414
Epoch 22: loss 16.129109946847805
Epoch 23: loss 16.12910632195195
Epoch 24: loss 16.12910966836868
Epoch 25: loss 16.129109052810545
Epoch 26: loss 16.129104249175313
Epoch 27: loss 16.12910477605574
Epoch 28: loss 16.129103976919186
Epoch 29: loss 16.129102041048483
Epoch 30: loss 16.1291046422613
Epoch 31: loss 16.12910101632828
Epoch 32: loss 16.12909878642096
Epoch 33: loss 16.129095741819597
Epoch 34: loss 16.12909867233268
Epoch 35: loss 16.129095215976335
Epoch 36: loss 16.129096590740126
Epoch 37: loss 16.12909711865772
Epoch 38: loss 16.12909803291972
Epoch 39: loss 16.129090407673853
Epoch 40: loss 16.129091431875473
Epoch 41: loss 16.129090367742958
Epoch 42: loss 16.12908553195557
Epoch 43: loss 16.12908655978727
Epoch 44: loss 16.12908607750499
Epoch 45: loss 16.12908051051544
Epoch 46: loss 16.129079313107066
Epoch 47: loss 16.129079647593162
Epoch 48: loss 16.129079477497907
Epoch 49: loss 16.129078572570403
-----------Time: 0:04:02.303243, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017186641693115-------------


Epoch 0: loss 16.12914908690709
Epoch 1: loss 16.12915030817029
Epoch 2: loss 16.129145906436957
Epoch 3: loss 16.129146141355097
Epoch 4: loss 16.129145593212765
Epoch 5: loss 16.129143746538354
Epoch 6: loss 16.12913712475078
Epoch 7: loss 16.129141555524768
Epoch 8: loss 16.129134679631477
Epoch 9: loss 16.12913701740408
Epoch 10: loss 16.129139445410143
Epoch 11: loss 16.129134890176214
Epoch 12: loss 16.12913270279271
Epoch 13: loss 16.129127129061576
Epoch 14: loss 16.129126435715975
Epoch 15: loss 16.12913032560779
Epoch 16: loss 16.129131995963945
Epoch 17: loss 16.129133397175476
Epoch 18: loss 16.12912703260512
Epoch 19: loss 16.1291254286276
Epoch 20: loss 16.129122841416525
Epoch 21: loss 16.129123397337604
Epoch 22: loss 16.129118359821398
Epoch 23: loss 16.129123558098367
Epoch 24: loss 16.129114925764124
Epoch 25: loss 16.12911872231098
Epoch 26: loss 16.129116602861863
Epoch 27: loss 16.129114734925544
Epoch 28: loss 16.129114011502125
Epoch 29: loss 16.129110360677117
Epoch 30: loss 16.12910895842842
Epoch 31: loss 16.12910933699408
Epoch 32: loss 16.129111057134217
Epoch 33: loss 16.129108234486417
Epoch 34: loss 16.129108265601403
Epoch 35: loss 16.129100729033247
Epoch 36: loss 16.129104450385555
Epoch 37: loss 16.129105717284062
Epoch 38: loss 16.129097643463815
Epoch 39: loss 16.129101835689575
Epoch 40: loss 16.129096914854564
Epoch 41: loss 16.129100992473457
Epoch 42: loss 16.12909812470893
Epoch 43: loss 16.12909600059256
Epoch 44: loss 16.12909185037203
Epoch 45: loss 16.12909444173177
Epoch 46: loss 16.12909370015794
Epoch 47: loss 16.129090963594937
Epoch 48: loss 16.12909219159971
Epoch 49: loss 16.12909065607516
-----------Time: 0:05:30.939792, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129160412761944
Epoch 1: loss 16.12916175641075
Epoch 2: loss 16.129162383896297
Epoch 3: loss 16.129163659610718
Epoch 4: loss 16.129162484501418
Epoch 5: loss 16.12915735882608
Epoch 6: loss 16.129153691924998
Epoch 7: loss 16.129160035233447
Epoch 8: loss 16.12915199149102
Epoch 9: loss 16.129154588555174
Epoch 10: loss 16.129151909554892
Epoch 11: loss 16.129147072211758
Epoch 12: loss 16.12914963608659
Epoch 13: loss 16.12914965060692
Epoch 14: loss 16.12914806166831
Epoch 15: loss 16.12914571611696
Epoch 16: loss 16.129147196671703
Epoch 17: loss 16.12914601067216
Epoch 18: loss 16.129142531498157
Epoch 19: loss 16.129144177480907
Epoch 20: loss 16.129138179030218
Epoch 21: loss 16.12913699562359
Epoch 22: loss 16.12913705733498
Epoch 23: loss 16.129136631059673
Epoch 24: loss 16.12913271627587
Epoch 25: loss 16.12913341947455
Epoch 26: loss 16.12913669432681
Epoch 27: loss 16.12912980339461
Epoch 28: loss 16.129130276342394
Epoch 29: loss 16.129130257673403
Epoch 30: loss 16.129130442288986
Epoch 31: loss 16.129126899847847
Epoch 32: loss 16.129125831566665
Epoch 33: loss 16.129125211341282
Epoch 34: loss 16.129124835368536
Epoch 35: loss 16.12912197486417
Epoch 36: loss 16.129120474084687
Epoch 37: loss 16.12911984659914
Epoch 38: loss 16.129116387131294
Epoch 39: loss 16.12911189360875
Epoch 40: loss 16.129111311758518
Epoch 41: loss 16.12911664434851
Epoch 42: loss 16.129111502078516
Epoch 43: loss 16.129109538204325
Epoch 44: loss 16.129110244514504
Epoch 45: loss 16.129115352558014
Epoch 46: loss 16.12911316724884
Epoch 47: loss 16.129108126621134
Epoch 48: loss 16.129102256260467
Epoch 49: loss 16.12910488184669
-----------Time: 0:06:50.908670, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129171631788676
Epoch 1: loss 16.129170475866953
Epoch 2: loss 16.129167982000837
Epoch 3: loss 16.129169933429033
Epoch 4: loss 16.12917258390724
Epoch 5: loss 16.12916438666422
Epoch 6: loss 16.12916718805011
Epoch 7: loss 16.129163642497474
Epoch 8: loss 16.12916272253106
Epoch 9: loss 16.12915953168926
Epoch 10: loss 16.129160442839762
Epoch 11: loss 16.12915913186169
Epoch 12: loss 16.12915778561997
Epoch 13: loss 16.1291595182061
Epoch 14: loss 16.129154409125423
Epoch 15: loss 16.129158256493422
Epoch 16: loss 16.12915258371292
Epoch 17: loss 16.129148419490647
Epoch 18: loss 16.129150932544338
Epoch 19: loss 16.129146184397495
Epoch 20: loss 16.129150312837535
Epoch 21: loss 16.12914263936344
Epoch 22: loss 16.129141291047386
Epoch 23: loss 16.129142257167697
Epoch 24: loss 16.12913919026726
Epoch 25: loss 16.12913824281594
Epoch 26: loss 16.129138409799697
Epoch 27: loss 16.12913991732076
Epoch 28: loss 16.12913811057725
Epoch 29: loss 16.12913883866792
Epoch 30: loss 16.12913846217659
Epoch 31: loss 16.129130793369743
Epoch 32: loss 16.12913497159376
Epoch 33: loss 16.129133025351397
Epoch 34: loss 16.12912678627815
Epoch 35: loss 16.129130304864464
Epoch 36: loss 16.129127214109204
Epoch 37: loss 16.12913295430551
Epoch 38: loss 16.129123879619886
Epoch 39: loss 16.12912857072259
Epoch 40: loss 16.129120586617216
Epoch 41: loss 16.129118481688423
Epoch 42: loss 16.129122620500127
Epoch 43: loss 16.129118932337136
Epoch 44: loss 16.12912200857207
Epoch 45: loss 16.129119901050363
Epoch 46: loss 16.1291131039817
Epoch 47: loss 16.129113252815053
Epoch 48: loss 16.12911278764601
Epoch 49: loss 16.12911447304108
-----------Time: 0:06:31.500501, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017180919647217-------------


Epoch 0: loss 16.129143551032527
Epoch 1: loss 16.129131423448207
Epoch 2: loss 16.12912082309111
Epoch 3: loss 16.12910712005134
Epoch 4: loss 16.129097710361034
Epoch 5: loss 16.129078232898472
Epoch 6: loss 16.129070898577726
Epoch 7: loss 16.12906040764166
Epoch 8: loss 16.12904916372294
Epoch 9: loss 16.129029865690132
Epoch 10: loss 16.12902243387576
Epoch 11: loss 16.12901361277727
Epoch 12: loss 16.12899876989185
Epoch 13: loss 16.128982844204927
Epoch 14: loss 16.128974621551333
Epoch 15: loss 16.128957361031514
Epoch 16: loss 16.128948550823267
Epoch 17: loss 16.128937118658882
Epoch 18: loss 16.128923869897907
Epoch 19: loss 16.128907892871254
Epoch 20: loss 16.1288978157645
Epoch 21: loss 16.128882034243677
Epoch 22: loss 16.128875428013597
Epoch 23: loss 16.128858645627393
Epoch 24: loss 16.128849026429517
Epoch 25: loss 16.1288334471561
Epoch 26: loss 16.128823488804876
Epoch 27: loss 16.128817497095767
Epoch 28: loss 16.128801931305514
Epoch 29: loss 16.128787865257575
Epoch 30: loss 16.128776990051435
Epoch 31: loss 16.12876733092266
Epoch 32: loss 16.128750987257757
Epoch 33: loss 16.128737015591945
Epoch 34: loss 16.128726057931093
Epoch 35: loss 16.128715038558852
Epoch 36: loss 16.128703393775396
Epoch 37: loss 16.12868651182124
Epoch 38: loss 16.128671288295827
Epoch 39: loss 16.128667629173492
Epoch 40: loss 16.128649496915486
Epoch 41: loss 16.128637513497267
Epoch 42: loss 16.128633666647847
Epoch 43: loss 16.12861369704993
Epoch 44: loss 16.128602426683468
Epoch 45: loss 16.12859063721533
Epoch 46: loss 16.12857766278481
Epoch 47: loss 16.12856620780277
Epoch 48: loss 16.12855130113163
Epoch 49: loss 16.128540766634586
-----------Time: 0:03:42.366987, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.0171403884887695-------------


Epoch 0: loss 16.129181906475587
Epoch 1: loss 16.129172103699293
Epoch 2: loss 16.129156765048435
Epoch 3: loss 16.12913938110584
Epoch 4: loss 16.12912886890787
Epoch 5: loss 16.129119528189115
Epoch 6: loss 16.12911367597886
Epoch 7: loss 16.12909042893576
Epoch 8: loss 16.12908541320004
Epoch 9: loss 16.12906469476813
Epoch 10: loss 16.129059494416826
Epoch 11: loss 16.129051795013577
Epoch 12: loss 16.129031685916804
Epoch 13: loss 16.12902291304654
Epoch 14: loss 16.129012187192334
Epoch 15: loss 16.128995010682978
Epoch 16: loss 16.128988319923852
Epoch 17: loss 16.128968305209202
Epoch 18: loss 16.12895845731618
Epoch 19: loss 16.128947580554293
Epoch 20: loss 16.128933870254357
Epoch 21: loss 16.128921536273968
Epoch 22: loss 16.128910630471424
Epoch 23: loss 16.128897029073944
Epoch 24: loss 16.12888329854927
Epoch 25: loss 16.128875401047274
Epoch 26: loss 16.12886243335834
Epoch 27: loss 16.128847586324255
Epoch 28: loss 16.128831827621088
Epoch 29: loss 16.12881979597464
Epoch 30: loss 16.128806311258355
Epoch 31: loss 16.1287989893836
Epoch 32: loss 16.128784061969135
Epoch 33: loss 16.12877350984027
Epoch 34: loss 16.12876003964431
Epoch 35: loss 16.128745501685753
Epoch 36: loss 16.128734303920925
Epoch 37: loss 16.128721785324952
Epoch 38: loss 16.128711186523603
Epoch 39: loss 16.12869752860056
Epoch 40: loss 16.128687740863178
Epoch 41: loss 16.12867438060684
Epoch 42: loss 16.128665404452
Epoch 43: loss 16.128650732699004
Epoch 44: loss 16.128641231219493
Epoch 45: loss 16.128623125927806
Epoch 46: loss 16.12861518382767
Epoch 47: loss 16.12860163013983
Epoch 48: loss 16.128588789503752
Epoch 49: loss 16.128579401593935
-----------Time: 0:04:40.551193, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017110824584961-------------


Epoch 0: loss 16.12916919963395
Epoch 1: loss 16.129156642144242
Epoch 2: loss 16.129146472729698
Epoch 3: loss 16.129138037457032
Epoch 4: loss 16.129124499845272
Epoch 5: loss 16.129111791966466
Epoch 6: loss 16.129097783481253
Epoch 7: loss 16.12908576220647
Epoch 8: loss 16.12907625191104
Epoch 9: loss 16.129060388454086
Epoch 10: loss 16.12904703130924
Epoch 11: loss 16.129033744173118
Epoch 12: loss 16.129023536902007
Epoch 13: loss 16.129012884686603
Epoch 14: loss 16.12900027948725
Epoch 15: loss 16.128989598749772
Epoch 16: loss 16.128979225013488
Epoch 17: loss 16.12895993890809
Epoch 18: loss 16.12895289914254
Epoch 19: loss 16.128939816328156
Epoch 20: loss 16.12892733144008
Epoch 21: loss 16.12890957463624
Epoch 22: loss 16.128900255179392
Epoch 23: loss 16.12888579708263
Epoch 24: loss 16.128877430262936
Epoch 25: loss 16.12886442419885
Epoch 26: loss 16.12885276541365
Epoch 27: loss 16.128840270672498
Epoch 28: loss 16.12882326477698
Epoch 29: loss 16.128814377299854
Epoch 30: loss 16.128804525258165
Epoch 31: loss 16.128787833105424
Epoch 32: loss 16.128773738535415
Epoch 33: loss 16.128763954946695
Epoch 34: loss 16.12875174594483
Epoch 35: loss 16.128739185862205
Epoch 36: loss 16.12872924047556
Epoch 37: loss 16.12871628212112
Epoch 38: loss 16.12869944839519
Epoch 39: loss 16.128684776642196
Epoch 40: loss 16.128678719591615
Epoch 41: loss 16.128670199271323
Epoch 42: loss 16.12865589519374
Epoch 43: loss 16.128642039171957
Epoch 44: loss 16.128629813575433
Epoch 45: loss 16.128614528338634
Epoch 46: loss 16.12860341458427
Epoch 47: loss 16.128592831340413
Epoch 48: loss 16.12857727229174
Epoch 49: loss 16.128566465019986
-----------Time: 0:06:13.922349, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017109394073486-------------


Epoch 0: loss 16.129170017958078
Epoch 1: loss 16.129156928920693
Epoch 2: loss 16.129148848358867
Epoch 3: loss 16.129136211007363
Epoch 4: loss 16.129120732339068
Epoch 5: loss 16.12910761374245
Epoch 6: loss 16.12909461753144
Epoch 7: loss 16.129086997471404
Epoch 8: loss 16.129077125204976
Epoch 9: loss 16.129057448087924
Epoch 10: loss 16.129038643746224
Epoch 11: loss 16.12903243423221
Epoch 12: loss 16.129023489192363
Epoch 13: loss 16.129007407411926
Epoch 14: loss 16.12899479287808
Epoch 15: loss 16.128984983360205
Epoch 16: loss 16.12897145715727
Epoch 17: loss 16.12896275481431
Epoch 18: loss 16.128950443651572
Epoch 19: loss 16.128930681486892
Epoch 20: loss 16.128923175515137
Epoch 21: loss 16.128910671958074
Epoch 22: loss 16.128896897353837
Epoch 23: loss 16.1288857581889
Epoch 24: loss 16.128874965956058
Epoch 25: loss 16.12886157510331
Epoch 26: loss 16.128847520982784
Epoch 27: loss 16.128830953808567
Epoch 28: loss 16.128823389755507
Epoch 29: loss 16.128813594239375
Epoch 30: loss 16.128798989902183
Epoch 31: loss 16.12878956206147
Epoch 32: loss 16.128771421506134
Epoch 33: loss 16.128762518471515
Epoch 34: loss 16.12874623288792
Epoch 35: loss 16.128738551635077
Epoch 36: loss 16.12872144357869
Epoch 37: loss 16.128709798276653
Epoch 38: loss 16.128695635772257
Epoch 39: loss 16.128689420553833
Epoch 40: loss 16.128680799109837
Epoch 41: loss 16.128663640232304
Epoch 42: loss 16.12865222621833
Epoch 43: loss 16.128641102610885
Epoch 44: loss 16.12862894754166
Epoch 45: loss 16.12861278019501
Epoch 46: loss 16.128598393144134
Epoch 47: loss 16.128591121053358
Epoch 48: loss 16.128574527431404
Epoch 49: loss 16.128566082824243
-----------Time: 0:05:45.761166, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.0171122550964355-------------


Epoch 0: loss 16.12917333844565
Epoch 1: loss 16.12915727325987
Epoch 2: loss 16.129147086213504
Epoch 3: loss 16.129136192856954
Epoch 4: loss 16.12912299491712
Epoch 5: loss 16.12910510950467
Epoch 6: loss 16.129098331623585
Epoch 7: loss 16.129085338005492
Epoch 8: loss 16.129069731765757
Epoch 9: loss 16.12905604324631
Epoch 10: loss 16.129048476081753
Epoch 11: loss 16.12903340813127
Epoch 12: loss 16.12902318063542
Epoch 13: loss 16.129007885026958
Epoch 14: loss 16.128998315094474
Epoch 15: loss 16.12898148033138
Epoch 16: loss 16.12897567583077
Epoch 17: loss 16.128955870105106
Epoch 18: loss 16.128948690840705
Epoch 19: loss 16.128935510532695
Epoch 20: loss 16.128921220975442
Epoch 21: loss 16.128910599875024
Epoch 22: loss 16.12890149718591
Epoch 23: loss 16.128883878325173
Epoch 24: loss 16.128870226106546
Epoch 25: loss 16.128865173551425
Epoch 26: loss 16.12884925668041
Epoch 27: loss 16.128832044907405
Epoch 28: loss 16.128822780420368
Epoch 29: loss 16.12881142500628
Epoch 30: loss 16.128795536138753
Epoch 31: loss 16.12878820285517
Epoch 32: loss 16.12877208736683
Epoch 33: loss 16.12875887801817
Epoch 34: loss 16.12875394110708
Epoch 35: loss 16.128737790355093
Epoch 36: loss 16.1287202835083
Epoch 37: loss 16.128709349702273
Epoch 38: loss 16.12869874519651
Epoch 39: loss 16.128688922195476
Epoch 40: loss 16.12867567187875
Epoch 41: loss 16.12866446322368
Epoch 42: loss 16.128649632784256
Epoch 43: loss 16.128639018425414
Epoch 44: loss 16.128623567242023
Epoch 45: loss 16.128615023585493
Epoch 46: loss 16.12859942875458
Epoch 47: loss 16.128588620964244
Epoch 48: loss 16.128572331231986
Epoch 49: loss 16.128567479368524
-----------Time: 0:06:56.708243, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017111301422119-------------


Epoch 0: loss 16.12914072734756
Epoch 1: loss 16.129019807770952
Epoch 2: loss 16.128897826654747
Epoch 3: loss 16.128772346139797
Epoch 4: loss 16.128645134075885
Epoch 5: loss 16.12852273135162
Epoch 6: loss 16.128399849975153
Epoch 7: loss 16.128275712590426
Epoch 8: loss 16.128153721621143
Epoch 9: loss 16.12803060169645
Epoch 10: loss 16.12790563457877
Epoch 11: loss 16.127778504450987
Epoch 12: loss 16.12765734891907
Epoch 13: loss 16.12753318041936
Epoch 14: loss 16.127409179940567
Epoch 15: loss 16.127278091468003
Epoch 16: loss 16.127163259538378
Epoch 17: loss 16.127034721457484
Epoch 18: loss 16.126916590820777
Epoch 19: loss 16.126789290079156
Epoch 20: loss 16.1266627573591
Epoch 21: loss 16.126542532165264
Epoch 22: loss 16.126421229874328
Epoch 23: loss 16.126297963190623
Epoch 24: loss 16.126168153025393
Epoch 25: loss 16.126045035175036
Epoch 26: loss 16.125922457688265
Epoch 27: loss 16.125795917708047
Epoch 28: loss 16.125668356119128
Epoch 29: loss 16.125545928502873
Epoch 30: loss 16.125431246962346
Epoch 31: loss 16.12530498960992
Epoch 32: loss 16.12517628506385
Epoch 33: loss 16.125052861249465
Epoch 34: loss 16.124927789378
Epoch 35: loss 16.124804611890585
Epoch 36: loss 16.124685608997105
Epoch 37: loss 16.124559615603474
Epoch 38: loss 16.124435959982446
Epoch 39: loss 16.124311081023887
Epoch 40: loss 16.124186666715786
Epoch 41: loss 16.124060576347116
Epoch 42: loss 16.1239396018007
Epoch 43: loss 16.123815441598317
Epoch 44: loss 16.123688840943878
Epoch 45: loss 16.1235706724506
Epoch 46: loss 16.12344199020361
Epoch 47: loss 16.123318180044816
Epoch 48: loss 16.12319606046692
Epoch 49: loss 16.123072782374386
-----------Time: 0:04:34.795471, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016439914703369-------------


Epoch 0: loss 16.129141069612405
Epoch 1: loss 16.129013596701196
Epoch 2: loss 16.128891999336478
Epoch 3: loss 16.128764540945596
Epoch 4: loss 16.128639199410916
Epoch 5: loss 16.128515829010592
Epoch 6: loss 16.128395258958992
Epoch 7: loss 16.12827385450719
Epoch 8: loss 16.128143775715913
Epoch 9: loss 16.128023759511063
Epoch 10: loss 16.127898595331803
Epoch 11: loss 16.127772112395725
Epoch 12: loss 16.127646395406884
Epoch 13: loss 16.1275277731534
Epoch 14: loss 16.127397425217495
Epoch 15: loss 16.127282443417357
Epoch 16: loss 16.12715714751799
Epoch 17: loss 16.127031275472802
Epoch 18: loss 16.12690612270237
Epoch 19: loss 16.12678365048797
Epoch 20: loss 16.12665765761292
Epoch 21: loss 16.126532338377313
Epoch 22: loss 16.12640876365525
Epoch 23: loss 16.12628350613103
Epoch 24: loss 16.126163117583516
Epoch 25: loss 16.12603795236709
Epoch 26: loss 16.125912940132675
Epoch 27: loss 16.125792015888823
Epoch 28: loss 16.125665221284304
Epoch 29: loss 16.12553764880514
Epoch 30: loss 16.125421351878263
Epoch 31: loss 16.125291956060927
Epoch 32: loss 16.1251741604289
Epoch 33: loss 16.12504479365222
Epoch 34: loss 16.124922048144526
Epoch 35: loss 16.124797986991513
Epoch 36: loss 16.124682271914875
Epoch 37: loss 16.12455285794713
Epoch 38: loss 16.124427852454296
Epoch 39: loss 16.124297976429013
Epoch 40: loss 16.124177361260685
Epoch 41: loss 16.124050469162544
Epoch 42: loss 16.12392687006707
Epoch 43: loss 16.123807507276926
Epoch 44: loss 16.123684220368478
Epoch 45: loss 16.123560581342108
Epoch 46: loss 16.123435986567088
Epoch 47: loss 16.123311289112618
Epoch 48: loss 16.123184148094587
Epoch 49: loss 16.123059757122725
-----------Time: 0:05:23.122763, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016427516937256-------------


Epoch 0: loss 16.12911148963252
Epoch 1: loss 16.12898726357008
Epoch 2: loss 16.128865362315672
Epoch 3: loss 16.128736534346828
Epoch 4: loss 16.12861607786493
Epoch 5: loss 16.128489277037414
Epoch 6: loss 16.12836505149356
Epoch 7: loss 16.128243444275764
Epoch 8: loss 16.12811378709255
Epoch 9: loss 16.127993978320937
Epoch 10: loss 16.127866115435236
Epoch 11: loss 16.127745264830182
Epoch 12: loss 16.127624440672864
Epoch 13: loss 16.127499028092302
Epoch 14: loss 16.127376653371524
Epoch 15: loss 16.127250709243285
Epoch 16: loss 16.12712351532978
Epoch 17: loss 16.12699971035682
Epoch 18: loss 16.12688187220098
Epoch 19: loss 16.12675475140769
Epoch 20: loss 16.12662438013555
Epoch 21: loss 16.126508720028717
Epoch 22: loss 16.12638453234143
Epoch 23: loss 16.126255507829594
Epoch 24: loss 16.12613635350985
Epoch 25: loss 16.12601195320349
Epoch 26: loss 16.125884519186016
Epoch 27: loss 16.125758706777884
Epoch 28: loss 16.12563696576565
Epoch 29: loss 16.125516640485273
Epoch 30: loss 16.12539153231299
Epoch 31: loss 16.12527161204601
Epoch 32: loss 16.125144060828752
Epoch 33: loss 16.125019232172757
Epoch 34: loss 16.124891568422967
Epoch 35: loss 16.12477136034237
Epoch 36: loss 16.124645712843662
Epoch 37: loss 16.12451909300165
Epoch 38: loss 16.12440035095547
Epoch 39: loss 16.12427255548557
Epoch 40: loss 16.124157883798123
Epoch 41: loss 16.12402715625939
Epoch 42: loss 16.123904014554213
Epoch 43: loss 16.123779481490583
Epoch 44: loss 16.123655138746948
Epoch 45: loss 16.12353403248043
Epoch 46: loss 16.12340429336108
Epoch 47: loss 16.123281617343522
Epoch 48: loss 16.12315965385914
Epoch 49: loss 16.123037478274274
-----------Time: 0:04:48.142609, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016416549682617-------------


Epoch 0: loss 16.129119882899953
Epoch 1: loss 16.129000297637656
Epoch 2: loss 16.12887050302992
Epoch 3: loss 16.128749794516633
Epoch 4: loss 16.12862462463296
Epoch 5: loss 16.128501792521888
Epoch 6: loss 16.128381500949413
Epoch 7: loss 16.128252562003787
Epoch 8: loss 16.12812968010874
Epoch 9: loss 16.128001027939565
Epoch 10: loss 16.127881182867135
Epoch 11: loss 16.12775911151747
Epoch 12: loss 16.12763388510824
Epoch 13: loss 16.12751101151052
Epoch 14: loss 16.12737857264757
Epoch 15: loss 16.127259835787218
Epoch 16: loss 16.12713647160989
Epoch 17: loss 16.12701219939356
Epoch 18: loss 16.1268905849156
Epoch 19: loss 16.126764780286216
Epoch 20: loss 16.126642639965
Epoch 21: loss 16.12651636601791
Epoch 22: loss 16.12639617297622
Epoch 23: loss 16.126269806202757
Epoch 24: loss 16.12614513156594
Epoch 25: loss 16.12601851016818
Epoch 26: loss 16.125895409431063
Epoch 27: loss 16.125773718202808
Epoch 28: loss 16.125649564742005
Epoch 29: loss 16.12552630324413
Epoch 30: loss 16.125401525409277
Epoch 31: loss 16.12527926270245
Epoch 32: loss 16.125153491780964
Epoch 33: loss 16.125026984990065
Epoch 34: loss 16.12491032298069
Epoch 35: loss 16.124777061644945
Epoch 36: loss 16.124659667914816
Epoch 37: loss 16.124530649625978
Epoch 38: loss 16.124409710343212
Epoch 39: loss 16.12428818195005
Epoch 40: loss 16.124163100225502
Epoch 41: loss 16.124037449615297
Epoch 42: loss 16.123910218882394
Epoch 43: loss 16.123787781931643
Epoch 44: loss 16.123663902282715
Epoch 45: loss 16.123542140008574
Epoch 46: loss 16.123417715847395
Epoch 47: loss 16.123294557547556
Epoch 48: loss 16.12317295810851
Epoch 49: loss 16.12304700412719
-----------Time: 0:05:45.932456, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016421794891357-------------


Epoch 0: loss 16.129112042960685
Epoch 1: loss 16.128991703678565
Epoch 2: loss 16.12886749109929
Epoch 3: loss 16.12874109476659
Epoch 4: loss 16.12861537777775
Epoch 5: loss 16.128496670476636
Epoch 6: loss 16.128375007770448
Epoch 7: loss 16.128244368390845
Epoch 8: loss 16.12812225711028
Epoch 9: loss 16.127998372794107
Epoch 10: loss 16.127877840599073
Epoch 11: loss 16.12775393865107
Epoch 12: loss 16.127626531599915
Epoch 13: loss 16.12750199646195
Epoch 14: loss 16.12737858301923
Epoch 15: loss 16.12725421123494
Epoch 16: loss 16.127131036340444
Epoch 17: loss 16.127007720909926
Epoch 18: loss 16.126884052842904
Epoch 19: loss 16.1267553923764
Epoch 20: loss 16.12663450547053
Epoch 21: loss 16.126510711387812
Epoch 22: loss 16.126390695182963
Epoch 23: loss 16.126264756759138
Epoch 24: loss 16.126137197763132
Epoch 25: loss 16.126016978273707
Epoch 26: loss 16.125890687213378
Epoch 27: loss 16.125766027096887
Epoch 28: loss 16.12564103612438
Epoch 29: loss 16.12551656477214
Epoch 30: loss 16.125398292043663
Epoch 31: loss 16.125268710054996
Epoch 32: loss 16.125146415196014
Epoch 33: loss 16.12501856683064
Epoch 34: loss 16.12489491587686
Epoch 35: loss 16.124779281699183
Epoch 36: loss 16.12465365286947
Epoch 37: loss 16.124529636314605
Epoch 38: loss 16.12440301336109
Epoch 39: loss 16.124281258347114
Epoch 40: loss 16.124154552938887
Epoch 41: loss 16.124027620391264
Epoch 42: loss 16.12390548733021
Epoch 43: loss 16.123783283223272
Epoch 44: loss 16.12365921221718
Epoch 45: loss 16.123531653221175
Epoch 46: loss 16.123410503912257
Epoch 47: loss 16.12328574526498
Epoch 48: loss 16.12316159958292
Epoch 49: loss 16.123039784413308
-----------Time: 0:07:15.896878, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016419887542725-------------


Epoch 0: loss 16.128802435886865
Epoch 1: loss 16.127561342074472
Epoch 2: loss 16.12632075491776
Epoch 3: loss 16.125078581415355
Epoch 4: loss 16.123839177146692
Epoch 5: loss 16.122602153174444
Epoch 6: loss 16.121360628938078
Epoch 7: loss 16.120121313347124
Epoch 8: loss 16.11888623354291
Epoch 9: loss 16.117644366004534
Epoch 10: loss 16.116407418782586
Epoch 11: loss 16.11516715366598
Epoch 12: loss 16.113932735055215
Epoch 13: loss 16.112689984888405
Epoch 14: loss 16.11144389867653
Epoch 15: loss 16.110202942288655
Epoch 16: loss 16.108966814428
Epoch 17: loss 16.107727041965337
Epoch 18: loss 16.106494029751932
Epoch 19: loss 16.105252222887778
Epoch 20: loss 16.10402073012285
Epoch 21: loss 16.1027802331996
Epoch 22: loss 16.10154119660635
Epoch 23: loss 16.100299052663186
Epoch 24: loss 16.099058076050568
Epoch 25: loss 16.09782517688828
Epoch 26: loss 16.096582426202886
Epoch 27: loss 16.095343866187505
Epoch 28: loss 16.094102101847163
Epoch 29: loss 16.092871517639825
Epoch 30: loss 16.091635599805326
Epoch 31: loss 16.090397190179043
Epoch 32: loss 16.089156249348658
Epoch 33: loss 16.087922729442404
Epoch 34: loss 16.086675968035333
Epoch 35: loss 16.085439532654902
Epoch 36: loss 16.084203529254193
Epoch 37: loss 16.082965960251112
Epoch 38: loss 16.081728834117996
Epoch 39: loss 16.0804883999433
Epoch 40: loss 16.07925773898566
Epoch 41: loss 16.078019466783896
Epoch 42: loss 16.07677577953743
Epoch 43: loss 16.0755473733791
Epoch 44: loss 16.07430372035912
Epoch 45: loss 16.073073368477004
Epoch 46: loss 16.07183217613382
Epoch 47: loss 16.070590300298115
Epoch 48: loss 16.06936032179583
Epoch 49: loss 16.06812269419286
-----------Time: 0:05:03.763020, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.009541034698486-------------


Epoch 0: loss 16.128600103431193
Epoch 1: loss 16.127361776778205
Epoch 2: loss 16.126122700254058
Epoch 3: loss 16.12488181750498
Epoch 4: loss 16.123647209611388
Epoch 5: loss 16.12240381329004
Epoch 6: loss 16.12116909908691
Epoch 7: loss 16.119930946677844
Epoch 8: loss 16.118684372479272
Epoch 9: loss 16.11745348697515
Epoch 10: loss 16.116209019779706
Epoch 11: loss 16.114967348784322
Epoch 12: loss 16.113729508043697
Epoch 13: loss 16.1124914577955
Epoch 14: loss 16.111252981272
Epoch 15: loss 16.11001627985844
Epoch 16: loss 16.108777886826815
Epoch 17: loss 16.107538175556957
Epoch 18: loss 16.106298196179637
Epoch 19: loss 16.10506156062613
Epoch 20: loss 16.103821886175673
Epoch 21: loss 16.10258304612542
Epoch 22: loss 16.10134606156549
Epoch 23: loss 16.100110756177628
Epoch 24: loss 16.098870414829307
Epoch 25: loss 16.09763740209732
Epoch 26: loss 16.096390503784313
Epoch 27: loss 16.095153491220895
Epoch 28: loss 16.093914928094016
Epoch 29: loss 16.09267730671404
Epoch 30: loss 16.091440360010676
Epoch 31: loss 16.090205846499202
Epoch 32: loss 16.088967069716087
Epoch 33: loss 16.087726739776596
Epoch 34: loss 16.086483209142223
Epoch 35: loss 16.085249940230188
Epoch 36: loss 16.084014156190126
Epoch 37: loss 16.082769005502158
Epoch 38: loss 16.08153793123379
Epoch 39: loss 16.08030035445196
Epoch 40: loss 16.079064074127878
Epoch 41: loss 16.07782530045626
Epoch 42: loss 16.07659086265792
Epoch 43: loss 16.07534882398712
Epoch 44: loss 16.07412262906712
Epoch 45: loss 16.072872505167243
Epoch 46: loss 16.0716405741996
Epoch 47: loss 16.070395561454735
Epoch 48: loss 16.06916223912864
Epoch 49: loss 16.067927262522463
-----------Time: 0:03:45.593741, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.00950813293457-------------


Epoch 0: loss 16.128556452217538
Epoch 1: loss 16.127319107242354
Epoch 2: loss 16.12607662103434
Epoch 3: loss 16.124839495419806
Epoch 4: loss 16.12359887196228
Epoch 5: loss 16.122362567783373
Epoch 6: loss 16.12112368728364
Epoch 7: loss 16.119881155958897
Epoch 8: loss 16.11863888807437
Epoch 9: loss 16.11739454067162
Epoch 10: loss 16.116159434419668
Epoch 11: loss 16.114920588146415
Epoch 12: loss 16.113681804103134
Epoch 13: loss 16.112444265177874
Epoch 14: loss 16.11120330101125
Epoch 15: loss 16.10996627392751
Epoch 16: loss 16.108724194807227
Epoch 17: loss 16.10748564931217
Epoch 18: loss 16.106242487390382
Epoch 19: loss 16.10500648710117
Epoch 20: loss 16.103763375481943
Epoch 21: loss 16.102529987295792
Epoch 22: loss 16.101291127020797
Epoch 23: loss 16.10005264634863
Epoch 24: loss 16.09881413663581
Epoch 25: loss 16.097579697800306
Epoch 26: loss 16.096333876584392
Epoch 27: loss 16.0951025238369
Epoch 28: loss 16.093865538239804
Epoch 29: loss 16.092621447017102
Epoch 30: loss 16.091385276632636
Epoch 31: loss 16.090151444539355
Epoch 32: loss 16.0889102973129
Epoch 33: loss 16.087667879039273
Epoch 34: loss 16.086432484973702
Epoch 35: loss 16.08519304388564
Epoch 36: loss 16.083957150424546
Epoch 37: loss 16.082717136302158
Epoch 38: loss 16.08148080567551
Epoch 39: loss 16.080237751100427
Epoch 40: loss 16.0790010087188
Epoch 41: loss 16.07776713617604
Epoch 42: loss 16.076531179447805
Epoch 43: loss 16.075290079930998
Epoch 44: loss 16.07405279822295
Epoch 45: loss 16.072814742788925
Epoch 46: loss 16.07157675010345
Epoch 47: loss 16.07034198559776
Epoch 48: loss 16.06910430146923
Epoch 49: loss 16.06786712295922
-----------Time: 0:05:12.729224, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009510040283203-------------


Epoch 0: loss 16.12856028454663
Epoch 1: loss 16.127321764980728
Epoch 2: loss 16.126078053360857
Epoch 3: loss 16.12483518962435
Epoch 4: loss 16.123606384675615
Epoch 5: loss 16.122359771583312
Epoch 6: loss 16.121122061007046
Epoch 7: loss 16.119880370824088
Epoch 8: loss 16.118644029307198
Epoch 9: loss 16.11740423765696
Epoch 10: loss 16.116166634427394
Epoch 11: loss 16.114924103102652
Epoch 12: loss 16.113680576616943
Epoch 13: loss 16.112454769597097
Epoch 14: loss 16.111207086667864
Epoch 15: loss 16.109964725438378
Epoch 16: loss 16.108729183058042
Epoch 17: loss 16.107487363229307
Epoch 18: loss 16.106248993533924
Epoch 19: loss 16.105006633341603
Epoch 20: loss 16.103771977738365
Epoch 21: loss 16.102538445904695
Epoch 22: loss 16.101297653907665
Epoch 23: loss 16.10005839899093
Epoch 24: loss 16.098818356865056
Epoch 25: loss 16.097578993045875
Epoch 26: loss 16.096338536572105
Epoch 27: loss 16.09509766678761
Epoch 28: loss 16.09386617402268
Epoch 29: loss 16.09262390250807
Epoch 30: loss 16.09139257828265
Epoch 31: loss 16.090150786975986
Epoch 32: loss 16.08890904337897
Epoch 33: loss 16.087672416122793
Epoch 34: loss 16.086432161896433
Epoch 35: loss 16.085194364198205
Epoch 36: loss 16.08395368577087
Epoch 37: loss 16.082720986263073
Epoch 38: loss 16.08148024612435
Epoch 39: loss 16.08024273260966
Epoch 40: loss 16.079006815812328
Epoch 41: loss 16.077768916990394
Epoch 42: loss 16.076526614360798
Epoch 43: loss 16.07529275115253
Epoch 44: loss 16.0740567544934
Epoch 45: loss 16.072819972699463
Epoch 46: loss 16.071583473014726
Epoch 47: loss 16.070342740654752
Epoch 48: loss 16.069103346239167
Epoch 49: loss 16.06786651673558
-----------Time: 0:06:23.332794, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.0095086097717285-------------


Epoch 0: loss 16.1285640930209
Epoch 1: loss 16.127326672851165
Epoch 2: loss 16.12608296226846
Epoch 3: loss 16.12484785601651
Epoch 4: loss 16.1236018781885
Epoch 5: loss 16.1223623676103
Epoch 6: loss 16.12112088486058
Epoch 7: loss 16.119880989493723
Epoch 8: loss 16.118646426716857
Epoch 9: loss 16.11740724129026
Epoch 10: loss 16.116170977560834
Epoch 11: loss 16.1149287221223
Epoch 12: loss 16.113689695900714
Epoch 13: loss 16.112454349026205
Epoch 14: loss 16.111207847429267
Epoch 15: loss 16.109965230019732
Epoch 16: loss 16.10873440985708
Epoch 17: loss 16.107495950965404
Epoch 18: loss 16.106255116444558
Epoch 19: loss 16.10501769212616
Epoch 20: loss 16.10377538379215
Epoch 21: loss 16.10253893544714
Epoch 22: loss 16.101298401185907
Epoch 23: loss 16.10006709770381
Epoch 24: loss 16.098824241746048
Epoch 25: loss 16.097587947938806
Epoch 26: loss 16.09634649422974
Epoch 27: loss 16.0951104426008
Epoch 28: loss 16.093873808603043
Epoch 29: loss 16.092625575975728
Epoch 30: loss 16.09138905606625
Epoch 31: loss 16.090152278420977
Epoch 32: loss 16.08891409593409
Epoch 33: loss 16.08767633194376
Epoch 34: loss 16.086438964669505
Epoch 35: loss 16.085197563855917
Epoch 36: loss 16.083962438934975
Epoch 37: loss 16.082726109864076
Epoch 38: loss 16.08148212754383
Epoch 39: loss 16.0802487295046
Epoch 40: loss 16.079013450045895
Epoch 41: loss 16.07777245995012
Epoch 42: loss 16.07653428887206
Epoch 43: loss 16.075299147356457
Epoch 44: loss 16.07405941897336
Epoch 45: loss 16.07281768056217
Epoch 46: loss 16.071579391247166
Epoch 47: loss 16.070340866495435
Epoch 48: loss 16.069107658776204
Epoch 49: loss 16.067869971017593
-----------Time: 0:07:43.926357, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.00950813293457-------------


Epoch 0: loss 16.123025483447226
Epoch 1: loss 16.11064620274704
Epoch 2: loss 16.098254272249356
Epoch 3: loss 16.085870377715352
Epoch 4: loss 16.07349753989157
Epoch 5: loss 16.061121380024463
Epoch 6: loss 16.048759757078752
Epoch 7: loss 16.036400609848744
Epoch 8: loss 16.02404129667215
Epoch 9: loss 16.01169316674005
Epoch 10: loss 15.999340323406177
Epoch 11: loss 15.987003645863204
Epoch 12: loss 15.974668057344736
Epoch 13: loss 15.962343557688644
Epoch 14: loss 15.950019850946108
Epoch 15: loss 15.937699915339858
Epoch 16: loss 15.925396672404935
Epoch 17: loss 15.91308994199868
Epoch 18: loss 15.900788087310708
Epoch 19: loss 15.888487293125172
Epoch 20: loss 15.876196301197345
Epoch 21: loss 15.86391735854779
Epoch 22: loss 15.851638566287333
Epoch 23: loss 15.839367435054976
Epoch 24: loss 15.827095072187763
Epoch 25: loss 15.814837898619476
Epoch 26: loss 15.802578840001631
Epoch 27: loss 15.79032307490504
Epoch 28: loss 15.778075837388903
Epoch 29: loss 15.765834057959868
Epoch 30: loss 15.753595248456232
Epoch 31: loss 15.741362631353365
Epoch 32: loss 15.7291329411335
Epoch 33: loss 15.716908242794533
Epoch 34: loss 15.70469418785506
Epoch 35: loss 15.692486839232204
Epoch 36: loss 15.680270513417346
Epoch 37: loss 15.66806658692435
Epoch 38: loss 15.655867745657211
Epoch 39: loss 15.64367805530741
Epoch 40: loss 15.631485775153621
Epoch 41: loss 15.619309442985829
Epoch 42: loss 15.607128010553273
Epoch 43: loss 15.59495372367722
Epoch 44: loss 15.582785378207715
Epoch 45: loss 15.57063425177138
Epoch 46: loss 15.558472797752659
Epoch 47: loss 15.54632105783252
Epoch 48: loss 15.53417709251018
Epoch 49: loss 15.522029063570688
-----------Time: 0:03:26.135459, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9404850006103516-------------


Epoch 0: loss 16.123052226258984
Epoch 1: loss 16.110663500086257
Epoch 2: loss 16.098275374951346
Epoch 3: loss 16.085897506871518
Epoch 4: loss 16.073521792985876
Epoch 5: loss 16.061149399069226
Epoch 6: loss 16.048781142408526
Epoch 7: loss 16.036422526726195
Epoch 8: loss 16.024068752535666
Epoch 9: loss 16.01171195967292
Epoch 10: loss 15.99936839327208
Epoch 11: loss 15.987034184703235
Epoch 12: loss 15.974703552802016
Epoch 13: loss 15.962377702236953
Epoch 14: loss 15.950060643729728
Epoch 15: loss 15.937742130596915
Epoch 16: loss 15.925425715132729
Epoch 17: loss 15.913117944967365
Epoch 18: loss 15.900810036340312
Epoch 19: loss 15.888510337920861
Epoch 20: loss 15.876215141839861
Epoch 21: loss 15.863922164257355
Epoch 22: loss 15.851631332053122
Epoch 23: loss 15.83934706770382
Epoch 24: loss 15.827063372758758
Epoch 25: loss 15.814777849808278
Epoch 26: loss 15.802504018832316
Epoch 27: loss 15.79022969157233
Epoch 28: loss 15.777965474089829
Epoch 29: loss 15.765703128692312
Epoch 30: loss 15.753433013364242
Epoch 31: loss 15.741173072117967
Epoch 32: loss 15.728907104417512
Epoch 33: loss 15.716643314247484
Epoch 34: loss 15.704374180078636
Epoch 35: loss 15.69210854686428
Epoch 36: loss 15.679844293080963
Epoch 37: loss 15.667579242235425
Epoch 38: loss 15.65531276373062
Epoch 39: loss 15.643048588771935
Epoch 40: loss 15.63077327257398
Epoch 41: loss 15.618487920237339
Epoch 42: loss 15.606203666259699
Epoch 43: loss 15.593917483152936
Epoch 44: loss 15.581623724584285
Epoch 45: loss 15.569321549411958
Epoch 46: loss 15.557019041827868
Epoch 47: loss 15.54469832004964
Epoch 48: loss 15.532372847013074
Epoch 49: loss 15.520035522278397
-----------Time: 0:04:20.485475, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.940182685852051-------------


Epoch 0: loss 16.123030304714284
Epoch 1: loss 16.110644609141183
Epoch 2: loss 16.098260607260475
Epoch 3: loss 16.085885589313353
Epoch 4: loss 16.073518645705057
Epoch 5: loss 16.061144204422337
Epoch 6: loss 16.0487725463751
Epoch 7: loss 16.03640806085069
Epoch 8: loss 16.024050510338043
Epoch 9: loss 16.011695240035273
Epoch 10: loss 15.999331708185176
Epoch 11: loss 15.986978821808906
Epoch 12: loss 15.974628670958477
Epoch 13: loss 15.962281752955077
Epoch 14: loss 15.949934101675177
Epoch 15: loss 15.937576207341936
Epoch 16: loss 15.925224378875185
Epoch 17: loss 15.912866285406032
Epoch 18: loss 15.900494984662553
Epoch 19: loss 15.888110260914175
Epoch 20: loss 15.875722700516253
Epoch 21: loss 15.86331747925236
Epoch 22: loss 15.85089093592583
Epoch 23: loss 15.838443081945488
Epoch 24: loss 15.825965540119977
Epoch 25: loss 15.8134665512533
Epoch 26: loss 15.800920358878754
Epoch 27: loss 15.788329622290457
Epoch 28: loss 15.775685031366063
Epoch 29: loss 15.76297953441261
Epoch 30: loss 15.750201850173394
Epoch 31: loss 15.737343430324636
Epoch 32: loss 15.724395922566965
Epoch 33: loss 15.711349120147855
Epoch 34: loss 15.698176277662633
Epoch 35: loss 15.684874508677778
Epoch 36: loss 15.671429251379912
Epoch 37: loss 15.657817460973344
Epoch 38: loss 15.644034258628293
Epoch 39: loss 15.630055367914213
Epoch 40: loss 15.615862013011474
Epoch 41: loss 15.601432830889372
Epoch 42: loss 15.586762817739597
Epoch 43: loss 15.571831351586695
Epoch 44: loss 15.556622467850005
Epoch 45: loss 15.541113916203145
Epoch 46: loss 15.52530229577816
Epoch 47: loss 15.509147870145199
Epoch 48: loss 15.49266335911048
Epoch 49: loss 15.475811300231037
-----------Time: 0:05:47.996489, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9342689514160156-------------


Epoch 0: loss 16.12303179927077
Epoch 1: loss 16.110652224015386
Epoch 2: loss 16.098268355410536
Epoch 3: loss 16.08589299364282
Epoch 4: loss 16.073516661606124
Epoch 5: loss 16.061139335445635
Epoch 6: loss 16.048767759334527
Epoch 7: loss 16.036408524982562
Epoch 8: loss 16.02403994939326
Epoch 9: loss 16.011683848320367
Epoch 10: loss 15.999316660459435
Epoch 11: loss 15.986946119440193
Epoch 12: loss 15.974560833029154
Epoch 13: loss 15.962167440645716
Epoch 14: loss 15.949756394656474
Epoch 15: loss 15.937321446135098
Epoch 16: loss 15.924851746323188
Epoch 17: loss 15.912348354686014
Epoch 18: loss 15.899788315624177
Epoch 19: loss 15.88715873145746
Epoch 20: loss 15.874454874782344
Epoch 21: loss 15.861644579963622
Epoch 22: loss 15.848706109035009
Epoch 23: loss 15.835614107431699
Epoch 24: loss 15.822331527577203
Epoch 25: loss 15.808848135234092
Epoch 26: loss 15.79510666793815
Epoch 27: loss 15.781084867065662
Epoch 28: loss 15.766732001187945
Epoch 29: loss 15.752003059366464
Epoch 30: loss 15.736880790415894
Epoch 31: loss 15.721296116474726
Epoch 32: loss 15.705228705974038
Epoch 33: loss 15.688608838527859
Epoch 34: loss 15.67141859501584
Epoch 35: loss 15.653609958273227
Epoch 36: loss 15.635140531020296
Epoch 37: loss 15.615990917212034
Epoch 38: loss 15.596123519055283
Epoch 39: loss 15.575496683437063
Epoch 40: loss 15.554091513189302
Epoch 41: loss 15.531885771987358
Epoch 42: loss 15.508842465668803
Epoch 43: loss 15.484955739949047
Epoch 44: loss 15.460200592381206
Epoch 45: loss 15.434562307132722
Epoch 46: loss 15.408018129814442
Epoch 47: loss 15.38055597069861
Epoch 48: loss 15.352177917082196
Epoch 49: loss 15.322860876459865
-----------Time: 0:06:45.556891, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.9140305519104004-------------


Epoch 0: loss 16.123029564696203
Epoch 1: loss 16.110648903527814
Epoch 2: loss 16.098262154712437
Epoch 3: loss 16.08588070166766
Epoch 4: loss 16.073503183112837
Epoch 5: loss 16.06112354770181
Epoch 6: loss 16.048749906592814
Epoch 7: loss 16.036357612568377
Epoch 8: loss 16.023972273780444
Epoch 9: loss 16.011552676355922
Epoch 10: loss 15.99912847132058
Epoch 11: loss 15.986653614199763
Epoch 12: loss 15.974140481497685
Epoch 13: loss 15.961556732298046
Epoch 14: loss 15.948866665266078
Epoch 15: loss 15.936039865503108
Epoch 16: loss 15.923034291996004
Epoch 17: loss 15.909796765603858
Epoch 18: loss 15.896267969236224
Epoch 19: loss 15.882358079114772
Epoch 20: loss 15.86800444054824
Epoch 21: loss 15.853120748344617
Epoch 22: loss 15.837619123930773
Epoch 23: loss 15.821429959971857
Epoch 24: loss 15.804461778928042
Epoch 25: loss 15.78662909963586
Epoch 26: loss 15.767885203981217
Epoch 27: loss 15.748155443482972
Epoch 28: loss 15.727402484306763
Epoch 29: loss 15.705571800032798
Epoch 30: loss 15.6826180649426
Epoch 31: loss 15.658521230095038
Epoch 32: loss 15.633242652752012
Epoch 33: loss 15.606770777844941
Epoch 34: loss 15.579073711994745
Epoch 35: loss 15.55014580575716
Epoch 36: loss 15.519962357982077
Epoch 37: loss 15.488511738406373
Epoch 38: loss 15.455783609075997
Epoch 39: loss 15.421789172941605
Epoch 40: loss 15.386522300869567
Epoch 41: loss 15.349964899495609
Epoch 42: loss 15.31212602587353
Epoch 43: loss 15.273007378362975
Epoch 44: loss 15.232593541562784
Epoch 45: loss 15.19090690581678
Epoch 46: loss 15.147930774823552
Epoch 47: loss 15.1036909973576
Epoch 48: loss 15.058189323118292
Epoch 49: loss 15.011426228164913
-----------Time: 0:06:33.689548, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.8730852603912354-------------


Epoch 0: loss 16.068086435899843
Epoch 1: loss 15.944889559701709
Epoch 2: loss 15.822207550953237
Epoch 3: loss 15.700097981712752
Epoch 4: loss 15.57857889037472
Epoch 5: loss 15.45753713507183
Epoch 6: loss 15.336934789229243
Epoch 7: loss 15.216798883472336
Epoch 8: loss 15.097078480494936
Epoch 9: loss 14.977749522708564
Epoch 10: loss 14.858716440952751
Epoch 11: loss 14.739911597989837
Epoch 12: loss 14.62106850693056
Epoch 13: loss 14.502073912856499
Epoch 14: loss 14.38268758681496
Epoch 15: loss 14.262574302171352
Epoch 16: loss 14.141309709118525
Epoch 17: loss 14.018371726197351
Epoch 18: loss 13.893194637848282
Epoch 19: loss 13.765150017294953
Epoch 20: loss 13.633495189237362
Epoch 21: loss 13.49744732033759
Epoch 22: loss 13.356272411190862
Epoch 23: loss 13.209250120315428
Epoch 24: loss 13.05576446070627
Epoch 25: loss 12.895335548529488
Epoch 26: loss 12.727542024646652
Epoch 27: loss 12.552054604348312
Epoch 28: loss 12.368715354707852
Epoch 29: loss 12.177486399452993
Epoch 30: loss 11.97840175296769
Epoch 31: loss 11.771462173420428
Epoch 32: loss 11.556780628434597
Epoch 33: loss 11.334543716654691
Epoch 34: loss 11.104961775903666
Epoch 35: loss 10.868378512168333
Epoch 36: loss 10.625028499771293
Epoch 37: loss 10.37522455114849
Epoch 38: loss 10.11939652501532
Epoch 39: loss 9.857976462284853
Epoch 40: loss 9.591423065782955
Epoch 41: loss 9.32015775998952
Epoch 42: loss 9.044700197320454
Epoch 43: loss 8.765690646397154
Epoch 44: loss 8.483607814388472
Epoch 45: loss 8.199093720393053
Epoch 46: loss 7.912749079921571
Epoch 47: loss 7.6252460827187
Epoch 48: loss 7.337237421619691
Epoch 49: loss 7.049340887780161
-----------Time: 0:04:20.993559, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.6349596977233887-------------


Epoch 0: loss 16.06791163035357
Epoch 1: loss 15.94472041863861
Epoch 2: loss 15.821955078253609
Epoch 3: loss 15.699600045482642
Epoch 4: loss 15.577258779018582
Epoch 5: loss 15.454326566890117
Epoch 6: loss 15.329323796100628
Epoch 7: loss 15.199677394744558
Epoch 8: loss 15.061447797487974
Epoch 9: loss 14.90942203849473
Epoch 10: loss 14.738137828065625
Epoch 11: loss 14.542758573974975
Epoch 12: loss 14.320063923415184
Epoch 13: loss 14.068628968720594
Epoch 14: loss 13.788381543348768
Epoch 15: loss 13.479955062845468
Epoch 16: loss 13.14449823544426
Epoch 17: loss 12.783387166511242
Epoch 18: loss 12.398058266922337
Epoch 19: loss 11.990424304503732
Epoch 20: loss 11.562399895830346
Epoch 21: loss 11.116160354386082
Epoch 22: loss 10.65422845833194
Epoch 23: loss 10.178935283808165
Epoch 24: loss 9.69288349099753
Epoch 25: loss 9.198696115212702
Epoch 26: loss 8.698986172222327
Epoch 27: loss 8.196624374182216
Epoch 28: loss 7.694684071149302
Epoch 29: loss 7.1959573095942915
Epoch 30: loss 6.703375133111984
Epoch 31: loss 6.220018027203442
Epoch 32: loss 5.748805551959355
Epoch 33: loss 5.292960026135841
Epoch 34: loss 4.855501423587353
Epoch 35: loss 4.439389749478231
Epoch 36: loss 4.047355953930123
Epoch 37: loss 3.6821690737520503
Epoch 38: loss 3.3460178292270846
Epoch 39: loss 3.0406640558673224
Epoch 40: loss 2.7670845044184276
Epoch 41: loss 2.5258103830131127
Epoch 42: loss 2.3162438826044984
Epoch 43: loss 2.136836957192538
Epoch 44: loss 1.9853324692426912
Epoch 45: loss 1.85859658706441
Epoch 46: loss 1.7533110723085803
Epoch 47: loss 1.6658778416844668
Epoch 48: loss 1.5930123049775955
Epoch 49: loss 1.531750065467486
-----------Time: 0:05:07.907517, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.237715482711792-------------


Epoch 0: loss 16.067738357738932
Epoch 1: loss 15.94437802777853
Epoch 2: loss 15.82053518269359
Epoch 3: loss 15.691951167265833
Epoch 4: loss 15.544958057061299
Epoch 5: loss 15.355482419850452
Epoch 6: loss 15.102766229381634
Epoch 7: loss 14.778882334710723
Epoch 8: loss 14.385463095929975
Epoch 9: loss 13.92783407674917
Epoch 10: loss 13.411733681770562
Epoch 11: loss 12.843606437529605
Epoch 12: loss 12.230091075783129
Epoch 13: loss 11.577844963052987
Epoch 14: loss 10.893852279003449
Epoch 15: loss 10.185070968698456
Epoch 16: loss 9.459264105982985
Epoch 17: loss 8.724258638063548
Epoch 18: loss 7.988150334474897
Epoch 19: loss 7.259788864523642
Epoch 20: loss 6.547947549897733
Epoch 21: loss 5.860504460503317
Epoch 22: loss 5.206625647749701
Epoch 23: loss 4.594783908326448
Epoch 24: loss 4.032672626807549
Epoch 25: loss 3.527289461869141
Epoch 26: loss 3.0837770147515484
Epoch 27: loss 2.7050500235005264
Epoch 28: loss 2.3908059595718925
Epoch 29: loss 2.1372728098858436
Epoch 30: loss 1.9374438570918695
Epoch 31: loss 1.7821136300754392
Epoch 32: loss 1.6617495154738102
Epoch 33: loss 1.5674617944559759
Epoch 34: loss 1.4923914801238216
Epoch 35: loss 1.4312908359038046
Epoch 36: loss 1.380678261214459
Epoch 37: loss 1.338093772862773
Epoch 38: loss 1.3016950627135608
Epoch 39: loss 1.2704234166661315
Epoch 40: loss 1.243403399684236
Epoch 41: loss 1.2198609728214207
Epoch 42: loss 1.1993009293124752
Epoch 43: loss 1.181207137110442
Epoch 44: loss 1.1653635520336094
Epoch 45: loss 1.1513205057839584
Epoch 46: loss 1.1389403803767297
Epoch 47: loss 1.127946901476986
Epoch 48: loss 1.1181543965648217
Epoch 49: loss 1.109511508314165
-----------Time: 0:05:38.316478, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.0618717670440674-------------


Epoch 0: loss 16.067801658066152
Epoch 1: loss 15.944165685113322
Epoch 2: loss 15.815300052615855
Epoch 3: loss 15.649841726571726
Epoch 4: loss 15.386593743988584
Epoch 5: loss 14.99847663414744
Epoch 6: loss 14.497267630775704
Epoch 7: loss 13.898546855692114
Epoch 8: loss 13.215465714193803
Epoch 9: loss 12.460706238386228
Epoch 10: loss 11.646591642876563
Epoch 11: loss 10.786363128736784
Epoch 12: loss 9.894172523253763
Epoch 13: loss 8.984354900497014
Epoch 14: loss 8.071810315263342
Epoch 15: loss 7.171606183116886
Epoch 16: loss 6.300192046256218
Epoch 17: loss 5.473508026886401
Epoch 18: loss 4.7073919498252765
Epoch 19: loss 4.015872068415523
Epoch 20: loss 3.4109604112346124
Epoch 21: loss 2.9007113128981556
Epoch 22: loss 2.487527355543617
Epoch 23: loss 2.1660666899424394
Epoch 24: loss 1.924462086419297
Epoch 25: loss 1.7462699270689208
Epoch 26: loss 1.6145497697277391
Epoch 27: loss 1.5153726716609413
Epoch 28: loss 1.438554205993001
Epoch 29: loss 1.3772464521298142
Epoch 30: loss 1.3273911580500102
Epoch 31: loss 1.2860134357729316
Epoch 32: loss 1.2514452476939668
Epoch 33: loss 1.2222630502478853
Epoch 34: loss 1.1974066212943484
Epoch 35: loss 1.1762850186045628
Epoch 36: loss 1.1582025542668897
Epoch 37: loss 1.1425236803866912
Epoch 38: loss 1.1289877211224326
Epoch 39: loss 1.1172833021015107
Epoch 40: loss 1.1070918223338
Epoch 41: loss 1.0981748505025015
Epoch 42: loss 1.0904064150592956
Epoch 43: loss 1.0835530255526677
Epoch 44: loss 1.0775207354038419
Epoch 45: loss 1.0722323774059808
Epoch 46: loss 1.0675141651906548
Epoch 47: loss 1.0633010461059453
Epoch 48: loss 1.0595959159522812
Epoch 49: loss 1.0562379824006733
-----------Time: 0:05:34.311786, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0370197296142578-------------


Epoch 0: loss 16.067645480024744
Epoch 1: loss 15.94238479815468
Epoch 2: loss 15.787245109977121
Epoch 3: loss 15.506799901679134
Epoch 4: loss 15.045263958858886
Epoch 5: loss 14.425627901866553
Epoch 6: loss 13.679523737162206
Epoch 7: loss 12.829213401686568
Epoch 8: loss 11.894463108699565
Epoch 9: loss 10.89555968014425
Epoch 10: loss 9.854079925864854
Epoch 11: loss 8.792734236351624
Epoch 12: loss 7.7348116775645455
Epoch 13: loss 6.704448696580382
Epoch 14: loss 5.726578066308839
Epoch 15: loss 4.824631786657585
Epoch 16: loss 4.02064784497525
Epoch 17: loss 3.3330028588646066
Epoch 18: loss 2.772566490811197
Epoch 19: loss 2.3392075145938205
Epoch 20: loss 2.020049813132107
Epoch 21: loss 1.7928951291560868
Epoch 22: loss 1.6324850471853367
Epoch 23: loss 1.5167748808147727
Epoch 24: loss 1.4301035789901502
Epoch 25: loss 1.362994890876799
Epoch 26: loss 1.3095894215092703
Epoch 27: loss 1.2662169956655331
Epoch 28: loss 1.2307863799432752
Epoch 29: loss 1.201326857448078
Epoch 30: loss 1.1767862423273971
Epoch 31: loss 1.156236360583894
Epoch 32: loss 1.1388802839207093
Epoch 33: loss 1.1242803875164469
Epoch 34: loss 1.1117384436663367
Epoch 35: loss 1.1010480645300578
Epoch 36: loss 1.0918774818064663
Epoch 37: loss 1.083985323164371
Epoch 38: loss 1.0772022159284973
Epoch 39: loss 1.071265440294443
Epoch 40: loss 1.0660762373619328
Epoch 41: loss 1.0616533140114561
Epoch 42: loss 1.0576976618084848
Epoch 43: loss 1.0542891658214593
Epoch 44: loss 1.0511930748261207
Epoch 45: loss 1.0485356322379784
Epoch 46: loss 1.0461205176324413
Epoch 47: loss 1.0440199680144273
Epoch 48: loss 1.0421581293591473
Epoch 49: loss 1.0404954101483157
-----------Time: 0:07:07.422040, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0294194221496582-------------


Epoch 0: loss 15.52683590623981
Epoch 1: loss 14.34700585396929
Epoch 2: loss 13.029108834175911
Epoch 3: loss 10.877284948692301
Epoch 4: loss 7.88149003764739
Epoch 5: loss 4.950309659554428
Epoch 6: loss 2.8409833650085963
Epoch 7: loss 1.7746968120903732
Epoch 8: loss 1.3764381782719466
Epoch 9: loss 1.2202481685156663
Epoch 10: loss 1.1424329904125332
Epoch 11: loss 1.0989412748340421
Epoch 12: loss 1.0729483602616112
Epoch 13: loss 1.0566471786911298
Epoch 14: loss 1.0462861766369205
Epoch 15: loss 1.039248289654863
Epoch 16: loss 1.0344397359532205
Epoch 17: loss 1.031179179245522
Epoch 18: loss 1.028664410535637
Epoch 19: loss 1.0270054453802602
Epoch 20: loss 1.0256523402441189
Epoch 21: loss 1.0247416967539245
Epoch 22: loss 1.0239432350068458
Epoch 23: loss 1.0233215667295223
Epoch 24: loss 1.0228600936327505
Epoch 25: loss 1.0225386112652892
Epoch 26: loss 1.0221086311042147
Epoch 27: loss 1.0219622700677222
Epoch 28: loss 1.0216756169403163
Epoch 29: loss 1.0213986107225714
Epoch 30: loss 1.0212608628651576
Epoch 31: loss 1.0210887662673398
Epoch 32: loss 1.0209282332623115
Epoch 33: loss 1.0208325346181288
Epoch 34: loss 1.0207175775479207
Epoch 35: loss 1.0205493346383352
Epoch 36: loss 1.0204445447138693
Epoch 37: loss 1.020307605716442
Epoch 38: loss 1.0201194926472446
Epoch 39: loss 1.019994402139196
Epoch 40: loss 1.0200115780003243
Epoch 41: loss 1.0199135820330194
Epoch 42: loss 1.0197087163961471
Epoch 43: loss 1.0196327435510064
Epoch 44: loss 1.0195953978216472
Epoch 45: loss 1.0194348413831453
Epoch 46: loss 1.019378668753721
Epoch 47: loss 1.019247941409459
Epoch 48: loss 1.019258765405517
Epoch 49: loss 1.019067990740983
-----------Time: 0:04:46.981432, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 1.0177572965621948-------------


Epoch 0: loss 15.514417456958267
Epoch 1: loss 12.957510504792603
Epoch 2: loss 7.288686405387244
Epoch 3: loss 3.0692036641298786
Epoch 4: loss 1.594383520558841
Epoch 5: loss 1.2477665017518484
Epoch 6: loss 1.1320070828219482
Epoch 7: loss 1.0817966606384908
Epoch 8: loss 1.0576566376719805
Epoch 9: loss 1.0452880515489584
Epoch 10: loss 1.0382958134274693
Epoch 11: loss 1.0341115290539105
Epoch 12: loss 1.0319897949598353
Epoch 13: loss 1.0301841416392656
Epoch 14: loss 1.029415841365781
Epoch 15: loss 1.0285446282117636
Epoch 16: loss 1.0280414017858293
Epoch 17: loss 1.0277294371360148
Epoch 18: loss 1.0274910724247326
Epoch 19: loss 1.0271649229390392
Epoch 20: loss 1.027118177534511
Epoch 21: loss 1.02663133924198
Epoch 22: loss 1.0264286955716235
Epoch 23: loss 1.0264010650104256
Epoch 24: loss 1.026460124754011
Epoch 25: loss 1.0260930383185967
Epoch 26: loss 1.025833312650943
Epoch 27: loss 1.0258506296639083
Epoch 28: loss 1.0255012212725292
Epoch 29: loss 1.025677937991986
Epoch 30: loss 1.0254616020748706
Epoch 31: loss 1.0252461453829853
Epoch 32: loss 1.0250117967736792
Epoch 33: loss 1.0250205172346363
Epoch 34: loss 1.0247147668309027
Epoch 35: loss 1.024871801117829
Epoch 36: loss 1.024548736456103
Epoch 37: loss 1.0243825610400932
Epoch 38: loss 1.0243661920942646
Epoch 39: loss 1.024226075252027
Epoch 40: loss 1.0240803767054414
Epoch 41: loss 1.0240082218635336
Epoch 42: loss 1.0238289002039433
Epoch 43: loss 1.0237357936649623
Epoch 44: loss 1.0236062361145226
Epoch 45: loss 1.0235245054736613
Epoch 46: loss 1.0233035666129717
Epoch 47: loss 1.0232272226243904
Epoch 48: loss 1.0231535648638428
Epoch 49: loss 1.0229589447111718
-----------Time: 0:04:30.672403, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 1.018186092376709-------------


Epoch 0: loss 15.080463313485955
Epoch 1: loss 7.785958722598401
Epoch 2: loss 2.3395999124219977
Epoch 3: loss 1.3152630570921449
Epoch 4: loss 1.1321697346926902
Epoch 5: loss 1.074271464587684
Epoch 6: loss 1.0520626698388167
Epoch 7: loss 1.04230393897975
Epoch 8: loss 1.037871969660188
Epoch 9: loss 1.0355111008237534
Epoch 10: loss 1.0343525375342355
Epoch 11: loss 1.0334780764618667
Epoch 12: loss 1.032921034334275
Epoch 13: loss 1.0325803167473822
Epoch 14: loss 1.0324585979693994
Epoch 15: loss 1.0321765359229793
Epoch 16: loss 1.0319106110805658
Epoch 17: loss 1.0319632920251716
Epoch 18: loss 1.0315808477513229
Epoch 19: loss 1.0314130202267986
Epoch 20: loss 1.0311579502685488
Epoch 21: loss 1.03088926484625
Epoch 22: loss 1.03106797020224
Epoch 23: loss 1.0306484571872294
Epoch 24: loss 1.0306754163777847
Epoch 25: loss 1.0303254445782817
Epoch 26: loss 1.0304003741314647
Epoch 27: loss 1.0301192172394296
Epoch 28: loss 1.0297659994986732
Epoch 29: loss 1.0297898828108198
Epoch 30: loss 1.0296948661682332
Epoch 31: loss 1.0294597490835993
Epoch 32: loss 1.0293462855975353
Epoch 33: loss 1.0289352826608011
Epoch 34: loss 1.0289612586178036
Epoch 35: loss 1.0290115719712771
Epoch 36: loss 1.02850728844611
Epoch 37: loss 1.0284065054835931
Epoch 38: loss 1.028449363297519
Epoch 39: loss 1.028245826951183
Epoch 40: loss 1.0278851040489587
Epoch 41: loss 1.0280053705997996
Epoch 42: loss 1.0277664422859245
Epoch 43: loss 1.0274913920663893
Epoch 44: loss 1.0276054811905493
Epoch 45: loss 1.0274014881345614
Epoch 46: loss 1.0271947618215354
Epoch 47: loss 1.0270269487849262
Epoch 48: loss 1.026888535066815
Epoch 49: loss 1.0267707141538622
-----------Time: 0:04:49.950183, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 1.0188648700714111-------------


Epoch 0: loss 14.10053658290944
Epoch 1: loss 4.7358612810677325
Epoch 2: loss 1.4957985600548764
Epoch 3: loss 1.152379829576315
Epoch 4: loss 1.0762943711016346
Epoch 5: loss 1.0521012828556204
Epoch 6: loss 1.0435197577650226
Epoch 7: loss 1.0396752238857025
Epoch 8: loss 1.0378848484121244
Epoch 9: loss 1.0368488480566376
Epoch 10: loss 1.036363836278599
Epoch 11: loss 1.0360440561601556
Epoch 12: loss 1.0356889738862836
Epoch 13: loss 1.0354450775139223
Epoch 14: loss 1.0355147934595226
Epoch 15: loss 1.0352116406061131
Epoch 16: loss 1.0349692869860558
Epoch 17: loss 1.0347853660129736
Epoch 18: loss 1.0345003590951996
Epoch 19: loss 1.0343072705777072
Epoch 20: loss 1.0342969915476619
Epoch 21: loss 1.0339042831206204
Epoch 22: loss 1.0339289965546605
Epoch 23: loss 1.0336935122483706
Epoch 24: loss 1.033530898039726
Epoch 25: loss 1.0332011558427443
Epoch 26: loss 1.032966700697023
Epoch 27: loss 1.0329652891138315
Epoch 28: loss 1.0327520298854107
Epoch 29: loss 1.0325903752499654
Epoch 30: loss 1.0322082761904026
Epoch 31: loss 1.0321342583063051
Epoch 32: loss 1.0320262087629566
Epoch 33: loss 1.031942960845186
Epoch 34: loss 1.031586532588884
Epoch 35: loss 1.0315138097040157
Epoch 36: loss 1.0312511730673781
Epoch 37: loss 1.0311918784056493
Epoch 38: loss 1.0308471243623938
Epoch 39: loss 1.0309456777092942
Epoch 40: loss 1.0306035928780648
Epoch 41: loss 1.0305298424207885
Epoch 42: loss 1.0302862785574274
Epoch 43: loss 1.0300881838980027
Epoch 44: loss 1.029995904337005
Epoch 45: loss 1.029791056915364
Epoch 46: loss 1.0297932099427394
Epoch 47: loss 1.0294600541725178
Epoch 48: loss 1.0293034031185007
Epoch 49: loss 1.029236388407433
-----------Time: 0:06:07.266539, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 1.0192461013793945-------------


Epoch 0: loss 13.107312752410989
Epoch 1: loss 3.231717679019594
Epoch 2: loss 1.2846252723126517
Epoch 3: loss 1.1021643199630249
Epoch 4: loss 1.0597143934886697
Epoch 5: loss 1.0469026892583224
Epoch 6: loss 1.0423187195703736
Epoch 7: loss 1.04028438656404
Epoch 8: loss 1.0394899508692506
Epoch 9: loss 1.0388162613498444
Epoch 10: loss 1.0384297123287216
Epoch 11: loss 1.0382712946241481
Epoch 12: loss 1.038131806985265
Epoch 13: loss 1.037940535194009
Epoch 14: loss 1.0375151988473388
Epoch 15: loss 1.0375125294084326
Epoch 16: loss 1.0371852262698158
Epoch 17: loss 1.0372381846563785
Epoch 18: loss 1.0367863030456992
Epoch 19: loss 1.0365035126493183
Epoch 20: loss 1.0367544712807706
Epoch 21: loss 1.0361928321733107
Epoch 22: loss 1.0361757224315271
Epoch 23: loss 1.035694625793818
Epoch 24: loss 1.0356943790454978
Epoch 25: loss 1.0355537905843444
Epoch 26: loss 1.0352660821079236
Epoch 27: loss 1.0351782109894268
Epoch 28: loss 1.0347640179239452
Epoch 29: loss 1.0348230219846708
Epoch 30: loss 1.0343041962550477
Epoch 31: loss 1.034321193626356
Epoch 32: loss 1.0342814274752121
Epoch 33: loss 1.0338493016117485
Epoch 34: loss 1.0339170294069868
Epoch 35: loss 1.0334799685447096
Epoch 36: loss 1.033293281611442
Epoch 37: loss 1.0332564987068011
Epoch 38: loss 1.0330091407598523
Epoch 39: loss 1.033010116441539
Epoch 40: loss 1.03261876258596
Epoch 41: loss 1.032411383538871
Epoch 42: loss 1.0321065905691813
Epoch 43: loss 1.0323570459157374
Epoch 44: loss 1.0319772583430977
Epoch 45: loss 1.0317774568646416
Epoch 46: loss 1.0318459847687766
Epoch 47: loss 1.031220890828745
Epoch 48: loss 1.0313304929450648
Epoch 49: loss 1.031046852072404
-----------Time: 0:07:59.168449, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 1.0199270248413086-------------


Epoch 0: loss 6.864305186621711
Epoch 1: loss 1.083839158184701
Epoch 2: loss 1.0574696959847663
Epoch 3: loss 1.054496454473032
Epoch 4: loss 1.0525992034310034
Epoch 5: loss 1.05090003200949
Epoch 6: loss 1.0485668407956694
Epoch 7: loss 1.0468749079515003
Epoch 8: loss 1.043011066477995
Epoch 9: loss 1.0400827614881216
Epoch 10: loss 1.0369017712443727
Epoch 11: loss 1.0337082102222246
Epoch 12: loss 1.0313015214252108
Epoch 13: loss 1.0293172990387713
Epoch 14: loss 1.0277861306821607
Epoch 15: loss 1.025921831220437
Epoch 16: loss 1.024987909539748
Epoch 17: loss 1.023509663171649
Epoch 18: loss 1.0225543990739343
Epoch 19: loss 1.021171637499313
Epoch 20: loss 1.020082936849589
Epoch 21: loss 1.0181558002201743
Epoch 22: loss 1.0172999015056678
Epoch 23: loss 1.0155122532281882
Epoch 24: loss 1.0144747087267056
Epoch 25: loss 1.0126037887090442
Epoch 26: loss 1.0114115415998877
Epoch 27: loss 1.0098524286800135
Epoch 28: loss 1.0087804288693003
Epoch 29: loss 1.0076570612960305
Epoch 30: loss 1.0062731635058426
Epoch 31: loss 1.0050132483033267
Epoch 32: loss 1.0042841947227799
Epoch 33: loss 1.0031808949865162
Epoch 34: loss 1.0020392194528045
Epoch 35: loss 1.0014561268867133
Epoch 36: loss 1.0005663810637153
Epoch 37: loss 1.000213865905822
Epoch 38: loss 0.9989748025407216
Epoch 39: loss 0.9991505908343765
Epoch 40: loss 0.998205996875597
Epoch 41: loss 0.9980665286835799
Epoch 42: loss 0.9976620773960547
Epoch 43: loss 0.9969193858579166
Epoch 44: loss 0.9967034337571161
Epoch 45: loss 0.9964971996118609
Epoch 46: loss 0.9961455942107776
Epoch 47: loss 0.9960289563155161
Epoch 48: loss 0.9954711816244763
Epoch 49: loss 0.9952782451814773
-----------Time: 0:03:51.257145, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.007472038269043-------------


Epoch 0: loss 4.52778943065329
Epoch 1: loss 1.0845782770418226
Epoch 2: loss 1.0784344045022962
Epoch 3: loss 1.0763498578400894
Epoch 4: loss 1.0728861128525478
Epoch 5: loss 1.0690595965919578
Epoch 6: loss 1.0656034778226258
Epoch 7: loss 1.0613264600954475
Epoch 8: loss 1.056297447157658
Epoch 9: loss 1.0523507865049582
Epoch 10: loss 1.0489118515375582
Epoch 11: loss 1.0457584879008113
Epoch 12: loss 1.0435249097260397
Epoch 13: loss 1.0415177176088144
Epoch 14: loss 1.0388771889001018
Epoch 15: loss 1.0368016347929212
Epoch 16: loss 1.0346501761315114
Epoch 17: loss 1.0319726588675482
Epoch 18: loss 1.0299054178094786
Epoch 19: loss 1.0270812609650766
Epoch 20: loss 1.025759875223389
Epoch 21: loss 1.0235259329708715
Epoch 22: loss 1.0217866049699644
Epoch 23: loss 1.0198126798612128
Epoch 24: loss 1.0177382285534007
Epoch 25: loss 1.0162890153841326
Epoch 26: loss 1.0145901524871506
Epoch 27: loss 1.0130656417651693
Epoch 28: loss 1.011687520828631
Epoch 29: loss 1.0103205050250121
Epoch 30: loss 1.0090143339604123
Epoch 31: loss 1.007719767262198
Epoch 32: loss 1.0069049634383773
Epoch 33: loss 1.0057561071817482
Epoch 34: loss 1.0050342280687101
Epoch 35: loss 1.003748632307089
Epoch 36: loss 1.002950939489098
Epoch 37: loss 1.0020402968091897
Epoch 38: loss 1.0015372869241101
Epoch 39: loss 1.0008596349760785
Epoch 40: loss 0.9998831044985848
Epoch 41: loss 0.999777307096806
Epoch 42: loss 0.9990990825033887
Epoch 43: loss 0.9984979434399711
Epoch 44: loss 0.9978365816131372
Epoch 45: loss 0.9978377059985271
Epoch 46: loss 0.9972687401870076
Epoch 47: loss 0.9971854741836514
Epoch 48: loss 0.996557261257991
Epoch 49: loss 0.996077528688297
-----------Time: 0:04:06.776145, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.007517695426941-------------


Epoch 0: loss 3.5562509142749654
Epoch 1: loss 1.1051137200381977
Epoch 2: loss 1.0999949644479239
Epoch 3: loss 1.0939911932255535
Epoch 4: loss 1.0867783092104137
Epoch 5: loss 1.081155595116929
Epoch 6: loss 1.0749771164253135
Epoch 7: loss 1.0709117482122097
Epoch 8: loss 1.0666491245367788
Epoch 9: loss 1.0627489053989119
Epoch 10: loss 1.058698065226463
Epoch 11: loss 1.0550300285114809
Epoch 12: loss 1.0512701225319656
Epoch 13: loss 1.0481901073533597
Epoch 14: loss 1.0449552057617835
Epoch 15: loss 1.0415105819378034
Epoch 16: loss 1.0386462323104253
Epoch 17: loss 1.0355771864283791
Epoch 18: loss 1.0327350013991166
Epoch 19: loss 1.029620630770938
Epoch 20: loss 1.0275642705585983
Epoch 21: loss 1.0248685652241232
Epoch 22: loss 1.0227695981596656
Epoch 23: loss 1.0203171667033657
Epoch 24: loss 1.0183501406296755
Epoch 25: loss 1.0160094913158033
Epoch 26: loss 1.0146646046845922
Epoch 27: loss 1.0128276193511427
Epoch 28: loss 1.0111352284684527
Epoch 29: loss 1.0095588762582548
Epoch 30: loss 1.0081899142770938
Epoch 31: loss 1.0071221188140214
Epoch 32: loss 1.0057535164054157
Epoch 33: loss 1.005039707709422
Epoch 34: loss 1.0040588106650123
Epoch 35: loss 1.0031666880784442
Epoch 36: loss 1.0022420462478172
Epoch 37: loss 1.0013402532208802
Epoch 38: loss 1.0010139120539094
Epoch 39: loss 1.0001738481122298
Epoch 40: loss 0.9994893133996815
Epoch 41: loss 0.9987893071817665
Epoch 42: loss 0.9982797352434047
Epoch 43: loss 0.9976174442453057
Epoch 44: loss 0.9971104479471324
Epoch 45: loss 0.9964807287644536
Epoch 46: loss 0.996178283452858
Epoch 47: loss 0.9956832285028492
Epoch 48: loss 0.9954657837254253
Epoch 49: loss 0.9949532231926723
-----------Time: 0:05:32.665973, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.0077779293060303-------------


Epoch 0: loss 3.1871256171004028
Epoch 1: loss 1.1232840354640954
Epoch 2: loss 1.118717389887218
Epoch 3: loss 1.1125084208430902
Epoch 4: loss 1.1059438954429046
Epoch 5: loss 1.099351961888847
Epoch 6: loss 1.0923849017546188
Epoch 7: loss 1.0863890312429743
Epoch 8: loss 1.0795546895203998
Epoch 9: loss 1.0750882612291142
Epoch 10: loss 1.0700033245493759
Epoch 11: loss 1.0648578657217165
Epoch 12: loss 1.0591330098483536
Epoch 13: loss 1.0544609267015441
Epoch 14: loss 1.0501404653560085
Epoch 15: loss 1.0464803297406893
Epoch 16: loss 1.0426034954593777
Epoch 17: loss 1.0389687167100767
Epoch 18: loss 1.035465482559588
Epoch 19: loss 1.0320759993198452
Epoch 20: loss 1.0292762042453198
Epoch 21: loss 1.0264844738834251
Epoch 22: loss 1.0237829668681346
Epoch 23: loss 1.0211560587444792
Epoch 24: loss 1.0188492108065514
Epoch 25: loss 1.0165208858144614
Epoch 26: loss 1.0147131060763115
Epoch 27: loss 1.0126368050150019
Epoch 28: loss 1.0112268393813688
Epoch 29: loss 1.009138040175705
Epoch 30: loss 1.0080279307303188
Epoch 31: loss 1.006792980247765
Epoch 32: loss 1.0057809291293014
Epoch 33: loss 1.004554639859586
Epoch 34: loss 1.0035528032075718
Epoch 35: loss 1.0025341676719297
Epoch 36: loss 1.0016040406258226
Epoch 37: loss 1.0008171167368471
Epoch 38: loss 1.0002802187103885
Epoch 39: loss 0.99999235469145
Epoch 40: loss 0.9989449544209641
Epoch 41: loss 0.9986174826780178
Epoch 42: loss 0.998177207799759
Epoch 43: loss 0.9977886276831116
Epoch 44: loss 0.9972378880579618
Epoch 45: loss 0.9967061719731055
Epoch 46: loss 0.9965018678320822
Epoch 47: loss 0.9959009297844377
Epoch 48: loss 0.9958298366765992
Epoch 49: loss 0.9954698940150565
-----------Time: 0:06:52.369528, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.0072970390319824-------------


Epoch 0: loss 2.975541432568922
Epoch 1: loss 1.1389740444902106
Epoch 2: loss 1.1326049791528454
Epoch 3: loss 1.1252545858609282
Epoch 4: loss 1.1171504241283203
Epoch 5: loss 1.108313495176781
Epoch 6: loss 1.1001919346247244
Epoch 7: loss 1.0928023889571703
Epoch 8: loss 1.0858552315884664
Epoch 9: loss 1.0798406949051054
Epoch 10: loss 1.0733986301160234
Epoch 11: loss 1.067994335509306
Epoch 12: loss 1.0627791749970819
Epoch 13: loss 1.057570472432583
Epoch 14: loss 1.0529628324729066
Epoch 15: loss 1.0483239309952401
Epoch 16: loss 1.0440058345506864
Epoch 17: loss 1.0403012706573531
Epoch 18: loss 1.0363327244366558
Epoch 19: loss 1.033127655342521
Epoch 20: loss 1.0293171449547684
Epoch 21: loss 1.0262699940076272
Epoch 22: loss 1.0228600544797266
Epoch 23: loss 1.020809119845811
Epoch 24: loss 1.0177847093833148
Epoch 25: loss 1.0152380415575215
Epoch 26: loss 1.01316266749591
Epoch 27: loss 1.0113782154405293
Epoch 28: loss 1.0096671097365457
Epoch 29: loss 1.0080239818496766
Epoch 30: loss 1.006516228492264
Epoch 31: loss 1.0053437916787828
Epoch 32: loss 1.004301021253369
Epoch 33: loss 1.002860726027984
Epoch 34: loss 1.0020707319974251
Epoch 35: loss 1.0011672321190452
Epoch 36: loss 1.0005284566928538
Epoch 37: loss 0.9996272353389588
Epoch 38: loss 0.9990620742419805
Epoch 39: loss 0.9982115442238144
Epoch 40: loss 0.9978990438128373
Epoch 41: loss 0.9973057876338512
Epoch 42: loss 0.9969611017842731
Epoch 43: loss 0.9962629785739967
Epoch 44: loss 0.9958895386853511
Epoch 45: loss 0.9954704512001834
Epoch 46: loss 0.995267563860594
Epoch 47: loss 0.994970974389336
Epoch 48: loss 0.9945751266222793
Epoch 49: loss 0.9943560640924712
-----------Time: 0:06:31.918993, Loss: regression, n_iter: 50, l2: 0.0001, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.007201910018921-------------


Epoch 0: loss 16.129421035513534
Epoch 1: loss 16.12942705730046
Epoch 2: loss 16.129418370514994
Epoch 3: loss 16.12942189895439
Epoch 4: loss 16.129416307072848
Epoch 5: loss 16.129418499123602
Epoch 6: loss 16.12941866766311
Epoch 7: loss 16.129413487017963
Epoch 8: loss 16.129414744581975
Epoch 9: loss 16.129417508629885
Epoch 10: loss 16.12941370222995
Epoch 11: loss 16.129410798164603
Epoch 12: loss 16.12940498951533
Epoch 13: loss 16.129411036712828
Epoch 14: loss 16.12940614854855
Epoch 15: loss 16.1294044320385
Epoch 16: loss 16.129403179141736
Epoch 17: loss 16.129401906538813
Epoch 18: loss 16.129400832034634
Epoch 19: loss 16.129401704291407
Epoch 20: loss 16.129400445690226
Epoch 21: loss 16.12939396703159
Epoch 22: loss 16.1294007962524
Epoch 23: loss 16.129392653460602
Epoch 24: loss 16.12939464222678
Epoch 25: loss 16.1293855877659
Epoch 26: loss 16.12939237653723
Epoch 27: loss 16.12938915043178
Epoch 28: loss 16.129385667109112
Epoch 29: loss 16.129384126917312
Epoch 30: loss 16.12939140886117
Epoch 31: loss 16.12938896996486
Epoch 32: loss 16.12938257583527
Epoch 33: loss 16.129380440310072
Epoch 34: loss 16.129383235472968
Epoch 35: loss 16.12937553606972
Epoch 36: loss 16.129379479894176
Epoch 37: loss 16.129378546444602
Epoch 38: loss 16.12937532863648
Epoch 39: loss 16.12937572535255
Epoch 40: loss 16.129376871421197
Epoch 41: loss 16.129374292507453
Epoch 42: loss 16.129376191558755
Epoch 43: loss 16.12937181316166
Epoch 44: loss 16.129365384287002
Epoch 45: loss 16.12936760071116
Epoch 46: loss 16.129364450318842
Epoch 47: loss 16.129363913585337
Epoch 48: loss 16.129365982213315
Epoch 49: loss 16.12936679690736
-----------Time: 0:03:46.272614, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017207145690918-------------


Epoch 0: loss 16.129162545694225
Epoch 1: loss 16.129163808444066
Epoch 2: loss 16.12915939789482
Epoch 3: loss 16.12915972200926
Epoch 4: loss 16.129157788212886
Epoch 5: loss 16.12915623972376
Epoch 6: loss 16.129149915602884
Epoch 7: loss 16.129154287776977
Epoch 8: loss 16.129151678785416
Epoch 9: loss 16.12914789572172
Epoch 10: loss 16.129150635396222
Epoch 11: loss 16.12914522242585
Epoch 12: loss 16.129147577311695
Epoch 13: loss 16.12914296244071
Epoch 14: loss 16.129141725620023
Epoch 15: loss 16.12914458145714
Epoch 16: loss 16.12913895949778
Epoch 17: loss 16.129140874106575
Epoch 18: loss 16.129137794760144
Epoch 19: loss 16.129132186802526
Epoch 20: loss 16.129137478424454
Epoch 21: loss 16.12913796329965
Epoch 22: loss 16.12913621360028
Epoch 23: loss 16.129127506071487
Epoch 24: loss 16.12913179371654
Epoch 25: loss 16.12912903174296
Epoch 26: loss 16.129126518170686
Epoch 27: loss 16.129122962246388
Epoch 28: loss 16.129122088433867
Epoch 29: loss 16.129118493097252
Epoch 30: loss 16.129116628272435
Epoch 31: loss 16.129124558963746
Epoch 32: loss 16.129125534418552
Epoch 33: loss 16.129116040717783
Epoch 34: loss 16.12911759024408
Epoch 35: loss 16.129112507092557
Epoch 36: loss 16.129115632592885
Epoch 37: loss 16.129118722829563
Epoch 38: loss 16.129118779873707
Epoch 39: loss 16.129117481860213
Epoch 40: loss 16.129113261630962
Epoch 41: loss 16.129106578650585
Epoch 42: loss 16.129107734053726
Epoch 43: loss 16.12910978245696
Epoch 44: loss 16.12910298486972
Epoch 45: loss 16.129107185911394
Epoch 46: loss 16.12910297501664
Epoch 47: loss 16.12910263949338
Epoch 48: loss 16.12909819523623
Epoch 49: loss 16.129099089273492
-----------Time: 0:04:41.288488, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017183780670166-------------


Epoch 0: loss 16.12918139100399
Epoch 1: loss 16.129179387717482
Epoch 2: loss 16.12917904078539
Epoch 3: loss 16.12917648935655
Epoch 4: loss 16.129180822118332
Epoch 5: loss 16.129175694887245
Epoch 6: loss 16.129174643200724
Epoch 7: loss 16.129170391337908
Epoch 8: loss 16.12917382695093
Epoch 9: loss 16.12917581986577
Epoch 10: loss 16.12917283801296
Epoch 11: loss 16.129168407238975
Epoch 12: loss 16.129169625909256
Epoch 13: loss 16.129164251832613
Epoch 14: loss 16.129163734286685
Epoch 15: loss 16.129165979232912
Epoch 16: loss 16.129157242663467
Epoch 17: loss 16.129161976808565
Epoch 18: loss 16.129155483111017
Epoch 19: loss 16.12915856919903
Epoch 20: loss 16.129159977152142
Epoch 21: loss 16.129158039207105
Epoch 22: loss 16.129154752427432
Epoch 23: loss 16.129155288642355
Epoch 24: loss 16.129157317339434
Epoch 25: loss 16.129151806356855
Epoch 26: loss 16.12915180376394
Epoch 27: loss 16.129147897277466
Epoch 28: loss 16.129151622259858
Epoch 29: loss 16.12914748241099
Epoch 30: loss 16.129146575927734
Epoch 31: loss 16.129148077744386
Epoch 32: loss 16.12914726823617
Epoch 33: loss 16.129143081714822
Epoch 34: loss 16.129139072548895
Epoch 35: loss 16.129142919398312
Epoch 36: loss 16.129136999772253
Epoch 37: loss 16.12914140876575
Epoch 38: loss 16.129132740649275
Epoch 39: loss 16.1291310402153
Epoch 40: loss 16.129135386460238
Epoch 41: loss 16.12913470867213
Epoch 42: loss 16.129130061648993
Epoch 43: loss 16.129126139605027
Epoch 44: loss 16.129127594230614
Epoch 45: loss 16.129125023095618
Epoch 46: loss 16.129129763982295
Epoch 47: loss 16.129119646944645
Epoch 48: loss 16.12912529638891
Epoch 49: loss 16.129124255592632
-----------Time: 0:06:17.244252, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017181873321533-------------


Epoch 0: loss 16.129179959196055
Epoch 1: loss 16.12917590958065
Epoch 2: loss 16.129175157116574
Epoch 3: loss 16.12917418581043
Epoch 4: loss 16.12917620672876
Epoch 5: loss 16.12917036644592
Epoch 6: loss 16.129173816579264
Epoch 7: loss 16.12917175469287
Epoch 8: loss 16.129172561089586
Epoch 9: loss 16.12916604042572
Epoch 10: loss 16.12916719219878
Epoch 11: loss 16.129163893491693
Epoch 12: loss 16.12916036349655
Epoch 13: loss 16.129162946558957
Epoch 14: loss 16.129162566437547
Epoch 15: loss 16.129162888996234
Epoch 16: loss 16.129157203769736
Epoch 17: loss 16.12915778198989
Epoch 18: loss 16.129156664443315
Epoch 19: loss 16.129156962628596
Epoch 20: loss 16.129160933419374
Epoch 21: loss 16.12915238457701
Epoch 22: loss 16.129154034189842
Epoch 23: loss 16.1291484516428
Epoch 24: loss 16.129148952594072
Epoch 25: loss 16.12914630107869
Epoch 26: loss 16.129149555706213
Epoch 27: loss 16.129147230379605
Epoch 28: loss 16.12914778422635
Epoch 29: loss 16.129144304533767
Epoch 30: loss 16.12914055310364
Epoch 31: loss 16.12913888378465
Epoch 32: loss 16.129139389403168
Epoch 33: loss 16.12913832786357
Epoch 34: loss 16.12913680634076
Epoch 35: loss 16.129137115934867
Epoch 36: loss 16.129128486712126
Epoch 37: loss 16.129135938751237
Epoch 38: loss 16.129133831229527
Epoch 39: loss 16.129129114197674
Epoch 40: loss 16.129133836933942
Epoch 41: loss 16.129126432604476
Epoch 42: loss 16.129124975904556
Epoch 43: loss 16.129125623614843
Epoch 44: loss 16.129117552387513
Epoch 45: loss 16.12911594996574
Epoch 46: loss 16.129123695004303
Epoch 47: loss 16.129120586617216
Epoch 48: loss 16.129119392838927
Epoch 49: loss 16.12911940061767
-----------Time: 0:05:36.087355, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129172859274867
Epoch 1: loss 16.129173233691866
Epoch 2: loss 16.129165676898968
Epoch 3: loss 16.12916785339223
Epoch 4: loss 16.129164327027162
Epoch 5: loss 16.129169895572467
Epoch 6: loss 16.129164253906946
Epoch 7: loss 16.129165164538865
Epoch 8: loss 16.129161278277135
Epoch 9: loss 16.129158872051562
Epoch 10: loss 16.12915002087525
Epoch 11: loss 16.129156714745875
Epoch 12: loss 16.129152429175157
Epoch 13: loss 16.12915437178744
Epoch 14: loss 16.129153849055676
Epoch 15: loss 16.129153637992356
Epoch 16: loss 16.129158271532333
Epoch 17: loss 16.129150264609308
Epoch 18: loss 16.129150335136607
Epoch 19: loss 16.129143400124846
Epoch 20: loss 16.129155483111017
Epoch 21: loss 16.129145551207532
Epoch 22: loss 16.129148040924985
Epoch 23: loss 16.12914669727618
Epoch 24: loss 16.129138836075004
Epoch 25: loss 16.129140730977642
Epoch 26: loss 16.129140748609466
Epoch 27: loss 16.129132854218973
Epoch 28: loss 16.129132434685246
Epoch 29: loss 16.12913448931148
Epoch 30: loss 16.1291373052177
Epoch 31: loss 16.129132438833913
Epoch 32: loss 16.12913545854329
Epoch 33: loss 16.12913121964505
Epoch 34: loss 16.129131344623577
Epoch 35: loss 16.129124585930064
Epoch 36: loss 16.12912190796695
Epoch 37: loss 16.1291292728841
Epoch 38: loss 16.129126248507475
Epoch 39: loss 16.129122964320718
Epoch 40: loss 16.129124686535185
Epoch 41: loss 16.129121527845538
Epoch 42: loss 16.12911706284507
Epoch 43: loss 16.129116578488457
Epoch 44: loss 16.12911785005421
Epoch 45: loss 16.129115106749627
Epoch 46: loss 16.129114800267015
Epoch 47: loss 16.129112743047866
Epoch 48: loss 16.129112399227274
Epoch 49: loss 16.129116136655657
-----------Time: 0:06:53.605212, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017181396484375-------------


Epoch 0: loss 16.129345020047342
Epoch 1: loss 16.129328023486323
Epoch 2: loss 16.129320492103997
Epoch 3: loss 16.129306010152412
Epoch 4: loss 16.129299220343917
Epoch 5: loss 16.129286713675352
Epoch 6: loss 16.12927078383976
Epoch 7: loss 16.12925636204382
Epoch 8: loss 16.12924272745702
Epoch 9: loss 16.12923791604304
Epoch 10: loss 16.12922036096802
Epoch 11: loss 16.129211959403257
Epoch 12: loss 16.129198322742123
Epoch 13: loss 16.12918427380743
Epoch 14: loss 16.129173252360857
Epoch 15: loss 16.129158882423223
Epoch 16: loss 16.129144368319487
Epoch 17: loss 16.129133636242283
Epoch 18: loss 16.129124177805167
Epoch 19: loss 16.129108755143847
Epoch 20: loss 16.129100447442624
Epoch 21: loss 16.129089385027985
Epoch 22: loss 16.129075772221675
Epoch 23: loss 16.129056662434532
Epoch 24: loss 16.129052530364408
Epoch 25: loss 16.129033187733455
Epoch 26: loss 16.129024122382326
Epoch 27: loss 16.129013920297048
Epoch 28: loss 16.129001219678404
Epoch 29: loss 16.12898251023741
Epoch 30: loss 16.12897387323592
Epoch 31: loss 16.128963532688957
Epoch 32: loss 16.128957413408404
Epoch 33: loss 16.12893954096053
Epoch 34: loss 16.128924453303892
Epoch 35: loss 16.128910266944672
Epoch 36: loss 16.12889981593951
Epoch 37: loss 16.12888592932132
Epoch 38: loss 16.12887726690926
Epoch 39: loss 16.128863450818375
Epoch 40: loss 16.12884648537234
Epoch 41: loss 16.128837524774998
Epoch 42: loss 16.12882411369751
Epoch 43: loss 16.128812723019774
Epoch 44: loss 16.12880609293487
Epoch 45: loss 16.12878140371218
Epoch 46: loss 16.128770755126855
Epoch 47: loss 16.128767664890177
Epoch 48: loss 16.12874851154205
Epoch 49: loss 16.128741445847346
-----------Time: 0:04:20.694465, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017160415649414-------------


Epoch 0: loss 16.129193737430377
Epoch 1: loss 16.129185173030518
Epoch 2: loss 16.12916981622925
Epoch 3: loss 16.12915991958942
Epoch 4: loss 16.129147379212952
Epoch 5: loss 16.12912996259962
Epoch 6: loss 16.129115575030163
Epoch 7: loss 16.129107239325453
Epoch 8: loss 16.129097233264584
Epoch 9: loss 16.129084829275474
Epoch 10: loss 16.129069234963147
Epoch 11: loss 16.12905931394991
Epoch 12: loss 16.12904494193794
Epoch 13: loss 16.129032641146868
Epoch 14: loss 16.129021800167216
Epoch 15: loss 16.12900898494171
Epoch 16: loss 16.128995959171462
Epoch 17: loss 16.128989380426287
Epoch 18: loss 16.128968198381084
Epoch 19: loss 16.128958228621034
Epoch 20: loss 16.12894794200671
Epoch 21: loss 16.128928861778803
Epoch 22: loss 16.12892532763499
Epoch 23: loss 16.1289099869098
Epoch 24: loss 16.128896659324194
Epoch 25: loss 16.128887200368496
Epoch 26: loss 16.128870709425993
Epoch 27: loss 16.128856921338595
Epoch 28: loss 16.128852250460636
Epoch 29: loss 16.128838432295417
Epoch 30: loss 16.12882163435172
Epoch 31: loss 16.12880509829249
Epoch 32: loss 16.128800062850612
Epoch 33: loss 16.1287891591224
Epoch 34: loss 16.12877951866262
Epoch 35: loss 16.12875987577205
Epoch 36: loss 16.128751447759548
Epoch 37: loss 16.128737206949108
Epoch 38: loss 16.12872273277627
Epoch 39: loss 16.128710610896366
Epoch 40: loss 16.12870150042851
Epoch 41: loss 16.12868682504543
Epoch 42: loss 16.12867179806301
Epoch 43: loss 16.12866146010896
Epoch 44: loss 16.12865036969084
Epoch 45: loss 16.12863167477017
Epoch 46: loss 16.12862081927019
Epoch 47: loss 16.12861451122539
Epoch 48: loss 16.128603200928033
Epoch 49: loss 16.128586938162094
-----------Time: 0:05:41.050082, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017111778259277-------------


Epoch 0: loss 16.129172392031496
Epoch 1: loss 16.129160339123143
Epoch 2: loss 16.1291478226015
Epoch 3: loss 16.129135325786017
Epoch 4: loss 16.12912126855399
Epoch 5: loss 16.129110388680605
Epoch 6: loss 16.129096692900998
Epoch 7: loss 16.129081455373843
Epoch 8: loss 16.129072253116775
Epoch 9: loss 16.12906049424504
Epoch 10: loss 16.129048598467364
Epoch 11: loss 16.12903364875383
Epoch 12: loss 16.129027583405918
Epoch 13: loss 16.129013968006692
Epoch 14: loss 16.128996204979853
Epoch 15: loss 16.128986464952114
Epoch 16: loss 16.1289694497221
Epoch 17: loss 16.128959654724554
Epoch 18: loss 16.128943740446456
Epoch 19: loss 16.12893256446212
Epoch 20: loss 16.128923704469898
Epoch 21: loss 16.128911405753154
Epoch 22: loss 16.128901814558766
Epoch 23: loss 16.128883259136952
Epoch 24: loss 16.12887523924935
Epoch 25: loss 16.128861965596386
Epoch 26: loss 16.128849943284433
Epoch 27: loss 16.128833576801874
Epoch 28: loss 16.12882335034319
Epoch 29: loss 16.128807998209172
Epoch 30: loss 16.128794394737355
Epoch 31: loss 16.12878690069301
Epoch 32: loss 16.12877342790414
Epoch 33: loss 16.128762403346066
Epoch 34: loss 16.128748251731917
Epoch 35: loss 16.128742068665645
Epoch 36: loss 16.12872353606149
Epoch 37: loss 16.128714278316032
Epoch 38: loss 16.128701490056844
Epoch 39: loss 16.12868999099524
Epoch 40: loss 16.12867874240927
Epoch 41: loss 16.128666556743646
Epoch 42: loss 16.12864897573947
Epoch 43: loss 16.12863645766208
Epoch 44: loss 16.128623801641584
Epoch 45: loss 16.128615623586136
Epoch 46: loss 16.128594204029874
Epoch 47: loss 16.1285879504363
Epoch 48: loss 16.128576916543732
Epoch 49: loss 16.12855926397509
-----------Time: 0:04:45.412724, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.01711368560791-------------


Epoch 0: loss 16.129167464454905
Epoch 1: loss 16.12915641604201
Epoch 2: loss 16.12914619684349
Epoch 3: loss 16.129135469952118
Epoch 4: loss 16.129120653514438
Epoch 5: loss 16.129107515730244
Epoch 6: loss 16.12909359125549
Epoch 7: loss 16.129085302741842
Epoch 8: loss 16.129069942310494
Epoch 9: loss 16.12905871706076
Epoch 10: loss 16.129041933637392
Epoch 11: loss 16.12903127364324
Epoch 12: loss 16.12901639030834
Epoch 13: loss 16.12900861052471
Epoch 14: loss 16.12899352131232
Epoch 15: loss 16.128979420519315
Epoch 16: loss 16.128966867178274
Epoch 17: loss 16.128954670622402
Epoch 18: loss 16.128945623421682
Epoch 19: loss 16.128941011143613
Epoch 20: loss 16.12892213108878
Epoch 21: loss 16.128907096327616
Epoch 22: loss 16.128896021466982
Epoch 23: loss 16.128882844789057
Epoch 24: loss 16.12887070735166
Epoch 25: loss 16.128857614684197
Epoch 26: loss 16.128845666011046
Epoch 27: loss 16.12883366651675
Epoch 28: loss 16.128822939625376
Epoch 29: loss 16.12880616657367
Epoch 30: loss 16.128796883417643
Epoch 31: loss 16.128782725580493
Epoch 32: loss 16.12877287094589
Epoch 33: loss 16.128754591410285
Epoch 34: loss 16.128744854494045
Epoch 35: loss 16.128735549038943
Epoch 36: loss 16.12872209180756
Epoch 37: loss 16.12870809836126
Epoch 38: loss 16.128698635775475
Epoch 39: loss 16.128681788047803
Epoch 40: loss 16.12867276885057
Epoch 41: loss 16.1286590289914
Epoch 42: loss 16.128650275308715
Epoch 43: loss 16.128635810988957
Epoch 44: loss 16.128628550825592
Epoch 45: loss 16.12860772245406
Epoch 46: loss 16.128597514145785
Epoch 47: loss 16.128587910505402
Epoch 48: loss 16.12856958274157
Epoch 49: loss 16.12855666172511
-----------Time: 0:05:55.657465, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017111301422119-------------


Epoch 0: loss 16.12916965546849
Epoch 1: loss 16.129154608779917
Epoch 2: loss 16.129145186643616
Epoch 3: loss 16.129135119908526
Epoch 4: loss 16.129121352045868
Epoch 5: loss 16.129105021345545
Epoch 6: loss 16.129093155127105
Epoch 7: loss 16.129082584329247
Epoch 8: loss 16.129067180336914
Epoch 9: loss 16.12905314955263
Epoch 10: loss 16.129041589298218
Epoch 11: loss 16.129031976841922
Epoch 12: loss 16.129017139142334
Epoch 13: loss 16.129006076209112
Epoch 14: loss 16.12899387602316
Epoch 15: loss 16.12897876191878
Epoch 16: loss 16.12896872007568
Epoch 17: loss 16.128964012896905
Epoch 18: loss 16.12894469256502
Epoch 19: loss 16.128935629288225
Epoch 20: loss 16.128920135062437
Epoch 21: loss 16.128906967719008
Epoch 22: loss 16.128893607462665
Epoch 23: loss 16.128880123783546
Epoch 24: loss 16.128869118931632
Epoch 25: loss 16.128854946574158
Epoch 26: loss 16.128848823144942
Epoch 27: loss 16.128837639900443
Epoch 28: loss 16.128819682923524
Epoch 29: loss 16.128805765708936
Epoch 30: loss 16.12879648670157
Epoch 31: loss 16.12878804365016
Epoch 32: loss 16.128768716058115
Epoch 33: loss 16.128760077500875
Epoch 34: loss 16.128740978085393
Epoch 35: loss 16.12872870114914
Epoch 36: loss 16.128715018334113
Epoch 37: loss 16.128708677099993
Epoch 38: loss 16.12869509540867
Epoch 39: loss 16.12868524129265
Epoch 40: loss 16.128671826585084
Epoch 41: loss 16.128657353967995
Epoch 42: loss 16.12864533580471
Epoch 43: loss 16.12863514046101
Epoch 44: loss 16.12862010569985
Epoch 45: loss 16.128613587628895
Epoch 46: loss 16.128598674216175
Epoch 47: loss 16.12858348388008
Epoch 48: loss 16.128575733137104
Epoch 49: loss 16.128563434420364
-----------Time: 0:07:20.617861, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017111778259277-------------


Epoch 0: loss 16.12937878862291
Epoch 1: loss 16.12925668356534
Epoch 2: loss 16.12912981169194
Epoch 3: loss 16.12900661449837
Epoch 4: loss 16.128882210043347
Epoch 5: loss 16.128753858133784
Epoch 6: loss 16.128634279094488
Epoch 7: loss 16.12851205372564
Epoch 8: loss 16.128387846332195
Epoch 9: loss 16.12825584411621
Epoch 10: loss 16.12813596378013
Epoch 11: loss 16.12801185751039
Epoch 12: loss 16.127889574060237
Epoch 13: loss 16.127764077469212
Epoch 14: loss 16.12763903826848
Epoch 15: loss 16.127519783862198
Epoch 16: loss 16.127397224525836
Epoch 17: loss 16.127274996564076
Epoch 18: loss 16.127146766521545
Epoch 19: loss 16.127019189893712
Epoch 20: loss 16.126898984924615
Epoch 21: loss 16.12677493466185
Epoch 22: loss 16.126648793990615
Epoch 23: loss 16.12652399696819
Epoch 24: loss 16.126403764514187
Epoch 25: loss 16.12627750612459
Epoch 26: loss 16.126157171509718
Epoch 27: loss 16.126027970161044
Epoch 28: loss 16.12590383485065
Epoch 29: loss 16.125781315445188
Epoch 30: loss 16.12565402040798
Epoch 31: loss 16.125532391409696
Epoch 32: loss 16.125407354801876
Epoch 33: loss 16.1252902234748
Epoch 34: loss 16.125165175458154
Epoch 35: loss 16.125040095289354
Epoch 36: loss 16.124912802326477
Epoch 37: loss 16.12478759614199
Epoch 38: loss 16.12466815763871
Epoch 39: loss 16.124546455520207
Epoch 40: loss 16.124419486153183
Epoch 41: loss 16.124293672707886
Epoch 42: loss 16.12417380948505
Epoch 43: loss 16.12405186155816
Epoch 44: loss 16.12392617360997
Epoch 45: loss 16.12379887286835
Epoch 46: loss 16.123675774205566
Epoch 47: loss 16.123549710803218
Epoch 48: loss 16.123425587420233
Epoch 49: loss 16.123303546148385
-----------Time: 0:05:00.361528, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016432762145996-------------


Epoch 0: loss 16.12911733250828
Epoch 1: loss 16.128991006702883
Epoch 2: loss 16.128866087294842
Epoch 3: loss 16.12873601420798
Epoch 4: loss 16.128615045365983
Epoch 5: loss 16.128491130971987
Epoch 6: loss 16.128368890045646
Epoch 7: loss 16.128247621462616
Epoch 8: loss 16.128119831697134
Epoch 9: loss 16.127999635025365
Epoch 10: loss 16.127872480524175
Epoch 11: loss 16.127751352477166
Epoch 12: loss 16.1276180610636
Epoch 13: loss 16.127500454714404
Epoch 14: loss 16.127374865297003
Epoch 15: loss 16.127255572515573
Epoch 16: loss 16.127132926057254
Epoch 17: loss 16.12700068166296
Epoch 18: loss 16.126884104701215
Epoch 19: loss 16.126754159704376
Epoch 20: loss 16.126629548853284
Epoch 21: loss 16.126506371884453
Epoch 22: loss 16.12638018142924
Epoch 23: loss 16.126263040767668
Epoch 24: loss 16.12613719361447
Epoch 25: loss 16.12601944621067
Epoch 26: loss 16.125890567420683
Epoch 27: loss 16.125765872559125
Epoch 28: loss 16.125641165251572
Epoch 29: loss 16.125519753021024
Epoch 30: loss 16.125392223584257
Epoch 31: loss 16.125271668052985
Epoch 32: loss 16.12514495279168
Epoch 33: loss 16.12502319051754
Epoch 34: loss 16.124895360302574
Epoch 35: loss 16.1247780018361
Epoch 36: loss 16.12465168432803
Epoch 37: loss 16.124527140374155
Epoch 38: loss 16.124403357181684
Epoch 39: loss 16.124279893436402
Epoch 40: loss 16.124153824848218
Epoch 41: loss 16.124029721689976
Epoch 42: loss 16.123903495971117
Epoch 43: loss 16.123784494633387
Epoch 44: loss 16.123658279804772
Epoch 45: loss 16.123536960400596
Epoch 46: loss 16.123409758189762
Epoch 47: loss 16.123291379670334
Epoch 48: loss 16.123165364496213
Epoch 49: loss 16.123036781298588
-----------Time: 0:03:49.181644, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016424655914307-------------


Epoch 0: loss 16.12911939854334
Epoch 1: loss 16.12899635018312
Epoch 2: loss 16.128872634925035
Epoch 3: loss 16.128747906874157
Epoch 4: loss 16.128619654532553
Epoch 5: loss 16.12849840565568
Epoch 6: loss 16.128374296793023
Epoch 7: loss 16.128254031668284
Epoch 8: loss 16.128128666797366
Epoch 9: loss 16.128001191811823
Epoch 10: loss 16.12787778511068
Epoch 11: loss 16.127756714626393
Epoch 12: loss 16.12763171898664
Epoch 13: loss 16.12750999197615
Epoch 14: loss 16.12738259062941
Epoch 15: loss 16.127257441489057
Epoch 16: loss 16.127137513961916
Epoch 17: loss 16.127015266294
Epoch 18: loss 16.126884702627525
Epoch 19: loss 16.12676354605844
Epoch 20: loss 16.126641154224423
Epoch 21: loss 16.12651522772801
Epoch 22: loss 16.126392023792857
Epoch 23: loss 16.126271331874232
Epoch 24: loss 16.126142220240432
Epoch 25: loss 16.12602310274009
Epoch 26: loss 16.125896508308646
Epoch 27: loss 16.125769418630345
Epoch 28: loss 16.12565051323049
Epoch 29: loss 16.12552577221504
Epoch 30: loss 16.125403380381016
Epoch 31: loss 16.125277933055386
Epoch 32: loss 16.12515415401158
Epoch 33: loss 16.125026903572518
Epoch 34: loss 16.12490516048595
Epoch 35: loss 16.12477867495696
Epoch 36: loss 16.12466015071568
Epoch 37: loss 16.124531617302036
Epoch 38: loss 16.124410322271267
Epoch 39: loss 16.124283254373456
Epoch 40: loss 16.124160132374435
Epoch 41: loss 16.124041350915938
Epoch 42: loss 16.123915445681433
Epoch 43: loss 16.12379075185704
Epoch 44: loss 16.123662028641984
Epoch 45: loss 16.123539085554132
Epoch 46: loss 16.123422244115
Epoch 47: loss 16.12329425158353
Epoch 48: loss 16.123171236931206
Epoch 49: loss 16.123050622800047
-----------Time: 0:05:14.488917, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016421318054199-------------


Epoch 0: loss 16.12911312679936
Epoch 1: loss 16.12898843453072
Epoch 2: loss 16.128861657558023
Epoch 3: loss 16.12873538412952
Epoch 4: loss 16.128613282183448
Epoch 5: loss 16.12849241394657
Epoch 6: loss 16.128366381140623
Epoch 7: loss 16.128244204000005
Epoch 8: loss 16.128117376206166
Epoch 9: loss 16.127995664234586
Epoch 10: loss 16.127870012587216
Epoch 11: loss 16.12775162162179
Epoch 12: loss 16.127620941792706
Epoch 13: loss 16.12750469309406
Epoch 14: loss 16.127376984746125
Epoch 15: loss 16.127251485043598
Epoch 16: loss 16.12712766399456
Epoch 17: loss 16.12700624139235
Epoch 18: loss 16.126880351196753
Epoch 19: loss 16.126758617444683
Epoch 20: loss 16.126629412465928
Epoch 21: loss 16.12650783791887
Epoch 22: loss 16.126381889641966
Epoch 23: loss 16.12626167170829
Epoch 24: loss 16.126136584797912
Epoch 25: loss 16.126016035489638
Epoch 26: loss 16.12588851953603
Epoch 27: loss 16.125763692954365
Epoch 28: loss 16.12564032203546
Epoch 29: loss 16.125517586380845
Epoch 30: loss 16.125394392817352
Epoch 31: loss 16.12527102397278
Epoch 32: loss 16.125142812599236
Epoch 33: loss 16.125021366660786
Epoch 34: loss 16.12489730602636
Epoch 35: loss 16.124771689124053
Epoch 36: loss 16.124652546213138
Epoch 37: loss 16.124520220919884
Epoch 38: loss 16.12439503444155
Epoch 39: loss 16.124277977271856
Epoch 40: loss 16.12415291058622
Epoch 41: loss 16.12403013240779
Epoch 42: loss 16.123906093553853
Epoch 43: loss 16.12377881148122
Epoch 44: loss 16.123658638664274
Epoch 45: loss 16.123532630231733
Epoch 46: loss 16.123410915667236
Epoch 47: loss 16.123286713459624
Epoch 48: loss 16.123161996298993
Epoch 49: loss 16.123038526330713
-----------Time: 0:06:31.153118, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016419410705566-------------


Epoch 0: loss 16.12911648566208
Epoch 1: loss 16.128993532202568
Epoch 2: loss 16.128871592054423
Epoch 3: loss 16.128745391227554
Epoch 4: loss 16.128620430332866
Epoch 5: loss 16.128498118879225
Epoch 6: loss 16.12837019272639
Epoch 7: loss 16.128249437022042
Epoch 8: loss 16.12813052539919
Epoch 9: loss 16.12799817158386
Epoch 10: loss 16.127878873616602
Epoch 11: loss 16.127753381692827
Epoch 12: loss 16.127628907747667
Epoch 13: loss 16.127506234323025
Epoch 14: loss 16.127382228139822
Epoch 15: loss 16.127258176839888
Epoch 16: loss 16.127134380164257
Epoch 17: loss 16.127010015640135
Epoch 18: loss 16.126890191829613
Epoch 19: loss 16.126766536208585
Epoch 20: loss 16.12663754903473
Epoch 21: loss 16.126509651922543
Epoch 22: loss 16.126387372102474
Epoch 23: loss 16.12626816385009
Epoch 24: loss 16.126139159562992
Epoch 25: loss 16.126019278189744
Epoch 26: loss 16.125896540979383
Epoch 27: loss 16.125770550697247
Epoch 28: loss 16.12564139550247
Epoch 29: loss 16.125521473679743
Epoch 30: loss 16.125398893081474
Epoch 31: loss 16.125272113515866
Epoch 32: loss 16.125152103534013
Epoch 33: loss 16.12502663079781
Epoch 34: loss 16.124900649331586
Epoch 35: loss 16.12478326908462
Epoch 36: loss 16.124654390813216
Epoch 37: loss 16.124533261210455
Epoch 38: loss 16.124407919157193
Epoch 39: loss 16.124282446939574
Epoch 40: loss 16.12415381136506
Epoch 41: loss 16.124037653937037
Epoch 42: loss 16.12391323651744
Epoch 43: loss 16.12378962290164
Epoch 44: loss 16.12366200478716
Epoch 45: loss 16.123540776135027
Epoch 46: loss 16.123420354916778
Epoch 47: loss 16.123291123490283
Epoch 48: loss 16.123172052143836
Epoch 49: loss 16.123046026079468
-----------Time: 0:07:31.417766, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016420841217041-------------


Epoch 0: loss 16.128594242405022
Epoch 1: loss 16.12735288618958
Epoch 2: loss 16.126110989091966
Epoch 3: loss 16.124872830459903
Epoch 4: loss 16.123635417550332
Epoch 5: loss 16.122393398067107
Epoch 6: loss 16.121153577376216
Epoch 7: loss 16.11991507233064
Epoch 8: loss 16.118677381979115
Epoch 9: loss 16.1174405073588
Epoch 10: loss 16.11619610394908
Epoch 11: loss 16.11496108533767
Epoch 12: loss 16.11372016006478
Epoch 13: loss 16.11247680211858
Epoch 14: loss 16.111247471845168
Epoch 15: loss 16.110005202404892
Epoch 16: loss 16.108766534524225
Epoch 17: loss 16.10752679213938
Epoch 18: loss 16.106288368511354
Epoch 19: loss 16.10505174955251
Epoch 20: loss 16.103810761531065
Epoch 21: loss 16.10258009642476
Epoch 22: loss 16.101336632169026
Epoch 23: loss 16.100100196270013
Epoch 24: loss 16.09886149727436
Epoch 25: loss 16.09762267485593
Epoch 26: loss 16.096387569641145
Epoch 27: loss 16.095142754995024
Epoch 28: loss 16.09390956127754
Epoch 29: loss 16.092670264874158
Epoch 30: loss 16.091431163458022
Epoch 31: loss 16.09019242712439
Epoch 32: loss 16.08896548077748
Epoch 33: loss 16.08772227892479
Epoch 34: loss 16.08648402020619
Epoch 35: loss 16.085249066417667
Epoch 36: loss 16.084007827920587
Epoch 37: loss 16.08277485356375
Epoch 38: loss 16.08153847833896
Epoch 39: loss 16.08030529966038
Epoch 40: loss 16.079062167297828
Epoch 41: loss 16.077825898901153
Epoch 42: loss 16.07658745401122
Epoch 43: loss 16.075353443525355
Epoch 44: loss 16.07411543735672
Epoch 45: loss 16.0728804529718
Epoch 46: loss 16.071645919235586
Epoch 47: loss 16.070412827160386
Epoch 48: loss 16.069175609756645
Epoch 49: loss 16.067941022606373
-----------Time: 0:03:29.684116, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.0095319747924805-------------


Epoch 0: loss 16.12855801626416
Epoch 1: loss 16.12731551709157
Epoch 2: loss 16.12607494186227
Epoch 3: loss 16.124842735008418
Epoch 4: loss 16.123601712241907
Epoch 5: loss 16.122360698809892
Epoch 6: loss 16.121125790656684
Epoch 7: loss 16.11988669131488
Epoch 8: loss 16.11864166404969
Epoch 9: loss 16.117402572486633
Epoch 10: loss 16.116164943327913
Epoch 11: loss 16.114932446067524
Epoch 12: loss 16.113694940850163
Epoch 13: loss 16.112451154554325
Epoch 14: loss 16.111213142681276
Epoch 15: loss 16.10997294083181
Epoch 16: loss 16.1087345135737
Epoch 17: loss 16.107496114837662
Epoch 18: loss 16.106262850592874
Epoch 19: loss 16.10502459135569
Epoch 20: loss 16.103788032552483
Epoch 21: loss 16.10255123727538
Epoch 22: loss 16.101311283308632
Epoch 23: loss 16.100075713961978
Epoch 24: loss 16.098838573308534
Epoch 25: loss 16.097596689175496
Epoch 26: loss 16.096360056214905
Epoch 27: loss 16.095120841747654
Epoch 28: loss 16.093890984075234
Epoch 29: loss 16.092651483350114
Epoch 30: loss 16.091413464735485
Epoch 31: loss 16.090176396683677
Epoch 32: loss 16.088942387234976
Epoch 33: loss 16.087704330763785
Epoch 34: loss 16.086464710245966
Epoch 35: loss 16.08523060589656
Epoch 36: loss 16.083993158760507
Epoch 37: loss 16.082758876018513
Epoch 38: loss 16.08152692845621
Epoch 39: loss 16.080285563424855
Epoch 40: loss 16.079053141877598
Epoch 41: loss 16.07781213000133
Epoch 42: loss 16.07658576291174
Epoch 43: loss 16.075343501768792
Epoch 44: loss 16.07410810770322
Epoch 45: loss 16.072877769304267
Epoch 46: loss 16.071637390617965
Epoch 47: loss 16.070401553682427
Epoch 48: loss 16.069169068349453
Epoch 49: loss 16.067929107641124
-----------Time: 0:04:29.154834, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.009523868560791-------------


Epoch 0: loss 16.12854605099635
Epoch 1: loss 16.127321322110763
Epoch 2: loss 16.126071242290454
Epoch 3: loss 16.124831264987467
Epoch 4: loss 16.123590578262803
Epoch 5: loss 16.12235747426019
Epoch 6: loss 16.121115450109716
Epoch 7: loss 16.11987231878433
Epoch 8: loss 16.118636429471902
Epoch 9: loss 16.11740038095446
Epoch 10: loss 16.116161686107475
Epoch 11: loss 16.114923578815137
Epoch 12: loss 16.11368426063127
Epoch 13: loss 16.11244650338252
Epoch 14: loss 16.111208286150568
Epoch 15: loss 16.109972497961845
Epoch 16: loss 16.10873331772108
Epoch 17: loss 16.107492697893633
Epoch 18: loss 16.106257731140534
Epoch 19: loss 16.105019285213434
Epoch 20: loss 16.103783222175668
Epoch 21: loss 16.102546455939223
Epoch 22: loss 16.1013045806221
Epoch 23: loss 16.100074466251044
Epoch 24: loss 16.098834624816828
Epoch 25: loss 16.09759600516439
Epoch 26: loss 16.0963591175795
Epoch 27: loss 16.095126087215686
Epoch 28: loss 16.093892458925563
Epoch 29: loss 16.09265054523329
Epoch 30: loss 16.091414176750078
Epoch 31: loss 16.090183320805195
Epoch 32: loss 16.08894228248119
Epoch 33: loss 16.08770730380068
Epoch 34: loss 16.08647426669529
Epoch 35: loss 16.085239953875476
Epoch 36: loss 16.083999190919098
Epoch 37: loss 16.082761352252806
Epoch 38: loss 16.081529425952407
Epoch 39: loss 16.080287214074854
Epoch 40: loss 16.079058565738215
Epoch 41: loss 16.07781822024123
Epoch 42: loss 16.076583523669925
Epoch 43: loss 16.075347419664094
Epoch 44: loss 16.0741139806568
Epoch 45: loss 16.0728775136428
Epoch 46: loss 16.071642243000007
Epoch 47: loss 16.070409669507903
Epoch 48: loss 16.069171963598887
Epoch 49: loss 16.067938975240303
-----------Time: 0:06:01.779191, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.009514808654785-------------


Epoch 0: loss 16.128559610388603
Epoch 1: loss 16.12731929237652
Epoch 2: loss 16.126075854568523
Epoch 3: loss 16.124839688851303
Epoch 4: loss 16.123602939209515
Epoch 5: loss 16.12236153372868
Epoch 6: loss 16.12112159480084
Epoch 7: loss 16.11988302493238
Epoch 8: loss 16.118647619976564
Epoch 9: loss 16.117413352792063
Epoch 10: loss 16.11617311930903
Epoch 11: loss 16.114929868190945
Epoch 12: loss 16.11369617404077
Epoch 13: loss 16.112459470552878
Epoch 14: loss 16.111212898947223
Epoch 15: loss 16.109976134785107
Epoch 16: loss 16.108740769241606
Epoch 17: loss 16.10750506091468
Epoch 18: loss 16.106265917493314
Epoch 19: loss 16.1050298834962
Epoch 20: loss 16.10379300783872
Epoch 21: loss 16.10255601342571
Epoch 22: loss 16.101323053589198
Epoch 23: loss 16.100085390722576
Epoch 24: loss 16.098845943411515
Epoch 25: loss 16.097608611400908
Epoch 26: loss 16.096372274032685
Epoch 27: loss 16.095133577111365
Epoch 28: loss 16.09390096783703
Epoch 29: loss 16.09266311828049
Epoch 30: loss 16.09143029068267
Epoch 31: loss 16.090189021070604
Epoch 32: loss 16.08895245241432
Epoch 33: loss 16.0877173420137
Epoch 34: loss 16.08648522591189
Epoch 35: loss 16.085251000214036
Epoch 36: loss 16.084012818764318
Epoch 37: loss 16.082781886069135
Epoch 38: loss 16.08154019121893
Epoch 39: loss 16.080305160161526
Epoch 40: loss 16.07907291389536
Epoch 41: loss 16.077836927089308
Epoch 42: loss 16.07660148012826
Epoch 43: loss 16.075369407068848
Epoch 44: loss 16.07412690063609
Epoch 45: loss 16.07289914218805
Epoch 46: loss 16.0716630817432
Epoch 47: loss 16.070427993123072
Epoch 48: loss 16.06918660319973
Epoch 49: loss 16.067960774917974
-----------Time: 0:06:21.241146, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.0095133781433105-------------


Epoch 0: loss 16.128562298204795
Epoch 1: loss 16.127320483043313
Epoch 2: loss 16.126082616892116
Epoch 3: loss 16.124835441655733
Epoch 4: loss 16.12359603583132
Epoch 5: loss 16.12236140148999
Epoch 6: loss 16.121120662907014
Epoch 7: loss 16.119888517245965
Epoch 8: loss 16.11864342360214
Epoch 9: loss 16.11740691613866
Epoch 10: loss 16.116171029419146
Epoch 11: loss 16.114933795939326
Epoch 12: loss 16.11369275709674
Epoch 13: loss 16.1124567791066
Epoch 14: loss 16.11121832280784
Epoch 15: loss 16.109982769018675
Epoch 16: loss 16.10874489871881
Epoch 17: loss 16.107509165499895
Epoch 18: loss 16.10627004697052
Epoch 19: loss 16.105031481769306
Epoch 20: loss 16.10379575321764
Epoch 21: loss 16.102557088967053
Epoch 22: loss 16.10131807467288
Epoch 23: loss 16.100086315874822
Epoch 24: loss 16.098845106936977
Epoch 25: loss 16.097607587717874
Epoch 26: loss 16.09636621127769
Epoch 27: loss 16.095139884637582
Epoch 28: loss 16.093902552626975
Epoch 29: loss 16.092667764785045
Epoch 30: loss 16.09143435533699
Epoch 31: loss 16.090193477773745
Epoch 32: loss 16.088956414389184
Epoch 33: loss 16.087720952389226
Epoch 34: loss 16.086491366454347
Epoch 35: loss 16.085251518278554
Epoch 36: loss 16.084016918682288
Epoch 37: loss 16.08278919757223
Epoch 38: loss 16.081551748880425
Epoch 39: loss 16.08030986630314
Epoch 40: loss 16.079079546054594
Epoch 41: loss 16.077841118277902
Epoch 42: loss 16.07660833268531
Epoch 43: loss 16.075377017794388
Epoch 44: loss 16.074140246890693
Epoch 45: loss 16.07290584020734
Epoch 46: loss 16.071674641479028
Epoch 47: loss 16.070439727621405
Epoch 48: loss 16.06920184850563
Epoch 49: loss 16.067967082962774
-----------Time: 0:06:37.525855, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.009511470794678-------------


Epoch 0: loss 16.12302306477566
Epoch 1: loss 16.110643966616724
Epoch 2: loss 16.098253777002498
Epoch 3: loss 16.085887161657304
Epoch 4: loss 16.073520956511338
Epoch 5: loss 16.061162023974735
Epoch 6: loss 16.048814692142024
Epoch 7: loss 16.03647482635017
Epoch 8: loss 16.02414310490586
Epoch 9: loss 16.011825420468835
Epoch 10: loss 15.999510124106974
Epoch 11: loss 15.987198981077144
Epoch 12: loss 15.97490518568909
Epoch 13: loss 15.96261464337281
Epoch 14: loss 15.950325903651375
Epoch 15: loss 15.938053445364872
Epoch 16: loss 15.92578486296844
Epoch 17: loss 15.91353046433831
Epoch 18: loss 15.901281017658178
Epoch 19: loss 15.889032864842873
Epoch 20: loss 15.876800252925838
Epoch 21: loss 15.864571546458109
Epoch 22: loss 15.852355399035837
Epoch 23: loss 15.840138094654675
Epoch 24: loss 15.827932289335116
Epoch 25: loss 15.815731990330889
Epoch 26: loss 15.803544574486555
Epoch 27: loss 15.791364741882855
Epoch 28: loss 15.779192689062786
Epoch 29: loss 15.767024062002658
Epoch 30: loss 15.754856741771937
Epoch 31: loss 15.742707565726633
Epoch 32: loss 15.730560891844775
Epoch 33: loss 15.71841928140828
Epoch 34: loss 15.706283692758715
Epoch 35: loss 15.694162253648967
Epoch 36: loss 15.682043744015136
Epoch 37: loss 15.669939729297466
Epoch 38: loss 15.657828328400736
Epoch 39: loss 15.645735181629043
Epoch 40: loss 15.633651815334566
Epoch 41: loss 15.621575848183728
Epoch 42: loss 15.60950647429839
Epoch 43: loss 15.597442838535029
Epoch 44: loss 15.585383689552627
Epoch 45: loss 15.573337450696439
Epoch 46: loss 15.56129610882044
Epoch 47: loss 15.549253518196343
Epoch 48: loss 15.537231767352603
Epoch 49: loss 15.52521211521466
-----------Time: 0:04:29.855500, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9408061504364014-------------


Epoch 0: loss 16.12303460999116
Epoch 1: loss 16.110653656860485
Epoch 2: loss 16.098281150929886
Epoch 3: loss 16.085909907749127
Epoch 4: loss 16.073551966743405
Epoch 5: loss 16.061203689015127
Epoch 6: loss 16.048857826846913
Epoch 7: loss 16.03652737696836
Epoch 8: loss 16.02420017238283
Epoch 9: loss 16.01188178370996
Epoch 10: loss 15.99957514301858
Epoch 11: loss 15.987276123942987
Epoch 12: loss 15.974991475323609
Epoch 13: loss 15.962720893270752
Epoch 14: loss 15.950444883727197
Epoch 15: loss 15.938168860181898
Epoch 16: loss 15.925908509773556
Epoch 17: loss 15.91366073811683
Epoch 18: loss 15.90142317165688
Epoch 19: loss 15.88918140467384
Epoch 20: loss 15.876948854986775
Epoch 21: loss 15.8647337784386
Epoch 22: loss 15.852514995577026
Epoch 23: loss 15.840308112123209
Epoch 24: loss 15.828109030752094
Epoch 25: loss 15.81591503045817
Epoch 26: loss 15.803720114346495
Epoch 27: loss 15.79153938511262
Epoch 28: loss 15.779362647931428
Epoch 29: loss 15.767195617070076
Epoch 30: loss 15.755036293909303
Epoch 31: loss 15.742869555010232
Epoch 32: loss 15.730705754402994
Epoch 33: loss 15.71855431318672
Epoch 34: loss 15.706407932304312
Epoch 35: loss 15.694258899906526
Epoch 36: loss 15.682107649528831
Epoch 37: loss 15.669961029579614
Epoch 38: loss 15.657822295723562
Epoch 39: loss 15.645671068682107
Epoch 40: loss 15.633520508019933
Epoch 41: loss 15.621355479407919
Epoch 42: loss 15.609184588213989
Epoch 43: loss 15.597003474710037
Epoch 44: loss 15.58480820959194
Epoch 45: loss 15.572597001155088
Epoch 46: loss 15.560363607733324
Epoch 47: loss 15.548116512827539
Epoch 48: loss 15.535839631545796
Epoch 49: loss 15.523520276234033
-----------Time: 0:05:17.010710, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.940455198287964-------------


Epoch 0: loss 16.123028355879004
Epoch 1: loss 16.1106581695706
Epoch 2: loss 16.09828424738956
Epoch 3: loss 16.085921259533134
Epoch 4: loss 16.073562777126657
Epoch 5: loss 16.06121838721586
Epoch 6: loss 16.04888013473602
Epoch 7: loss 16.03655780172011
Epoch 8: loss 16.02424018832897
Epoch 9: loss 16.01193577391483
Epoch 10: loss 15.999626823972918
Epoch 11: loss 15.987329919160612
Epoch 12: loss 15.975050355248506
Epoch 13: loss 15.962765797899753
Epoch 14: loss 15.950488887058775
Epoch 15: loss 15.938215201804663
Epoch 16: loss 15.92593939347135
Epoch 17: loss 15.91366777477088
Epoch 18: loss 15.901386562283108
Epoch 19: loss 15.8891032246418
Epoch 20: loss 15.876802257768093
Epoch 21: loss 15.864482034866805
Epoch 22: loss 15.852125787034899
Epoch 23: loss 15.839735337091957
Epoch 24: loss 15.827291162458692
Epoch 25: loss 15.814765104072906
Epoch 26: loss 15.802142569524818
Epoch 27: loss 15.789395238472885
Epoch 28: loss 15.776485410444499
Epoch 29: loss 15.763381809174463
Epoch 30: loss 15.750055424087135
Epoch 31: loss 15.73645345149577
Epoch 32: loss 15.72256101169555
Epoch 33: loss 15.708339809399055
Epoch 34: loss 15.693766468954578
Epoch 35: loss 15.678832880241059
Epoch 36: loss 15.663525824056753
Epoch 37: loss 15.647822511786025
Epoch 38: loss 15.631730212408153
Epoch 39: loss 15.615236455036804
Epoch 40: loss 15.598365154650109
Epoch 41: loss 15.581102470783774
Epoch 42: loss 15.563446135673917
Epoch 43: loss 15.545408856680757
Epoch 44: loss 15.526993734412637
Epoch 45: loss 15.508205051328774
Epoch 46: loss 15.489037921857808
Epoch 47: loss 15.469496299677274
Epoch 48: loss 15.449591424038644
Epoch 49: loss 15.429306871415221
-----------Time: 0:05:17.716003, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9280107021331787-------------


Epoch 0: loss 16.123025041614426
Epoch 1: loss 16.110648987538273
Epoch 2: loss 16.098271130348692
Epoch 3: loss 16.085909679053984
Epoch 4: loss 16.0735564893066
Epoch 5: loss 16.061213139673495
Epoch 6: loss 16.048880756517153
Epoch 7: loss 16.03655246601862
Epoch 8: loss 16.02423068425654
Epoch 9: loss 16.01190823196652
Epoch 10: loss 15.999582634470007
Epoch 11: loss 15.987250818124991
Epoch 12: loss 15.974914525370679
Epoch 13: loss 15.962548915039527
Epoch 14: loss 15.950141726789946
Epoch 15: loss 15.937667427664545
Epoch 16: loss 15.925088242514747
Epoch 17: loss 15.912356327901138
Epoch 18: loss 15.89940928790542
Epoch 19: loss 15.886182492553313
Epoch 20: loss 15.872598399151402
Epoch 21: loss 15.858580294739234
Epoch 22: loss 15.844068772511484
Epoch 23: loss 15.829015149961288
Epoch 24: loss 15.813382145631177
Epoch 25: loss 15.797150635732264
Epoch 26: loss 15.780322003325669
Epoch 27: loss 15.762874178551927
Epoch 28: loss 15.74481456988917
Epoch 29: loss 15.726148271379165
Epoch 30: loss 15.706892233429038
Epoch 31: loss 15.687041585506337
Epoch 32: loss 15.666614664709392
Epoch 33: loss 15.645610902152546
Epoch 34: loss 15.624037676754693
Epoch 35: loss 15.601895293442693
Epoch 36: loss 15.579181528532226
Epoch 37: loss 15.555915334679758
Epoch 38: loss 15.532089216285195
Epoch 39: loss 15.507714026774188
Epoch 40: loss 15.482782906329522
Epoch 41: loss 15.457318602080187
Epoch 42: loss 15.431301807696045
Epoch 43: loss 15.404740075302746
Epoch 44: loss 15.377634389689593
Epoch 45: loss 15.350003374212783
Epoch 46: loss 15.32183615885201
Epoch 47: loss 15.293145648029073
Epoch 48: loss 15.263923861268683
Epoch 49: loss 15.234193999460043
-----------Time: 0:05:47.992791, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.902677536010742-------------


Epoch 0: loss 16.123026092782364
Epoch 1: loss 16.110654395322815
Epoch 2: loss 16.09828797029762
Epoch 3: loss 16.08592766092289
Epoch 4: loss 16.0735826601212
Epoch 5: loss 16.061241796575466
Epoch 6: loss 16.048911385072063
Epoch 7: loss 16.0365819111669
Epoch 8: loss 16.02424618003808
Epoch 9: loss 16.01190065183738
Epoch 10: loss 15.999526110468123
Epoch 11: loss 15.987110879513283
Epoch 12: loss 15.974594575675562
Epoch 13: loss 15.961896924365792
Epoch 14: loss 15.948926189789765
Epoch 15: loss 15.935542174562803
Epoch 16: loss 15.921598442226989
Epoch 17: loss 15.906960231185673
Epoch 18: loss 15.891520121617445
Epoch 19: loss 15.875230336461525
Epoch 20: loss 15.858072650114954
Epoch 21: loss 15.84005597079818
Epoch 22: loss 15.82119858686272
Epoch 23: loss 15.801528591510717
Epoch 24: loss 15.781063520629617
Epoch 25: loss 15.759795793053117
Epoch 26: loss 15.737739468606158
Epoch 27: loss 15.71491046052967
Epoch 28: loss 15.691330611543723
Epoch 29: loss 15.666979098462617
Epoch 30: loss 15.641862567447333
Epoch 31: loss 15.615997873485703
Epoch 32: loss 15.58938530750285
Epoch 33: loss 15.562036217134112
Epoch 34: loss 15.53394010573901
Epoch 35: loss 15.505111838502039
Epoch 36: loss 15.47556144430172
Epoch 37: loss 15.445272683708353
Epoch 38: loss 15.414271207397693
Epoch 39: loss 15.38255161277103
Epoch 40: loss 15.350119845902162
Epoch 41: loss 15.316972220702429
Epoch 42: loss 15.283124868217664
Epoch 43: loss 15.248585732622338
Epoch 44: loss 15.213368108831325
Epoch 45: loss 15.177458163640498
Epoch 46: loss 15.140867152895988
Epoch 47: loss 15.103582671052934
Epoch 48: loss 15.065629103444333
Epoch 49: loss 15.027015815680931
-----------Time: 0:07:15.973075, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.8756723403930664-------------


Epoch 0: loss 16.068030154076265
Epoch 1: loss 15.945036936350787
Epoch 2: loss 15.822761055955166
Epoch 3: loss 15.701256978634973
Epoch 4: loss 15.580564584986162
Epoch 5: loss 15.460578732804283
Epoch 6: loss 15.341227938002254
Epoch 7: loss 15.222464731039075
Epoch 8: loss 15.104230406752354
Epoch 9: loss 14.986327793625401
Epoch 10: loss 14.868529526159257
Epoch 11: loss 14.750449978702415
Epoch 12: loss 14.631540041244698
Epoch 13: loss 14.511018289438468
Epoch 14: loss 14.387909344190357
Epoch 15: loss 14.261147777563597
Epoch 16: loss 14.129641860642467
Epoch 17: loss 13.99242381053881
Epoch 18: loss 13.848770420080687
Epoch 19: loss 13.698499057784296
Epoch 20: loss 13.54190865309748
Epoch 21: loss 13.37952523026666
Epoch 22: loss 13.211755205976893
Epoch 23: loss 13.038894452628654
Epoch 24: loss 12.861409501059669
Epoch 25: loss 12.679401712485019
Epoch 26: loss 12.493168450750172
Epoch 27: loss 12.30309247633502
Epoch 28: loss 12.109355060998482
Epoch 29: loss 11.912300250399301
Epoch 30: loss 11.71220069686118
Epoch 31: loss 11.509220110326439
Epoch 32: loss 11.303689123820584
Epoch 33: loss 11.095957988367708
Epoch 34: loss 10.886316394857767
Epoch 35: loss 10.675046702971466
Epoch 36: loss 10.462367180704486
Epoch 37: loss 10.248630432411016
Epoch 38: loss 10.034103509717301
Epoch 39: loss 9.819069318890637
Epoch 40: loss 9.603817132843002
Epoch 41: loss 9.388663206883525
Epoch 42: loss 9.173913347909087
Epoch 43: loss 8.959867428670181
Epoch 44: loss 8.746835544745906
Epoch 45: loss 8.535013067132432
Epoch 46: loss 8.324806936802327
Epoch 47: loss 8.116471151867401
Epoch 48: loss 7.910270396103478
Epoch 49: loss 7.706389948879655
-----------Time: 0:05:24.124119, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 20, rmse: 2.7646713256835938-------------


Epoch 0: loss 16.067809650468853
Epoch 1: loss 15.944897328076511
Epoch 2: loss 15.822818046681839
Epoch 3: loss 15.70144104451962
Epoch 4: loss 15.58028149306612
Epoch 5: loss 15.457956621088108
Epoch 6: loss 15.329864413793999
Epoch 7: loss 15.187153526335193
Epoch 8: loss 15.022643564316551
Epoch 9: loss 14.835885503228557
Epoch 10: loss 14.629585982795122
Epoch 11: loss 14.405743930312587
Epoch 12: loss 14.165955436689948
Epoch 13: loss 13.911370337041841
Epoch 14: loss 13.643078206089802
Epoch 15: loss 13.361959627492716
Epoch 16: loss 13.069103732583573
Epoch 17: loss 12.76574740998443
Epoch 18: loss 12.452797363866212
Epoch 19: loss 12.131393750508627
Epoch 20: loss 11.802620042465898
Epoch 21: loss 11.467714656624475
Epoch 22: loss 11.127878994965048
Epoch 23: loss 10.784124266524364
Epoch 24: loss 10.437715358225919
Epoch 25: loss 10.089848796332122
Epoch 26: loss 9.741852795060009
Epoch 27: loss 9.394903042447378
Epoch 28: loss 9.049987087177154
Epoch 29: loss 8.708538599672883
Epoch 30: loss 8.371819402809827
Epoch 31: loss 8.04073830231692
Epoch 32: loss 7.716354937187805
Epoch 33: loss 7.399711729457289
Epoch 34: loss 7.091867996856789
Epoch 35: loss 6.793804888753803
Epoch 36: loss 6.506481411276854
Epoch 37: loss 6.230384594075637
Epoch 38: loss 5.966401016407521
Epoch 39: loss 5.715058539858845
Epoch 40: loss 5.476826960525803
Epoch 41: loss 5.251920013728513
Epoch 42: loss 5.040670502762746
Epoch 43: loss 4.843341263952043
Epoch 44: loss 4.659509345070703
Epoch 45: loss 4.489066208880384
Epoch 46: loss 4.332011966487517
Epoch 47: loss 4.187430813624977
Epoch 48: loss 4.055297087819243
Epoch 49: loss 3.934416348938063
-----------Time: 0:03:49.064423, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 50, rmse: 1.9816765785217285-------------


Epoch 0: loss 16.06783758498457
Epoch 1: loss 15.945002453686298
Epoch 2: loss 15.822352635983606
Epoch 3: loss 15.693287366106823
Epoch 4: loss 15.529139174398615
Epoch 5: loss 15.307003482260608
Epoch 6: loss 15.03906951642931
Epoch 7: loss 14.734347612070868
Epoch 8: loss 14.396649596093464
Epoch 9: loss 14.028810608445333
Epoch 10: loss 13.633774740790594
Epoch 11: loss 13.214678870652797
Epoch 12: loss 12.774510321894569
Epoch 13: loss 12.316455670450614
Epoch 14: loss 11.84381976602128
Epoch 15: loss 11.359868123260382
Epoch 16: loss 10.868130317260675
Epoch 17: loss 10.37221392268004
Epoch 18: loss 9.87558264885343
Epoch 19: loss 9.381443026274557
Epoch 20: loss 8.893335086227177
Epoch 21: loss 8.414271544476707
Epoch 22: loss 7.947806141310376
Epoch 23: loss 7.496971458374903
Epoch 24: loss 7.064524785664627
Epoch 25: loss 6.652884259568796
Epoch 26: loss 6.264271242453911
Epoch 27: loss 5.900667252522438
Epoch 28: loss 5.563400238995451
Epoch 29: loss 5.25330060950565
Epoch 30: loss 4.970405902468426
Epoch 31: loss 4.715003235823697
Epoch 32: loss 4.4860048237023245
Epoch 33: loss 4.282217033016999
Epoch 34: loss 4.102554475308242
Epoch 35: loss 3.9444989858599318
Epoch 36: loss 3.8065795803796085
Epoch 37: loss 3.6862463905734817
Epoch 38: loss 3.581385161709435
Epoch 39: loss 3.4902794429826764
Epoch 40: loss 3.411193254656478
Epoch 41: loss 3.3420286507889134
Epoch 42: loss 3.2819440681173337
Epoch 43: loss 3.22919770127214
Epoch 44: loss 3.1831235179050648
Epoch 45: loss 3.14230738521076
Epoch 46: loss 3.1066619106581057
Epoch 47: loss 3.0748303724800263
Epoch 48: loss 3.0468824931368226
Epoch 49: loss 3.021761844442097
-----------Time: 0:05:15.246870, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.7475773096084595-------------


Epoch 0: loss 16.067791462722454
Epoch 1: loss 15.944695351368841
Epoch 2: loss 15.813919568852668
Epoch 3: loss 15.621203644499953
Epoch 4: loss 15.335866459301466
Epoch 5: loss 14.986583851289982
Epoch 6: loss 14.58494117377957
Epoch 7: loss 14.13669103743785
Epoch 8: loss 13.647279360814222
Epoch 9: loss 13.122309598668624
Epoch 10: loss 12.567717898598568
Epoch 11: loss 11.989760665416458
Epoch 12: loss 11.394256391623799
Epoch 13: loss 10.787729485777813
Epoch 14: loss 10.176859399817312
Epoch 15: loss 9.568043198515502
Epoch 16: loss 8.967675479745788
Epoch 17: loss 8.38171784005261
Epoch 18: loss 7.815594423458977
Epoch 19: loss 7.275151703654585
Epoch 20: loss 6.765108271874183
Epoch 21: loss 6.289476344091469
Epoch 22: loss 5.851523505403271
Epoch 23: loss 5.453668849762527
Epoch 24: loss 5.0965548235803535
Epoch 25: loss 4.780403154320274
Epoch 26: loss 4.503581878735748
Epoch 27: loss 4.264342037603348
Epoch 28: loss 4.058946061095445
Epoch 29: loss 3.883996340293739
Epoch 30: loss 3.7362731949929118
Epoch 31: loss 3.6114214003377794
Epoch 32: loss 3.506170301089927
Epoch 33: loss 3.416625915758632
Epoch 34: loss 3.341316796178854
Epoch 35: loss 3.2770256345332998
Epoch 36: loss 3.2223844427074537
Epoch 37: loss 3.17569336624104
Epoch 38: loss 3.135322824387916
Epoch 39: loss 3.100294742200477
Epoch 40: loss 3.069934315756652
Epoch 41: loss 3.04386446941411
Epoch 42: loss 3.020767934559609
Epoch 43: loss 3.000377260646851
Epoch 44: loss 2.982801173678425
Epoch 45: loss 2.9670029269929423
Epoch 46: loss 2.9532975909371557
Epoch 47: loss 2.94059520418583
Epoch 48: loss 2.9293137837389747
Epoch 49: loss 2.9194840959649557
-----------Time: 0:06:21.625606, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.7194877862930298-------------


Epoch 0: loss 16.067819049787495
Epoch 1: loss 15.943633566478336
Epoch 2: loss 15.780038065596596
Epoch 3: loss 15.488652050397913
Epoch 4: loss 15.097384968810006
Epoch 5: loss 14.633979099869014
Epoch 6: loss 14.108182945998225
Epoch 7: loss 13.528751340101698
Epoch 8: loss 12.904695940769646
Epoch 9: loss 12.24553663202963
Epoch 10: loss 11.560540912331545
Epoch 11: loss 10.859405588105426
Epoch 12: loss 10.152358207059594
Epoch 13: loss 9.449118671759502
Epoch 14: loss 8.760073678398859
Epoch 15: loss 8.093753947195246
Epoch 16: loss 7.45856263859237
Epoch 17: loss 6.862956963135147
Epoch 18: loss 6.3128221912705555
Epoch 19: loss 5.813095101070767
Epoch 20: loss 5.366691243979126
Epoch 21: loss 4.974822361226831
Epoch 22: loss 4.636519849267973
Epoch 23: loss 4.348919186273173
Epoch 24: loss 4.1072971981592055
Epoch 25: loss 3.9064466443769694
Epoch 26: loss 3.7406580184969713
Epoch 27: loss 3.604048480800859
Epoch 28: loss 3.491437571407855
Epoch 29: loss 3.398748307334398
Epoch 30: loss 3.3219300498775715
Epoch 31: loss 3.258004977172843
Epoch 32: loss 3.204150508317434
Epoch 33: loss 3.159107441614347
Epoch 34: loss 3.1207727396728036
Epoch 35: loss 3.088177085246905
Epoch 36: loss 3.060090431251754
Epoch 37: loss 3.0364812735048825
Epoch 38: loss 3.0158525145916006
Epoch 39: loss 2.9978368568835276
Epoch 40: loss 2.982213644439465
Epoch 41: loss 2.9690085145816525
Epoch 42: loss 2.9565123474552557
Epoch 43: loss 2.9456892043368335
Epoch 44: loss 2.9365109231823356
Epoch 45: loss 2.9277360952697804
Epoch 46: loss 2.920317846254657
Epoch 47: loss 2.913767421809534
Epoch 48: loss 2.907634322444922
Epoch 49: loss 2.901682318909911
-----------Time: 0:07:49.630000, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.7147624492645264-------------


Epoch 0: loss 15.529645922911303
Epoch 1: loss 14.371544763275693
Epoch 2: loss 13.006083211022398
Epoch 3: loss 10.880641594472431
Epoch 4: loss 8.557079834287228
Epoch 5: loss 6.517065175270632
Epoch 6: loss 4.9601930919842205
Epoch 7: loss 3.9252297690279008
Epoch 8: loss 3.3172473399259266
Epoch 9: loss 2.985269855325024
Epoch 10: loss 2.805771516936833
Epoch 11: loss 2.7051166458710694
Epoch 12: loss 2.6435367473770315
Epoch 13: loss 2.606055605080933
Epoch 14: loss 2.5800868833499346
Epoch 15: loss 2.561214276018708
Epoch 16: loss 2.54640843688048
Epoch 17: loss 2.5353661764569098
Epoch 18: loss 2.5245993198293135
Epoch 19: loss 2.515319411167313
Epoch 20: loss 2.506576168076897
Epoch 21: loss 2.4988297709568226
Epoch 22: loss 2.4912235013423505
Epoch 23: loss 2.483387470375007
Epoch 24: loss 2.4765524118733575
Epoch 25: loss 2.469234318626906
Epoch 26: loss 2.4623038811551416
Epoch 27: loss 2.4547456734332136
Epoch 28: loss 2.44831413157029
Epoch 29: loss 2.441802682534322
Epoch 30: loss 2.434020253223463
Epoch 31: loss 2.427433983018179
Epoch 32: loss 2.421004830526878
Epoch 33: loss 2.4136132212551216
Epoch 34: loss 2.4064784366323484
Epoch 35: loss 2.399308116102815
Epoch 36: loss 2.3918972781587384
Epoch 37: loss 2.38399667529842
Epoch 38: loss 2.3762466379842917
Epoch 39: loss 2.3678756045413576
Epoch 40: loss 2.3595161952682004
Epoch 41: loss 2.3502075078372013
Epoch 42: loss 2.3407064368376074
Epoch 43: loss 2.330376394680494
Epoch 44: loss 2.3191031362130112
Epoch 45: loss 2.3072297689770798
Epoch 46: loss 2.294244048905282
Epoch 47: loss 2.2802182300509544
Epoch 48: loss 2.265780755914768
Epoch 49: loss 2.250791565369756
-----------Time: 0:03:23.971440, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 20, rmse: 1.5026428699493408-------------


Epoch 0: loss 15.519946807749314
Epoch 1: loss 13.26485647799983
Epoch 2: loss 9.22380672504618
Epoch 3: loss 5.959589849547759
Epoch 4: loss 4.111363401226274
Epoch 5: loss 3.296681736525535
Epoch 6: loss 2.9727497758134156
Epoch 7: loss 2.832436982155365
Epoch 8: loss 2.765144266208402
Epoch 9: loss 2.726682110154285
Epoch 10: loss 2.702755206690462
Epoch 11: loss 2.6849946995154874
Epoch 12: loss 2.6704576841315997
Epoch 13: loss 2.6564942668999842
Epoch 14: loss 2.6461130311270002
Epoch 15: loss 2.634773089485106
Epoch 16: loss 2.624705848128592
Epoch 17: loss 2.6134251435857028
Epoch 18: loss 2.603592525557891
Epoch 19: loss 2.5936424692018063
Epoch 20: loss 2.5843212651195184
Epoch 21: loss 2.574978483572416
Epoch 22: loss 2.566218359435363
Epoch 23: loss 2.5573974879556793
Epoch 24: loss 2.547934834758765
Epoch 25: loss 2.539104531289703
Epoch 26: loss 2.5304590015712156
Epoch 27: loss 2.5221444769616097
Epoch 28: loss 2.513474938271809
Epoch 29: loss 2.5050948486825964
Epoch 30: loss 2.4962819794586912
Epoch 31: loss 2.4877134046196745
Epoch 32: loss 2.4786172012974217
Epoch 33: loss 2.469639298966425
Epoch 34: loss 2.4612483221140162
Epoch 35: loss 2.450919073777979
Epoch 36: loss 2.4415328743796167
Epoch 37: loss 2.4310209937819067
Epoch 38: loss 2.419634828754195
Epoch 39: loss 2.407692011962318
Epoch 40: loss 2.3947063681222356
Epoch 41: loss 2.3806042955127857
Epoch 42: loss 2.3641342223500867
Epoch 43: loss 2.346294528249167
Epoch 44: loss 2.3262115082966366
Epoch 45: loss 2.304459334068029
Epoch 46: loss 2.2826849033548626
Epoch 47: loss 2.2629011558280685
Epoch 48: loss 2.245985467191492
Epoch 49: loss 2.2317334566510456
-----------Time: 0:04:18.225796, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 50, rmse: 1.4965704679489136-------------


Epoch 0: loss 15.16988846891402
Epoch 1: loss 10.020256317588801
Epoch 2: loss 5.433787773457994
Epoch 3: loss 3.606886708263213
Epoch 4: loss 3.073956244273183
Epoch 5: loss 2.907403817573535
Epoch 6: loss 2.839727401863044
Epoch 7: loss 2.8050358462683724
Epoch 8: loss 2.7847902813704524
Epoch 9: loss 2.7672538500625588
Epoch 10: loss 2.750710808498306
Epoch 11: loss 2.7385709262399844
Epoch 12: loss 2.724023333316656
Epoch 13: loss 2.711849647348766
Epoch 14: loss 2.6985748388509245
Epoch 15: loss 2.685419936099734
Epoch 16: loss 2.67467884618086
Epoch 17: loss 2.661722683271291
Epoch 18: loss 2.652151431254293
Epoch 19: loss 2.6413080338358297
Epoch 20: loss 2.6302723945516036
Epoch 21: loss 2.620433671244984
Epoch 22: loss 2.6094466817968884
Epoch 23: loss 2.6000718206993714
Epoch 24: loss 2.59005320856531
Epoch 25: loss 2.5801464226532396
Epoch 26: loss 2.571946428676479
Epoch 27: loss 2.560861296210359
Epoch 28: loss 2.551784699348213
Epoch 29: loss 2.5427868704616925
Epoch 30: loss 2.5333292289814784
Epoch 31: loss 2.5231011410392195
Epoch 32: loss 2.5136051343925105
Epoch 33: loss 2.503509217465034
Epoch 34: loss 2.49361674587256
Epoch 35: loss 2.4817168949867736
Epoch 36: loss 2.470466282913515
Epoch 37: loss 2.4578805488370694
Epoch 38: loss 2.4434907381919624
Epoch 39: loss 2.427645879312985
Epoch 40: loss 2.4099996385527667
Epoch 41: loss 2.3891308786040613
Epoch 42: loss 2.364345276725234
Epoch 43: loss 2.337053930065825
Epoch 44: loss 2.3097727878253185
Epoch 45: loss 2.285976649756792
Epoch 46: loss 2.2658329721116837
Epoch 47: loss 2.248906914276426
Epoch 48: loss 2.2344095185115456
Epoch 49: loss 2.2221284962141237
-----------Time: 0:05:46.711784, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 100, rmse: 1.4936766624450684-------------


Epoch 0: loss 14.578550050412915
Epoch 1: loss 7.908816705779448
Epoch 2: loss 4.120842349470926
Epoch 3: loss 3.1793616304195336
Epoch 4: loss 2.958209834583691
Epoch 5: loss 2.8849701196879
Epoch 6: loss 2.851003913402298
Epoch 7: loss 2.8279190566502166
Epoch 8: loss 2.8112320478614614
Epoch 9: loss 2.7955969736069166
Epoch 10: loss 2.7785509194026115
Epoch 11: loss 2.765383092782755
Epoch 12: loss 2.75116856452108
Epoch 13: loss 2.735652542166116
Epoch 14: loss 2.7250531417251347
Epoch 15: loss 2.7111818283520814
Epoch 16: loss 2.697522728387398
Epoch 17: loss 2.687517380701452
Epoch 18: loss 2.6734938261105743
Epoch 19: loss 2.6639819425237508
Epoch 20: loss 2.6516118600616125
Epoch 21: loss 2.6423571024205517
Epoch 22: loss 2.630057812644581
Epoch 23: loss 2.620231429022768
Epoch 24: loss 2.610491948519277
Epoch 25: loss 2.6014987083152947
Epoch 26: loss 2.5901618181459924
Epoch 27: loss 2.5803775197465244
Epoch 28: loss 2.571023862993283
Epoch 29: loss 2.5611499424146658
Epoch 30: loss 2.5512292965914387
Epoch 31: loss 2.540886096295745
Epoch 32: loss 2.5299770225319542
Epoch 33: loss 2.5193791538557972
Epoch 34: loss 2.508318334898915
Epoch 35: loss 2.4974074381858906
Epoch 36: loss 2.484424950053602
Epoch 37: loss 2.4695860019775107
Epoch 38: loss 2.4533746901123727
Epoch 39: loss 2.435084666941333
Epoch 40: loss 2.412887195972984
Epoch 41: loss 2.3859987312065902
Epoch 42: loss 2.3555742299316886
Epoch 43: loss 2.325514175698704
Epoch 44: loss 2.2990603448256905
Epoch 45: loss 2.2773079949918813
Epoch 46: loss 2.2586333771125333
Epoch 47: loss 2.2430177069410413
Epoch 48: loss 2.22949055681545
Epoch 49: loss 2.2179674256295208
-----------Time: 0:06:48.072200, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 150, rmse: 1.4924445152282715-------------


Epoch 0: loss 13.957370333855668
Epoch 1: loss 6.52460075824918
Epoch 2: loss 3.5938100852416612
Epoch 3: loss 3.0584026402011393
Epoch 4: loss 2.9339868567312717
Epoch 5: loss 2.891105523376506
Epoch 6: loss 2.8664081604860607
Epoch 7: loss 2.8470265832914485
Epoch 8: loss 2.8295531490693087
Epoch 9: loss 2.813295892945705
Epoch 10: loss 2.7965552796482585
Epoch 11: loss 2.7828033936289485
Epoch 12: loss 2.7671883735013694
Epoch 13: loss 2.7530624752397315
Epoch 14: loss 2.7399909323878493
Epoch 15: loss 2.7261403437993526
Epoch 16: loss 2.714497036749802
Epoch 17: loss 2.7006406929260884
Epoch 18: loss 2.689511488675423
Epoch 19: loss 2.6779034421650074
Epoch 20: loss 2.666119102811995
Epoch 21: loss 2.655094454507584
Epoch 22: loss 2.644805914037185
Epoch 23: loss 2.6336291689403426
Epoch 24: loss 2.623573235936499
Epoch 25: loss 2.6122025091276018
Epoch 26: loss 2.6028341168356435
Epoch 27: loss 2.5933927412328672
Epoch 28: loss 2.5827675102974426
Epoch 29: loss 2.573685405693863
Epoch 30: loss 2.5623129474656485
Epoch 31: loss 2.5513616365346654
Epoch 32: loss 2.542260696085463
Epoch 33: loss 2.5306772177085857
Epoch 34: loss 2.5187527435898067
Epoch 35: loss 2.506209260423006
Epoch 36: loss 2.493021985381242
Epoch 37: loss 2.477281028126296
Epoch 38: loss 2.459695710249087
Epoch 39: loss 2.4384986043559786
Epoch 40: loss 2.4126044572080327
Epoch 41: loss 2.3812320487793532
Epoch 42: loss 2.3480611168346437
Epoch 43: loss 2.3185192979892486
Epoch 44: loss 2.293561506232469
Epoch 45: loss 2.272580476024995
Epoch 46: loss 2.254647297078724
Epoch 47: loss 2.2396367450587578
Epoch 48: loss 2.2267751458418505
Epoch 49: loss 2.215806076162337
-----------Time: 0:06:26.817428, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.0001, embedding_dim: 200, rmse: 1.491783618927002-------------


Epoch 0: loss 8.223206227858991
Epoch 1: loss 2.6817961064546636
Epoch 2: loss 2.504262091349881
Epoch 3: loss 2.415913741182801
Epoch 4: loss 2.344327749280323
Epoch 5: loss 2.2595172183807937
Epoch 6: loss 2.182897817381962
Epoch 7: loss 2.150774315623501
Epoch 8: loss 2.141797465951369
Epoch 9: loss 2.1374139880408016
Epoch 10: loss 2.1357396058636944
Epoch 11: loss 2.1365517560005705
Epoch 12: loss 2.135116300549178
Epoch 13: loss 2.1360507218198586
Epoch 14: loss 2.1351841696583103
Epoch 15: loss 2.135123777156164
Epoch 16: loss 2.135888429618906
Epoch 17: loss 2.134466472042845
Epoch 18: loss 2.135760004265014
Epoch 19: loss 2.135501977799702
Epoch 20: loss 2.135392055166617
Epoch 21: loss 2.135517801973987
Epoch 22: loss 2.135095244908605
Epoch 23: loss 2.1363087930129625
Epoch 24: loss 2.134610907936822
Epoch 25: loss 2.1354837779962477
Epoch 26: loss 2.1353910337523834
Epoch 27: loss 2.135255963663617
Epoch 28: loss 2.135867803689252
Epoch 29: loss 2.1356403593008384
Epoch 30: loss 2.135751789973572
Epoch 31: loss 2.1348194708443
Epoch 32: loss 2.1351323850521773
Epoch 33: loss 2.1357578709437965
Epoch 34: loss 2.135683575488096
Epoch 35: loss 2.1351311321554136
Epoch 36: loss 2.135399123778351
Epoch 37: loss 2.1351265636976158
Epoch 38: loss 2.135095186373538
Epoch 39: loss 2.1350598276277286
Epoch 40: loss 2.135435976626887
Epoch 41: loss 2.1356703464332774
Epoch 42: loss 2.1362876035779506
Epoch 43: loss 2.1346742397031426
Epoch 44: loss 2.134473266129652
Epoch 45: loss 2.1362868592167366
Epoch 46: loss 2.135754131052143
Epoch 47: loss 2.1350798321651854
Epoch 48: loss 2.1350517110892
Epoch 49: loss 2.135425364860962
-----------Time: 0:04:12.352803, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.001, embedding_dim: 20, rmse: 1.4643930196762085-------------


Epoch 0: loss 6.172715289103978
Epoch 1: loss 2.7154313881414103
Epoch 2: loss 2.573551376595842
Epoch 3: loss 2.4762591650591004
Epoch 4: loss 2.37487436702162
Epoch 5: loss 2.2483932112402862
Epoch 6: loss 2.1700610457197875
Epoch 7: loss 2.1482806301687383
Epoch 8: loss 2.1400519867446386
Epoch 9: loss 2.1377433329707194
Epoch 10: loss 2.1364897787084782
Epoch 11: loss 2.134651185508135
Epoch 12: loss 2.1361094588102887
Epoch 13: loss 2.134912223385494
Epoch 14: loss 2.1355199233029705
Epoch 15: loss 2.1359829362674274
Epoch 16: loss 2.1350541176388873
Epoch 17: loss 2.1354787671870676
Epoch 18: loss 2.1356619293110213
Epoch 19: loss 2.1354174466156453
Epoch 20: loss 2.1353483382873786
Epoch 21: loss 2.135474844105935
Epoch 22: loss 2.1353158185247367
Epoch 23: loss 2.1355172787884644
Epoch 24: loss 2.1350219953053666
Epoch 25: loss 2.1355453649897367
Epoch 26: loss 2.135161238756434
Epoch 27: loss 2.135575853656854
Epoch 28: loss 2.1357984881870378
Epoch 29: loss 2.1347698391358967
Epoch 30: loss 2.13519372695033
Epoch 31: loss 2.1354899632664983
Epoch 32: loss 2.1351634260751164
Epoch 33: loss 2.1363157505182677
Epoch 34: loss 2.1347154554561016
Epoch 35: loss 2.1354885459140696
Epoch 36: loss 2.136937680291118
Epoch 37: loss 2.1337048217873007
Epoch 38: loss 2.135603315030316
Epoch 39: loss 2.1360342891531316
Epoch 40: loss 2.1351742741852884
Epoch 41: loss 2.1354080200065666
Epoch 42: loss 2.134670668156527
Epoch 43: loss 2.1358441731539544
Epoch 44: loss 2.1355322890465773
Epoch 45: loss 2.135293709966007
Epoch 46: loss 2.135163865509469
Epoch 47: loss 2.1354077948118566
Epoch 48: loss 2.1352403327911817
Epoch 49: loss 2.1351975878663136
-----------Time: 0:05:05.891212, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.001, embedding_dim: 50, rmse: 1.4646668434143066-------------


Epoch 0: loss 5.3487254913375715
Epoch 1: loss 2.770928401931464
Epoch 2: loss 2.6256260086233816
Epoch 3: loss 2.5115465094953726
Epoch 4: loss 2.383357033643987
Epoch 5: loss 2.2317395382694993
Epoch 6: loss 2.1650728557082606
Epoch 7: loss 2.145986135787715
Epoch 8: loss 2.1406951681824724
Epoch 9: loss 2.137291836505224
Epoch 10: loss 2.1362890102346026
Epoch 11: loss 2.1352553216577417
Epoch 12: loss 2.135209308687016
Epoch 13: loss 2.135797763531982
Epoch 14: loss 2.1353927874059506
Epoch 15: loss 2.135509637401699
Epoch 16: loss 2.1354359170546537
Epoch 17: loss 2.135100868488538
Epoch 18: loss 2.1354757683506604
Epoch 19: loss 2.135435567140709
Epoch 20: loss 2.1365609229939864
Epoch 21: loss 2.1344804078616026
Epoch 22: loss 2.1349697160357817
Epoch 23: loss 2.1361857479587076
Epoch 24: loss 2.134237669874236
Epoch 25: loss 2.1358231430535994
Epoch 26: loss 2.1355318044306726
Epoch 27: loss 2.135878979673588
Epoch 28: loss 2.1351034523936443
Epoch 29: loss 2.135526743383755
Epoch 30: loss 2.1357519479469476
Epoch 31: loss 2.1350704907982037
Epoch 32: loss 2.134799385278234
Epoch 33: loss 2.1355772381440787
Epoch 34: loss 2.135487537723705
Epoch 35: loss 2.1347342129308546
Epoch 36: loss 2.1358951412510003
Epoch 37: loss 2.1359054399872033
Epoch 38: loss 2.1346654605450612
Epoch 39: loss 2.135349244057581
Epoch 40: loss 2.1354142647842274
Epoch 41: loss 2.135455573622852
Epoch 42: loss 2.136373631781764
Epoch 43: loss 2.1346471661001916
Epoch 44: loss 2.1356275504927975
Epoch 45: loss 2.135701735295829
Epoch 46: loss 2.135506110842167
Epoch 47: loss 2.135522294070424
Epoch 48: loss 2.1352933024893384
Epoch 49: loss 2.1346933467677265
-----------Time: 0:05:46.693118, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.001, embedding_dim: 100, rmse: 1.4646514654159546-------------


Epoch 0: loss 5.004670006740087
Epoch 1: loss 2.802948612956783
Epoch 2: loss 2.651176173379721
Epoch 3: loss 2.5309344258497695
Epoch 4: loss 2.3860948354151663
Epoch 5: loss 2.221732324721568
Epoch 6: loss 2.1631451938514026
Epoch 7: loss 2.1454870094742184
Epoch 8: loss 2.14021996890416
Epoch 9: loss 2.1379151987990586
Epoch 10: loss 2.1356736743106604
Epoch 11: loss 2.135165491008188
Epoch 12: loss 2.1361523163973346
Epoch 13: loss 2.135791149263864
Epoch 14: loss 2.135028079322267
Epoch 15: loss 2.135036416582727
Epoch 16: loss 2.1358842373931455
Epoch 17: loss 2.1353188537267838
Epoch 18: loss 2.1349220810019496
Epoch 19: loss 2.135129510221953
Epoch 20: loss 2.135472159919822
Epoch 21: loss 2.1352457795342783
Epoch 22: loss 2.1353446437717456
Epoch 23: loss 2.1359209449735914
Epoch 24: loss 2.1357309260142765
Epoch 25: loss 2.135067534550433
Epoch 26: loss 2.1357933422221382
Epoch 27: loss 2.134428598428304
Epoch 28: loss 2.135722429030223
Epoch 29: loss 2.135631500864366
Epoch 30: loss 2.134476658376162
Epoch 31: loss 2.135903301480155
Epoch 32: loss 2.1358218441325856
Epoch 33: loss 2.135351853567728
Epoch 34: loss 2.134777494913343
Epoch 35: loss 2.1356248198934975
Epoch 36: loss 2.1359060345427245
Epoch 37: loss 2.135609796541161
Epoch 38: loss 2.135942759236413
Epoch 39: loss 2.1351435691393736
Epoch 40: loss 2.134559009242408
Epoch 41: loss 2.135368056826257
Epoch 42: loss 2.1354421407000537
Epoch 43: loss 2.1354018778435364
Epoch 44: loss 2.1358382801701024
Epoch 45: loss 2.135429596952798
Epoch 46: loss 2.1345328138599324
Epoch 47: loss 2.1355333829976217
Epoch 48: loss 2.1353016486305334
Epoch 49: loss 2.13488977210816
-----------Time: 0:05:26.917016, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4648663997650146-------------


Epoch 0: loss 4.8287682276565524
Epoch 1: loss 2.827495466826596
Epoch 2: loss 2.6694690425347996
Epoch 3: loss 2.543094283151134
Epoch 4: loss 2.383322047176019
Epoch 5: loss 2.2172561659639203
Epoch 6: loss 2.1621727153229413
Epoch 7: loss 2.145122903141397
Epoch 8: loss 2.1395772497831578
Epoch 9: loss 2.137597976332452
Epoch 10: loss 2.1358448462099924
Epoch 11: loss 2.1355744575015096
Epoch 12: loss 2.13547419380273
Epoch 13: loss 2.1356883048363002
Epoch 14: loss 2.134396244936368
Epoch 15: loss 2.1361609153477894
Epoch 16: loss 2.1352168827228017
Epoch 17: loss 2.136305174664222
Epoch 18: loss 2.1349885464362828
Epoch 19: loss 2.1356518485093647
Epoch 20: loss 2.1347731823114806
Epoch 21: loss 2.135304499605935
Epoch 22: loss 2.135459687672211
Epoch 23: loss 2.1350832663521033
Epoch 24: loss 2.1358180832383167
Epoch 25: loss 2.135159247980745
Epoch 26: loss 2.1359426804766053
Epoch 27: loss 2.1349782908721227
Epoch 28: loss 2.1348773801112837
Epoch 29: loss 2.1352600464683436
Epoch 30: loss 2.1358838190262315
Epoch 31: loss 2.135330897689579
Epoch 32: loss 2.1348604489565544
Epoch 33: loss 2.1358828989301704
Epoch 34: loss 2.1350808014618177
Epoch 35: loss 2.1363148065673845
Epoch 36: loss 2.1347731862656767
Epoch 37: loss 2.135601984410911
Epoch 38: loss 2.1357629098861097
Epoch 39: loss 2.1349110905407174
Epoch 40: loss 2.135760040371362
Epoch 41: loss 2.1347073623186352
Epoch 42: loss 2.1357430233911305
Epoch 43: loss 2.1352581203210734
Epoch 44: loss 2.1352489611064045
Epoch 45: loss 2.135491307369064
Epoch 46: loss 2.1352546097727947
Epoch 47: loss 2.1359073305467082
Epoch 48: loss 2.135103502047976
Epoch 49: loss 2.1360920369463035
-----------Time: 0:07:10.880176, Loss: regression, n_iter: 50, l2: 0.001, batch_size: 512, learning_rate: 0.001, embedding_dim: 200, rmse: 1.4642051458358765-------------


Epoch 0: loss 16.129198368735935
Epoch 1: loss 16.129129845163096
Epoch 2: loss 16.129054001103277
Epoch 3: loss 16.128771792287413
Epoch 4: loss 16.12920622203661
Epoch 5: loss 16.129659577037977
Epoch 6: loss 16.129102527576944
Epoch 7: loss 16.12905944222989
Epoch 8: loss 16.129173246673915
Epoch 9: loss 16.129283740209498
Epoch 10: loss 16.129529357993086
Epoch 11: loss 16.12934700820757
Epoch 12: loss 16.129464683325395
Epoch 13: loss 16.12946628798609
Epoch 14: loss 16.129245186888653
Epoch 15: loss 16.129402996146162
Epoch 16: loss 16.12958213246387
Epoch 17: loss 16.1297495852346
Epoch 18: loss 16.12936362079952
Epoch 19: loss 16.129292467366096
Epoch 20: loss 16.12937090707862
Epoch 21: loss 16.129242881484654
Epoch 22: loss 16.12928088851597
Epoch 23: loss 16.129429062553076
Epoch 24: loss 16.129269079540087
Epoch 25: loss 16.129406715475994
Epoch 26: loss 16.12932552565699
Epoch 27: loss 16.12945331386898
Epoch 28: loss 16.129337619698568
Epoch 29: loss 16.129566341897714
Epoch 30: loss 16.12926726963209
Epoch 31: loss 16.129289655063463
Epoch 32: loss 16.129015835471776
Epoch 33: loss 16.129554198099218
Epoch 34: loss 16.12929913168368
Epoch 35: loss 16.129499707014663
Epoch 36: loss 16.129402128509852
Epoch 37: loss 16.12923868428106
Epoch 38: loss 16.129269794795825
Epoch 39: loss 16.12955815481103
Epoch 40: loss 16.1290816369264
Epoch 41: loss 16.129206824302674
Epoch 42: loss 16.129249835014342
Epoch 43: loss 16.128971285405367
Epoch 44: loss 16.12943662042203
Epoch 45: loss 16.129251825291178
Epoch 46: loss 16.129131737999295
Epoch 47: loss 16.129205026833908
Epoch 48: loss 16.12919088964877
Epoch 49: loss 16.129206232402634
-----------Time: 0:02:34.289363, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017206192016602-------------


Epoch 0: loss 16.128812710098597
Epoch 1: loss 16.12913486024608
Epoch 2: loss 16.129154674903205
Epoch 3: loss 16.12915985584259
Epoch 4: loss 16.129266420654627
Epoch 5: loss 16.129247233142024
Epoch 6: loss 16.12918761709462
Epoch 7: loss 16.129098587450773
Epoch 8: loss 16.12910728143609
Epoch 9: loss 16.12921015076015
Epoch 10: loss 16.12912132843681
Epoch 11: loss 16.1293224003004
Epoch 12: loss 16.12911068667536
Epoch 13: loss 16.129233607001925
Epoch 14: loss 16.12910325734512
Epoch 15: loss 16.128639237777048
Epoch 16: loss 16.12928428131601
Epoch 17: loss 16.12934052736863
Epoch 18: loss 16.12915268773618
Epoch 19: loss 16.129109918552896
Epoch 20: loss 16.12908026135486
Epoch 21: loss 16.12898880917093
Epoch 22: loss 16.129135928983274
Epoch 23: loss 16.129145847196163
Epoch 24: loss 16.129329800605774
Epoch 25: loss 16.128978935531947
Epoch 26: loss 16.128945900046308
Epoch 27: loss 16.128942901155224
Epoch 28: loss 16.129242677273957
Epoch 29: loss 16.12914383204087
Epoch 30: loss 16.129072556288346
Epoch 31: loss 16.129195070266725
Epoch 32: loss 16.128774603553442
Epoch 33: loss 16.128756857954937
Epoch 34: loss 16.129298043251037
Epoch 35: loss 16.129060715177786
Epoch 36: loss 16.129098810320315
Epoch 37: loss 16.129227512815724
Epoch 38: loss 16.129520473272905
Epoch 39: loss 16.129313563263935
Epoch 40: loss 16.128809011500813
Epoch 41: loss 16.129012159679245
Epoch 42: loss 16.12905342578888
Epoch 43: loss 16.12897819954416
Epoch 44: loss 16.12924469553906
Epoch 45: loss 16.129414719084036
Epoch 46: loss 16.12873080709706
Epoch 47: loss 16.12921051979065
Epoch 48: loss 16.12933888953665
Epoch 49: loss 16.12917947043543
-----------Time: 0:03:03.322335, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129148025098054
Epoch 1: loss 16.129205684039903
Epoch 2: loss 16.129198283734528
Epoch 3: loss 16.129264209581457
Epoch 4: loss 16.129291159173718
Epoch 5: loss 16.129361037586047
Epoch 6: loss 16.129351402365643
Epoch 7: loss 16.12927894488625
Epoch 8: loss 16.129240531506746
Epoch 9: loss 16.129536660857823
Epoch 10: loss 16.129146617391836
Epoch 11: loss 16.12898531893025
Epoch 12: loss 16.129108974208002
Epoch 13: loss 16.12904842832814
Epoch 14: loss 16.129171074991643
Epoch 15: loss 16.12919157380643
Epoch 16: loss 16.129291558265685
Epoch 17: loss 16.129223168414573
Epoch 18: loss 16.12937085421189
Epoch 19: loss 16.129100531080496
Epoch 20: loss 16.129284901204315
Epoch 21: loss 16.12913241593734
Epoch 22: loss 16.129308474582174
Epoch 23: loss 16.129434835392495
Epoch 24: loss 16.129394624544226
Epoch 25: loss 16.129114739791206
Epoch 26: loss 16.129321347112242
Epoch 27: loss 16.12932349702586
Epoch 28: loss 16.12938399522201
Epoch 29: loss 16.129342703197313
Epoch 30: loss 16.129467652155004
Epoch 31: loss 16.129212559824406
Epoch 32: loss 16.128978205763776
Epoch 33: loss 16.1291583351467
Epoch 34: loss 16.12921812534332
Epoch 35: loss 16.129327867342077
Epoch 36: loss 16.129183976546578
Epoch 37: loss 16.129090428352356
Epoch 38: loss 16.129039149699004
Epoch 39: loss 16.12896322582079
Epoch 40: loss 16.129209749594978
Epoch 41: loss 16.12881735822429
Epoch 42: loss 16.12899692784185
Epoch 43: loss 16.1294020248496
Epoch 44: loss 16.128651881217955
Epoch 45: loss 16.129213914663897
Epoch 46: loss 16.129227167627086
Epoch 47: loss 16.129059665099433
Epoch 48: loss 16.1290795367697
Epoch 49: loss 16.129053682866303
-----------Time: 0:03:03.399957, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.01718282699585-------------


Epoch 0: loss 16.129141295474508
Epoch 1: loss 16.128843825796377
Epoch 2: loss 16.129228413623313
Epoch 3: loss 16.12920784017314
Epoch 4: loss 16.129289609452954
Epoch 5: loss 16.12892701211183
Epoch 6: loss 16.129094236830007
Epoch 7: loss 16.1294186986011
Epoch 8: loss 16.12911124540412
Epoch 9: loss 16.12885641222415
Epoch 10: loss 16.12881187045056
Epoch 11: loss 16.12898907246797
Epoch 12: loss 16.12885737211808
Epoch 13: loss 16.129680357808653
Epoch 14: loss 16.12917531469594
Epoch 15: loss 16.129261835761692
Epoch 16: loss 16.128950214385988
Epoch 17: loss 16.128987704152646
Epoch 18: loss 16.129231326476386
Epoch 19: loss 16.12910615361255
Epoch 20: loss 16.128860964982405
Epoch 21: loss 16.128682403979095
Epoch 22: loss 16.129284776812014
Epoch 23: loss 16.129100827548815
Epoch 24: loss 16.12894007537676
Epoch 25: loss 16.12900235445603
Epoch 26: loss 16.129080064400384
Epoch 27: loss 16.128854607499164
Epoch 28: loss 16.12939371959023
Epoch 29: loss 16.12943106941555
Epoch 30: loss 16.129080441723698
Epoch 31: loss 16.129418873786925
Epoch 32: loss 16.129314469254535
Epoch 33: loss 16.129179479764854
Epoch 34: loss 16.12934680192367
Epoch 35: loss 16.128896563986075
Epoch 36: loss 16.129332217962844
Epoch 37: loss 16.129140539791273
Epoch 38: loss 16.129449450451396
Epoch 39: loss 16.128821400974108
Epoch 40: loss 16.12891131898631
Epoch 41: loss 16.129279334648796
Epoch 42: loss 16.128995674589405
Epoch 43: loss 16.12905840770058
Epoch 44: loss 16.129435069664666
Epoch 45: loss 16.129464873023654
Epoch 46: loss 16.12899172824362
Epoch 47: loss 16.1289341211319
Epoch 48: loss 16.129380984928297
Epoch 49: loss 16.129080148365187
-----------Time: 0:02:40.420218, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129066895401998
Epoch 1: loss 16.129274412860042
Epoch 2: loss 16.12927602685016
Epoch 3: loss 16.128909708106
Epoch 4: loss 16.129098826905956
Epoch 5: loss 16.12929986870807
Epoch 6: loss 16.12901103392891
Epoch 7: loss 16.129281878471375
Epoch 8: loss 16.129187394225077
Epoch 9: loss 16.128907350871874
Epoch 10: loss 16.129033119782157
Epoch 11: loss 16.129060732800028
Epoch 12: loss 16.12950160710708
Epoch 13: loss 16.12908662091131
Epoch 14: loss 16.1285914618036
Epoch 15: loss 16.128871126796888
Epoch 16: loss 16.129043671359188
Epoch 17: loss 16.12953719056171
Epoch 18: loss 16.12948632136635
Epoch 19: loss 16.129316326846247
Epoch 20: loss 16.128987296767857
Epoch 21: loss 16.12915349421294
Epoch 22: loss 16.129535613889278
Epoch 23: loss 16.129061303968015
Epoch 24: loss 16.129369345955226
Epoch 25: loss 16.12888297205386
Epoch 26: loss 16.129123417190883
Epoch 27: loss 16.12939105759496
Epoch 28: loss 16.129338908195496
Epoch 29: loss 16.129418049687924
Epoch 30: loss 16.129358215953992
Epoch 31: loss 16.12914246165234
Epoch 32: loss 16.12922792331032
Epoch 33: loss 16.12903413876243
Epoch 34: loss 16.12879549420398
Epoch 35: loss 16.12879809089329
Epoch 36: loss 16.129307832925218
Epoch 37: loss 16.129412849053093
Epoch 38: loss 16.129092564790145
Epoch 39: loss 16.129392817746037
Epoch 40: loss 16.129156519019084
Epoch 41: loss 16.129317063870637
Epoch 42: loss 16.12895752450694
Epoch 43: loss 16.129056918102762
Epoch 44: loss 16.129023558160533
Epoch 45: loss 16.129169030811475
Epoch 46: loss 16.129169284779092
Epoch 47: loss 16.129263385482457
Epoch 48: loss 16.129061920746512
Epoch 49: loss 16.12875150286633
-----------Time: 0:03:34.700813, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017184734344482-------------


Epoch 0: loss 16.12864408493042
Epoch 1: loss 16.12941826633785
Epoch 2: loss 16.129349994659425
Epoch 3: loss 16.129411001827407
Epoch 4: loss 16.129409572352532
Epoch 5: loss 16.129051860519077
Epoch 6: loss 16.129041293393012
Epoch 7: loss 16.129046923181285
Epoch 8: loss 16.12910352997158
Epoch 9: loss 16.12933910825978
Epoch 10: loss 16.129158662713092
Epoch 11: loss 16.129120604888254
Epoch 12: loss 16.129110238863074
Epoch 13: loss 16.129027527311575
Epoch 14: loss 16.128977715450784
Epoch 15: loss 16.12915039788122
Epoch 16: loss 16.12879653495291
Epoch 17: loss 16.129148568277774
Epoch 18: loss 16.129025344226672
Epoch 19: loss 16.12906891055729
Epoch 20: loss 16.128860578329668
Epoch 21: loss 16.12863346286442
Epoch 22: loss 16.12874450268953
Epoch 23: loss 16.128843421521395
Epoch 24: loss 16.129020439023556
Epoch 25: loss 16.12909974637239
Epoch 26: loss 16.129249985321707
Epoch 27: loss 16.129126345592997
Epoch 28: loss 16.129109412690866
Epoch 29: loss 16.12861215757287
Epoch 30: loss 16.12857839335566
Epoch 31: loss 16.12892028041508
Epoch 32: loss 16.12894461673239
Epoch 33: loss 16.129419411783633
Epoch 34: loss 16.128892583432403
Epoch 35: loss 16.129175724153935
Epoch 36: loss 16.1291967485262
Epoch 37: loss 16.12905118983725
Epoch 38: loss 16.12899293381235
Epoch 39: loss 16.129188459852468
Epoch 40: loss 16.128905422791192
Epoch 41: loss 16.12908484210139
Epoch 42: loss 16.128628437415415
Epoch 43: loss 16.129013998612113
Epoch 44: loss 16.128911648625913
Epoch 45: loss 16.129024468297544
Epoch 46: loss 16.128858065605165
Epoch 47: loss 16.128702289125194
Epoch 48: loss 16.128811485871026
Epoch 49: loss 16.129133204791856
-----------Time: 0:02:13.677203, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.01718807220459-------------


Epoch 0: loss 16.12911964084791
Epoch 1: loss 16.129133936633234
Epoch 2: loss 16.129171028344526
Epoch 3: loss 16.128683964065882
Epoch 4: loss 16.12908061068991
Epoch 5: loss 16.12928135809691
Epoch 6: loss 16.12913408383079
Epoch 7: loss 16.129146790504457
Epoch 8: loss 16.12877978760263
Epoch 9: loss 16.12890162260636
Epoch 10: loss 16.12939905187358
Epoch 11: loss 16.129169158313584
Epoch 12: loss 16.129232865831128
Epoch 13: loss 16.12898292230523
Epoch 14: loss 16.129168778917062
Epoch 15: loss 16.129137518094932
Epoch 16: loss 16.129420129112575
Epoch 17: loss 16.12937851988751
Epoch 18: loss 16.129065133177715
Epoch 19: loss 16.129139234708703
Epoch 20: loss 16.128996506981228
Epoch 21: loss 16.12925460960554
Epoch 22: loss 16.12941465896109
Epoch 23: loss 16.129106578619584
Epoch 24: loss 16.128875999865325
Epoch 25: loss 16.128721956584766
Epoch 26: loss 16.129106606607852
Epoch 27: loss 16.12921797607256
Epoch 28: loss 16.1292281192282
Epoch 29: loss 16.129010650385982
Epoch 30: loss 16.128717843345974
Epoch 31: loss 16.128685100182242
Epoch 32: loss 16.128965596530747
Epoch 33: loss 16.128733755194624
Epoch 34: loss 16.129001579077347
Epoch 35: loss 16.129000100882156
Epoch 36: loss 16.129145391091058
Epoch 37: loss 16.128839955122576
Epoch 38: loss 16.128936229581416
Epoch 39: loss 16.12879189926645
Epoch 40: loss 16.128846372728763
Epoch 41: loss 16.12885867720065
Epoch 42: loss 16.129117893136065
Epoch 43: loss 16.128908216434976
Epoch 44: loss 16.12889370088992
Epoch 45: loss 16.12872063284335
Epoch 46: loss 16.128491711616515
Epoch 47: loss 16.128971735290857
Epoch 48: loss 16.128838948581528
Epoch 49: loss 16.12883810064067
-----------Time: 0:02:33.748460, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017132759094238-------------


Epoch 0: loss 16.12948537391165
Epoch 1: loss 16.128879834258036
Epoch 2: loss 16.129016901099163
Epoch 3: loss 16.129274678230285
Epoch 4: loss 16.129289106700732
Epoch 5: loss 16.128940618556477
Epoch 6: loss 16.129099766067835
Epoch 7: loss 16.129076738979506
Epoch 8: loss 16.12947639797045
Epoch 9: loss 16.129289856164352
Epoch 10: loss 16.129072603972062
Epoch 11: loss 16.12907049137613
Epoch 12: loss 16.12914486553358
Epoch 13: loss 16.12912629790928
Epoch 14: loss 16.12901281066563
Epoch 15: loss 16.12897140150485
Epoch 16: loss 16.129188332350356
Epoch 17: loss 16.128753448569256
Epoch 18: loss 16.128934221682343
Epoch 19: loss 16.128629767376445
Epoch 20: loss 16.1291393984919
Epoch 21: loss 16.128907607949298
Epoch 22: loss 16.128938437544782
Epoch 23: loss 16.129110931313555
Epoch 24: loss 16.12886629726576
Epoch 25: loss 16.128916928042536
Epoch 26: loss 16.128854957870814
Epoch 27: loss 16.129080501846644
Epoch 28: loss 16.129173056975656
Epoch 29: loss 16.12875769760298
Epoch 30: loss 16.129124351169754
Epoch 31: loss 16.12928084808847
Epoch 32: loss 16.129129453327344
Epoch 33: loss 16.128825063290805
Epoch 34: loss 16.129037884007328
Epoch 35: loss 16.12895616448444
Epoch 36: loss 16.129053221578182
Epoch 37: loss 16.12865734618643
Epoch 38: loss 16.128649336358777
Epoch 39: loss 16.128760690274447
Epoch 40: loss 16.128887318528218
Epoch 41: loss 16.128936409950256
Epoch 42: loss 16.12889037028603
Epoch 43: loss 16.12871054255444
Epoch 44: loss 16.128909940304965
Epoch 45: loss 16.12907732984294
Epoch 46: loss 16.128707143534786
Epoch 47: loss 16.128703888602878
Epoch 48: loss 16.128557810576066
Epoch 49: loss 16.12862208822499
-----------Time: 0:03:03.467546, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017141342163086-------------


Epoch 0: loss 16.128995641418125
Epoch 1: loss 16.12931737070498
Epoch 2: loss 16.128961852322455
Epoch 3: loss 16.129069905695708
Epoch 4: loss 16.12877308907716
Epoch 5: loss 16.129092539911685
Epoch 6: loss 16.129395978347116
Epoch 7: loss 16.129111289978027
Epoch 8: loss 16.12883547388989
Epoch 9: loss 16.129194038847217
Epoch 10: loss 16.128810305180757
Epoch 11: loss 16.12918107924254
Epoch 12: loss 16.12896726027779
Epoch 13: loss 16.129208409267925
Epoch 14: loss 16.129196492485377
Epoch 15: loss 16.128966159405916
Epoch 16: loss 16.129178716825404
Epoch 17: loss 16.128994462801064
Epoch 18: loss 16.12906856847846
Epoch 19: loss 16.12917230958524
Epoch 20: loss 16.12883313842442
Epoch 21: loss 16.128680485227832
Epoch 22: loss 16.129003231421763
Epoch 23: loss 16.12899438298267
Epoch 24: loss 16.128843994762587
Epoch 25: loss 16.129049182974775
Epoch 26: loss 16.129083864585212
Epoch 27: loss 16.129185564621636
Epoch 28: loss 16.129150388551796
Epoch 29: loss 16.12877410080122
Epoch 30: loss 16.129119856461234
Epoch 31: loss 16.129006585867508
Epoch 32: loss 16.128954170061196
Epoch 33: loss 16.128998795799586
Epoch 34: loss 16.128889869607015
Epoch 35: loss 16.129069699411808
Epoch 36: loss 16.128862492934516
Epoch 37: loss 16.128800192086594
Epoch 38: loss 16.128714055600373
Epoch 39: loss 16.129163613526718
Epoch 40: loss 16.128878882656927
Epoch 41: loss 16.129029413928155
Epoch 42: loss 16.12875115767769
Epoch 43: loss 16.128649570630945
Epoch 44: loss 16.128791724080624
Epoch 45: loss 16.128940990696783
Epoch 46: loss 16.129171077064846
Epoch 47: loss 16.128996755765833
Epoch 48: loss 16.12898772281149
Epoch 49: loss 16.12931431480076
-----------Time: 0:03:35.024379, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017141342163086-------------


Epoch 0: loss 16.12872935792674
Epoch 1: loss 16.12892553391664
Epoch 2: loss 16.129345370375592
Epoch 3: loss 16.128995389523713
Epoch 4: loss 16.12913390242535
Epoch 5: loss 16.129081527046534
Epoch 6: loss 16.129149309448575
Epoch 7: loss 16.12893643586532
Epoch 8: loss 16.12940543734509
Epoch 9: loss 16.128951593067335
Epoch 10: loss 16.12911537937496
Epoch 11: loss 16.129163318095
Epoch 12: loss 16.128959288804428
Epoch 13: loss 16.128971346564914
Epoch 14: loss 16.12892778645391
Epoch 15: loss 16.128746640163918
Epoch 16: loss 16.12915127899336
Epoch 17: loss 16.128995349096215
Epoch 18: loss 16.128972362435384
Epoch 19: loss 16.12895929087763
Epoch 20: loss 16.129071687615436
Epoch 21: loss 16.128949319798014
Epoch 22: loss 16.128756359349126
Epoch 23: loss 16.129234239329463
Epoch 24: loss 16.129442439908566
Epoch 25: loss 16.128737770992778
Epoch 26: loss 16.128903712397037
Epoch 27: loss 16.128991906539255
Epoch 28: loss 16.129013780925586
Epoch 29: loss 16.128845821256224
Epoch 30: loss 16.129141475843348
Epoch 31: loss 16.129040022518325
Epoch 32: loss 16.128944021722546
Epoch 33: loss 16.129070488266322
Epoch 34: loss 16.129063892364503
Epoch 35: loss 16.12871600544971
Epoch 36: loss 16.128760705823485
Epoch 37: loss 16.12884475355563
Epoch 38: loss 16.12878680643828
Epoch 39: loss 16.129064942442852
Epoch 40: loss 16.128628669614375
Epoch 41: loss 16.129027857987776
Epoch 42: loss 16.128853525286136
Epoch 43: loss 16.128971223209216
Epoch 44: loss 16.12897878418798
Epoch 45: loss 16.129152289680814
Epoch 46: loss 16.128729017921117
Epoch 47: loss 16.1290268379709
Epoch 48: loss 16.12882933512978
Epoch 49: loss 16.128748545439347
-----------Time: 0:04:02.327387, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0171403884887695-------------


Epoch 0: loss 16.129124709834223
Epoch 1: loss 16.129412503864454
Epoch 2: loss 16.128631802227186
Epoch 3: loss 16.128569648576818
Epoch 4: loss 16.128978152897048
Epoch 5: loss 16.128925061225893
Epoch 6: loss 16.12888230862825
Epoch 7: loss 16.12875102084616
Epoch 8: loss 16.128695445475373
Epoch 9: loss 16.12858259263246
Epoch 10: loss 16.128565414055533
Epoch 11: loss 16.128348637663798
Epoch 12: loss 16.12871935367584
Epoch 13: loss 16.128347896493
Epoch 14: loss 16.12798601648082
Epoch 15: loss 16.12840328734854
Epoch 16: loss 16.127790701907614
Epoch 17: loss 16.127950184241584
Epoch 18: loss 16.127690691533296
Epoch 19: loss 16.12767057833464
Epoch 20: loss 16.12765884399414
Epoch 21: loss 16.127711292971735
Epoch 22: loss 16.127550501408784
Epoch 23: loss 16.127427371688512
Epoch 24: loss 16.12757826888043
Epoch 25: loss 16.127319297583206
Epoch 26: loss 16.12719794874606
Epoch 27: loss 16.127026269746864
Epoch 28: loss 16.127193378365558
Epoch 29: loss 16.126856919993525
Epoch 30: loss 16.126911828828895
Epoch 31: loss 16.127089430974877
Epoch 32: loss 16.126835273659747
Epoch 33: loss 16.127284215844195
Epoch 34: loss 16.126810575568157
Epoch 35: loss 16.126711329169897
Epoch 36: loss 16.126310485342273
Epoch 37: loss 16.126585982156836
Epoch 38: loss 16.126362470958544
Epoch 39: loss 16.12650098696999
Epoch 40: loss 16.126326264505803
Epoch 41: loss 16.12627893530804
Epoch 42: loss 16.126167364742447
Epoch 43: loss 16.125872894992
Epoch 44: loss 16.125587652040565
Epoch 45: loss 16.125433093568553
Epoch 46: loss 16.125266705388608
Epoch 47: loss 16.125853889921437
Epoch 48: loss 16.12579127601955
Epoch 49: loss 16.125044657873072
-----------Time: 0:01:46.830040, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016727924346924-------------


Epoch 0: loss 16.129346438076187
Epoch 1: loss 16.129081946870556
Epoch 2: loss 16.12895925563315
Epoch 3: loss 16.12888596887174
Epoch 4: loss 16.128536869132
Epoch 5: loss 16.128746169546375
Epoch 6: loss 16.12878890659498
Epoch 7: loss 16.128548152550408
Epoch 8: loss 16.12871944282366
Epoch 9: loss 16.128690793203273
Epoch 10: loss 16.128847945254783
Epoch 11: loss 16.1281676945479
Epoch 12: loss 16.12823148603025
Epoch 13: loss 16.12826833932296
Epoch 14: loss 16.128217487749847
Epoch 15: loss 16.128051701835965
Epoch 16: loss 16.1282303550969
Epoch 17: loss 16.12785475357719
Epoch 18: loss 16.12775747568711
Epoch 19: loss 16.12758922369584
Epoch 20: loss 16.127972211008487
Epoch 21: loss 16.12790091348731
Epoch 22: loss 16.12740809606469
Epoch 23: loss 16.127202363636183
Epoch 24: loss 16.127209111918575
Epoch 25: loss 16.127373228902403
Epoch 26: loss 16.127230594469154
Epoch 27: loss 16.126916527748108
Epoch 28: loss 16.127017641067503
Epoch 29: loss 16.127258341208748
Epoch 30: loss 16.12686132762743
Epoch 31: loss 16.127302766882856
Epoch 32: loss 16.126553782172824
Epoch 33: loss 16.126890731894452
Epoch 34: loss 16.126917302090188
Epoch 35: loss 16.126484030226003
Epoch 36: loss 16.126097185715384
Epoch 37: loss 16.126650277428006
Epoch 38: loss 16.125776785352954
Epoch 39: loss 16.12609512391298
Epoch 40: loss 16.126242763063182
Epoch 41: loss 16.125729369080585
Epoch 42: loss 16.12602898556253
Epoch 43: loss 16.12568163353464
Epoch 44: loss 16.125701899113864
Epoch 45: loss 16.125962908371633
Epoch 46: loss 16.1256963159727
Epoch 47: loss 16.125635631188104
Epoch 48: loss 16.12551971829456
Epoch 49: loss 16.12556171209916
-----------Time: 0:02:15.159884, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016714572906494-------------


Epoch 0: loss 16.129385361464127
Epoch 1: loss 16.12905327444491
Epoch 2: loss 16.1290421537731
Epoch 3: loss 16.128924192552983
Epoch 4: loss 16.128868178699328
Epoch 5: loss 16.128871392167135
Epoch 6: loss 16.128790332960047
Epoch 7: loss 16.12876871979755
Epoch 8: loss 16.128687097715297
Epoch 9: loss 16.12820610170779
Epoch 10: loss 16.128162321837053
Epoch 11: loss 16.128139447129293
Epoch 12: loss 16.12837792168493
Epoch 13: loss 16.127805626910664
Epoch 14: loss 16.128060562714285
Epoch 15: loss 16.128015820876413
Epoch 16: loss 16.127787664662236
Epoch 17: loss 16.1279171725978
Epoch 18: loss 16.12750690501669
Epoch 19: loss 16.12772756866787
Epoch 20: loss 16.12734882313272
Epoch 21: loss 16.127682486824366
Epoch 22: loss 16.12729156224624
Epoch 23: loss 16.127726255292476
Epoch 24: loss 16.1272296262824
Epoch 25: loss 16.126858322516732
Epoch 26: loss 16.12744348152824
Epoch 27: loss 16.127177957866504
Epoch 28: loss 16.12725775345512
Epoch 29: loss 16.12679087701051
Epoch 30: loss 16.126632079870806
Epoch 31: loss 16.126718513861945
Epoch 32: loss 16.126506523464037
Epoch 33: loss 16.12650039403335
Epoch 34: loss 16.12656046203945
Epoch 35: loss 16.12653824246448
Epoch 36: loss 16.12609410700591
Epoch 37: loss 16.12587155777475
Epoch 38: loss 16.12639093813689
Epoch 39: loss 16.12617896847103
Epoch 40: loss 16.12616359047268
Epoch 41: loss 16.125956337348274
Epoch 42: loss 16.125704472997914
Epoch 43: loss 16.125745791974275
Epoch 44: loss 16.125878329899
Epoch 45: loss 16.12560935124107
Epoch 46: loss 16.125769475232
Epoch 47: loss 16.125388569417208
Epoch 48: loss 16.125400718398716
Epoch 49: loss 16.125317302994105
-----------Time: 0:02:45.342674, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016713619232178-------------


Epoch 0: loss 16.128917194449382
Epoch 1: loss 16.12876374203226
Epoch 2: loss 16.129231395928755
Epoch 3: loss 16.128807896116506
Epoch 4: loss 16.12910578561866
Epoch 5: loss 16.129181077169335
Epoch 6: loss 16.128245138085408
Epoch 7: loss 16.128450018426648
Epoch 8: loss 16.128149997669716
Epoch 9: loss 16.12853702876879
Epoch 10: loss 16.128411563583043
Epoch 11: loss 16.128423524939496
Epoch 12: loss 16.12816596860471
Epoch 13: loss 16.128376320134038
Epoch 14: loss 16.127795804065208
Epoch 15: loss 16.127873245529507
Epoch 16: loss 16.127643221357594
Epoch 17: loss 16.128144668496173
Epoch 18: loss 16.12752729602482
Epoch 19: loss 16.127780388749166
Epoch 20: loss 16.12739522146142
Epoch 21: loss 16.127492991737697
Epoch 22: loss 16.127758140149325
Epoch 23: loss 16.127526968458426
Epoch 24: loss 16.127373988732046
Epoch 25: loss 16.12729329129924
Epoch 26: loss 16.12731632668039
Epoch 27: loss 16.126857916168543
Epoch 28: loss 16.126841449737547
Epoch 29: loss 16.12724523233331
Epoch 30: loss 16.127115186401035
Epoch 31: loss 16.12657925978951
Epoch 32: loss 16.126556730270387
Epoch 33: loss 16.12630448134049
Epoch 34: loss 16.126703675933506
Epoch 35: loss 16.126652280144068
Epoch 36: loss 16.126402566743934
Epoch 37: loss 16.126116592987724
Epoch 38: loss 16.126129622044772
Epoch 39: loss 16.126196066192957
Epoch 40: loss 16.125760676549827
Epoch 41: loss 16.12578587013742
Epoch 42: loss 16.125938353331193
Epoch 43: loss 16.1259066965269
Epoch 44: loss 16.125981019890826
Epoch 45: loss 16.126163229735003
Epoch 46: loss 16.125366587224214
Epoch 47: loss 16.125634920078774
Epoch 48: loss 16.125412432007167
Epoch 49: loss 16.125387131649514
-----------Time: 0:03:05.401932, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016715049743652-------------


Epoch 0: loss 16.129136742716252
Epoch 1: loss 16.128772328210914
Epoch 2: loss 16.129096405402475
Epoch 3: loss 16.12904505211374
Epoch 4: loss 16.128986076686694
Epoch 5: loss 16.128905148091523
Epoch 6: loss 16.128512291286302
Epoch 7: loss 16.128314995765685
Epoch 8: loss 16.12886025387308
Epoch 9: loss 16.12851870785589
Epoch 10: loss 16.128157538953033
Epoch 11: loss 16.128094137233237
Epoch 12: loss 16.128497664824778
Epoch 13: loss 16.12821401409481
Epoch 14: loss 16.12774933213773
Epoch 15: loss 16.128099275671918
Epoch 16: loss 16.127909442652825
Epoch 17: loss 16.128103579645572
Epoch 18: loss 16.12813172755034
Epoch 19: loss 16.127594028348508
Epoch 20: loss 16.12771377252496
Epoch 21: loss 16.127395366585773
Epoch 22: loss 16.1276440889939
Epoch 23: loss 16.127350403951564
Epoch 24: loss 16.127381981974064
Epoch 25: loss 16.1269749040189
Epoch 26: loss 16.127253143683724
Epoch 27: loss 16.12678837257883
Epoch 28: loss 16.127036789189216
Epoch 29: loss 16.126761960983277
Epoch 30: loss 16.12697026003962
Epoch 31: loss 16.12679455694945
Epoch 32: loss 16.126776984463568
Epoch 33: loss 16.126234899396483
Epoch 34: loss 16.126737669239873
Epoch 35: loss 16.126538943207784
Epoch 36: loss 16.12630986545397
Epoch 37: loss 16.12587901612987
Epoch 38: loss 16.126287940274114
Epoch 39: loss 16.12626391597416
Epoch 40: loss 16.126389025605242
Epoch 41: loss 16.12596802711487
Epoch 42: loss 16.1255949953328
Epoch 43: loss 16.125794499853384
Epoch 44: loss 16.12579440759576
Epoch 45: loss 16.125967676743215
Epoch 46: loss 16.125800901910534
Epoch 47: loss 16.125725562676138
Epoch 48: loss 16.125410010503685
Epoch 49: loss 16.12539470921392
-----------Time: 0:03:58.266223, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016714572906494-------------


Epoch 0: loss 16.128965577871902
Epoch 1: loss 16.12797650254291
Epoch 2: loss 16.127483014438464
Epoch 3: loss 16.12706647955853
Epoch 4: loss 16.125542699772378
Epoch 5: loss 16.12513963450556
Epoch 6: loss 16.12460863797561
Epoch 7: loss 16.123774453868037
Epoch 8: loss 16.122893210079358
Epoch 9: loss 16.121874805118726
Epoch 10: loss 16.121696667049243
Epoch 11: loss 16.120323361521184
Epoch 12: loss 16.119690498061804
Epoch 13: loss 16.11890653423641
Epoch 14: loss 16.118045609930288
Epoch 15: loss 16.117751175424328
Epoch 16: loss 16.116790302940036
Epoch 17: loss 16.116205232039743
Epoch 18: loss 16.11546790599823
Epoch 19: loss 16.11457716382068
Epoch 20: loss 16.11387198800626
Epoch 21: loss 16.112739164932915
Epoch 22: loss 16.112278402369956
Epoch 23: loss 16.111170644345492
Epoch 24: loss 16.110567679612533
Epoch 25: loss 16.109638582105223
Epoch 26: loss 16.10903038356615
Epoch 27: loss 16.108297943032305
Epoch 28: loss 16.107569012434585
Epoch 29: loss 16.1068370072738
Epoch 30: loss 16.10606409466785
Epoch 31: loss 16.10536898633708
Epoch 32: loss 16.104569289995275
Epoch 33: loss 16.103844098422837
Epoch 34: loss 16.103198881771252
Epoch 35: loss 16.101973365700765
Epoch 36: loss 16.101271073714546
Epoch 37: loss 16.10089030162148
Epoch 38: loss 16.0999594066454
Epoch 39: loss 16.09928657801255
Epoch 40: loss 16.098541712760927
Epoch 41: loss 16.097891602308852
Epoch 42: loss 16.097017050826032
Epoch 43: loss 16.09623264229816
Epoch 44: loss 16.09540483744248
Epoch 45: loss 16.094740054918372
Epoch 46: loss 16.093958617293318
Epoch 47: loss 16.093030392605325
Epoch 48: loss 16.092402181418045
Epoch 49: loss 16.091795868458956
-----------Time: 0:02:40.043369, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012479305267334-------------


Epoch 0: loss 16.1288780481919
Epoch 1: loss 16.128160452842714
Epoch 2: loss 16.127105913991514
Epoch 3: loss 16.126686861204064
Epoch 4: loss 16.125606276678003
Epoch 5: loss 16.125474102600762
Epoch 6: loss 16.124424257485764
Epoch 7: loss 16.123395381803096
Epoch 8: loss 16.12306160512178
Epoch 9: loss 16.12191334600034
Epoch 10: loss 16.12136875650157
Epoch 11: loss 16.12033087377963
Epoch 12: loss 16.119807990737584
Epoch 13: loss 16.11880050327467
Epoch 14: loss 16.118387898154882
Epoch 15: loss 16.117003220060596
Epoch 16: loss 16.116593874019124
Epoch 17: loss 16.116256136479585
Epoch 18: loss 16.11510966238768
Epoch 19: loss 16.114089983442554
Epoch 20: loss 16.113557649695355
Epoch 21: loss 16.11293035797451
Epoch 22: loss 16.111894100645316
Epoch 23: loss 16.111350547749062
Epoch 24: loss 16.110157404775205
Epoch 25: loss 16.10959315818289
Epoch 26: loss 16.10898312382076
Epoch 27: loss 16.108183719800866
Epoch 28: loss 16.107312759109167
Epoch 29: loss 16.106998059023983
Epoch 30: loss 16.10607498003089
Epoch 31: loss 16.105314916113148
Epoch 32: loss 16.10416429353797
Epoch 33: loss 16.103703473961872
Epoch 34: loss 16.102943206870037
Epoch 35: loss 16.102345744423243
Epoch 36: loss 16.10121346867603
Epoch 37: loss 16.100698421312416
Epoch 38: loss 16.099934009883714
Epoch 39: loss 16.09891961761143
Epoch 40: loss 16.09821735361348
Epoch 41: loss 16.09758164468019
Epoch 42: loss 16.096855508762857
Epoch 43: loss 16.095754311395726
Epoch 44: loss 16.095314733878425
Epoch 45: loss 16.094598182387973
Epoch 46: loss 16.093680849282638
Epoch 47: loss 16.093069775208182
Epoch 48: loss 16.092162084579467
Epoch 49: loss 16.091711670419443
-----------Time: 0:03:05.288962, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012460708618164-------------


Epoch 0: loss 16.12884216412254
Epoch 1: loss 16.128308634136033
Epoch 2: loss 16.127359541602758
Epoch 3: loss 16.126699253787166
Epoch 4: loss 16.125283184258834
Epoch 5: loss 16.125226746434752
Epoch 6: loss 16.12417233404906
Epoch 7: loss 16.123166032459427
Epoch 8: loss 16.122938442230225
Epoch 9: loss 16.12168270815974
Epoch 10: loss 16.1212236829426
Epoch 11: loss 16.120423831110415
Epoch 12: loss 16.119827616733055
Epoch 13: loss 16.118847370147705
Epoch 14: loss 16.11834538190261
Epoch 15: loss 16.117391813319664
Epoch 16: loss 16.116668756111807
Epoch 17: loss 16.11558536964914
Epoch 18: loss 16.11514997896941
Epoch 19: loss 16.11433536073436
Epoch 20: loss 16.1134430916413
Epoch 21: loss 16.112680451766305
Epoch 22: loss 16.111815441173057
Epoch 23: loss 16.111089436904244
Epoch 24: loss 16.110361674557563
Epoch 25: loss 16.109462432239365
Epoch 26: loss 16.109128739522852
Epoch 27: loss 16.107932763514313
Epoch 28: loss 16.107527260158374
Epoch 29: loss 16.106740428053815
Epoch 30: loss 16.10584129354228
Epoch 31: loss 16.10505919974783
Epoch 32: loss 16.104281853592916
Epoch 33: loss 16.103649142514104
Epoch 34: loss 16.10290161423061
Epoch 35: loss 16.102020205622136
Epoch 36: loss 16.101164992995884
Epoch 37: loss 16.10060447713603
Epoch 38: loss 16.10028890941454
Epoch 39: loss 16.09873130943464
Epoch 40: loss 16.098306500393413
Epoch 41: loss 16.097437328877657
Epoch 42: loss 16.096829612358757
Epoch 43: loss 16.095796196357064
Epoch 44: loss 16.09532330761785
Epoch 45: loss 16.094138577710027
Epoch 46: loss 16.093671657728112
Epoch 47: loss 16.092804621613546
Epoch 48: loss 16.092057699742526
Epoch 49: loss 16.091344411476797
-----------Time: 0:02:34.168374, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012458801269531-------------


Epoch 0: loss 16.12890322726706
Epoch 1: loss 16.128066333480504
Epoch 2: loss 16.12711117060288
Epoch 3: loss 16.126168093474014
Epoch 4: loss 16.12564794395281
Epoch 5: loss 16.12503343457761
Epoch 6: loss 16.12420026115749
Epoch 7: loss 16.12337827682495
Epoch 8: loss 16.12246273185896
Epoch 9: loss 16.12222835291987
Epoch 10: loss 16.12115997231525
Epoch 11: loss 16.12004029750824
Epoch 12: loss 16.11964238933895
Epoch 13: loss 16.118952268102895
Epoch 14: loss 16.11794277585071
Epoch 15: loss 16.1175775476124
Epoch 16: loss 16.116481734358747
Epoch 17: loss 16.116042553860208
Epoch 18: loss 16.115234808299853
Epoch 19: loss 16.113807821273802
Epoch 20: loss 16.113523564131363
Epoch 21: loss 16.112645058009935
Epoch 22: loss 16.11213224141494
Epoch 23: loss 16.111251310680224
Epoch 24: loss 16.110576710493667
Epoch 25: loss 16.109826163623644
Epoch 26: loss 16.109177902470464
Epoch 27: loss 16.108208083069844
Epoch 28: loss 16.107349456911503
Epoch 29: loss 16.10672076370405
Epoch 30: loss 16.105921984755476
Epoch 31: loss 16.105094233803126
Epoch 32: loss 16.104307289745496
Epoch 33: loss 16.103465083371038
Epoch 34: loss 16.10297655126323
Epoch 35: loss 16.101910747652468
Epoch 36: loss 16.101367369942043
Epoch 37: loss 16.100459794376206
Epoch 38: loss 16.099968621005182
Epoch 39: loss 16.098892963450886
Epoch 40: loss 16.097932920248613
Epoch 41: loss 16.09766275779061
Epoch 42: loss 16.09647440288378
Epoch 43: loss 16.096265070334724
Epoch 44: loss 16.09507516881694
Epoch 45: loss 16.094732200581095
Epoch 46: loss 16.093773083064868
Epoch 47: loss 16.093100044001705
Epoch 48: loss 16.09198692985203
Epoch 49: loss 16.091558764291847
-----------Time: 0:02:47.819042, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012455463409424-------------


Epoch 0: loss 16.129049084497534
Epoch 1: loss 16.127800911405814
Epoch 2: loss 16.127466750144958
Epoch 3: loss 16.126356665984446
Epoch 4: loss 16.12578960294309
Epoch 5: loss 16.1251877380454
Epoch 6: loss 16.124173911758092
Epoch 7: loss 16.123415372682654
Epoch 8: loss 16.122521003432897
Epoch 9: loss 16.121788359724956
Epoch 10: loss 16.121236725475477
Epoch 11: loss 16.119993678383207
Epoch 12: loss 16.1195186107055
Epoch 13: loss 16.11901490273683
Epoch 14: loss 16.117875937793567
Epoch 15: loss 16.117371393286664
Epoch 16: loss 16.11649246423141
Epoch 17: loss 16.115567193860592
Epoch 18: loss 16.11456993040831
Epoch 19: loss 16.114273356354754
Epoch 20: loss 16.113849289520928
Epoch 21: loss 16.11294880327971
Epoch 22: loss 16.111922842523327
Epoch 23: loss 16.11137368057085
Epoch 24: loss 16.110500904788143
Epoch 25: loss 16.10945881035017
Epoch 26: loss 16.10886952669724
Epoch 27: loss 16.108262664338817
Epoch 28: loss 16.107194588495336
Epoch 29: loss 16.1066058760104
Epoch 30: loss 16.10562513600225
Epoch 31: loss 16.105516155906347
Epoch 32: loss 16.104501741865406
Epoch 33: loss 16.103725617864857
Epoch 34: loss 16.102774358832317
Epoch 35: loss 16.102106430219568
Epoch 36: loss 16.100862930131996
Epoch 37: loss 16.100705811251764
Epoch 38: loss 16.09977087352587
Epoch 39: loss 16.099007782728776
Epoch 40: loss 16.098357342637104
Epoch 41: loss 16.097595942538717
Epoch 42: loss 16.096657557072845
Epoch 43: loss 16.095716165459674
Epoch 44: loss 16.095220423781353
Epoch 45: loss 16.094280129930247
Epoch 46: loss 16.09365459525067
Epoch 47: loss 16.09278271198273
Epoch 48: loss 16.09210768367933
Epoch 49: loss 16.09132731479147
-----------Time: 0:03:35.437238, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012454509735107-------------


Epoch 0: loss 16.125890921509786
Epoch 1: loss 16.117841824241307
Epoch 2: loss 16.11014694130939
Epoch 3: loss 16.10274253513502
Epoch 4: loss 16.09483985175257
Epoch 5: loss 16.087563716846965
Epoch 6: loss 16.079773283004762
Epoch 7: loss 16.0720220202985
Epoch 8: loss 16.064226118378016
Epoch 9: loss 16.05662924621416
Epoch 10: loss 16.049475914499034
Epoch 11: loss 16.04142436877541
Epoch 12: loss 16.034008382714312
Epoch 13: loss 16.026243850459224
Epoch 14: loss 16.01922291569088
Epoch 15: loss 16.01128048274828
Epoch 16: loss 16.00346170404683
Epoch 17: loss 15.995927513164023
Epoch 18: loss 15.988294997422592
Epoch 19: loss 15.980917119979859
Epoch 20: loss 15.97293815612793
Epoch 21: loss 15.965463738856108
Epoch 22: loss 15.957784654783167
Epoch 23: loss 15.950043530049532
Epoch 24: loss 15.942932830686155
Epoch 25: loss 15.935097771105559
Epoch 26: loss 15.92776361030081
Epoch 27: loss 15.91985248690066
Epoch 28: loss 15.912239894659622
Epoch 29: loss 15.905085146945456
Epoch 30: loss 15.897472509093907
Epoch 31: loss 15.889616101721058
Epoch 32: loss 15.8819414159526
Epoch 33: loss 15.874373644331227
Epoch 34: loss 15.866975054533585
Epoch 35: loss 15.859266064478003
Epoch 36: loss 15.852112868557805
Epoch 37: loss 15.844214955620144
Epoch 38: loss 15.836428843373838
Epoch 39: loss 15.829436363344607
Epoch 40: loss 15.821817696612815
Epoch 41: loss 15.81380217489989
Epoch 42: loss 15.806252920109293
Epoch 43: loss 15.798757790482563
Epoch 44: loss 15.791067025972449
Epoch 45: loss 15.78392914067144
Epoch 46: loss 15.776078229365142
Epoch 47: loss 15.768676514210908
Epoch 48: loss 15.761353993415833
Epoch 49: loss 15.753676669493965
-----------Time: 0:02:15.889737, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.969928503036499-------------


Epoch 0: loss 16.125290460171907
Epoch 1: loss 16.118019581877668
Epoch 2: loss 16.11028078120688
Epoch 3: loss 16.102172959369163
Epoch 4: loss 16.094900399705637
Epoch 5: loss 16.08724878663602
Epoch 6: loss 16.07951854933863
Epoch 7: loss 16.071848805054373
Epoch 8: loss 16.064223568335823
Epoch 9: loss 16.0563540790392
Epoch 10: loss 16.04897853706194
Epoch 11: loss 16.041559613269307
Epoch 12: loss 16.033998210533806
Epoch 13: loss 16.026152120465817
Epoch 14: loss 16.01878633706466
Epoch 15: loss 16.010776393309882
Epoch 16: loss 16.003478813171387
Epoch 17: loss 15.996137589993685
Epoch 18: loss 15.987921780088673
Epoch 19: loss 15.980819344520569
Epoch 20: loss 15.973059396121814
Epoch 21: loss 15.965454224918199
Epoch 22: loss 15.957763560958531
Epoch 23: loss 15.950157358335412
Epoch 24: loss 15.942034124291462
Epoch 25: loss 15.935253506121429
Epoch 26: loss 15.927640434970026
Epoch 27: loss 15.920036834219228
Epoch 28: loss 15.912188979853754
Epoch 29: loss 15.90446581425874
Epoch 30: loss 15.896958501442619
Epoch 31: loss 15.889491171422213
Epoch 32: loss 15.882069292275801
Epoch 33: loss 15.87396087542824
Epoch 34: loss 15.866653468297875
Epoch 35: loss 15.859280759355297
Epoch 36: loss 15.851304823419323
Epoch 37: loss 15.844178985512775
Epoch 38: loss 15.836541422553685
Epoch 39: loss 15.828972243226092
Epoch 40: loss 15.82129187894904
Epoch 41: loss 15.813530858703281
Epoch 42: loss 15.806069336766782
Epoch 43: loss 15.798425098087476
Epoch 44: loss 15.790732543364815
Epoch 45: loss 15.783392672953399
Epoch 46: loss 15.77572772399239
Epoch 47: loss 15.768397192333056
Epoch 48: loss 15.760716274510258
Epoch 49: loss 15.753080999332926
-----------Time: 0:02:41.017265, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.969867467880249-------------


Epoch 0: loss 16.125105561380803
Epoch 1: loss 16.1175339481105
Epoch 2: loss 16.110002009764962
Epoch 3: loss 16.102756355119787
Epoch 4: loss 16.09446918135104
Epoch 5: loss 16.087430272931638
Epoch 6: loss 16.079809502933337
Epoch 7: loss 16.072077089807262
Epoch 8: loss 16.064369529226553
Epoch 9: loss 16.05691637578218
Epoch 10: loss 16.049091272768766
Epoch 11: loss 16.041284462679986
Epoch 12: loss 16.033995863665705
Epoch 13: loss 16.026305523126023
Epoch 14: loss 16.018192137842593
Epoch 15: loss 16.01090729754904
Epoch 16: loss 16.003066511776137
Epoch 17: loss 15.99573514254197
Epoch 18: loss 15.988050702343816
Epoch 19: loss 15.98060048352117
Epoch 20: loss 15.973067647477855
Epoch 21: loss 15.965143773866737
Epoch 22: loss 15.957474662946618
Epoch 23: loss 15.950077529575514
Epoch 24: loss 15.942589715252751
Epoch 25: loss 15.9343643364699
Epoch 26: loss 15.92698189901269
Epoch 27: loss 15.919714187539142
Epoch 28: loss 15.911759565187538
Epoch 29: loss 15.904339972786282
Epoch 30: loss 15.896498277912968
Epoch 31: loss 15.889001135204149
Epoch 32: loss 15.881323807135873
Epoch 33: loss 15.873349286162336
Epoch 34: loss 15.86566426857658
Epoch 35: loss 15.857894199827443
Epoch 36: loss 15.849933358897333
Epoch 37: loss 15.842517988578132
Epoch 38: loss 15.834635128145633
Epoch 39: loss 15.82718752467114
Epoch 40: loss 15.819289776553278
Epoch 41: loss 15.811217074808868
Epoch 42: loss 15.803491102094236
Epoch 43: loss 15.795457885576331
Epoch 44: loss 15.787826307960179
Epoch 45: loss 15.779545416002689
Epoch 46: loss 15.771862261191659
Epoch 47: loss 15.763742188785388
Epoch 48: loss 15.755692706937376
Epoch 49: loss 15.747306727326434
-----------Time: 0:03:05.035071, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.969143867492676-------------


Epoch 0: loss 16.125204372406007
Epoch 1: loss 16.117793571430703
Epoch 2: loss 16.10989801572717
Epoch 3: loss 16.10269511782605
Epoch 4: loss 16.094837921598682
Epoch 5: loss 16.08730355553005
Epoch 6: loss 16.079556725336158
Epoch 7: loss 16.071818038691646
Epoch 8: loss 16.064469532344653
Epoch 9: loss 16.056937556681426
Epoch 10: loss 16.048985747669054
Epoch 11: loss 16.041405733771946
Epoch 12: loss 16.03391946191373
Epoch 13: loss 16.02614328653916
Epoch 14: loss 16.01865084896917
Epoch 15: loss 16.01086948332579
Epoch 16: loss 16.00324433679166
Epoch 17: loss 15.995570591221686
Epoch 18: loss 15.98818099187768
Epoch 19: loss 15.980218626105268
Epoch 20: loss 15.972426391684492
Epoch 21: loss 15.965025592886883
Epoch 22: loss 15.95719744744508
Epoch 23: loss 15.949526518324147
Epoch 24: loss 15.942189141978389
Epoch 25: loss 15.933875493381334
Epoch 26: loss 15.926104842061582
Epoch 27: loss 15.91837275961171
Epoch 28: loss 15.910481668555219
Epoch 29: loss 15.902864116171132
Epoch 30: loss 15.89473110385563
Epoch 31: loss 15.886757542776024
Epoch 32: loss 15.87915741879007
Epoch 33: loss 15.870960074922312
Epoch 34: loss 15.862388557973116
Epoch 35: loss 15.85405379689258
Epoch 36: loss 15.845653475885806
Epoch 37: loss 15.837279654585798
Epoch 38: loss 15.828691359188246
Epoch 39: loss 15.819885710011357
Epoch 40: loss 15.81122682509215
Epoch 41: loss 15.801806274704312
Epoch 42: loss 15.792596893725188
Epoch 43: loss 15.783669831441797
Epoch 44: loss 15.773533765129422
Epoch 45: loss 15.763989459949991
Epoch 46: loss 15.754031028954879
Epoch 47: loss 15.743412173312644
Epoch 48: loss 15.733074628788492
Epoch 49: loss 15.722239289076432
-----------Time: 0:03:40.540254, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.965765953063965-------------


Epoch 0: loss 16.125552678108214
Epoch 1: loss 16.11776860278586
Epoch 2: loss 16.109957090668058
Epoch 3: loss 16.102426614968675
Epoch 4: loss 16.09491676662279
Epoch 5: loss 16.086929789833402
Epoch 6: loss 16.079630152038906
Epoch 7: loss 16.07166631429092
Epoch 8: loss 16.06428016061368
Epoch 9: loss 16.056816132172294
Epoch 10: loss 16.048931509515512
Epoch 11: loss 16.04108649958735
Epoch 12: loss 16.033628707346708
Epoch 13: loss 16.02644677680472
Epoch 14: loss 16.01808373513429
Epoch 15: loss 16.010175280985624
Epoch 16: loss 16.00266148733056
Epoch 17: loss 15.99539335084998
Epoch 18: loss 15.987450533327848
Epoch 19: loss 15.9796581454899
Epoch 20: loss 15.971749374140863
Epoch 21: loss 15.963672880504442
Epoch 22: loss 15.955718474802763
Epoch 23: loss 15.947869308098502
Epoch 24: loss 15.939489676641381
Epoch 25: loss 15.931448966523876
Epoch 26: loss 15.922333806494008
Epoch 27: loss 15.914058668717095
Epoch 28: loss 15.905375160341677
Epoch 29: loss 15.896589495824731
Epoch 30: loss 15.887329117111538
Epoch 31: loss 15.878108172831327
Epoch 32: loss 15.868557632487754
Epoch 33: loss 15.858300282644189
Epoch 34: loss 15.848092703197313
Epoch 35: loss 15.837750735490218
Epoch 36: loss 15.826605732544609
Epoch 37: loss 15.8152413741402
Epoch 38: loss 15.803425529728765
Epoch 39: loss 15.791292403055275
Epoch 40: loss 15.778978910653487
Epoch 41: loss 15.766060350252234
Epoch 42: loss 15.752173073395438
Epoch 43: loss 15.738524119750313
Epoch 44: loss 15.724476534387339
Epoch 45: loss 15.708912267892257
Epoch 46: loss 15.693673234400542
Epoch 47: loss 15.677652266751165
Epoch 48: loss 15.661353478224381
Epoch 49: loss 15.644090433742688
-----------Time: 0:03:36.408585, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.9555201530456543-------------


Epoch 0: loss 16.091551057152127
Epoch 1: loss 16.015424681746442
Epoch 2: loss 15.939053929370383
Epoch 3: loss 15.863667573099551
Epoch 4: loss 15.788099010094353
Epoch 5: loss 15.712829442646193
Epoch 6: loss 15.637606659142868
Epoch 7: loss 15.562557075334633
Epoch 8: loss 15.487936792166337
Epoch 9: loss 15.413271483131076
Epoch 10: loss 15.338889309634332
Epoch 11: loss 15.263803434371948
Epoch 12: loss 15.189922400142835
Epoch 13: loss 15.115506681152013
Epoch 14: loss 15.041488805024521
Epoch 15: loss 14.966585008994393
Epoch 16: loss 14.892099565008412
Epoch 17: loss 14.81778915654058
Epoch 18: loss 14.74282017376112
Epoch 19: loss 14.66779302617778
Epoch 20: loss 14.592257988971213
Epoch 21: loss 14.515846511592036
Epoch 22: loss 14.438675545609515
Epoch 23: loss 14.36119923280633
Epoch 24: loss 14.282744723817576
Epoch 25: loss 14.202884508215863
Epoch 26: loss 14.121498003213302
Epoch 27: loss 14.039124122909877
Epoch 28: loss 13.95474972932235
Epoch 29: loss 13.868601576141689
Epoch 30: loss 13.779940630042034
Epoch 31: loss 13.689485531267913
Epoch 32: loss 13.59634276473004
Epoch 33: loss 13.500827575766522
Epoch 34: loss 13.402062342477882
Epoch 35: loss 13.30077875904415
Epoch 36: loss 13.196763680292213
Epoch 37: loss 13.089410113251727
Epoch 38: loss 12.979137961760811
Epoch 39: loss 12.865899115023405
Epoch 40: loss 12.749124732224837
Epoch 41: loss 12.629122775533926
Epoch 42: loss 12.505620036954465
Epoch 43: loss 12.379024720191955
Epoch 44: loss 12.249173303272412
Epoch 45: loss 12.116171206598697
Epoch 46: loss 11.979273405282393
Epoch 47: loss 11.839093452951182
Epoch 48: loss 11.69639453680619
Epoch 49: loss 11.549769220144853
-----------Time: 0:01:48.674748, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 20, rmse: 3.3917531967163086-------------


Epoch 0: loss 16.091372735603997
Epoch 1: loss 16.015241039317587
Epoch 2: loss 15.93914044836293
Epoch 3: loss 15.863632595020793
Epoch 4: loss 15.787584955795952
Epoch 5: loss 15.712542604363483
Epoch 6: loss 15.636519310785376
Epoch 7: loss 15.560562415744947
Epoch 8: loss 15.484160674136618
Epoch 9: loss 15.40601981204489
Epoch 10: loss 15.32598876953125
Epoch 11: loss 15.242936036897742
Epoch 12: loss 15.155875805149908
Epoch 13: loss 15.06216854945473
Epoch 14: loss 14.960984702732253
Epoch 15: loss 14.850826770326366
Epoch 16: loss 14.72955735662709
Epoch 17: loss 14.597174047387165
Epoch 18: loss 14.45288754546124
Epoch 19: loss 14.29547989472099
Epoch 20: loss 14.125101383872654
Epoch 21: loss 13.941704317797786
Epoch 22: loss 13.74567189320274
Epoch 23: loss 13.537301042805547
Epoch 24: loss 13.316290798394576
Epoch 25: loss 13.083843347300654
Epoch 26: loss 12.839361156588016
Epoch 27: loss 12.58508297049481
Epoch 28: loss 12.319918088291002
Epoch 29: loss 12.045073261468307
Epoch 30: loss 11.760944430724434
Epoch 31: loss 11.46877194798511
Epoch 32: loss 11.168483316380044
Epoch 33: loss 10.860759501871856
Epoch 34: loss 10.546796641142471
Epoch 35: loss 10.226680161641992
Epoch 36: loss 9.901957690197488
Epoch 37: loss 9.57262849289438
Epoch 38: loss 9.24002695394599
Epoch 39: loss 8.904452907520792
Epoch 40: loss 8.566836784196937
Epoch 41: loss 8.22835138414217
Epoch 42: loss 7.889192645446114
Epoch 43: loss 7.551191782951355
Epoch 44: loss 7.214026211137357
Epoch 45: loss 6.879174571451934
Epoch 46: loss 6.547691485674485
Epoch 47: loss 6.220165464670762
Epoch 48: loss 5.897721941056459
Epoch 49: loss 5.581017040688058
-----------Time: 0:02:25.880460, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 50, rmse: 2.3377246856689453-------------


Epoch 0: loss 16.091322111046832
Epoch 1: loss 16.01536690981492
Epoch 2: loss 15.939223278087118
Epoch 3: loss 15.862064548160719
Epoch 4: loss 15.783949014414912
Epoch 5: loss 15.701099172882412
Epoch 6: loss 15.608832205896793
Epoch 7: loss 15.497820041490638
Epoch 8: loss 15.362061355424965
Epoch 9: loss 15.196817053919252
Epoch 10: loss 14.999075512264087
Epoch 11: loss 14.768827257985654
Epoch 12: loss 14.50696323228919
Epoch 13: loss 14.214606961996658
Epoch 14: loss 13.893753485057665
Epoch 15: loss 13.546586627545564
Epoch 16: loss 13.175512232987778
Epoch 17: loss 12.782117437279743
Epoch 18: loss 12.36881815765215
Epoch 19: loss 11.937497291357621
Epoch 20: loss 11.490099123249884
Epoch 21: loss 11.028917895192686
Epoch 22: loss 10.555965938775437
Epoch 23: loss 10.073454290887584
Epoch 24: loss 9.583311951678732
Epoch 25: loss 9.088692788455797
Epoch 26: loss 8.591471714558809
Epoch 27: loss 8.093773451577063
Epoch 28: loss 7.59847268436266
Epoch 29: loss 7.107855332416037
Epoch 30: loss 6.624317792187567
Epoch 31: loss 6.150651177116062
Epoch 32: loss 5.689232867697011
Epoch 33: loss 5.242806602042655
Epoch 34: loss 4.813906974377839
Epoch 35: loss 4.404431495459184
Epoch 36: loss 4.017290934013284
Epoch 37: loss 3.654079491418341
Epoch 38: loss 3.317154512975527
Epoch 39: loss 3.008047427042671
Epoch 40: loss 2.7276102998982306
Epoch 41: loss 2.476848680558412
Epoch 42: loss 2.2557113766670227
Epoch 43: loss 2.06361167145812
Epoch 44: loss 1.8990547028572664
Epoch 45: loss 1.7602677912815758
Epoch 46: loss 1.6445501556862956
Epoch 47: loss 1.5488372638173726
Epoch 48: loss 1.4705628015424894
Epoch 49: loss 1.40621824523677
-----------Time: 0:02:45.415364, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.184414267539978-------------


Epoch 0: loss 16.09139632349429
Epoch 1: loss 16.01490199773208
Epoch 2: loss 15.938218269140824
Epoch 3: loss 15.857122608889703
Epoch 4: loss 15.760711862729943
Epoch 5: loss 15.630042094769685
Epoch 6: loss 15.445799076038858
Epoch 7: loss 15.200874708009803
Epoch 8: loss 14.898240342347519
Epoch 9: loss 14.54395635024361
Epoch 10: loss 14.144758639128312
Epoch 11: loss 13.704344663412675
Epoch 12: loss 13.228650288996489
Epoch 13: loss 12.720916714875594
Epoch 14: loss 12.185116737821827
Epoch 15: loss 11.625187302672344
Epoch 16: loss 11.044517578249392
Epoch 17: loss 10.448486206842505
Epoch 18: loss 9.839984407632247
Epoch 19: loss 9.223952300652213
Epoch 20: loss 8.604608144967452
Epoch 21: loss 7.986272102335225
Epoch 22: loss 7.373810083969779
Epoch 23: loss 6.7718974994576495
Epoch 24: loss 6.184942971105161
Epoch 25: loss 5.6175203281900155
Epoch 26: loss 5.07459767434908
Epoch 27: loss 4.560157421360845
Epoch 28: loss 4.078780513483545
Epoch 29: loss 3.6342338471308997
Epoch 30: loss 3.2292830065540645
Epoch 31: loss 2.8667739956275278
Epoch 32: loss 2.5482540737027706
Epoch 33: loss 2.2740510279717654
Epoch 34: loss 2.0427605279113936
Epoch 35: loss 1.8518638377604277
Epoch 36: loss 1.697033725225407
Epoch 37: loss 1.5735263302274372
Epoch 38: loss 1.475846360170323
Epoch 39: loss 1.3990562051534652
Epoch 40: loss 1.338252429469772
Epoch 41: loss 1.289663814202599
Epoch 42: loss 1.2501760630503944
Epoch 43: loss 1.2173119000766588
Epoch 44: loss 1.189567188465077
Epoch 45: loss 1.165781720954439
Epoch 46: loss 1.1451629865428676
Epoch 47: loss 1.1270714985287709
Epoch 48: loss 1.1112850783311803
Epoch 49: loss 1.0973820566483166
-----------Time: 0:03:11.385228, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0542036294937134-------------


Epoch 0: loss 16.091099916333736
Epoch 1: loss 16.014672071000803
Epoch 2: loss 15.935617291409036
Epoch 3: loss 15.840018960703974
Epoch 4: loss 15.690509389794391
Epoch 5: loss 15.455518253989842
Epoch 6: loss 15.133445735599684
Epoch 7: loss 14.736086927289548
Epoch 8: loss 14.274526800280032
Epoch 9: loss 13.75771353866743
Epoch 10: loss 13.193185651820638
Epoch 11: loss 12.586749400263248
Epoch 12: loss 11.944625292653622
Epoch 13: loss 11.274096530416738
Epoch 14: loss 10.580860827280128
Epoch 15: loss 9.871655045384946
Epoch 16: loss 9.153553504529206
Epoch 17: loss 8.432491115901781
Epoch 18: loss 7.715912984246793
Epoch 19: loss 7.011192953068277
Epoch 20: loss 6.32540480416754
Epoch 21: loss 5.665613301940586
Epoch 22: loss 5.038538008669148
Epoch 23: loss 4.451100235399992
Epoch 24: loss 3.9091162720452184
Epoch 25: loss 3.4181108523970063
Epoch 26: loss 2.98242608256962
Epoch 27: loss 2.604420372195866
Epoch 28: loss 2.2850306792103727
Epoch 29: loss 2.022013059388036
Epoch 30: loss 1.8114097662594006
Epoch 31: loss 1.646250010314195
Epoch 32: loss 1.5193537590296373
Epoch 33: loss 1.4226647903089937
Epoch 34: loss 1.3485510995854502
Epoch 35: loss 1.291469161277232
Epoch 36: loss 1.2463476375393245
Epoch 37: loss 1.2098156843496406
Epoch 38: loss 1.1795473014530928
Epoch 39: loss 1.1540140079415362
Epoch 40: loss 1.1324146507226902
Epoch 41: loss 1.113589208540709
Epoch 42: loss 1.0974808972166932
Epoch 43: loss 1.0834707582126493
Epoch 44: loss 1.0712123434828675
Epoch 45: loss 1.060632911130138
Epoch 46: loss 1.051436938021494
Epoch 47: loss 1.043310989180337
Epoch 48: loss 1.0362998115627662
Epoch 49: loss 1.030104315345702
-----------Time: 0:03:56.208477, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.023127794265747-------------


Epoch 0: loss 15.753977677096492
Epoch 1: loss 15.014377954731817
Epoch 2: loss 14.272448795774709
Epoch 3: loss 13.390867523525072
Epoch 4: loss 12.057833009180815
Epoch 5: loss 10.15495576236559
Epoch 6: loss 7.950478467215662
Epoch 7: loss 5.799743552311607
Epoch 8: loss 3.9769332349300384
Epoch 9: loss 2.645356821236403
Epoch 10: loss 1.8300637821788373
Epoch 11: loss 1.4151980944301772
Epoch 12: loss 1.2260357925425405
Epoch 13: loss 1.13438133098509
Epoch 14: loss 1.0822905603958213
Epoch 15: loss 1.0494355906610904
Epoch 16: loss 1.0280777821722238
Epoch 17: loss 1.013870758206948
Epoch 18: loss 1.004307418089846
Epoch 19: loss 0.9978060649141022
Epoch 20: loss 0.9933031763071599
Epoch 21: loss 0.9900635382403498
Epoch 22: loss 0.9878543360725693
Epoch 23: loss 0.9861648277744003
Epoch 24: loss 0.9849684257870135
Epoch 25: loss 0.9839021782512251
Epoch 26: loss 0.9831675030615019
Epoch 27: loss 0.9826056313903435
Epoch 28: loss 0.9821103907149771
Epoch 29: loss 0.981699596086274
Epoch 30: loss 0.981248600003512
Epoch 31: loss 0.9810137039293414
Epoch 32: loss 0.9807169224257054
Epoch 33: loss 0.9804804874503095
Epoch 34: loss 0.9801282549033994
Epoch 35: loss 0.9799632082814755
Epoch 36: loss 0.9796555554089339
Epoch 37: loss 0.9795789487983869
Epoch 38: loss 0.9792974864011226
Epoch 39: loss 0.9790117575422578
Epoch 40: loss 0.9788054135830506
Epoch 41: loss 0.9785464700149453
Epoch 42: loss 0.9782791639799657
Epoch 43: loss 0.9780482738562252
Epoch 44: loss 0.9778435409716938
Epoch 45: loss 0.9774879561170288
Epoch 46: loss 0.9772370163513266
Epoch 47: loss 0.9770438586888106
Epoch 48: loss 0.9766949814946755
Epoch 49: loss 0.976380908165289
-----------Time: 0:02:49.804669, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9995318651199341-------------


Epoch 0: loss 15.753821173958157
Epoch 1: loss 14.867936542759772
Epoch 2: loss 12.551049148518107
Epoch 3: loss 8.448825423613838
Epoch 4: loss 4.678481324859288
Epoch 5: loss 2.419456489448962
Epoch 6: loss 1.4965174868055011
Epoch 7: loss 1.2041014167277708
Epoch 8: loss 1.0975421148149864
Epoch 9: loss 1.0467689013999442
Epoch 10: loss 1.0199778795890186
Epoch 11: loss 1.005346675735453
Epoch 12: loss 0.9971289703379507
Epoch 13: loss 0.9923923551388408
Epoch 14: loss 0.9896288325605185
Epoch 15: loss 0.9878740922912307
Epoch 16: loss 0.9867476010452146
Epoch 17: loss 0.9860311808793442
Epoch 18: loss 0.9854740734981454
Epoch 19: loss 0.9851495763529902
Epoch 20: loss 0.9847104570787886
Epoch 21: loss 0.9845070528595344
Epoch 22: loss 0.9843319608465485
Epoch 23: loss 0.9841121812229571
Epoch 24: loss 0.9839916662677475
Epoch 25: loss 0.9837584205295729
Epoch 26: loss 0.9835772578483043
Epoch 27: loss 0.9833928778767586
Epoch 28: loss 0.9831480050864427
Epoch 29: loss 0.9830361558691315
Epoch 30: loss 0.9828464492507603
Epoch 31: loss 0.9826594324863476
Epoch 32: loss 0.982397734924503
Epoch 33: loss 0.9822336977590685
Epoch 34: loss 0.9820252884989199
Epoch 35: loss 0.9818389290052911
Epoch 36: loss 0.981504523365394
Epoch 37: loss 0.9812380370238553
Epoch 38: loss 0.9810116461437681
Epoch 39: loss 0.9807057083948799
Epoch 40: loss 0.9804172396659852
Epoch 41: loss 0.9800930080854374
Epoch 42: loss 0.9798241788278456
Epoch 43: loss 0.9794767383000125
Epoch 44: loss 0.9791389850170716
Epoch 45: loss 0.97885494750479
Epoch 46: loss 0.9783173279917758
Epoch 47: loss 0.9779047548123028
Epoch 48: loss 0.977380867431993
Epoch 49: loss 0.976971567519333
-----------Time: 0:02:48.110906, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.998993992805481-------------


Epoch 0: loss 15.707691830137502
Epoch 1: loss 12.405635535198709
Epoch 2: loss 6.0130600986273395
Epoch 3: loss 2.335851213724717
Epoch 4: loss 1.3281540077665577
Epoch 5: loss 1.113631525506144
Epoch 6: loss 1.0428104826937552
Epoch 7: loss 1.0133264959506367
Epoch 8: loss 1.000360103275465
Epoch 9: loss 0.9943399335379186
Epoch 10: loss 0.9912582480389139
Epoch 11: loss 0.9897423713751461
Epoch 12: loss 0.9888829544834469
Epoch 13: loss 0.9883144287311513
Epoch 14: loss 0.987946047147979
Epoch 15: loss 0.987812792736551
Epoch 16: loss 0.9875419527292252
Epoch 17: loss 0.9874103283752566
Epoch 18: loss 0.9873126401849415
Epoch 19: loss 0.9871368395893471
Epoch 20: loss 0.9870025268715361
Epoch 21: loss 0.9867565789948339
Epoch 22: loss 0.9867854425440664
Epoch 23: loss 0.9865991363058919
Epoch 24: loss 0.9864640637584354
Epoch 25: loss 0.986288060442261
Epoch 26: loss 0.986133910326854
Epoch 27: loss 0.985978295168151
Epoch 28: loss 0.9858270154699035
Epoch 29: loss 0.9855975980991902
Epoch 30: loss 0.9854212000318195
Epoch 31: loss 0.9851889511165411
Epoch 32: loss 0.9849669893150744
Epoch 33: loss 0.9847819702132888
Epoch 34: loss 0.9845105940233106
Epoch 35: loss 0.9841823202760324
Epoch 36: loss 0.9839305271273074
Epoch 37: loss 0.9834884345531464
Epoch 38: loss 0.9832165837287903
Epoch 39: loss 0.9827902188119682
Epoch 40: loss 0.9823508431082186
Epoch 41: loss 0.981911164716534
Epoch 42: loss 0.9813988832028016
Epoch 43: loss 0.9808620993857798
Epoch 44: loss 0.9802362075966338
Epoch 45: loss 0.9797767147421836
Epoch 46: loss 0.9790049390948337
Epoch 47: loss 0.9782438521151957
Epoch 48: loss 0.9775037297736043
Epoch 49: loss 0.9765638658533926
-----------Time: 0:02:23.349759, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9984278678894043-------------


Epoch 0: loss 15.472378137837286
Epoch 1: loss 9.408333761795708
Epoch 2: loss 3.082407616273217
Epoch 3: loss 1.3651347952044528
Epoch 4: loss 1.1009975375040717
Epoch 5: loss 1.0316825173471285
Epoch 6: loss 1.006855812020924
Epoch 7: loss 0.9972490176558495
Epoch 8: loss 0.993291540508685
Epoch 9: loss 0.9913975613920585
Epoch 10: loss 0.9905627943251444
Epoch 11: loss 0.9900668384588283
Epoch 12: loss 0.989742172671401
Epoch 13: loss 0.9895342901349068
Epoch 14: loss 0.9894128568794416
Epoch 15: loss 0.9893334027217782
Epoch 16: loss 0.9891616973540057
Epoch 17: loss 0.9890802256439043
Epoch 18: loss 0.9889029099889424
Epoch 19: loss 0.9888386962854344
Epoch 20: loss 0.9887382066120272
Epoch 21: loss 0.9886105620990628
Epoch 22: loss 0.9884921122504318
Epoch 23: loss 0.9883513530959254
Epoch 24: loss 0.9882071839078613
Epoch 25: loss 0.9879757623309674
Epoch 26: loss 0.9878761267532473
Epoch 27: loss 0.98763623399579
Epoch 28: loss 0.9874671300468237
Epoch 29: loss 0.9872730794808139
Epoch 30: loss 0.9870029005019562
Epoch 31: loss 0.9867391989930816
Epoch 32: loss 0.9864863652250041
Epoch 33: loss 0.9861990992141807
Epoch 34: loss 0.9857819069986758
Epoch 35: loss 0.985474194586277
Epoch 36: loss 0.9850153079499369
Epoch 37: loss 0.9846267656139706
Epoch 38: loss 0.984103365112906
Epoch 39: loss 0.9837062829214593
Epoch 40: loss 0.9831379780950753
Epoch 41: loss 0.9825368250193803
Epoch 42: loss 0.9817019704243412
Epoch 43: loss 0.9810650846232538
Epoch 44: loss 0.9801652836410896
Epoch 45: loss 0.9791886605646299
Epoch 46: loss 0.9782648879548778
Epoch 47: loss 0.9770220350960027
Epoch 48: loss 0.9757681374964506
Epoch 49: loss 0.9745025560907695
-----------Time: 0:03:00.640375, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9980441331863403-------------


Epoch 0: loss 15.049498779877373
Epoch 1: loss 7.111617778435997
Epoch 2: loss 1.957315014237943
Epoch 3: loss 1.1644471453583758
Epoch 4: loss 1.0448087847103242
Epoch 5: loss 1.0102694331303887
Epoch 6: loss 0.9985572612156038
Epoch 7: loss 0.9943005463351374
Epoch 8: loss 0.9926507436710855
Epoch 9: loss 0.9918059718349705
Epoch 10: loss 0.9914068406042845
Epoch 11: loss 0.9911973617647005
Epoch 12: loss 0.9909369329395501
Epoch 13: loss 0.9909002377935078
Epoch 14: loss 0.9908480800364329
Epoch 15: loss 0.9906748895411907
Epoch 16: loss 0.9905333550080009
Epoch 17: loss 0.9904603409378425
Epoch 18: loss 0.990368280786535
Epoch 19: loss 0.9902148273328076
Epoch 20: loss 0.9901563111854637
Epoch 21: loss 0.9899982197777085
Epoch 22: loss 0.9898268102951672
Epoch 23: loss 0.989653412349846
Epoch 24: loss 0.989501426530921
Epoch 25: loss 0.9893081716869189
Epoch 26: loss 0.9891962179671163
Epoch 27: loss 0.9889366567134857
Epoch 28: loss 0.98868595627339
Epoch 29: loss 0.9884623049393945
Epoch 30: loss 0.9882701905525249
Epoch 31: loss 0.9878454058714535
Epoch 32: loss 0.9875627736682477
Epoch 33: loss 0.9871796622872353
Epoch 34: loss 0.9867187411888786
Epoch 35: loss 0.9862646949679955
Epoch 36: loss 0.9857436626501705
Epoch 37: loss 0.9852239238827125
Epoch 38: loss 0.9845978477078935
Epoch 39: loss 0.9837556457389955
Epoch 40: loss 0.9830626376297162
Epoch 41: loss 0.9822307550388834
Epoch 42: loss 0.9811916786043541
Epoch 43: loss 0.980123177624267
Epoch 44: loss 0.978950446649738
Epoch 45: loss 0.977573659238608
Epoch 46: loss 0.9760269823281661
Epoch 47: loss 0.9743269714972247
Epoch 48: loss 0.9724744437829308
Epoch 49: loss 0.9704259073604709
-----------Time: 0:03:36.514282, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9974012970924377-------------


Epoch 0: loss 10.030975942767185
Epoch 1: loss 1.1171960865673811
Epoch 2: loss 1.009100341602512
Epoch 3: loss 1.0043235772329828
Epoch 4: loss 1.0023487918402838
Epoch 5: loss 0.9993923790428949
Epoch 6: loss 0.9952933840129686
Epoch 7: loss 0.9898512961423915
Epoch 8: loss 0.9827678975203763
Epoch 9: loss 0.9729661302722019
Epoch 10: loss 0.9617614937217339
Epoch 11: loss 0.9488126993179321
Epoch 12: loss 0.9349549029832301
Epoch 13: loss 0.9200751182177792
Epoch 14: loss 0.9041856163869734
Epoch 15: loss 0.8883389977657277
Epoch 16: loss 0.8723187642252963
Epoch 17: loss 0.8565223961420682
Epoch 18: loss 0.8415149239742238
Epoch 19: loss 0.8273442027361496
Epoch 20: loss 0.814220905692681
Epoch 21: loss 0.8023762019432109
Epoch 22: loss 0.7916241871921913
Epoch 23: loss 0.7821531380648198
Epoch 24: loss 0.77344360863385
Epoch 25: loss 0.7657979904957439
Epoch 26: loss 0.7590962331580079
Epoch 27: loss 0.7529990062765454
Epoch 28: loss 0.7475073967938838
Epoch 29: loss 0.74275053234204
Epoch 30: loss 0.738258915686089
Epoch 31: loss 0.7343935371093128
Epoch 32: loss 0.7307749704174373
Epoch 33: loss 0.7274307745954265
Epoch 34: loss 0.7243967568096907
Epoch 35: loss 0.7217495153779568
Epoch 36: loss 0.7189427760632142
Epoch 37: loss 0.7165250505442204
Epoch 38: loss 0.7143259331583977
Epoch 39: loss 0.7121393435675165
Epoch 40: loss 0.7101189809648887
Epoch 41: loss 0.7083039444425832
Epoch 42: loss 0.7064812075832616
Epoch 43: loss 0.7048348303722298
Epoch 44: loss 0.7032765376826992
Epoch 45: loss 0.7016805749872457
Epoch 46: loss 0.7001889952498933
Epoch 47: loss 0.6988183243767075
Epoch 48: loss 0.6974514141030933
Epoch 49: loss 0.696220760954463
-----------Time: 0:02:17.484423, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.001, embedding_dim: 20, rmse: 1.101578950881958-------------


Epoch 0: loss 6.35718452140041
Epoch 1: loss 1.0259293057996295
Epoch 2: loss 1.0122388877946398
Epoch 3: loss 1.0012465173135634
Epoch 4: loss 0.991797475905522
Epoch 5: loss 0.9836740938217744
Epoch 6: loss 0.9745535105466843
Epoch 7: loss 0.9631570589931114
Epoch 8: loss 0.9472199773658877
Epoch 9: loss 0.9266675999631052
Epoch 10: loss 0.9009563103318214
Epoch 11: loss 0.8705390507112378
Epoch 12: loss 0.834310626530129
Epoch 13: loss 0.7950487597481064
Epoch 14: loss 0.7534038012442381
Epoch 15: loss 0.7123557401740033
Epoch 16: loss 0.6735517904810283
Epoch 17: loss 0.6382667229875274
Epoch 18: loss 0.6074794176480044
Epoch 19: loss 0.5812355374512466
Epoch 20: loss 0.5587311545467895
Epoch 21: loss 0.539915716810071
Epoch 22: loss 0.5236981840885204
Epoch 23: loss 0.5102189753042615
Epoch 24: loss 0.4983722687739393
Epoch 25: loss 0.48796981971548953
Epoch 26: loss 0.4789483777530815
Epoch 27: loss 0.47069397649687267
Epoch 28: loss 0.4635175513184589
Epoch 29: loss 0.45667392125596173
Epoch 30: loss 0.45099740397670995
Epoch 31: loss 0.4453682628986628
Epoch 32: loss 0.4401665798347929
Epoch 33: loss 0.43569643050432205
Epoch 34: loss 0.4314467501057231
Epoch 35: loss 0.42728475584931996
Epoch 36: loss 0.42358527598173723
Epoch 37: loss 0.4200619029286115
Epoch 38: loss 0.41679919235732243
Epoch 39: loss 0.4136314855645532
Epoch 40: loss 0.41068214029073713
Epoch 41: loss 0.40801846161484717
Epoch 42: loss 0.40525927993914357
Epoch 43: loss 0.4026775302446407
Epoch 44: loss 0.4005083368524261
Epoch 45: loss 0.3982306288636249
Epoch 46: loss 0.3961023760230645
Epoch 47: loss 0.39399546239039174
Epoch 48: loss 0.3921389648771804
Epoch 49: loss 0.39018868467082146
-----------Time: 0:02:46.989387, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3154696226119995-------------


Epoch 0: loss 4.84682483083528
Epoch 1: loss 1.0353923585751783
Epoch 2: loss 1.0302261482114377
Epoch 3: loss 1.0205309217390808
Epoch 4: loss 1.0061619021322417
Epoch 5: loss 0.9893412073021349
Epoch 6: loss 0.9665427667939145
Epoch 7: loss 0.93245275694391
Epoch 8: loss 0.8861507119044013
Epoch 9: loss 0.8272917943156284
Epoch 10: loss 0.7572363195859868
Epoch 11: loss 0.679586338284223
Epoch 12: loss 0.6006980985403061
Epoch 13: loss 0.5270081280366234
Epoch 14: loss 0.46302246266733044
Epoch 15: loss 0.40975511741379034
Epoch 16: loss 0.3662933179217836
Epoch 17: loss 0.3311299349302831
Epoch 18: loss 0.3024617355802785
Epoch 19: loss 0.27902539273642973
Epoch 20: loss 0.25965071558628394
Epoch 21: loss 0.24335469153264294
Epoch 22: loss 0.22937013859982075
Epoch 23: loss 0.21739491021827512
Epoch 24: loss 0.20692813033642976
Epoch 25: loss 0.19785557531792183
Epoch 26: loss 0.18960471762263256
Epoch 27: loss 0.18220754894225494
Epoch 28: loss 0.17575626577372136
Epoch 29: loss 0.16979988281331632
Epoch 30: loss 0.16438420336369586
Epoch 31: loss 0.15940573665758836
Epoch 32: loss 0.15493006372581358
Epoch 33: loss 0.1507661655302281
Epoch 34: loss 0.14689599022917124
Epoch 35: loss 0.14327120401127183
Epoch 36: loss 0.13997227287162906
Epoch 37: loss 0.13683054117566865
Epoch 38: loss 0.1339288930611118
Epoch 39: loss 0.13130456329039905
Epoch 40: loss 0.12869399469345807
Epoch 41: loss 0.1263202310256336
Epoch 42: loss 0.1240148882665064
Epoch 43: loss 0.12188332120523504
Epoch 44: loss 0.11984879046516574
Epoch 45: loss 0.11800123926900004
Epoch 46: loss 0.11614767304095237
Epoch 47: loss 0.11447528143129919
Epoch 48: loss 0.11268625874882159
Epoch 49: loss 0.11123680426212756
-----------Time: 0:03:13.496647, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5153837203979492-------------


Epoch 0: loss 4.177530185554339
Epoch 1: loss 1.0432990876876789
Epoch 2: loss 1.0306797047672065
Epoch 3: loss 1.0068168728247933
Epoch 4: loss 0.9804470408221949
Epoch 5: loss 0.9544153617128082
Epoch 6: loss 0.9173519796651343
Epoch 7: loss 0.8602162994768309
Epoch 8: loss 0.7787576678006546
Epoch 9: loss 0.6771013635656108
Epoch 10: loss 0.568091622610455
Epoch 11: loss 0.46564189082254537
Epoch 12: loss 0.378613022390915
Epoch 13: loss 0.3090057089600874
Epoch 14: loss 0.25460933764343674
Epoch 15: loss 0.2127716988325119
Epoch 16: loss 0.18042182383006033
Epoch 17: loss 0.15543905341106912
Epoch 18: loss 0.13579172269319711
Epoch 19: loss 0.11982790967044624
Epoch 20: loss 0.10706807954803757
Epoch 21: loss 0.0965471825197987
Epoch 22: loss 0.0875795229824017
Epoch 23: loss 0.08006091548696809
Epoch 24: loss 0.0737375787175868
Epoch 25: loss 0.06818662061882408
Epoch 26: loss 0.06334112877755062
Epoch 27: loss 0.059086443495977185
Epoch 28: loss 0.05540096300613621
Epoch 29: loss 0.05217142542824149
Epoch 30: loss 0.049187833099099604
Epoch 31: loss 0.04655684513323333
Epoch 32: loss 0.044226562430191296
Epoch 33: loss 0.042143690861437634
Epoch 34: loss 0.040211622860363644
Epoch 35: loss 0.03851351635935514
Epoch 36: loss 0.03681755335394131
Epoch 37: loss 0.03537773942858305
Epoch 38: loss 0.034100301861358075
Epoch 39: loss 0.032867068724463815
Epoch 40: loss 0.03178030895474165
Epoch 41: loss 0.030732616518988558
Epoch 42: loss 0.029790793454436505
Epoch 43: loss 0.028830362050591602
Epoch 44: loss 0.028097628119766065
Epoch 45: loss 0.02731492464511615
Epoch 46: loss 0.026576408589987652
Epoch 47: loss 0.025907236280972544
Epoch 48: loss 0.025241296762681525
Epoch 49: loss 0.024674864686296687
-----------Time: 0:03:41.006686, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4565104246139526-------------


Epoch 0: loss 3.853886251281137
Epoch 1: loss 1.0503146395087242
Epoch 2: loss 1.0325953451187715
Epoch 3: loss 1.0089471379052037
Epoch 4: loss 0.975833423241325
Epoch 5: loss 0.9252718793309254
Epoch 6: loss 0.851928019070107
Epoch 7: loss 0.7517337550287662
Epoch 8: loss 0.6287036963778994
Epoch 9: loss 0.4999269914691863
Epoch 10: loss 0.384474559776161
Epoch 11: loss 0.29074759127005284
Epoch 12: loss 0.21958288053131622
Epoch 13: loss 0.1675828906181066
Epoch 14: loss 0.13013899105236582
Epoch 15: loss 0.10336847523146349
Epoch 16: loss 0.08359019306447843
Epoch 17: loss 0.06917244832881767
Epoch 18: loss 0.05820997021687419
Epoch 19: loss 0.04967987078201511
Epoch 20: loss 0.043225371050041005
Epoch 21: loss 0.03812364616998188
Epoch 22: loss 0.03408344341441989
Epoch 23: loss 0.030796327227321656
Epoch 24: loss 0.028204216682554586
Epoch 25: loss 0.0260994300853623
Epoch 26: loss 0.024343604606616755
Epoch 27: loss 0.022924106689574925
Epoch 28: loss 0.021741571062770873
Epoch 29: loss 0.020855495590797586
Epoch 30: loss 0.02002014279284555
Epoch 31: loss 0.01929487781599164
Epoch 32: loss 0.018728386598598698
Epoch 33: loss 0.01825659586035687
Epoch 34: loss 0.017817314895396324
Epoch 35: loss 0.017471075028626493
Epoch 36: loss 0.01717744004993659
Epoch 37: loss 0.01683358861659856
Epoch 38: loss 0.016592094080482164
Epoch 39: loss 0.016389401530361045
Epoch 40: loss 0.016162297910119854
Epoch 41: loss 0.015948283974000294
Epoch 42: loss 0.015732733052952783
Epoch 43: loss 0.015541503225129259
Epoch 44: loss 0.015369362726240702
Epoch 45: loss 0.015322616310668705
Epoch 46: loss 0.015133259925262436
Epoch 47: loss 0.014971701312891167
Epoch 48: loss 0.01480126887883829
Epoch 49: loss 0.014691998496773126
-----------Time: 0:03:08.936082, Loss: regression, n_iter: 50, l2: 1e-10, batch_size: 1024, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2835272550582886-------------


Epoch 0: loss 16.129332991268324
Epoch 1: loss 16.129066424784455
Epoch 2: loss 16.129435901019885
Epoch 3: loss 16.129139124828836
Epoch 4: loss 16.129235389958257
Epoch 5: loss 16.129336178821067
Epoch 6: loss 16.129249674340954
Epoch 7: loss 16.12940435513206
Epoch 8: loss 16.129597292775692
Epoch 9: loss 16.129491394499073
Epoch 10: loss 16.12911658598029
Epoch 11: loss 16.129476170954497
Epoch 12: loss 16.129288412177043
Epoch 13: loss 16.129348648112753
Epoch 14: loss 16.12897950048032
Epoch 15: loss 16.12931595677915
Epoch 16: loss 16.12921433241471
Epoch 17: loss 16.12936382190041
Epoch 18: loss 16.12897585785907
Epoch 19: loss 16.129648308132005
Epoch 20: loss 16.129060582492663
Epoch 21: loss 16.12930372590604
Epoch 22: loss 16.129277906210525
Epoch 23: loss 16.12945159725521
Epoch 24: loss 16.129256106459575
Epoch 25: loss 16.12890100997427
Epoch 26: loss 16.129340571942535
Epoch 27: loss 16.129349105254462
Epoch 28: loss 16.128855503123738
Epoch 29: loss 16.129266095161437
Epoch 30: loss 16.129456905696703
Epoch 31: loss 16.129136649422023
Epoch 32: loss 16.12957630675772
Epoch 33: loss 16.12919623748116
Epoch 34: loss 16.12895605045816
Epoch 35: loss 16.12921459882156
Epoch 36: loss 16.12928032149439
Epoch 37: loss 16.129530658929244
Epoch 38: loss 16.129177359912706
Epoch 39: loss 16.129303426327912
Epoch 40: loss 16.129459043171096
Epoch 41: loss 16.1293190251226
Epoch 42: loss 16.12898453422215
Epoch 43: loss 16.12925985066787
Epoch 44: loss 16.129606174386065
Epoch 45: loss 16.129049748959748
Epoch 46: loss 16.12936527521714
Epoch 47: loss 16.129217307463936
Epoch 48: loss 16.12888071640678
Epoch 49: loss 16.12927551580512
-----------Time: 0:01:52.751798, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017210483551025-------------


Epoch 0: loss 16.12900402234948
Epoch 1: loss 16.129132140201072
Epoch 2: loss 16.129267848056294
Epoch 3: loss 16.129068430610324
Epoch 4: loss 16.129145136086837
Epoch 5: loss 16.129429906347525
Epoch 6: loss 16.129095591669497
Epoch 7: loss 16.1290851344233
Epoch 8: loss 16.12927269106326
Epoch 9: loss 16.129434505752894
Epoch 10: loss 16.129427086788674
Epoch 11: loss 16.12894117002902
Epoch 12: loss 16.129120066891545
Epoch 13: loss 16.12943447361822
Epoch 14: loss 16.12893046192501
Epoch 15: loss 16.129277378579843
Epoch 16: loss 16.129080078912818
Epoch 17: loss 16.12927841829217
Epoch 18: loss 16.12912602528282
Epoch 19: loss 16.129429939518804
Epoch 20: loss 16.12939868491629
Epoch 21: loss 16.129492193719614
Epoch 22: loss 16.12915992114855
Epoch 23: loss 16.128803085244222
Epoch 24: loss 16.129419329892034
Epoch 25: loss 16.129003776674686
Epoch 26: loss 16.128956709737363
Epoch 27: loss 16.128971610898557
Epoch 28: loss 16.129168808978537
Epoch 29: loss 16.129402973340905
Epoch 30: loss 16.129154439594434
Epoch 31: loss 16.1289751280909
Epoch 32: loss 16.129090052065642
Epoch 33: loss 16.128933917957802
Epoch 34: loss 16.129358784012172
Epoch 35: loss 16.12895411512126
Epoch 36: loss 16.129480952801913
Epoch 37: loss 16.129176549289536
Epoch 38: loss 16.12928150944088
Epoch 39: loss 16.129134674694228
Epoch 40: loss 16.129208895434505
Epoch 41: loss 16.12896242245384
Epoch 42: loss 16.12910487237184
Epoch 43: loss 16.129019881331402
Epoch 44: loss 16.129076623916625
Epoch 45: loss 16.12908008824224
Epoch 46: loss 16.129168681476425
Epoch 47: loss 16.129029865886853
Epoch 48: loss 16.129024892267974
Epoch 49: loss 16.1291697408842
-----------Time: 0:02:27.230012, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.0171918869018555-------------


Epoch 0: loss 16.129110560209856
Epoch 1: loss 16.129208022615185
Epoch 2: loss 16.129333031695822
Epoch 3: loss 16.12921850784965
Epoch 4: loss 16.129376943215078
Epoch 5: loss 16.12877326218978
Epoch 6: loss 16.129140951322473
Epoch 7: loss 16.12911619932755
Epoch 8: loss 16.12904124052628
Epoch 9: loss 16.129136397527613
Epoch 10: loss 16.129238948614702
Epoch 11: loss 16.12891140398772
Epoch 12: loss 16.129368211912073
Epoch 13: loss 16.129516434669494
Epoch 14: loss 16.128973918375763
Epoch 15: loss 16.129127542868904
Epoch 16: loss 16.12952429211658
Epoch 17: loss 16.129480817006982
Epoch 18: loss 16.12929768976958
Epoch 19: loss 16.129338673923325
Epoch 20: loss 16.12946718050086
Epoch 21: loss 16.129108735789423
Epoch 22: loss 16.129309925825698
Epoch 23: loss 16.12914961213651
Epoch 24: loss 16.129091181962387
Epoch 25: loss 16.129318875851837
Epoch 26: loss 16.129213858687358
Epoch 27: loss 16.12941925940306
Epoch 28: loss 16.12909581868545
Epoch 29: loss 16.129175678543422
Epoch 30: loss 16.129354012530783
Epoch 31: loss 16.12925054197726
Epoch 32: loss 16.129329071874203
Epoch 33: loss 16.12916058353756
Epoch 34: loss 16.12931923451631
Epoch 35: loss 16.12906464493793
Epoch 36: loss 16.129214042166005
Epoch 37: loss 16.129057393903317
Epoch 38: loss 16.129418826103212
Epoch 39: loss 16.128875814313474
Epoch 40: loss 16.12860880623693
Epoch 41: loss 16.129247374119966
Epoch 42: loss 16.12876904218093
Epoch 43: loss 16.128888148846833
Epoch 44: loss 16.12902566453685
Epoch 45: loss 16.129122265525485
Epoch 46: loss 16.129096006310505
Epoch 47: loss 16.12925361654033
Epoch 48: loss 16.129393537148186
Epoch 49: loss 16.12932236816572
-----------Time: 0:02:46.120566, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.12910870572795
Epoch 1: loss 16.12896949622942
Epoch 2: loss 16.129184662777444
Epoch 3: loss 16.129288466080375
Epoch 4: loss 16.12904916535253
Epoch 5: loss 16.12938621147819
Epoch 6: loss 16.12923605442047
Epoch 7: loss 16.12900185999663
Epoch 8: loss 16.129064518472422
Epoch 9: loss 16.129103034475577
Epoch 10: loss 16.129151410641878
Epoch 11: loss 16.12918659085813
Epoch 12: loss 16.12920655063961
Epoch 13: loss 16.12929838222006
Epoch 14: loss 16.129013928123143
Epoch 15: loss 16.129159549008246
Epoch 16: loss 16.1290975684705
Epoch 17: loss 16.129139514591383
Epoch 18: loss 16.12911690214406
Epoch 19: loss 16.12910385131836
Epoch 20: loss 16.12932792124541
Epoch 21: loss 16.128883315169293
Epoch 22: loss 16.129279626970707
Epoch 23: loss 16.129441468612008
Epoch 24: loss 16.129136266915694
Epoch 25: loss 16.12897393703461
Epoch 26: loss 16.129159816451693
Epoch 27: loss 16.12924307429272
Epoch 28: loss 16.129071118520653
Epoch 29: loss 16.12890001172605
Epoch 30: loss 16.128921188478884
Epoch 31: loss 16.12921373118525
Epoch 32: loss 16.12905597790428
Epoch 33: loss 16.12924605037855
Epoch 34: loss 16.129329321695412
Epoch 35: loss 16.129335223073543
Epoch 36: loss 16.12918585901675
Epoch 37: loss 16.129323728188226
Epoch 38: loss 16.129297924041747
Epoch 39: loss 16.12919902490533
Epoch 40: loss 16.129151450032772
Epoch 41: loss 16.129159310589667
Epoch 42: loss 16.129548156779745
Epoch 43: loss 16.12918403148651
Epoch 44: loss 16.129455004567685
Epoch 45: loss 16.129168259579203
Epoch 46: loss 16.129096669736114
Epoch 47: loss 16.129243018316185
Epoch 48: loss 16.129193127673606
Epoch 49: loss 16.129057222863903
-----------Time: 0:03:18.922718, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017183303833008-------------


Epoch 0: loss 16.12920567885689
Epoch 1: loss 16.129211491087208
Epoch 2: loss 16.129608756562938
Epoch 3: loss 16.12943904192551
Epoch 4: loss 16.129483132777008
Epoch 5: loss 16.128871322714765
Epoch 6: loss 16.128868355958357
Epoch 7: loss 16.129236265887386
Epoch 8: loss 16.128998710798182
Epoch 9: loss 16.12901121740756
Epoch 10: loss 16.129055075023484
Epoch 11: loss 16.12959121828494
Epoch 12: loss 16.129075395542642
Epoch 13: loss 16.12937140983084
Epoch 14: loss 16.129130644383636
Epoch 15: loss 16.129148479129956
Epoch 16: loss 16.129258959189706
Epoch 17: loss 16.1293650938117
Epoch 18: loss 16.129237824937572
Epoch 19: loss 16.12917492493339
Epoch 20: loss 16.129149992569634
Epoch 21: loss 16.1294209635776
Epoch 22: loss 16.12899146390998
Epoch 23: loss 16.128964542305987
Epoch 24: loss 16.129375080440354
Epoch 25: loss 16.129371015921883
Epoch 26: loss 16.12931895048722
Epoch 27: loss 16.129034667429718
Epoch 28: loss 16.12905896746594
Epoch 29: loss 16.129108270354894
Epoch 30: loss 16.12900173145792
Epoch 31: loss 16.1292343782342
Epoch 32: loss 16.129383104780445
Epoch 33: loss 16.12903112846872
Epoch 34: loss 16.1294525820276
Epoch 35: loss 16.12948671631191
Epoch 36: loss 16.12923819085826
Epoch 37: loss 16.128862310492472
Epoch 38: loss 16.128889904851498
Epoch 39: loss 16.129266592730648
Epoch 40: loss 16.129259357245072
Epoch 41: loss 16.12913951251818
Epoch 42: loss 16.129149622502535
Epoch 43: loss 16.12899954629981
Epoch 44: loss 16.128935507069464
Epoch 45: loss 16.128939113409622
Epoch 46: loss 16.12915108514869
Epoch 47: loss 16.128801213140072
Epoch 48: loss 16.129367775502413
Epoch 49: loss 16.1290855469911
-----------Time: 0:04:04.566334, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017183780670166-------------


Epoch 0: loss 16.12928729368293
Epoch 1: loss 16.129509712302166
Epoch 2: loss 16.129239267888277
Epoch 3: loss 16.129538092405898
Epoch 4: loss 16.1295530785685
Epoch 5: loss 16.129373490292092
Epoch 6: loss 16.12933929692144
Epoch 7: loss 16.129177689552307
Epoch 8: loss 16.129658490678537
Epoch 9: loss 16.129486698689668
Epoch 10: loss 16.129333186149598
Epoch 11: loss 16.12917324771052
Epoch 12: loss 16.129565830852673
Epoch 13: loss 16.128777248963065
Epoch 14: loss 16.129073024832685
Epoch 15: loss 16.129238191894864
Epoch 16: loss 16.129450253818344
Epoch 17: loss 16.129155979985775
Epoch 18: loss 16.12946767496026
Epoch 19: loss 16.129076863371807
Epoch 20: loss 16.128981539477472
Epoch 21: loss 16.129419174401658
Epoch 22: loss 16.12911720275879
Epoch 23: loss 16.12918693397356
Epoch 24: loss 16.129131373115207
Epoch 25: loss 16.129325584743334
Epoch 26: loss 16.12913458969282
Epoch 27: loss 16.129110972777656
Epoch 28: loss 16.12926633876303
Epoch 29: loss 16.12898845050646
Epoch 30: loss 16.129337698480356
Epoch 31: loss 16.128996303807135
Epoch 32: loss 16.129116880375406
Epoch 33: loss 16.12867646631987
Epoch 34: loss 16.128878938633463
Epoch 35: loss 16.129162174722424
Epoch 36: loss 16.129115603281104
Epoch 37: loss 16.129132877225462
Epoch 38: loss 16.129046124997345
Epoch 39: loss 16.1291805692341
Epoch 40: loss 16.129272775028063
Epoch 41: loss 16.128924898479294
Epoch 42: loss 16.12876375136168
Epoch 43: loss 16.12936236443727
Epoch 44: loss 16.1292451236559
Epoch 45: loss 16.128769426760467
Epoch 46: loss 16.12857542556265
Epoch 47: loss 16.12911279305168
Epoch 48: loss 16.128717316751896
Epoch 49: loss 16.12924971373185
-----------Time: 0:02:52.645549, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.01715087890625-------------


Epoch 0: loss 16.12880188693171
Epoch 1: loss 16.129029804727306
Epoch 2: loss 16.129243120939837
Epoch 3: loss 16.128930115699767
Epoch 4: loss 16.128779277594194
Epoch 5: loss 16.129093112116276
Epoch 6: loss 16.129109275859335
Epoch 7: loss 16.129261952897778
Epoch 8: loss 16.12919919076173
Epoch 9: loss 16.129238488363182
Epoch 10: loss 16.129114724242168
Epoch 11: loss 16.12904976243558
Epoch 12: loss 16.129064479081528
Epoch 13: loss 16.128983937139097
Epoch 14: loss 16.129103375517804
Epoch 15: loss 16.128980608608412
Epoch 16: loss 16.128966794843258
Epoch 17: loss 16.128877572391342
Epoch 18: loss 16.129229287479234
Epoch 19: loss 16.12914896115013
Epoch 20: loss 16.129212045669554
Epoch 21: loss 16.128997510412464
Epoch 22: loss 16.129100242904993
Epoch 23: loss 16.128949401689614
Epoch 24: loss 16.12927316582721
Epoch 25: loss 16.128640876645626
Epoch 26: loss 16.12861935366755
Epoch 27: loss 16.12898377024609
Epoch 28: loss 16.128939229509104
Epoch 29: loss 16.128531111841617
Epoch 30: loss 16.129140042222065
Epoch 31: loss 16.129048253142315
Epoch 32: loss 16.128972720063253
Epoch 33: loss 16.128750786573992
Epoch 34: loss 16.129365073079647
Epoch 35: loss 16.12904149138409
Epoch 36: loss 16.128929825451063
Epoch 37: loss 16.12904579743095
Epoch 38: loss 16.128960408335146
Epoch 39: loss 16.128932431469792
Epoch 40: loss 16.129217291914898
Epoch 41: loss 16.129009496647377
Epoch 42: loss 16.1287261299465
Epoch 43: loss 16.12878773316093
Epoch 44: loss 16.12911764849787
Epoch 45: loss 16.12890492003897
Epoch 46: loss 16.12899198843085
Epoch 47: loss 16.128840308604033
Epoch 48: loss 16.1293016920919
Epoch 49: loss 16.128651357733684
-----------Time: 0:02:19.533778, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017141819000244-------------


Epoch 0: loss 16.12923159495644
Epoch 1: loss 16.129080448979916
Epoch 2: loss 16.129153471407683
Epoch 3: loss 16.12915109862452
Epoch 4: loss 16.129327953380088
Epoch 5: loss 16.129165272090745
Epoch 6: loss 16.12914129962092
Epoch 7: loss 16.128879787610924
Epoch 8: loss 16.128857799198318
Epoch 9: loss 16.129095791733782
Epoch 10: loss 16.12909194075543
Epoch 11: loss 16.128964994264685
Epoch 12: loss 16.128622121396273
Epoch 13: loss 16.129237723350524
Epoch 14: loss 16.128874089406885
Epoch 15: loss 16.128964577550473
Epoch 16: loss 16.128899067381155
Epoch 17: loss 16.128999511055323
Epoch 18: loss 16.12865817650505
Epoch 19: loss 16.128877153603927
Epoch 20: loss 16.12911765160768
Epoch 21: loss 16.129069179037344
Epoch 22: loss 16.128939592319988
Epoch 23: loss 16.128964636636816
Epoch 24: loss 16.12916217886883
Epoch 25: loss 16.128901591508285
Epoch 26: loss 16.128643952245298
Epoch 27: loss 16.129065063725346
Epoch 28: loss 16.129168891906737
Epoch 29: loss 16.12912376134292
Epoch 30: loss 16.1288248145062
Epoch 31: loss 16.128889355452163
Epoch 32: loss 16.12885608051134
Epoch 33: loss 16.128803179575048
Epoch 34: loss 16.128923069912453
Epoch 35: loss 16.128239291647205
Epoch 36: loss 16.12903442693793
Epoch 37: loss 16.128846028576728
Epoch 38: loss 16.12894600888957
Epoch 39: loss 16.12909030603326
Epoch 40: loss 16.129275580074477
Epoch 41: loss 16.12901288944742
Epoch 42: loss 16.128634299402652
Epoch 43: loss 16.128994284505428
Epoch 44: loss 16.12854770577472
Epoch 45: loss 16.1288904822391
Epoch 46: loss 16.12921656007352
Epoch 47: loss 16.128772451566615
Epoch 48: loss 16.129080176353455
Epoch 49: loss 16.128987468843874
-----------Time: 0:02:28.925805, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0171427726745605-------------


Epoch 0: loss 16.129235638742863
Epoch 1: loss 16.129219216885772
Epoch 2: loss 16.1292847301649
Epoch 3: loss 16.12929774781932
Epoch 4: loss 16.12916687882465
Epoch 5: loss 16.12941978081413
Epoch 6: loss 16.12897579566292
Epoch 7: loss 16.129066590640857
Epoch 8: loss 16.12920812834864
Epoch 9: loss 16.129060045532558
Epoch 10: loss 16.129122009484664
Epoch 11: loss 16.128756740818854
Epoch 12: loss 16.1294308859369
Epoch 13: loss 16.128980246834132
Epoch 14: loss 16.1290130884751
Epoch 15: loss 16.12916450811469
Epoch 16: loss 16.128798269188923
Epoch 17: loss 16.129219159872637
Epoch 18: loss 16.12898901856464
Epoch 19: loss 16.12892780200295
Epoch 20: loss 16.128938781696817
Epoch 21: loss 16.128725910186766
Epoch 22: loss 16.129051394047945
Epoch 23: loss 16.12896717009337
Epoch 24: loss 16.129118802236473
Epoch 25: loss 16.12934696674347
Epoch 26: loss 16.1290523228438
Epoch 27: loss 16.128961919701617
Epoch 28: loss 16.12891257016555
Epoch 29: loss 16.12919842471247
Epoch 30: loss 16.129221399970678
Epoch 31: loss 16.12895518178525
Epoch 32: loss 16.12887232199959
Epoch 33: loss 16.12915797337242
Epoch 34: loss 16.129065522940262
Epoch 35: loss 16.12892478963603
Epoch 36: loss 16.128757416683694
Epoch 37: loss 16.128622366034467
Epoch 38: loss 16.12880240212316
Epoch 39: loss 16.12886992952098
Epoch 40: loss 16.128777376465177
Epoch 41: loss 16.1287458077721
Epoch 42: loss 16.12905995534814
Epoch 43: loss 16.128738903999327
Epoch 44: loss 16.12837246293607
Epoch 45: loss 16.128742404606033
Epoch 46: loss 16.12908601657204
Epoch 47: loss 16.12855699373328
Epoch 48: loss 16.128985580154087
Epoch 49: loss 16.128822927889615
-----------Time: 0:03:00.110955, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017141819000244-------------


Epoch 0: loss 16.129138651101485
Epoch 1: loss 16.129436412064926
Epoch 2: loss 16.12918002501778
Epoch 3: loss 16.128764168075893
Epoch 4: loss 16.128844925631647
Epoch 5: loss 16.129072876598524
Epoch 6: loss 16.128891763479814
Epoch 7: loss 16.12921911218892
Epoch 8: loss 16.129112244688947
Epoch 9: loss 16.129207233760667
Epoch 10: loss 16.129449290814605
Epoch 11: loss 16.129322657377823
Epoch 12: loss 16.12925630134085
Epoch 13: loss 16.12870530149211
Epoch 14: loss 16.129428711144822
Epoch 15: loss 16.129195121060246
Epoch 16: loss 16.12880115820014
Epoch 17: loss 16.129260616717133
Epoch 18: loss 16.12889409687208
Epoch 19: loss 16.129056473400283
Epoch 20: loss 16.128984446110934
Epoch 21: loss 16.129247429059898
Epoch 22: loss 16.12909256271694
Epoch 23: loss 16.129093544379526
Epoch 24: loss 16.128989892420563
Epoch 25: loss 16.128615639520728
Epoch 26: loss 16.12876402191494
Epoch 27: loss 16.12892989904984
Epoch 28: loss 16.128957122305167
Epoch 29: loss 16.12887981041618
Epoch 30: loss 16.129027981343476
Epoch 31: loss 16.128969437143077
Epoch 32: loss 16.12896482426187
Epoch 33: loss 16.128850025716034
Epoch 34: loss 16.129214548028035
Epoch 35: loss 16.12912527270939
Epoch 36: loss 16.12876955529918
Epoch 37: loss 16.128681181824724
Epoch 38: loss 16.128820823586505
Epoch 39: loss 16.12902589880902
Epoch 40: loss 16.128904856806216
Epoch 41: loss 16.128934185401253
Epoch 42: loss 16.12881546124168
Epoch 43: loss 16.129109790014184
Epoch 44: loss 16.12876088100931
Epoch 45: loss 16.128541437439296
Epoch 46: loss 16.129050978370334
Epoch 47: loss 16.129085751201796
Epoch 48: loss 16.129367476960887
Epoch 49: loss 16.128761870964713
-----------Time: 0:03:32.995407, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.0171403884887695-------------


Epoch 0: loss 16.12905858392301
Epoch 1: loss 16.12956193841022
Epoch 2: loss 16.128999647886857
Epoch 3: loss 16.129365859860957
Epoch 4: loss 16.129092014354207
Epoch 5: loss 16.128743955363397
Epoch 6: loss 16.129192863339963
Epoch 7: loss 16.128917346829954
Epoch 8: loss 16.12876993884211
Epoch 9: loss 16.128732009555982
Epoch 10: loss 16.128530297072036
Epoch 11: loss 16.128585525180984
Epoch 12: loss 16.128559051389278
Epoch 13: loss 16.128394076098566
Epoch 14: loss 16.128323851460994
Epoch 15: loss 16.128294221214627
Epoch 16: loss 16.12817051099694
Epoch 17: loss 16.128017398585445
Epoch 18: loss 16.127927593562912
Epoch 19: loss 16.128174037518708
Epoch 20: loss 16.12792587902235
Epoch 21: loss 16.127830081400663
Epoch 22: loss 16.127608022482498
Epoch 23: loss 16.127915337811345
Epoch 24: loss 16.127508195586827
Epoch 25: loss 16.127402561643848
Epoch 26: loss 16.127529713381893
Epoch 27: loss 16.12760883103246
Epoch 28: loss 16.127091772659966
Epoch 29: loss 16.12723035397737
Epoch 30: loss 16.126969666066376
Epoch 31: loss 16.127092279558596
Epoch 32: loss 16.127015293162803
Epoch 33: loss 16.12663451588672
Epoch 34: loss 16.12682554203531
Epoch 35: loss 16.12650591497836
Epoch 36: loss 16.126678574603538
Epoch 37: loss 16.126604101968848
Epoch 38: loss 16.12619278119958
Epoch 39: loss 16.126372657651487
Epoch 40: loss 16.126059661740843
Epoch 41: loss 16.126206629172614
Epoch 42: loss 16.126189017295836
Epoch 43: loss 16.126289406030075
Epoch 44: loss 16.126297991172127
Epoch 45: loss 16.125966653616533
Epoch 46: loss 16.126221679604573
Epoch 47: loss 16.125819600146748
Epoch 48: loss 16.126029475875523
Epoch 49: loss 16.125590208302373
-----------Time: 0:02:23.634374, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016791820526123-------------


Epoch 0: loss 16.129071675176206
Epoch 1: loss 16.128975101139236
Epoch 2: loss 16.128849985288536
Epoch 3: loss 16.12899982618249
Epoch 4: loss 16.129143353130505
Epoch 5: loss 16.12883661000625
Epoch 6: loss 16.12865813607755
Epoch 7: loss 16.128705023682635
Epoch 8: loss 16.128417559292007
Epoch 9: loss 16.128470021745432
Epoch 10: loss 16.12825211753016
Epoch 11: loss 16.12858572524527
Epoch 12: loss 16.12823951555335
Epoch 13: loss 16.127934415444084
Epoch 14: loss 16.12822447652402
Epoch 15: loss 16.128030166418657
Epoch 16: loss 16.127924411193185
Epoch 17: loss 16.127840979202933
Epoch 18: loss 16.127644177105115
Epoch 19: loss 16.127597921827565
Epoch 20: loss 16.127603371247
Epoch 21: loss 16.127282557280168
Epoch 22: loss 16.127324098089467
Epoch 23: loss 16.12743342648382
Epoch 24: loss 16.127216209536012
Epoch 25: loss 16.127078191093776
Epoch 26: loss 16.12722672275875
Epoch 27: loss 16.127014018141704
Epoch 28: loss 16.126975239878117
Epoch 29: loss 16.126818355270053
Epoch 30: loss 16.126736252204232
Epoch 31: loss 16.126673164574996
Epoch 32: loss 16.126915357423865
Epoch 33: loss 16.12671365634255
Epoch 34: loss 16.126677076712898
Epoch 35: loss 16.126474669705267
Epoch 36: loss 16.126266404856807
Epoch 37: loss 16.125993649855904
Epoch 38: loss 16.126345729827882
Epoch 39: loss 16.126339893755706
Epoch 40: loss 16.126112007058186
Epoch 41: loss 16.125947563544564
Epoch 42: loss 16.125949540345566
Epoch 43: loss 16.125775730091593
Epoch 44: loss 16.1254658833794
Epoch 45: loss 16.125881747577502
Epoch 46: loss 16.125645714220795
Epoch 47: loss 16.12556448397429
Epoch 48: loss 16.125319681996885
Epoch 49: loss 16.125235130475915
-----------Time: 0:02:46.821301, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016726016998291-------------


Epoch 0: loss 16.129246868257937
Epoch 1: loss 16.12903823852539
Epoch 2: loss 16.12912549972534
Epoch 3: loss 16.128646878574205
Epoch 4: loss 16.12903615495433
Epoch 5: loss 16.12878406877103
Epoch 6: loss 16.1288332949514
Epoch 7: loss 16.128571224212646
Epoch 8: loss 16.128690202339836
Epoch 9: loss 16.12814800635628
Epoch 10: loss 16.1285808791285
Epoch 11: loss 16.12827016374339
Epoch 12: loss 16.128389925542084
Epoch 13: loss 16.12798821200495
Epoch 14: loss 16.128159711671913
Epoch 15: loss 16.128233662895536
Epoch 16: loss 16.127821443391884
Epoch 17: loss 16.12786055440488
Epoch 18: loss 16.127796036264172
Epoch 19: loss 16.12754865107329
Epoch 20: loss 16.12766296552575
Epoch 21: loss 16.127516474931138
Epoch 22: loss 16.127663140711576
Epoch 23: loss 16.127328025776407
Epoch 24: loss 16.127356082460153
Epoch 25: loss 16.127150749123615
Epoch 26: loss 16.127074436519457
Epoch 27: loss 16.12682261052339
Epoch 28: loss 16.12731325522713
Epoch 29: loss 16.12649297092272
Epoch 30: loss 16.126737110511115
Epoch 31: loss 16.12693021401115
Epoch 32: loss 16.12684273927108
Epoch 33: loss 16.126499920306
Epoch 34: loss 16.126641009164892
Epoch 35: loss 16.12633430024852
Epoch 36: loss 16.126368461484493
Epoch 37: loss 16.126259680416272
Epoch 38: loss 16.126052502963855
Epoch 39: loss 16.126201852508213
Epoch 40: loss 16.126199650764466
Epoch 41: loss 16.12585020168968
Epoch 42: loss 16.125901025274526
Epoch 43: loss 16.125450029580488
Epoch 44: loss 16.126376114720884
Epoch 45: loss 16.125215195572896
Epoch 46: loss 16.125389708643375
Epoch 47: loss 16.12584474397742
Epoch 48: loss 16.125297898831576
Epoch 49: loss 16.12539606716322
-----------Time: 0:03:18.635728, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016714572906494-------------


Epoch 0: loss 16.12933818050053
Epoch 1: loss 16.128736069928046
Epoch 2: loss 16.128972672379536
Epoch 3: loss 16.12902925843778
Epoch 4: loss 16.128899393910945
Epoch 5: loss 16.128708238187045
Epoch 6: loss 16.128630909712417
Epoch 7: loss 16.128196534903154
Epoch 8: loss 16.12849875636723
Epoch 9: loss 16.12831425148508
Epoch 10: loss 16.128106059198796
Epoch 11: loss 16.12839502044346
Epoch 12: loss 16.127845149454863
Epoch 13: loss 16.128340319965197
Epoch 14: loss 16.128283402194146
Epoch 15: loss 16.12817643829014
Epoch 16: loss 16.127908001775328
Epoch 17: loss 16.12782623871513
Epoch 18: loss 16.12778568578803
Epoch 19: loss 16.128042125701903
Epoch 20: loss 16.127381652334464
Epoch 21: loss 16.127571688527645
Epoch 22: loss 16.127478715647822
Epoch 23: loss 16.127256363371146
Epoch 24: loss 16.127576899528503
Epoch 25: loss 16.127195114674777
Epoch 26: loss 16.126822840649147
Epoch 27: loss 16.1269603708516
Epoch 28: loss 16.1267607771832
Epoch 29: loss 16.12680645092674
Epoch 30: loss 16.127082273234493
Epoch 31: loss 16.12700484835583
Epoch 32: loss 16.126414637980254
Epoch 33: loss 16.12680624982585
Epoch 34: loss 16.12649748325348
Epoch 35: loss 16.126575751926588
Epoch 36: loss 16.126110972528874
Epoch 37: loss 16.126153766590615
Epoch 38: loss 16.126283350198165
Epoch 39: loss 16.126206727649855
Epoch 40: loss 16.125797867774963
Epoch 41: loss 16.125931909809943
Epoch 42: loss 16.126061660310498
Epoch 43: loss 16.125931784381038
Epoch 44: loss 16.125487806486046
Epoch 45: loss 16.125902045291404
Epoch 46: loss 16.12544829638108
Epoch 47: loss 16.12563911728237
Epoch 48: loss 16.12575132328531
Epoch 49: loss 16.12537077717159
-----------Time: 0:03:24.600306, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016714096069336-------------


Epoch 0: loss 16.129094827693443
Epoch 1: loss 16.12887759105019
Epoch 2: loss 16.129060806398808
Epoch 3: loss 16.128783714252968
Epoch 4: loss 16.128561037519702
Epoch 5: loss 16.128943923245306
Epoch 6: loss 16.128803060365758
Epoch 7: loss 16.128822738191356
Epoch 8: loss 16.128491010873216
Epoch 9: loss 16.128703585914945
Epoch 10: loss 16.12835335524186
Epoch 11: loss 16.12820495004239
Epoch 12: loss 16.12830066266267
Epoch 13: loss 16.128503580715346
Epoch 14: loss 16.127947262059088
Epoch 15: loss 16.127795386314393
Epoch 16: loss 16.12801586130391
Epoch 17: loss 16.12782366586768
Epoch 18: loss 16.127822563959203
Epoch 19: loss 16.127613023053044
Epoch 20: loss 16.127641437364662
Epoch 21: loss 16.127221236021622
Epoch 22: loss 16.127841966048532
Epoch 23: loss 16.127527619444805
Epoch 24: loss 16.12709544637929
Epoch 25: loss 16.127329021951425
Epoch 26: loss 16.126782468090887
Epoch 27: loss 16.127118204987568
Epoch 28: loss 16.127043048195215
Epoch 29: loss 16.127087749605593
Epoch 30: loss 16.127120225325875
Epoch 31: loss 16.126755838808805
Epoch 32: loss 16.126641228924626
Epoch 33: loss 16.12673148901566
Epoch 34: loss 16.126343326983246
Epoch 35: loss 16.126495794627978
Epoch 36: loss 16.12594163728797
Epoch 37: loss 16.126168518481048
Epoch 38: loss 16.12626114098922
Epoch 39: loss 16.126006976417873
Epoch 40: loss 16.126185830779697
Epoch 41: loss 16.126145317243495
Epoch 42: loss 16.12584097178086
Epoch 43: loss 16.125878949787307
Epoch 44: loss 16.125391368244006
Epoch 45: loss 16.125771066416863
Epoch 46: loss 16.12522974014282
Epoch 47: loss 16.12567155153855
Epoch 48: loss 16.12537916224936
Epoch 49: loss 16.1255414320075
-----------Time: 0:03:07.186601, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.0167155265808105-------------


Epoch 0: loss 16.12901736549709
Epoch 1: loss 16.128085911792258
Epoch 2: loss 16.127230480442876
Epoch 3: loss 16.12663741837377
Epoch 4: loss 16.125741976240406
Epoch 5: loss 16.124900813724683
Epoch 6: loss 16.124593999074854
Epoch 7: loss 16.12343500904415
Epoch 8: loss 16.122682198234227
Epoch 9: loss 16.121747966434643
Epoch 10: loss 16.121261149904
Epoch 11: loss 16.120330419747727
Epoch 12: loss 16.119800321952155
Epoch 13: loss 16.118973623151366
Epoch 14: loss 16.118041071684463
Epoch 15: loss 16.11737319801165
Epoch 16: loss 16.1168182424877
Epoch 17: loss 16.11596876434658
Epoch 18: loss 16.11491580216781
Epoch 19: loss 16.114131248515584
Epoch 20: loss 16.11333661597708
Epoch 21: loss 16.11287245232126
Epoch 22: loss 16.11194633193638
Epoch 23: loss 16.110728281477222
Epoch 24: loss 16.11040138265361
Epoch 25: loss 16.10996875140978
Epoch 26: loss 16.1090273162593
Epoch 27: loss 16.10828965746838
Epoch 28: loss 16.107437753677367
Epoch 29: loss 16.106903558192045
Epoch 30: loss 16.10577942703081
Epoch 31: loss 16.105217640296274
Epoch 32: loss 16.104095253737075
Epoch 33: loss 16.103197034545566
Epoch 34: loss 16.102602733736454
Epoch 35: loss 16.10199746981911
Epoch 36: loss 16.10121792710346
Epoch 37: loss 16.10083631950876
Epoch 38: loss 16.099674112900445
Epoch 39: loss 16.09916719975679
Epoch 40: loss 16.098468355510544
Epoch 41: loss 16.09744927779488
Epoch 42: loss 16.097080145711484
Epoch 43: loss 16.096167104140573
Epoch 44: loss 16.09537029162697
Epoch 45: loss 16.094419434796208
Epoch 46: loss 16.093888376070105
Epoch 47: loss 16.092881953197978
Epoch 48: loss 16.092084144509357
Epoch 49: loss 16.091487356890802
-----------Time: 0:01:55.259886, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012422561645508-------------


Epoch 0: loss 16.128720716808154
Epoch 1: loss 16.12833607715109
Epoch 2: loss 16.127673474602076
Epoch 3: loss 16.12682566953742
Epoch 4: loss 16.126110968382463
Epoch 5: loss 16.12524855447852
Epoch 6: loss 16.124283249481863
Epoch 7: loss 16.123788793190666
Epoch 8: loss 16.122557253422944
Epoch 9: loss 16.121873461681865
Epoch 10: loss 16.12113251167795
Epoch 11: loss 16.120480680465697
Epoch 12: loss 16.119868655826735
Epoch 13: loss 16.11875781494638
Epoch 14: loss 16.118086012550023
Epoch 15: loss 16.11735307859338
Epoch 16: loss 16.116714552174443
Epoch 17: loss 16.115710277142732
Epoch 18: loss 16.11509050079014
Epoch 19: loss 16.114374390892362
Epoch 20: loss 16.113858115154763
Epoch 21: loss 16.11238626293514
Epoch 22: loss 16.112338011161143
Epoch 23: loss 16.111420027069425
Epoch 24: loss 16.110804798292076
Epoch 25: loss 16.10966754166976
Epoch 26: loss 16.109108057229417
Epoch 27: loss 16.10807415402454
Epoch 28: loss 16.107531946638357
Epoch 29: loss 16.106530109695765
Epoch 30: loss 16.106044946546138
Epoch 31: loss 16.104956232983135
Epoch 32: loss 16.10448952239493
Epoch 33: loss 16.103424533553746
Epoch 34: loss 16.102723824459574
Epoch 35: loss 16.102251242554708
Epoch 36: loss 16.101315670428068
Epoch 37: loss 16.100528161422066
Epoch 38: loss 16.099332897559457
Epoch 39: loss 16.099030569325322
Epoch 40: loss 16.098299677475637
Epoch 41: loss 16.097926405201786
Epoch 42: loss 16.096880897231724
Epoch 43: loss 16.095849631143654
Epoch 44: loss 16.09521789758102
Epoch 45: loss 16.09424641650656
Epoch 46: loss 16.09370073546534
Epoch 47: loss 16.092505893499954
Epoch 48: loss 16.09197877386342
Epoch 49: loss 16.091340614401776
-----------Time: 0:02:29.033553, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012454986572266-------------


Epoch 0: loss 16.128855029396387
Epoch 1: loss 16.12763148598049
Epoch 2: loss 16.127146126912987
Epoch 3: loss 16.12674174827078
Epoch 4: loss 16.125739765167236
Epoch 5: loss 16.124920283193173
Epoch 6: loss 16.124748162601303
Epoch 7: loss 16.12333152397819
Epoch 8: loss 16.122961457915927
Epoch 9: loss 16.122095555844513
Epoch 10: loss 16.12122847722924
Epoch 11: loss 16.12016503188921
Epoch 12: loss 16.119613641241322
Epoch 13: loss 16.119076189787492
Epoch 14: loss 16.11772109736567
Epoch 15: loss 16.11713556828706
Epoch 16: loss 16.11681887274203
Epoch 17: loss 16.116109695641892
Epoch 18: loss 16.114939943603847
Epoch 19: loss 16.114184734095698
Epoch 20: loss 16.113008035784183
Epoch 21: loss 16.112527824484783
Epoch 22: loss 16.111874087997105
Epoch 23: loss 16.111419953470644
Epoch 24: loss 16.110852345176365
Epoch 25: loss 16.109755568918974
Epoch 26: loss 16.109066849169523
Epoch 27: loss 16.108091828097468
Epoch 28: loss 16.107114693392877
Epoch 29: loss 16.106516363309776
Epoch 30: loss 16.105676375264707
Epoch 31: loss 16.105108894472536
Epoch 32: loss 16.104460359656294
Epoch 33: loss 16.10347249404244
Epoch 34: loss 16.1027601470118
Epoch 35: loss 16.101983561723127
Epoch 36: loss 16.100872207724528
Epoch 37: loss 16.100428045314292
Epoch 38: loss 16.09977009192757
Epoch 39: loss 16.099141733542734
Epoch 40: loss 16.098461022584335
Epoch 41: loss 16.097410433188728
Epoch 42: loss 16.096835244220237
Epoch 43: loss 16.095825204641923
Epoch 44: loss 16.09559486741605
Epoch 45: loss 16.09469617553379
Epoch 46: loss 16.09373880987582
Epoch 47: loss 16.092554272776066
Epoch 48: loss 16.092116929137187
Epoch 49: loss 16.09161332068236
-----------Time: 0:02:46.079093, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012454509735107-------------


Epoch 0: loss 16.128651947560517
Epoch 1: loss 16.127895905660548
Epoch 2: loss 16.12727007658585
Epoch 3: loss 16.126541896488355
Epoch 4: loss 16.125552840854812
Epoch 5: loss 16.12512777680936
Epoch 6: loss 16.123837748817774
Epoch 7: loss 16.123747364334438
Epoch 8: loss 16.122636218692946
Epoch 9: loss 16.121542506632597
Epoch 10: loss 16.121265733760335
Epoch 11: loss 16.120637323545374
Epoch 12: loss 16.11949867476588
Epoch 13: loss 16.11887114877286
Epoch 14: loss 16.11811569566312
Epoch 15: loss 16.117155162147853
Epoch 16: loss 16.116657276775527
Epoch 17: loss 16.115799574230028
Epoch 18: loss 16.11520242172739
Epoch 19: loss 16.114726877212526
Epoch 20: loss 16.1139683422835
Epoch 21: loss 16.112936424172442
Epoch 22: loss 16.11217447882113
Epoch 23: loss 16.1110420320345
Epoch 24: loss 16.110670572778453
Epoch 25: loss 16.10964290888413
Epoch 26: loss 16.1091101905574
Epoch 27: loss 16.108371514859407
Epoch 28: loss 16.10754499020784
Epoch 29: loss 16.10676906834478
Epoch 30: loss 16.105882630140886
Epoch 31: loss 16.10530604797861
Epoch 32: loss 16.104235721671063
Epoch 33: loss 16.10329296692558
Epoch 34: loss 16.103025365912394
Epoch 35: loss 16.101774458263233
Epoch 36: loss 16.1012919270474
Epoch 37: loss 16.100588013814843
Epoch 38: loss 16.099952008413233
Epoch 39: loss 16.099028201725172
Epoch 40: loss 16.098279824464218
Epoch 41: loss 16.097368128403374
Epoch 42: loss 16.097104394954183
Epoch 43: loss 16.09597421625386
Epoch 44: loss 16.095187752143197
Epoch 45: loss 16.093893765366595
Epoch 46: loss 16.093743409281192
Epoch 47: loss 16.092743709812993
Epoch 48: loss 16.092125941359477
Epoch 49: loss 16.09145065183225
-----------Time: 0:03:22.791943, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012458324432373-------------


Epoch 0: loss 16.128598735643468
Epoch 1: loss 16.12811477806257
Epoch 2: loss 16.127346938589344
Epoch 3: loss 16.126522716231968
Epoch 4: loss 16.125752802517102
Epoch 5: loss 16.124904638787974
Epoch 6: loss 16.124190633193308
Epoch 7: loss 16.123795152747114
Epoch 8: loss 16.12258727654167
Epoch 9: loss 16.121997796970867
Epoch 10: loss 16.12094833228899
Epoch 11: loss 16.120096860761226
Epoch 12: loss 16.11953765931337
Epoch 13: loss 16.118574957225633
Epoch 14: loss 16.118273434431657
Epoch 15: loss 16.11742442276167
Epoch 16: loss 16.1165896633397
Epoch 17: loss 16.116040255712427
Epoch 18: loss 16.11486542950506
Epoch 19: loss 16.1142235994339
Epoch 20: loss 16.113604631631272
Epoch 21: loss 16.112342175193454
Epoch 22: loss 16.11197808618131
Epoch 23: loss 16.111145392708156
Epoch 24: loss 16.110347345600957
Epoch 25: loss 16.10988852770432
Epoch 26: loss 16.10893377221149
Epoch 27: loss 16.107981337671696
Epoch 28: loss 16.10741576111835
Epoch 29: loss 16.10681535990342
Epoch 30: loss 16.106028480115143
Epoch 31: loss 16.105129384994505
Epoch 32: loss 16.103972885919653
Epoch 33: loss 16.103435766178627
Epoch 34: loss 16.10284081645634
Epoch 35: loss 16.101835175182508
Epoch 36: loss 16.101172709465025
Epoch 37: loss 16.100210487324258
Epoch 38: loss 16.099367180077927
Epoch 39: loss 16.098999791559965
Epoch 40: loss 16.098207461315653
Epoch 41: loss 16.097611771459164
Epoch 42: loss 16.096777263931607
Epoch 43: loss 16.09604930981346
Epoch 44: loss 16.094885848916096
Epoch 45: loss 16.094346516028693
Epoch 46: loss 16.093507793675297
Epoch 47: loss 16.092603516578674
Epoch 48: loss 16.09221675914267
Epoch 49: loss 16.0911423475846
-----------Time: 0:04:07.493080, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012455463409424-------------


Epoch 0: loss 16.12532849104508
Epoch 1: loss 16.117700748858244
Epoch 2: loss 16.109911057223446
Epoch 3: loss 16.102314850558404
Epoch 4: loss 16.09520466638648
Epoch 5: loss 16.08714952054231
Epoch 6: loss 16.07967092887215
Epoch 7: loss 16.071973324858625
Epoch 8: loss 16.06427939041801
Epoch 9: loss 16.056403786203134
Epoch 10: loss 16.04944219900214
Epoch 11: loss 16.041650635263196
Epoch 12: loss 16.03437657356262
Epoch 13: loss 16.026305367635644
Epoch 14: loss 16.018318836585337
Epoch 15: loss 16.01111799530361
Epoch 16: loss 16.00343422786049
Epoch 17: loss 15.995847894834435
Epoch 18: loss 15.988193177140277
Epoch 19: loss 15.980493145403655
Epoch 20: loss 15.973006218412648
Epoch 21: loss 15.965267400119616
Epoch 22: loss 15.957777932415837
Epoch 23: loss 15.950354315923608
Epoch 24: loss 15.942539060634115
Epoch 25: loss 15.9349312948144
Epoch 26: loss 15.927507909484532
Epoch 27: loss 15.919904348124629
Epoch 28: loss 15.912129661311274
Epoch 29: loss 15.904903972667196
Epoch 30: loss 15.896870423399884
Epoch 31: loss 15.889611173712689
Epoch 32: loss 15.88216932338217
Epoch 33: loss 15.87453764003256
Epoch 34: loss 15.866563502601956
Epoch 35: loss 15.859365535819013
Epoch 36: loss 15.851783277677454
Epoch 37: loss 15.843816855679387
Epoch 38: loss 15.836393988650778
Epoch 39: loss 15.829007201609404
Epoch 40: loss 15.821083092689515
Epoch 41: loss 15.813678399376247
Epoch 42: loss 15.805981411104616
Epoch 43: loss 15.799291862612185
Epoch 44: loss 15.790973381374194
Epoch 45: loss 15.78381810810255
Epoch 46: loss 15.775989431920259
Epoch 47: loss 15.768612666752027
Epoch 48: loss 15.761141954297605
Epoch 49: loss 15.753355717658996
-----------Time: 0:02:52.302406, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9698948860168457-------------


Epoch 0: loss 16.12548203157342
Epoch 1: loss 16.11785226179206
Epoch 2: loss 16.11008332501287
Epoch 3: loss 16.102236000351283
Epoch 4: loss 16.09474516640539
Epoch 5: loss 16.08712508678436
Epoch 6: loss 16.079124371901802
Epoch 7: loss 16.07216777905174
Epoch 8: loss 16.064262321720953
Epoch 9: loss 16.05658525902292
Epoch 10: loss 16.04906253192736
Epoch 11: loss 16.041703462600708
Epoch 12: loss 16.033537188820215
Epoch 13: loss 16.026434709714806
Epoch 14: loss 16.018615127646406
Epoch 15: loss 16.01103539466858
Epoch 16: loss 16.003502794970636
Epoch 17: loss 15.995611098538275
Epoch 18: loss 15.988540003610694
Epoch 19: loss 15.98077648618947
Epoch 20: loss 15.973008951933487
Epoch 21: loss 15.965509929864304
Epoch 22: loss 15.957417649808137
Epoch 23: loss 15.95022607471632
Epoch 24: loss 15.942602922605431
Epoch 25: loss 15.934858717089114
Epoch 26: loss 15.927191655532173
Epoch 27: loss 15.919849539839703
Epoch 28: loss 15.911972025166387
Epoch 29: loss 15.904476592851722
Epoch 30: loss 15.897118011764858
Epoch 31: loss 15.889590878071992
Epoch 32: loss 15.881712457408076
Epoch 33: loss 15.874201758011528
Epoch 34: loss 15.866498200789742
Epoch 35: loss 15.859228067812712
Epoch 36: loss 15.850930828633516
Epoch 37: loss 15.843828261416892
Epoch 38: loss 15.83626781857532
Epoch 39: loss 15.828835524683413
Epoch 40: loss 15.821004012356633
Epoch 41: loss 15.81364137193431
Epoch 42: loss 15.80573271875796
Epoch 43: loss 15.798283531354821
Epoch 44: loss 15.790386061046435
Epoch 45: loss 15.783125142429187
Epoch 46: loss 15.775686374954555
Epoch 47: loss 15.76789043571638
Epoch 48: loss 15.760307070483332
Epoch 49: loss 15.753024163453475
-----------Time: 0:02:03.653147, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9698190689086914-------------


Epoch 0: loss 16.125332433244456
Epoch 1: loss 16.117763077694438
Epoch 2: loss 16.110282879290374
Epoch 3: loss 16.102351579458816
Epoch 4: loss 16.094671572809634
Epoch 5: loss 16.08729835074881
Epoch 6: loss 16.079796460400456
Epoch 7: loss 16.071913741982502
Epoch 8: loss 16.06434690952301
Epoch 9: loss 16.056589496654013
Epoch 10: loss 16.04891008708788
Epoch 11: loss 16.04136247220247
Epoch 12: loss 16.03390694182852
Epoch 13: loss 16.026070977293926
Epoch 14: loss 16.018865798867267
Epoch 15: loss 16.010756480175516
Epoch 16: loss 16.00320995994236
Epoch 17: loss 15.996137252061263
Epoch 18: loss 15.988172055327373
Epoch 19: loss 15.980615103763082
Epoch 20: loss 15.972953036557072
Epoch 21: loss 15.965307936461075
Epoch 22: loss 15.95742809979812
Epoch 23: loss 15.949922722318899
Epoch 24: loss 15.942054024986598
Epoch 25: loss 15.934806667203489
Epoch 26: loss 15.927174715373827
Epoch 27: loss 15.91931420823802
Epoch 28: loss 15.911643095638441
Epoch 29: loss 15.904038808656775
Epoch 30: loss 15.896268530513929
Epoch 31: loss 15.888466095924377
Epoch 32: loss 15.880978198673414
Epoch 33: loss 15.87310236433278
Epoch 34: loss 15.865448663545692
Epoch 35: loss 15.858227579489999
Epoch 36: loss 15.850511629685112
Epoch 37: loss 15.84226923921834
Epoch 38: loss 15.834613418579101
Epoch 39: loss 15.826840792531552
Epoch 40: loss 15.818979267452074
Epoch 41: loss 15.811368784697159
Epoch 42: loss 15.803232036466184
Epoch 43: loss 15.795318450098453
Epoch 44: loss 15.787497531849406
Epoch 45: loss 15.77957576772441
Epoch 46: loss 15.771681332588196
Epoch 47: loss 15.763461754633033
Epoch 48: loss 15.75550606250763
Epoch 49: loss 15.747324337130008
-----------Time: 0:02:32.583878, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9690842628479004-------------


Epoch 0: loss 16.124987588758053
Epoch 1: loss 16.11762206347092
Epoch 2: loss 16.11026843423429
Epoch 3: loss 16.102739324777023
Epoch 4: loss 16.094765882906707
Epoch 5: loss 16.087236788998478
Epoch 6: loss 16.079537017449088
Epoch 7: loss 16.071793963598168
Epoch 8: loss 16.0640524553216
Epoch 9: loss 16.056582000981205
Epoch 10: loss 16.04917503128881
Epoch 11: loss 16.041693295603213
Epoch 12: loss 16.033948899352033
Epoch 13: loss 16.026411435915076
Epoch 14: loss 16.018605533890103
Epoch 15: loss 16.01086249144181
Epoch 16: loss 16.003181542520938
Epoch 17: loss 15.99599133056143
Epoch 18: loss 15.987865414826766
Epoch 19: loss 15.980230523192365
Epoch 20: loss 15.972475760916005
Epoch 21: loss 15.96452137698298
Epoch 22: loss 15.957246088981629
Epoch 23: loss 15.949323275814885
Epoch 24: loss 15.941598924346593
Epoch 25: loss 15.93430860042572
Epoch 26: loss 15.926333998597187
Epoch 27: loss 15.918542057534923
Epoch 28: loss 15.910380602919536
Epoch 29: loss 15.902643891002821
Epoch 30: loss 15.894720970029416
Epoch 31: loss 15.886554664114247
Epoch 32: loss 15.878540833100029
Epoch 33: loss 15.870564227518829
Epoch 34: loss 15.862058573183806
Epoch 35: loss 15.85374640174534
Epoch 36: loss 15.845396702185921
Epoch 37: loss 15.837145253886348
Epoch 38: loss 15.828132260364034
Epoch 39: loss 15.819651981022046
Epoch 40: loss 15.810354775967806
Epoch 41: loss 15.800871183561242
Epoch 42: loss 15.792067561978879
Epoch 43: loss 15.782242377944614
Epoch 44: loss 15.772979494799738
Epoch 45: loss 15.762545143003049
Epoch 46: loss 15.75285272287286
Epoch 47: loss 15.743000478329867
Epoch 48: loss 15.731559770003608
Epoch 49: loss 15.720717375174813
-----------Time: 0:03:02.846075, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.965590476989746-------------


Epoch 0: loss 16.125582171523053
Epoch 1: loss 16.117580779739047
Epoch 2: loss 16.110103178024293
Epoch 3: loss 16.102567776389744
Epoch 4: loss 16.094968559430992
Epoch 5: loss 16.087015981259555
Epoch 6: loss 16.079524627975797
Epoch 7: loss 16.072312071012416
Epoch 8: loss 16.064128098280534
Epoch 9: loss 16.056712497835573
Epoch 10: loss 16.04867666804272
Epoch 11: loss 16.041156463001084
Epoch 12: loss 16.03368115943411
Epoch 13: loss 16.026133695892664
Epoch 14: loss 16.018264314402703
Epoch 15: loss 16.01059253630431
Epoch 16: loss 16.003030909662662
Epoch 17: loss 15.995300728341807
Epoch 18: loss 15.98795746098394
Epoch 19: loss 15.97965054512024
Epoch 20: loss 15.971732837220896
Epoch 21: loss 15.963760532503542
Epoch 22: loss 15.955768203735351
Epoch 23: loss 15.947817118271537
Epoch 24: loss 15.939071386793385
Epoch 25: loss 15.931437327550805
Epoch 26: loss 15.923163364244544
Epoch 27: loss 15.914506603323895
Epoch 28: loss 15.905510534410892
Epoch 29: loss 15.896652097287385
Epoch 30: loss 15.88753488789434
Epoch 31: loss 15.877962223343227
Epoch 32: loss 15.868537232150203
Epoch 33: loss 15.858524419950403
Epoch 34: loss 15.848048721189084
Epoch 35: loss 15.837571736004042
Epoch 36: loss 15.826757803170578
Epoch 37: loss 15.81515843246294
Epoch 38: loss 15.803870738070945
Epoch 39: loss 15.791961781874948
Epoch 40: loss 15.778853631019592
Epoch 41: loss 15.765978981100995
Epoch 42: loss 15.75260229629019
Epoch 43: loss 15.738499681845955
Epoch 44: loss 15.724316707901332
Epoch 45: loss 15.709558625843213
Epoch 46: loss 15.694107030785602
Epoch 47: loss 15.678353794761327
Epoch 48: loss 15.661572396236917
Epoch 49: loss 15.644752762628638
-----------Time: 0:03:37.963400, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.955641746520996-------------


Epoch 0: loss 16.09132479066434
Epoch 1: loss 16.01579966545105
Epoch 2: loss 15.939424544831981
Epoch 3: loss 15.863627517741659
Epoch 4: loss 15.788112788615019
Epoch 5: loss 15.712957443361697
Epoch 6: loss 15.63801049667856
Epoch 7: loss 15.563222463234611
Epoch 8: loss 15.488078339203545
Epoch 9: loss 15.413610198186792
Epoch 10: loss 15.339602081672005
Epoch 11: loss 15.265301224459773
Epoch 12: loss 15.19066798997962
Epoch 13: loss 15.11658919479536
Epoch 14: loss 15.042471655555394
Epoch 15: loss 14.969062138640362
Epoch 16: loss 14.894698577341826
Epoch 17: loss 14.821139665271925
Epoch 18: loss 14.747317908121191
Epoch 19: loss 14.67266292779342
Epoch 20: loss 14.59833843604378
Epoch 21: loss 14.523664609245632
Epoch 22: loss 14.448438798862956
Epoch 23: loss 14.373103044344031
Epoch 24: loss 14.296717370074727
Epoch 25: loss 14.21947079948757
Epoch 26: loss 14.141654171114382
Epoch 27: loss 14.062061793907828
Epoch 28: loss 13.98108771054641
Epoch 29: loss 13.899044104244398
Epoch 30: loss 13.814810868968134
Epoch 31: loss 13.729019376505976
Epoch 32: loss 13.64081261779951
Epoch 33: loss 13.550756410930468
Epoch 34: loss 13.45770200646442
Epoch 35: loss 13.361710910175157
Epoch 36: loss 13.263247937741488
Epoch 37: loss 13.16149784585704
Epoch 38: loss 13.057025838934857
Epoch 39: loss 12.949191138018733
Epoch 40: loss 12.838450059683426
Epoch 41: loss 12.723526030001432
Epoch 42: loss 12.606373612777046
Epoch 43: loss 12.48474424507307
Epoch 44: loss 12.36027700693711
Epoch 45: loss 12.232098233181498
Epoch 46: loss 12.100534667139469
Epoch 47: loss 11.965753134437229
Epoch 48: loss 11.827310258409252
Epoch 49: loss 11.685174720183662
-----------Time: 0:02:28.309987, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 20, rmse: 3.41188645362854-------------


Epoch 0: loss 16.09125823041667
Epoch 1: loss 16.015202082758364
Epoch 2: loss 15.939112699550131
Epoch 3: loss 15.863396565810493
Epoch 4: loss 15.787802161341128
Epoch 5: loss 15.711914261527683
Epoch 6: loss 15.636276106212451
Epoch 7: loss 15.55970045690951
Epoch 8: loss 15.482766061243803
Epoch 9: loss 15.403526765367259
Epoch 10: loss 15.32271625166354
Epoch 11: loss 15.23808356264363
Epoch 12: loss 15.14888902436132
Epoch 13: loss 15.053172944939655
Epoch 14: loss 14.950315679674564
Epoch 15: loss 14.83830342811087
Epoch 16: loss 14.716860489223315
Epoch 17: loss 14.58380942551986
Epoch 18: loss 14.439529323577881
Epoch 19: loss 14.283136896465136
Epoch 20: loss 14.113554855014966
Epoch 21: loss 13.931724931882774
Epoch 22: loss 13.737327051162719
Epoch 23: loss 13.531128521587538
Epoch 24: loss 13.312525951344034
Epoch 25: loss 13.082565266153086
Epoch 26: loss 12.841426311368528
Epoch 27: loss 12.589041526421257
Epoch 28: loss 12.326476557358452
Epoch 29: loss 12.054665093836578
Epoch 30: loss 11.773069525801617
Epoch 31: loss 11.483198862490447
Epoch 32: loss 11.1851471102756
Epoch 33: loss 10.879520034790039
Epoch 34: loss 10.567582014332647
Epoch 35: loss 10.24950638335684
Epoch 36: loss 9.926493848925052
Epoch 37: loss 9.599138785445172
Epoch 38: loss 9.267636155045551
Epoch 39: loss 8.93361337184906
Epoch 40: loss 8.597212066857711
Epoch 41: loss 8.259487326248832
Epoch 42: loss 7.921796608489493
Epoch 43: loss 7.58422177615373
Epoch 44: loss 7.247765974376513
Epoch 45: loss 6.913738166249317
Epoch 46: loss 6.5825498710507935
Epoch 47: loss 6.255491735105929
Epoch 48: loss 5.9328473101491515
Epoch 49: loss 5.616374014771503
-----------Time: 0:02:48.826871, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 50, rmse: 2.3452417850494385-------------


Epoch 0: loss 16.09155797958374
Epoch 1: loss 16.015202310810917
Epoch 2: loss 15.938755560957867
Epoch 3: loss 15.861988794285319
Epoch 4: loss 15.783380499093429
Epoch 5: loss 15.699112030734186
Epoch 6: loss 15.603492589618849
Epoch 7: loss 15.487722671550253
Epoch 8: loss 15.34503272201704
Epoch 9: loss 15.171499268904975
Epoch 10: loss 14.966277893729831
Epoch 11: loss 14.73042936221413
Epoch 12: loss 14.46450748650924
Epoch 13: loss 14.16984776413959
Epoch 14: loss 13.848408069817916
Epoch 15: loss 13.501846814155579
Epoch 16: loss 13.131954366227856
Epoch 17: loss 12.739914034760517
Epoch 18: loss 12.328107372574184
Epoch 19: loss 11.898592855619347
Epoch 20: loss 11.452804772750191
Epoch 21: loss 10.993229435837787
Epoch 22: loss 10.521445863143258
Epoch 23: loss 10.04021218030349
Epoch 24: loss 9.552048542188562
Epoch 25: loss 9.058623669458472
Epoch 26: loss 8.562678301852682
Epoch 27: loss 8.066423750442008
Epoch 28: loss 7.572207545197529
Epoch 29: loss 7.082441508769989
Epoch 30: loss 6.600794509182806
Epoch 31: loss 6.12850231035896
Epoch 32: loss 5.668599115247312
Epoch 33: loss 5.223539935505909
Epoch 34: loss 4.7958928517673325
Epoch 35: loss 4.388047649808552
Epoch 36: loss 4.002360116398853
Epoch 37: loss 3.6408396536889285
Epoch 38: loss 3.3053936268972315
Epoch 39: loss 2.997696754206782
Epoch 40: loss 2.7188427951024927
Epoch 41: loss 2.4693723212117735
Epoch 42: loss 2.249499393157337
Epoch 43: loss 2.0584737171297487
Epoch 44: loss 1.895074970955434
Epoch 45: loss 1.7570079794396525
Epoch 46: loss 1.642064111388248
Epoch 47: loss 1.5469474188659502
Epoch 48: loss 1.4690014503572297
Epoch 49: loss 1.405166550434154
-----------Time: 0:03:21.716715, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.1839276552200317-------------


Epoch 0: loss 16.091322864656863
Epoch 1: loss 16.014970964970797
Epoch 2: loss 15.938098766492761
Epoch 3: loss 15.856822677280592
Epoch 4: loss 15.760699212032815
Epoch 5: loss 15.628797557042992
Epoch 6: loss 15.442513086484826
Epoch 7: loss 15.19510832351187
Epoch 8: loss 14.892416866965917
Epoch 9: loss 14.53882872229037
Epoch 10: loss 14.140551825191663
Epoch 11: loss 13.70126902538797
Epoch 12: loss 13.226184406487839
Epoch 13: loss 12.719098592841107
Epoch 14: loss 12.18373988814976
Epoch 15: loss 11.624176308383113
Epoch 16: loss 11.044009731126868
Epoch 17: loss 10.44807524577431
Epoch 18: loss 9.840306369118068
Epoch 19: loss 9.22485019331393
Epoch 20: loss 8.605575177462205
Epoch 21: loss 7.987399002261784
Epoch 22: loss 7.375321515228437
Epoch 23: loss 6.773443455799766
Epoch 24: loss 6.186421121203381
Epoch 25: loss 5.619289815425873
Epoch 26: loss 5.076057924913323
Epoch 27: loss 4.561812823233397
Epoch 28: loss 4.080187467129334
Epoch 29: loss 3.6354118062102274
Epoch 30: loss 3.230549658381421
Epoch 31: loss 2.8677680303221162
Epoch 32: loss 2.5491620429184128
Epoch 33: loss 2.2746156618646953
Epoch 34: loss 2.042985314908235
Epoch 35: loss 1.8518740161605503
Epoch 36: loss 1.6968970745801926
Epoch 37: loss 1.573251712710961
Epoch 38: loss 1.4755761083053507
Epoch 39: loss 1.3987111124007598
Epoch 40: loss 1.3380456496839939
Epoch 41: loss 1.2893921609805978
Epoch 42: loss 1.2499270862859229
Epoch 43: loss 1.2171675693729649
Epoch 44: loss 1.1893418371677398
Epoch 45: loss 1.16562590987786
Epoch 46: loss 1.1449781880430554
Epoch 47: loss 1.1270427175838014
Epoch 48: loss 1.1111560015574746
Epoch 49: loss 1.0971975423071696
-----------Time: 0:03:08.925303, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.0541778802871704-------------


Epoch 0: loss 16.091454973428146
Epoch 1: loss 16.01504686397055
Epoch 2: loss 15.934948290949283
Epoch 3: loss 15.835735246409541
Epoch 4: loss 15.678322923701742
Epoch 5: loss 15.433250267609306
Epoch 6: loss 15.099268429175668
Epoch 7: loss 14.688046781913094
Epoch 8: loss 14.213651958755825
Epoch 9: loss 13.685246935098068
Epoch 10: loss 13.109937269791313
Epoch 11: loss 12.49499547585197
Epoch 12: loss 11.846406451515529
Epoch 13: loss 11.169533227837604
Epoch 14: loss 10.472000039142111
Epoch 15: loss 9.758950358888377
Epoch 16: loss 9.038476860004923
Epoch 17: loss 8.316836296475453
Epoch 18: loss 7.600644607647606
Epoch 19: loss 6.89769168159236
Epoch 20: loss 6.214439060377038
Epoch 21: loss 5.558946787792704
Epoch 22: loss 4.936988931116851
Epoch 23: loss 4.356134752346121
Epoch 24: loss 3.8220393030539803
Epoch 25: loss 3.3398695137189782
Epoch 26: loss 2.9138988735883133
Epoch 27: loss 2.5458297353723776
Epoch 28: loss 2.2361516099909076
Epoch 29: loss 1.9825946665328482
Epoch 30: loss 1.7802334736222807
Epoch 31: loss 1.6224122614964196
Epoch 32: loss 1.5013029713993487
Epoch 33: loss 1.4089373675377472
Epoch 34: loss 1.3382286759822264
Epoch 35: loss 1.2833334469276927
Epoch 36: loss 1.2398861492457598
Epoch 37: loss 1.2044838450525117
Epoch 38: loss 1.1751161614189978
Epoch 39: loss 1.1502697930387829
Epoch 40: loss 1.1291133415439853
Epoch 41: loss 1.1108502946470096
Epoch 42: loss 1.0950444535716721
Epoch 43: loss 1.0814510067519933
Epoch 44: loss 1.069425052922705
Epoch 45: loss 1.059075765117355
Epoch 46: loss 1.0500342617216318
Epoch 47: loss 1.0421600806324378
Epoch 48: loss 1.0350903685974038
Epoch 49: loss 1.0291617984357087
-----------Time: 0:03:13.563911, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.022708773612976-------------


Epoch 0: loss 15.755263128487961
Epoch 1: loss 15.016147657062696
Epoch 2: loss 14.277295997868414
Epoch 3: loss 13.42739513853322
Epoch 4: loss 12.216331523397695
Epoch 5: loss 10.475008365382319
Epoch 6: loss 8.375810054074163
Epoch 7: loss 6.233644788679869
Epoch 8: loss 4.3411236884801285
Epoch 9: loss 2.900616440306539
Epoch 10: loss 1.976248661979385
Epoch 11: loss 1.484734250669894
Epoch 12: loss 1.2575111693662147
Epoch 13: loss 1.150441733520964
Epoch 14: loss 1.091963865186857
Epoch 15: loss 1.0557654103507166
Epoch 16: loss 1.0322672424109085
Epoch 17: loss 1.0167562800257102
Epoch 18: loss 1.0062619263063306
Epoch 19: loss 0.9990698942023775
Epoch 20: loss 0.9942416693853295
Epoch 21: loss 0.99078245629435
Epoch 22: loss 0.9882640880087148
Epoch 23: loss 0.9864714264221813
Epoch 24: loss 0.9850650042943333
Epoch 25: loss 0.9840509316843489
Epoch 26: loss 0.9832707264501116
Epoch 27: loss 0.982652973027333
Epoch 28: loss 0.9821764783366866
Epoch 29: loss 0.9816971520366876
Epoch 30: loss 0.9813264938800231
Epoch 31: loss 0.9810685679964397
Epoch 32: loss 0.9806932678041251
Epoch 33: loss 0.9804453449404757
Epoch 34: loss 0.9801443228903024
Epoch 35: loss 0.9799468644935152
Epoch 36: loss 0.9796942158237748
Epoch 37: loss 0.9794774770088818
Epoch 38: loss 0.9792348546178444
Epoch 39: loss 0.9790458390246267
Epoch 40: loss 0.9787607902418012
Epoch 41: loss 0.9785800403226976
Epoch 42: loss 0.9783278625944386
Epoch 43: loss 0.9780891595327336
Epoch 44: loss 0.977845521400804
Epoch 45: loss 0.9775694195343101
Epoch 46: loss 0.9773821867678476
Epoch 47: loss 0.9770788625530574
Epoch 48: loss 0.9768264716734056
Epoch 49: loss 0.976548078202683
-----------Time: 0:01:55.963471, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9997021555900574-------------


Epoch 0: loss 15.753852505269258
Epoch 1: loss 14.847915443130162
Epoch 2: loss 12.425284157628598
Epoch 3: loss 8.320545752152153
Epoch 4: loss 4.613911115345748
Epoch 5: loss 2.3922610453937363
Epoch 6: loss 1.4876892928196037
Epoch 7: loss 1.2012832281382189
Epoch 8: loss 1.0964227717207826
Epoch 9: loss 1.046175812120023
Epoch 10: loss 1.0195607353811678
Epoch 11: loss 1.0051651603501777
Epoch 12: loss 0.9970122382044793
Epoch 13: loss 0.9924467020708582
Epoch 14: loss 0.9895754786289257
Epoch 15: loss 0.9878596322044082
Epoch 16: loss 0.9866878251666608
Epoch 17: loss 0.9858974774894507
Epoch 18: loss 0.9854178695575051
Epoch 19: loss 0.9849522678100544
Epoch 20: loss 0.9846945733479832
Epoch 21: loss 0.9843838137129078
Epoch 22: loss 0.9841505342851514
Epoch 23: loss 0.9839589238166809
Epoch 24: loss 0.9838234414225039
Epoch 25: loss 0.9835315421871517
Epoch 26: loss 0.9833656031152477
Epoch 27: loss 0.9831900445015535
Epoch 28: loss 0.9829078402856122
Epoch 29: loss 0.9828014294738355
Epoch 30: loss 0.9825054004788398
Epoch 31: loss 0.9823382962657058
Epoch 32: loss 0.9821096100884935
Epoch 33: loss 0.9817629472069118
Epoch 34: loss 0.981549487489721
Epoch 35: loss 0.9812608882136967
Epoch 36: loss 0.9808976391087407
Epoch 37: loss 0.9806560911562132
Epoch 38: loss 0.9802736528541731
Epoch 39: loss 0.9798869206205658
Epoch 40: loss 0.9795735966900121
Epoch 41: loss 0.9791715995773025
Epoch 42: loss 0.9787984377664068
Epoch 43: loss 0.978274537363778
Epoch 44: loss 0.9777842519075974
Epoch 45: loss 0.9772828886042471
Epoch 46: loss 0.9767817787502123
Epoch 47: loss 0.9761849504450093
Epoch 48: loss 0.975591128287108
Epoch 49: loss 0.9749462416638499
-----------Time: 0:02:29.938064, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9985367655754089-------------


Epoch 0: loss 15.698300436268681
Epoch 1: loss 12.232209795454274
Epoch 2: loss 5.835133764536485
Epoch 3: loss 2.27355182831702
Epoch 4: loss 1.3147261987561765
Epoch 5: loss 1.1095123693346978
Epoch 6: loss 1.041251273194085
Epoch 7: loss 1.0128069326929423
Epoch 8: loss 1.000081320884435
Epoch 9: loss 0.9941573374297308
Epoch 10: loss 0.9911980759190476
Epoch 11: loss 0.9897885706761609
Epoch 12: loss 0.9888507354518642
Epoch 13: loss 0.9883510371265204
Epoch 14: loss 0.9880630036411078
Epoch 15: loss 0.9877722917043644
Epoch 16: loss 0.9876173851930576
Epoch 17: loss 0.9874815927251526
Epoch 18: loss 0.9873993002202199
Epoch 19: loss 0.9871355847172115
Epoch 20: loss 0.9871323367175848
Epoch 21: loss 0.9869810811203459
Epoch 22: loss 0.9869354402241499
Epoch 23: loss 0.9867348188291425
Epoch 24: loss 0.9865841607036798
Epoch 25: loss 0.9864147537428399
Epoch 26: loss 0.9862967719202457
Epoch 27: loss 0.9861357754339343
Epoch 28: loss 0.9860815957836483
Epoch 29: loss 0.9859001668898956
Epoch 30: loss 0.9856645122170449
Epoch 31: loss 0.9855496503088785
Epoch 32: loss 0.9853056772895481
Epoch 33: loss 0.9851290818789731
Epoch 34: loss 0.9848811977583429
Epoch 35: loss 0.9846959500209145
Epoch 36: loss 0.9843562247312587
Epoch 37: loss 0.9841404326584028
Epoch 38: loss 0.9838557291290034
Epoch 39: loss 0.9835576412470445
Epoch 40: loss 0.9832663208894108
Epoch 41: loss 0.9827838259546653
Epoch 42: loss 0.9824827135257099
Epoch 43: loss 0.9820372791393943
Epoch 44: loss 0.981636005056941
Epoch 45: loss 0.9811577461983847
Epoch 46: loss 0.9805865754251895
Epoch 47: loss 0.9800511912807174
Epoch 48: loss 0.9794675639142161
Epoch 49: loss 0.9788166417375855
-----------Time: 0:02:50.973228, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9991381764411926-------------


Epoch 0: loss 15.455234207277712
Epoch 1: loss 9.357652267684108
Epoch 2: loss 3.0700132311686223
Epoch 3: loss 1.364307127698608
Epoch 4: loss 1.100941210142944
Epoch 5: loss 1.031633531464183
Epoch 6: loss 1.0068461223788883
Epoch 7: loss 0.9972689856653628
Epoch 8: loss 0.9933224426015563
Epoch 9: loss 0.9914647722373838
Epoch 10: loss 0.9905333800808244
Epoch 11: loss 0.9899909347295761
Epoch 12: loss 0.9897692570219869
Epoch 13: loss 0.9895453941562902
Epoch 14: loss 0.9894010953929113
Epoch 15: loss 0.9892251118369725
Epoch 16: loss 0.9891384054137313
Epoch 17: loss 0.9889513022225837
Epoch 18: loss 0.9888729673364888
Epoch 19: loss 0.9886847588679065
Epoch 20: loss 0.9885879018384477
Epoch 21: loss 0.98847696904255
Epoch 22: loss 0.988330428107925
Epoch 23: loss 0.9880925247202749
Epoch 24: loss 0.9880147729230964
Epoch 25: loss 0.9877216835384783
Epoch 26: loss 0.9875310882925987
Epoch 27: loss 0.9872990732607634
Epoch 28: loss 0.987020540950091
Epoch 29: loss 0.9866598293833111
Epoch 30: loss 0.9863388289575992
Epoch 31: loss 0.9859715590010518
Epoch 32: loss 0.9855379205035126
Epoch 33: loss 0.9850809763307157
Epoch 34: loss 0.9845848868722501
Epoch 35: loss 0.9839679500978926
Epoch 36: loss 0.9833747144626535
Epoch 37: loss 0.982693640563799
Epoch 38: loss 0.9819740323916726
Epoch 39: loss 0.9811811935642492
Epoch 40: loss 0.9802906809293706
Epoch 41: loss 0.9793506790114486
Epoch 42: loss 0.9782944652697314
Epoch 43: loss 0.9771631039355112
Epoch 44: loss 0.975944363941317
Epoch 45: loss 0.9747220588119133
Epoch 46: loss 0.9733891214365544
Epoch 47: loss 0.9718066100193107
Epoch 48: loss 0.9702708486629569
Epoch 49: loss 0.9685939056069954
-----------Time: 0:03:23.234106, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9962798953056335-------------


Epoch 0: loss 15.041437099290931
Epoch 1: loss 7.105562568488328
Epoch 2: loss 1.9580488048169924
Epoch 3: loss 1.1648792301183162
Epoch 4: loss 1.044754002340462
Epoch 5: loss 1.010087195492309
Epoch 6: loss 0.9985888209679853
Epoch 7: loss 0.994356453548307
Epoch 8: loss 0.9925361865888471
Epoch 9: loss 0.9918134079679198
Epoch 10: loss 0.9914228617497113
Epoch 11: loss 0.9911508692347485
Epoch 12: loss 0.9910350257935732
Epoch 13: loss 0.9909133559983709
Epoch 14: loss 0.9907556211170943
Epoch 15: loss 0.990670264026393
Epoch 16: loss 0.990612595366395
Epoch 17: loss 0.990454235802526
Epoch 18: loss 0.9903547897287037
Epoch 19: loss 0.9901528390205425
Epoch 20: loss 0.9901027987184732
Epoch 21: loss 0.9899315675963526
Epoch 22: loss 0.9897934546289237
Epoch 23: loss 0.9895517169781353
Epoch 24: loss 0.98947432157786
Epoch 25: loss 0.9892232510706652
Epoch 26: loss 0.9891510270212007
Epoch 27: loss 0.9888487865743429
Epoch 28: loss 0.9886253208569858
Epoch 29: loss 0.9882948600727579
Epoch 30: loss 0.9879896621989167
Epoch 31: loss 0.9876459535697232
Epoch 32: loss 0.987287793016952
Epoch 33: loss 0.9869346265559611
Epoch 34: loss 0.9864458476071772
Epoch 35: loss 0.9859872387803119
Epoch 36: loss 0.9854059906109519
Epoch 37: loss 0.9848379023697065
Epoch 38: loss 0.9840247154883717
Epoch 39: loss 0.9833030490771584
Epoch 40: loss 0.9824526390951612
Epoch 41: loss 0.981487916280394
Epoch 42: loss 0.9803220229304355
Epoch 43: loss 0.9791660209064899
Epoch 44: loss 0.9778901927497076
Epoch 45: loss 0.9763219238623329
Epoch 46: loss 0.9745763795531315
Epoch 47: loss 0.9727995722190194
Epoch 48: loss 0.9707533005786979
Epoch 49: loss 0.9685288629454115
-----------Time: 0:04:13.947531, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9968442320823669-------------


Epoch 0: loss 10.049161286457725
Epoch 1: loss 1.1273107557193092
Epoch 2: loss 1.0075527713350627
Epoch 3: loss 0.9983899645183397
Epoch 4: loss 0.9888097846637601
Epoch 5: loss 0.9787885805187018
Epoch 6: loss 0.9697244813908701
Epoch 7: loss 0.9612498688956965
Epoch 8: loss 0.9527019060824229
Epoch 9: loss 0.9443482042654701
Epoch 10: loss 0.9355208591274593
Epoch 11: loss 0.9259707919281462
Epoch 12: loss 0.915730742138365
Epoch 13: loss 0.9044088662318561
Epoch 14: loss 0.892105668200099
Epoch 15: loss 0.8791687273460885
Epoch 16: loss 0.8657727168953937
Epoch 17: loss 0.8521629420311555
Epoch 18: loss 0.8390258274648501
Epoch 19: loss 0.8262802340414213
Epoch 20: loss 0.8140712599391523
Epoch 21: loss 0.8027514425308808
Epoch 22: loss 0.7921531241873037
Epoch 23: loss 0.7827662642235341
Epoch 24: loss 0.774077254274617
Epoch 25: loss 0.7664413810424183
Epoch 26: loss 0.7594621401118196
Epoch 27: loss 0.7533481081542761
Epoch 28: loss 0.7478183367977972
Epoch 29: loss 0.7429399372442909
Epoch 30: loss 0.7385195145140524
Epoch 31: loss 0.7345278609706007
Epoch 32: loss 0.7309400709426921
Epoch 33: loss 0.7276721375791922
Epoch 34: loss 0.7245912189716878
Epoch 35: loss 0.7219346374921177
Epoch 36: loss 0.7195282971729403
Epoch 37: loss 0.7168778907345689
Epoch 38: loss 0.7148291748502981
Epoch 39: loss 0.7126918955341629
Epoch 40: loss 0.710519825829112
Epoch 41: loss 0.7089097088445788
Epoch 42: loss 0.7070158044281213
Epoch 43: loss 0.705401007571946
Epoch 44: loss 0.703773516157399
Epoch 45: loss 0.702198703263117
Epoch 46: loss 0.7008258761271187
Epoch 47: loss 0.6995303843980251
Epoch 48: loss 0.6981285860357077
Epoch 49: loss 0.6969173228611116
-----------Time: 0:02:36.339122, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0981571674346924-------------


Epoch 0: loss 6.581509593388309
Epoch 1: loss 1.0268754766039225
Epoch 2: loss 1.0155910219835198
Epoch 3: loss 1.0061906612437703
Epoch 4: loss 0.9961575836591099
Epoch 5: loss 0.985432319537453
Epoch 6: loss 0.9746531314176062
Epoch 7: loss 0.9610305999284205
Epoch 8: loss 0.9436926709569019
Epoch 9: loss 0.9210368292487185
Epoch 10: loss 0.893425750149333
Epoch 11: loss 0.8607845618672992
Epoch 12: loss 0.8243973460534345
Epoch 13: loss 0.7848824023552563
Epoch 14: loss 0.7442227770452914
Epoch 15: loss 0.704617334060047
Epoch 16: loss 0.6668358886371488
Epoch 17: loss 0.6326079101018284
Epoch 18: loss 0.6028310256807701
Epoch 19: loss 0.5769678268095721
Epoch 20: loss 0.5551260225474834
Epoch 21: loss 0.5366761402267477
Epoch 22: loss 0.5209997280136399
Epoch 23: loss 0.5076282458136911
Epoch 24: loss 0.49581233859062196
Epoch 25: loss 0.4857193624843722
Epoch 26: loss 0.4768471455120522
Epoch 27: loss 0.4688077917241532
Epoch 28: loss 0.46164561966839046
Epoch 29: loss 0.45514344773862675
Epoch 30: loss 0.44927061737879465
Epoch 31: loss 0.4439033925857233
Epoch 32: loss 0.4389665832986002
Epoch 33: loss 0.4343218317174393
Epoch 34: loss 0.43012235631113466
Epoch 35: loss 0.4262003683849521
Epoch 36: loss 0.42266268040175026
Epoch 37: loss 0.41904380600089614
Epoch 38: loss 0.4158433713666771
Epoch 39: loss 0.4127462959807852
Epoch 40: loss 0.4099975063748982
Epoch 41: loss 0.40719936697379405
Epoch 42: loss 0.404558576449104
Epoch 43: loss 0.4021744171562402
Epoch 44: loss 0.3997917067097581
Epoch 45: loss 0.3976164986257968
Epoch 46: loss 0.39523292974285457
Epoch 47: loss 0.3932691759713318
Epoch 48: loss 0.39149241726035655
Epoch 49: loss 0.38960782998929855
-----------Time: 0:01:59.288655, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3115016222000122-------------


Epoch 0: loss 4.85650043565294
Epoch 1: loss 1.0354111318355022
Epoch 2: loss 1.0317951581400373
Epoch 3: loss 1.0251751270631084
Epoch 4: loss 1.0124604391015095
Epoch 5: loss 0.9954421544204588
Epoch 6: loss 0.9729634938032731
Epoch 7: loss 0.9401496678590775
Epoch 8: loss 0.8919094859257989
Epoch 9: loss 0.8291285328890966
Epoch 10: loss 0.7549076373810354
Epoch 11: loss 0.6751861464070237
Epoch 12: loss 0.5961087616889373
Epoch 13: loss 0.5235648319773052
Epoch 14: loss 0.46055708161514736
Epoch 15: loss 0.40787694295463356
Epoch 16: loss 0.3647118076682091
Epoch 17: loss 0.32968255430459975
Epoch 18: loss 0.3012147236291481
Epoch 19: loss 0.27789798401620075
Epoch 20: loss 0.2584560201705798
Epoch 21: loss 0.24216711409389974
Epoch 22: loss 0.22832924864538337
Epoch 23: loss 0.21631520317624445
Epoch 24: loss 0.2060269456680702
Epoch 25: loss 0.19669510563430578
Epoch 26: loss 0.18870313264753508
Epoch 27: loss 0.18146379847565422
Epoch 28: loss 0.17494911776612634
Epoch 29: loss 0.1690332464225914
Epoch 30: loss 0.1637792717827403
Epoch 31: loss 0.15889732616589122
Epoch 32: loss 0.15440963568246882
Epoch 33: loss 0.15021682182407897
Epoch 34: loss 0.14648316509859718
Epoch 35: loss 0.14290644716795373
Epoch 36: loss 0.13974166046828032
Epoch 37: loss 0.13662382207163
Epoch 38: loss 0.13382953365697808
Epoch 39: loss 0.13105440818421218
Epoch 40: loss 0.12862676959484814
Epoch 41: loss 0.12616663754148327
Epoch 42: loss 0.12393653349869925
Epoch 43: loss 0.12175764480362768
Epoch 44: loss 0.11981389130587163
Epoch 45: loss 0.11795649285549703
Epoch 46: loss 0.11606017462099376
Epoch 47: loss 0.1143639383027735
Epoch 48: loss 0.11278301019059575
Epoch 49: loss 0.11114210980417935
-----------Time: 0:02:37.126960, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5124258995056152-------------


Epoch 0: loss 4.214453217970289
Epoch 1: loss 1.0442039030401602
Epoch 2: loss 1.0337248553400453
Epoch 3: loss 1.015233731334624
Epoch 4: loss 0.9905255693456401
Epoch 5: loss 0.9559926084202269
Epoch 6: loss 0.9108175361286039
Epoch 7: loss 0.8472556887113529
Epoch 8: loss 0.7654659497997035
Epoch 9: loss 0.6660521717175193
Epoch 10: loss 0.5608298585790655
Epoch 11: loss 0.46125965086014376
Epoch 12: loss 0.37529746161206906
Epoch 13: loss 0.306057657852121
Epoch 14: loss 0.25228729596280536
Epoch 15: loss 0.21065094725269337
Epoch 16: loss 0.17873268660319888
Epoch 17: loss 0.15423266553198514
Epoch 18: loss 0.13468721467677666
Epoch 19: loss 0.11910413193475941
Epoch 20: loss 0.1064641893800834
Epoch 21: loss 0.09592612861939098
Epoch 22: loss 0.08732956388722295
Epoch 23: loss 0.07977544942303844
Epoch 24: loss 0.07341183377429843
Epoch 25: loss 0.06795329991323144
Epoch 26: loss 0.06318493560361474
Epoch 27: loss 0.05906804008004458
Epoch 28: loss 0.055309285583865386
Epoch 29: loss 0.0520687373355031
Epoch 30: loss 0.04910672932215359
Epoch 31: loss 0.046531971548076555
Epoch 32: loss 0.044186999539480264
Epoch 33: loss 0.042126174497863524
Epoch 34: loss 0.04020901615326495
Epoch 35: loss 0.03845207900335283
Epoch 36: loss 0.03687413629022953
Epoch 37: loss 0.03545688057075376
Epoch 38: loss 0.03413217328286365
Epoch 39: loss 0.03292506324410763
Epoch 40: loss 0.03178092293117357
Epoch 41: loss 0.030786816449835896
Epoch 42: loss 0.029865654090257442
Epoch 43: loss 0.02894885037296816
Epoch 44: loss 0.028154848137384524
Epoch 45: loss 0.027391965646782646
Epoch 46: loss 0.02666636706005944
Epoch 47: loss 0.025999118079957754
Epoch 48: loss 0.02537415052521164
Epoch 49: loss 0.024808579383660916
-----------Time: 0:03:02.826187, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4590593576431274-------------


Epoch 0: loss 3.875242039302121
Epoch 1: loss 1.0515505876230158
Epoch 2: loss 1.0382227825081867
Epoch 3: loss 1.0168780126649402
Epoch 4: loss 0.9852704487417056
Epoch 5: loss 0.934355774651403
Epoch 6: loss 0.8590705471842185
Epoch 7: loss 0.7573660826553469
Epoch 8: loss 0.6328959235678548
Epoch 9: loss 0.5025602148278899
Epoch 10: loss 0.3854700999739377
Epoch 11: loss 0.2908919167907342
Epoch 12: loss 0.21945393700962482
Epoch 13: loss 0.16764139905571937
Epoch 14: loss 0.13019430594437797
Epoch 15: loss 0.10329438982450444
Epoch 16: loss 0.08368152596218431
Epoch 17: loss 0.06919243052682798
Epoch 18: loss 0.058302874057351246
Epoch 19: loss 0.049952411461297586
Epoch 20: loss 0.043353727775747364
Epoch 21: loss 0.03823572879697642
Epoch 22: loss 0.03408719595805135
Epoch 23: loss 0.03086414509655341
Epoch 24: loss 0.028174204146489502
Epoch 25: loss 0.02610602767571159
Epoch 26: loss 0.02434465792030096
Epoch 27: loss 0.022927902881865917
Epoch 28: loss 0.02171942202941231
Epoch 29: loss 0.02075253949581605
Epoch 30: loss 0.020057563827899486
Epoch 31: loss 0.01935912499042309
Epoch 32: loss 0.018770321680273375
Epoch 33: loss 0.018219419359229504
Epoch 34: loss 0.017842861965460623
Epoch 35: loss 0.017497089903032326
Epoch 36: loss 0.0171275374641561
Epoch 37: loss 0.016853540786542
Epoch 38: loss 0.01661405908247537
Epoch 39: loss 0.016351633247635933
Epoch 40: loss 0.016165705614358834
Epoch 41: loss 0.015976872140258228
Epoch 42: loss 0.01574768611513402
Epoch 43: loss 0.015554403265654717
Epoch 44: loss 0.01538294853095937
Epoch 45: loss 0.01528702836341994
Epoch 46: loss 0.01512506501749158
Epoch 47: loss 0.014983279796560174
Epoch 48: loss 0.014869437522619315
Epoch 49: loss 0.01473057274909123
-----------Time: 0:03:35.672132, Loss: regression, n_iter: 50, l2: 1e-09, batch_size: 1024, learning_rate: 0.001, embedding_dim: 200, rmse: 1.282199501991272-------------


Epoch 0: loss 16.128942802677983
Epoch 1: loss 16.129042345544566
Epoch 2: loss 16.128859150928
Epoch 3: loss 16.129249295981033
Epoch 4: loss 16.129442405700683
Epoch 5: loss 16.129047153307045
Epoch 6: loss 16.129132491609326
Epoch 7: loss 16.12939368330914
Epoch 8: loss 16.12904508528502
Epoch 9: loss 16.129338410626286
Epoch 10: loss 16.129529033536496
Epoch 11: loss 16.129444558724114
Epoch 12: loss 16.129363268354666
Epoch 13: loss 16.129483836630115
Epoch 14: loss 16.129331295386606
Epoch 15: loss 16.129113047019295
Epoch 16: loss 16.129248522675557
Epoch 17: loss 16.129503159937652
Epoch 18: loss 16.1291335116262
Epoch 19: loss 16.12918767099795
Epoch 20: loss 16.129149981167007
Epoch 21: loss 16.129050756537396
Epoch 22: loss 16.129325712245443
Epoch 23: loss 16.12951741218567
Epoch 24: loss 16.12920595044675
Epoch 25: loss 16.129335314294565
Epoch 26: loss 16.129110290693202
Epoch 27: loss 16.129017779101495
Epoch 28: loss 16.129287232523378
Epoch 29: loss 16.129065167385598
Epoch 30: loss 16.12915167082911
Epoch 31: loss 16.12945613031802
Epoch 32: loss 16.129304876534835
Epoch 33: loss 16.129250252765157
Epoch 34: loss 16.1290913115377
Epoch 35: loss 16.12914122291233
Epoch 36: loss 16.129235808745676
Epoch 37: loss 16.129305786671846
Epoch 38: loss 16.12936991505001
Epoch 39: loss 16.128948476003565
Epoch 40: loss 16.12929632041765
Epoch 41: loss 16.12931741527889
Epoch 42: loss 16.12918951926024
Epoch 43: loss 16.129311368776403
Epoch 44: loss 16.12885866579802
Epoch 45: loss 16.129028988921124
Epoch 46: loss 16.129211059860562
Epoch 47: loss 16.129224264103435
Epoch 48: loss 16.129309598259304
Epoch 49: loss 16.129186893546063
-----------Time: 0:02:33.262311, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017190456390381-------------


Epoch 0: loss 16.129352569580078
Epoch 1: loss 16.129079570977584
Epoch 2: loss 16.129329874204554
Epoch 3: loss 16.128781000427576
Epoch 4: loss 16.128784885613815
Epoch 5: loss 16.1294575214386
Epoch 6: loss 16.12906658338464
Epoch 7: loss 16.129134432129238
Epoch 8: loss 16.129462726219842
Epoch 9: loss 16.129065026407655
Epoch 10: loss 16.12927885055542
Epoch 11: loss 16.129220345745917
Epoch 12: loss 16.129251754802205
Epoch 13: loss 16.128927766758462
Epoch 14: loss 16.129304249390312
Epoch 15: loss 16.129333829879762
Epoch 16: loss 16.129454505961874
Epoch 17: loss 16.12926615424778
Epoch 18: loss 16.129305198918217
Epoch 19: loss 16.129336014001264
Epoch 20: loss 16.129506639812302
Epoch 21: loss 16.12906260697738
Epoch 22: loss 16.12951153361279
Epoch 23: loss 16.129240757486095
Epoch 24: loss 16.128916731088058
Epoch 25: loss 16.129095977285633
Epoch 26: loss 16.129041722546454
Epoch 27: loss 16.129343104362487
Epoch 28: loss 16.12936510728753
Epoch 29: loss 16.129099415696185
Epoch 30: loss 16.12937557282655
Epoch 31: loss 16.128761590045432
Epoch 32: loss 16.129427225693412
Epoch 33: loss 16.129301892156185
Epoch 34: loss 16.128879057842752
Epoch 35: loss 16.128831485043403
Epoch 36: loss 16.129359056638634
Epoch 37: loss 16.129284019055575
Epoch 38: loss 16.129085374915082
Epoch 39: loss 16.129344392859416
Epoch 40: loss 16.129153991782147
Epoch 41: loss 16.129075162307075
Epoch 42: loss 16.12918094448421
Epoch 43: loss 16.129547972264497
Epoch 44: loss 16.12897746666618
Epoch 45: loss 16.128831161623417
Epoch 46: loss 16.129010491785795
Epoch 47: loss 16.128881257513296
Epoch 48: loss 16.12913768498794
Epoch 49: loss 16.12936306518057
-----------Time: 0:02:53.705770, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017177581787109-------------


Epoch 0: loss 16.12954055848329
Epoch 1: loss 16.129129859675533
Epoch 2: loss 16.128835525720017
Epoch 3: loss 16.129408815632697
Epoch 4: loss 16.12914386002914
Epoch 5: loss 16.12907237695611
Epoch 6: loss 16.129270010409147
Epoch 7: loss 16.129419720691182
Epoch 8: loss 16.12910616086877
Epoch 9: loss 16.129170487238014
Epoch 10: loss 16.129261430450107
Epoch 11: loss 16.12962092420329
Epoch 12: loss 16.128889038251796
Epoch 13: loss 16.12874904508176
Epoch 14: loss 16.12920060261436
Epoch 15: loss 16.128939394328906
Epoch 16: loss 16.128873825073242
Epoch 17: loss 16.12908817477848
Epoch 18: loss 16.12928367075713
Epoch 19: loss 16.128952619303828
Epoch 20: loss 16.129168952029683
Epoch 21: loss 16.12886477138685
Epoch 22: loss 16.12931707838307
Epoch 23: loss 16.12924721448318
Epoch 24: loss 16.12951605112656
Epoch 25: loss 16.129115180347277
Epoch 26: loss 16.12954690767371
Epoch 27: loss 16.12899440060491
Epoch 28: loss 16.12931582927704
Epoch 29: loss 16.12910180610159
Epoch 30: loss 16.12887758172077
Epoch 31: loss 16.12908038885697
Epoch 32: loss 16.129234108717544
Epoch 33: loss 16.12912962022035
Epoch 34: loss 16.129053768904313
Epoch 35: loss 16.12906169787697
Epoch 36: loss 16.128778010865915
Epoch 37: loss 16.12928673806398
Epoch 38: loss 16.129262060704438
Epoch 39: loss 16.12897097753442
Epoch 40: loss 16.12906957709271
Epoch 41: loss 16.12905571046083
Epoch 42: loss 16.12916829275048
Epoch 43: loss 16.129259465051735
Epoch 44: loss 16.129076367875804
Epoch 45: loss 16.12919814379319
Epoch 46: loss 16.129293389942337
Epoch 47: loss 16.129149335363635
Epoch 48: loss 16.129242310316666
Epoch 49: loss 16.129246255625848
-----------Time: 0:03:22.179307, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017183303833008-------------


Epoch 0: loss 16.12862785899121
Epoch 1: loss 16.12923261497332
Epoch 2: loss 16.12917048516481
Epoch 3: loss 16.12898222985475
Epoch 4: loss 16.12899667180103
Epoch 5: loss 16.128949079306228
Epoch 6: loss 16.12903884286466
Epoch 7: loss 16.129087381777556
Epoch 8: loss 16.129331982654072
Epoch 9: loss 16.129360413551332
Epoch 10: loss 16.128931028946585
Epoch 11: loss 16.129185576024263
Epoch 12: loss 16.129574194161787
Epoch 13: loss 16.12911653207696
Epoch 14: loss 16.12926510002302
Epoch 15: loss 16.1292665336443
Epoch 16: loss 16.129243878696276
Epoch 17: loss 16.129410104129626
Epoch 18: loss 16.129388632981673
Epoch 19: loss 16.129234971170842
Epoch 20: loss 16.129497423379316
Epoch 21: loss 16.12883529663086
Epoch 22: loss 16.128873916294264
Epoch 23: loss 16.12916317919026
Epoch 24: loss 16.12909803701484
Epoch 25: loss 16.12955872701562
Epoch 26: loss 16.129247143994206
Epoch 27: loss 16.129093841884448
Epoch 28: loss 16.129144248755082
Epoch 29: loss 16.129058179648027
Epoch 30: loss 16.129101218347966
Epoch 31: loss 16.129074248023656
Epoch 32: loss 16.128983252981435
Epoch 33: loss 16.12895276028177
Epoch 34: loss 16.129258592232414
Epoch 35: loss 16.12901778428451
Epoch 36: loss 16.128991063781406
Epoch 37: loss 16.129091690934224
Epoch 38: loss 16.129171596402706
Epoch 39: loss 16.129114017279253
Epoch 40: loss 16.12938814474189
Epoch 41: loss 16.129243585337765
Epoch 42: loss 16.129177113201308
Epoch 43: loss 16.129025983810426
Epoch 44: loss 16.12888125129368
Epoch 45: loss 16.128676569980122
Epoch 46: loss 16.129175267012222
Epoch 47: loss 16.12931875767915
Epoch 48: loss 16.12896310246509
Epoch 49: loss 16.12917395674664
-----------Time: 0:02:56.057869, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.01718282699585-------------


Epoch 0: loss 16.12926470922387
Epoch 1: loss 16.12953658207603
Epoch 2: loss 16.12928390191949
Epoch 3: loss 16.128711577083752
Epoch 4: loss 16.129280260334845
Epoch 5: loss 16.129254335942477
Epoch 6: loss 16.129146772882212
Epoch 7: loss 16.129471086419148
Epoch 8: loss 16.129287004470825
Epoch 9: loss 16.129169475513955
Epoch 10: loss 16.12896259142005
Epoch 11: loss 16.128824487976406
Epoch 12: loss 16.129398543938347
Epoch 13: loss 16.128983031148497
Epoch 14: loss 16.12954111306564
Epoch 15: loss 16.129006814956664
Epoch 16: loss 16.12896334917649
Epoch 17: loss 16.129274650242017
Epoch 18: loss 16.129109327689463
Epoch 19: loss 16.12936812172765
Epoch 20: loss 16.12896711619004
Epoch 21: loss 16.129093961093737
Epoch 22: loss 16.128714648537013
Epoch 23: loss 16.12904968365379
Epoch 24: loss 16.129420907601066
Epoch 25: loss 16.128932584886964
Epoch 26: loss 16.128612218732417
Epoch 27: loss 16.1292377129845
Epoch 28: loss 16.129193139076232
Epoch 29: loss 16.128667731907058
Epoch 30: loss 16.128789235197978
Epoch 31: loss 16.129293584823607
Epoch 32: loss 16.12904287006544
Epoch 33: loss 16.129251540225486
Epoch 34: loss 16.12921156572259
Epoch 35: loss 16.129149605916894
Epoch 36: loss 16.12885658222696
Epoch 37: loss 16.129068420244298
Epoch 38: loss 16.12896314392919
Epoch 39: loss 16.129687687625054
Epoch 40: loss 16.12911652378414
Epoch 41: loss 16.129137517058332
Epoch 42: loss 16.129149799761564
Epoch 43: loss 16.129223095852396
Epoch 44: loss 16.128841704907625
Epoch 45: loss 16.129271878366886
Epoch 46: loss 16.129123858783554
Epoch 47: loss 16.129584555003955
Epoch 48: loss 16.129076430071954
Epoch 49: loss 16.129187096720155
-----------Time: 0:03:18.524734, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017183780670166-------------


Epoch 0: loss 16.129099257096
Epoch 1: loss 16.128998354206914
Epoch 2: loss 16.128972622622616
Epoch 3: loss 16.12915302152219
Epoch 4: loss 16.129390257337818
Epoch 5: loss 16.129415029028188
Epoch 6: loss 16.129109419947085
Epoch 7: loss 16.129047215503196
Epoch 8: loss 16.129310474188433
Epoch 9: loss 16.129159643339076
Epoch 10: loss 16.129236389243083
Epoch 11: loss 16.129268911610477
Epoch 12: loss 16.129653249616208
Epoch 13: loss 16.12932258170584
Epoch 14: loss 16.12936571991962
Epoch 15: loss 16.129157940201136
Epoch 16: loss 16.129266204004704
Epoch 17: loss 16.12917675557344
Epoch 18: loss 16.12922666694807
Epoch 19: loss 16.129058664778004
Epoch 20: loss 16.129026543575783
Epoch 21: loss 16.129264279033826
Epoch 22: loss 16.129088178924892
Epoch 23: loss 16.129181188085806
Epoch 24: loss 16.12938040132108
Epoch 25: loss 16.12915193930916
Epoch 26: loss 16.12916546697202
Epoch 27: loss 16.129304330245308
Epoch 28: loss 16.129081790343577
Epoch 29: loss 16.129063481869906
Epoch 30: loss 16.129318006142327
Epoch 31: loss 16.12902158446934
Epoch 32: loss 16.128966016354767
Epoch 33: loss 16.129103131916214
Epoch 34: loss 16.129060827130857
Epoch 35: loss 16.12907359185426
Epoch 36: loss 16.128939132068467
Epoch 37: loss 16.129205870628358
Epoch 38: loss 16.129251309063125
Epoch 39: loss 16.128931147119275
Epoch 40: loss 16.1291550853978
Epoch 41: loss 16.12878583514172
Epoch 42: loss 16.129186161704684
Epoch 43: loss 16.12910685331925
Epoch 44: loss 16.129098398789115
Epoch 45: loss 16.129022798330887
Epoch 46: loss 16.128941333812215
Epoch 47: loss 16.128971856573354
Epoch 48: loss 16.128937744057698
Epoch 49: loss 16.12903254446776
-----------Time: 0:01:57.548688, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.01717472076416-------------


Epoch 0: loss 16.129614037016164
Epoch 1: loss 16.129275497146274
Epoch 2: loss 16.129016525849053
Epoch 3: loss 16.12910202689793
Epoch 4: loss 16.129317195519157
Epoch 5: loss 16.12892957770306
Epoch 6: loss 16.129021147023078
Epoch 7: loss 16.129238848064258
Epoch 8: loss 16.12917902054994
Epoch 9: loss 16.129326942692632
Epoch 10: loss 16.1291711817617
Epoch 11: loss 16.129084289592246
Epoch 12: loss 16.1291074089382
Epoch 13: loss 16.128825086096057
Epoch 14: loss 16.129236109360406
Epoch 15: loss 16.128903790142225
Epoch 16: loss 16.129201642326688
Epoch 17: loss 16.12909688638604
Epoch 18: loss 16.129107867116513
Epoch 19: loss 16.128870551482493
Epoch 20: loss 16.12891445989194
Epoch 21: loss 16.128878727166548
Epoch 22: loss 16.12933014475781
Epoch 23: loss 16.12891634858173
Epoch 24: loss 16.128975472242935
Epoch 25: loss 16.129008132478464
Epoch 26: loss 16.129153924402985
Epoch 27: loss 16.128598548018413
Epoch 28: loss 16.12892176793969
Epoch 29: loss 16.12913898281429
Epoch 30: loss 16.128867551554805
Epoch 31: loss 16.128549091712287
Epoch 32: loss 16.12889814376831
Epoch 33: loss 16.12902913611868
Epoch 34: loss 16.12861075815947
Epoch 35: loss 16.129148069671963
Epoch 36: loss 16.128805658091668
Epoch 37: loss 16.128754900849383
Epoch 38: loss 16.128859252515046
Epoch 39: loss 16.12910641587299
Epoch 40: loss 16.128903720689856
Epoch 41: loss 16.128585051453634
Epoch 42: loss 16.128695504561716
Epoch 43: loss 16.129125410577526
Epoch 44: loss 16.128891182982404
Epoch 45: loss 16.12904567822166
Epoch 46: loss 16.128719942466073
Epoch 47: loss 16.128743225595226
Epoch 48: loss 16.128724121010823
Epoch 49: loss 16.129111877731656
-----------Time: 0:02:29.725765, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017144203186035-------------


Epoch 0: loss 16.12908642084702
Epoch 1: loss 16.129116942571557
Epoch 2: loss 16.12903936116592
Epoch 3: loss 16.129016702071482
Epoch 4: loss 16.129532311273657
Epoch 5: loss 16.12941701308541
Epoch 6: loss 16.12918022300886
Epoch 7: loss 16.129123648353247
Epoch 8: loss 16.129239061604377
Epoch 9: loss 16.129524748221687
Epoch 10: loss 16.129311317982882
Epoch 11: loss 16.12927416614864
Epoch 12: loss 16.12920954538428
Epoch 13: loss 16.129041356625763
Epoch 14: loss 16.129037276558254
Epoch 15: loss 16.128737036041592
Epoch 16: loss 16.128985838268115
Epoch 17: loss 16.129075387249824
Epoch 18: loss 16.12917214787525
Epoch 19: loss 16.128789942160896
Epoch 20: loss 16.129412692526113
Epoch 21: loss 16.128891895128334
Epoch 22: loss 16.128811369771544
Epoch 23: loss 16.12935152986775
Epoch 24: loss 16.12886251055676
Epoch 25: loss 16.128998105422312
Epoch 26: loss 16.128811118913735
Epoch 27: loss 16.128731641562087
Epoch 28: loss 16.128817044133726
Epoch 29: loss 16.128928309938182
Epoch 30: loss 16.12919553881106
Epoch 31: loss 16.129014479595682
Epoch 32: loss 16.128662971828295
Epoch 33: loss 16.129172974047453
Epoch 34: loss 16.128436244052388
Epoch 35: loss 16.128974718632904
Epoch 36: loss 16.12903085687886
Epoch 37: loss 16.128831996088444
Epoch 38: loss 16.129181150768115
Epoch 39: loss 16.128677081025167
Epoch 40: loss 16.12855285976244
Epoch 41: loss 16.12877308804056
Epoch 42: loss 16.128953428890394
Epoch 43: loss 16.128557540022808
Epoch 44: loss 16.128785343792128
Epoch 45: loss 16.128912007290385
Epoch 46: loss 16.128853113754936
Epoch 47: loss 16.129070058076277
Epoch 48: loss 16.128720946933914
Epoch 49: loss 16.12941898263019
-----------Time: 0:02:55.440907, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017143249511719-------------


Epoch 0: loss 16.129140184236608
Epoch 1: loss 16.128898041144662
Epoch 2: loss 16.128831659192624
Epoch 3: loss 16.129241861467776
Epoch 4: loss 16.12913607099782
Epoch 5: loss 16.12870488996091
Epoch 6: loss 16.12914567719335
Epoch 7: loss 16.129131640558658
Epoch 8: loss 16.129307396515557
Epoch 9: loss 16.128942838959073
Epoch 10: loss 16.12923093256743
Epoch 11: loss 16.128880771346715
Epoch 12: loss 16.129089108757352
Epoch 13: loss 16.128677801463915
Epoch 14: loss 16.129385828971863
Epoch 15: loss 16.128902518230937
Epoch 16: loss 16.128900356914688
Epoch 17: loss 16.12919262388478
Epoch 18: loss 16.129151230273038
Epoch 19: loss 16.12873542619788
Epoch 20: loss 16.12913263051406
Epoch 21: loss 16.129046576956043
Epoch 22: loss 16.12903347844663
Epoch 23: loss 16.128932217929673
Epoch 24: loss 16.129068319693857
Epoch 25: loss 16.128903583858325
Epoch 26: loss 16.129226566397627
Epoch 27: loss 16.128956912911455
Epoch 28: loss 16.128781531168066
Epoch 29: loss 16.129073695514514
Epoch 30: loss 16.129044370029284
Epoch 31: loss 16.128863348131596
Epoch 32: loss 16.129087990263233
Epoch 33: loss 16.12891324395719
Epoch 34: loss 16.128886185521665
Epoch 35: loss 16.12906505232272
Epoch 36: loss 16.12897875930952
Epoch 37: loss 16.129182665244393
Epoch 38: loss 16.12913587818975
Epoch 39: loss 16.12920773340308
Epoch 40: loss 16.129003882408142
Epoch 41: loss 16.128990885485774
Epoch 42: loss 16.128683568083723
Epoch 43: loss 16.12911406185316
Epoch 44: loss 16.129191530269125
Epoch 45: loss 16.128790059296982
Epoch 46: loss 16.128778154953665
Epoch 47: loss 16.128725851100423
Epoch 48: loss 16.1286145168802
Epoch 49: loss 16.129138167008108
-----------Time: 0:03:23.837097, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017139911651611-------------


Epoch 0: loss 16.12912281492482
Epoch 1: loss 16.1292472642401
Epoch 2: loss 16.129013772632764
Epoch 3: loss 16.128834611436595
Epoch 4: loss 16.129178461821184
Epoch 5: loss 16.12922977157261
Epoch 6: loss 16.1293421631274
Epoch 7: loss 16.128899227017943
Epoch 8: loss 16.129038861523505
Epoch 9: loss 16.12932360586913
Epoch 10: loss 16.128756265018296
Epoch 11: loss 16.128946077305336
Epoch 12: loss 16.128792322200276
Epoch 13: loss 16.128796559831372
Epoch 14: loss 16.128876416579537
Epoch 15: loss 16.129218716206758
Epoch 16: loss 16.12933019036832
Epoch 17: loss 16.129160884152288
Epoch 18: loss 16.12929508478745
Epoch 19: loss 16.129027849694957
Epoch 20: loss 16.129022476984108
Epoch 21: loss 16.12929738500844
Epoch 22: loss 16.12918817167697
Epoch 23: loss 16.129039780989938
Epoch 24: loss 16.12864711491958
Epoch 25: loss 16.129112887382508
Epoch 26: loss 16.12894501686096
Epoch 27: loss 16.128752999720366
Epoch 28: loss 16.12888390188632
Epoch 29: loss 16.12909877196602
Epoch 30: loss 16.12870601674785
Epoch 31: loss 16.12892637771109
Epoch 32: loss 16.128783414674842
Epoch 33: loss 16.129043093971585
Epoch 34: loss 16.12868088639301
Epoch 35: loss 16.1288586979327
Epoch 36: loss 16.128796434402467
Epoch 37: loss 16.128564207450204
Epoch 38: loss 16.128753908820773
Epoch 39: loss 16.128599046624224
Epoch 40: loss 16.128707087558247
Epoch 41: loss 16.128465973812602
Epoch 42: loss 16.128925310010494
Epoch 43: loss 16.12883147156757
Epoch 44: loss 16.128757235278254
Epoch 45: loss 16.12882155024487
Epoch 46: loss 16.128941700769506
Epoch 47: loss 16.128692816651387
Epoch 48: loss 16.128401674395022
Epoch 49: loss 16.12877367683079
-----------Time: 0:04:19.799605, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017140865325928-------------


Epoch 0: loss 16.129534895523733
Epoch 1: loss 16.129430668250375
Epoch 2: loss 16.12888556874317
Epoch 3: loss 16.129182221578514
Epoch 4: loss 16.129085112654643
Epoch 5: loss 16.12903033339459
Epoch 6: loss 16.129188118810237
Epoch 7: loss 16.128893583753836
Epoch 8: loss 16.128431421777478
Epoch 9: loss 16.128769245355027
Epoch 10: loss 16.128899941237076
Epoch 11: loss 16.128501444277557
Epoch 12: loss 16.128333333264226
Epoch 13: loss 16.12875134530275
Epoch 14: loss 16.127965313455334
Epoch 15: loss 16.127950884984887
Epoch 16: loss 16.128084313351174
Epoch 17: loss 16.128018443480784
Epoch 18: loss 16.127980690417083
Epoch 19: loss 16.12794050341067
Epoch 20: loss 16.12787137342536
Epoch 21: loss 16.12791972056679
Epoch 22: loss 16.12771050100741
Epoch 23: loss 16.127833884695303
Epoch 24: loss 16.127474206426868
Epoch 25: loss 16.127535990010138
Epoch 26: loss 16.127468792251918
Epoch 27: loss 16.12723113661227
Epoch 28: loss 16.12680712679158
Epoch 29: loss 16.127293256054752
Epoch 30: loss 16.126968846113787
Epoch 31: loss 16.126883046523385
Epoch 32: loss 16.12703288119772
Epoch 33: loss 16.126710534095764
Epoch 34: loss 16.126953447383382
Epoch 35: loss 16.126592287809952
Epoch 36: loss 16.126604589172032
Epoch 37: loss 16.12660871899646
Epoch 38: loss 16.12645373240761
Epoch 39: loss 16.12624103297358
Epoch 40: loss 16.12635471820831
Epoch 41: loss 16.126699154273325
Epoch 42: loss 16.126233748767685
Epoch 43: loss 16.126371446899746
Epoch 44: loss 16.126229976571125
Epoch 45: loss 16.12603593805562
Epoch 46: loss 16.125807546532673
Epoch 47: loss 16.125631506546682
Epoch 48: loss 16.125823284232098
Epoch 49: loss 16.125499209113745
-----------Time: 0:02:20.481600, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016716003417969-------------


Epoch 0: loss 16.129247790834178
Epoch 1: loss 16.129101499267247
Epoch 2: loss 16.12914525633273
Epoch 3: loss 16.128742071856625
Epoch 4: loss 16.128589170912036
Epoch 5: loss 16.128396677970887
Epoch 6: loss 16.128484602596448
Epoch 7: loss 16.12850483085798
Epoch 8: loss 16.128551462422248
Epoch 9: loss 16.128290228221726
Epoch 10: loss 16.12806362794793
Epoch 11: loss 16.128119371248328
Epoch 12: loss 16.128339077078778
Epoch 13: loss 16.128329894853675
Epoch 14: loss 16.128003372316776
Epoch 15: loss 16.127948870866195
Epoch 16: loss 16.12790209106777
Epoch 17: loss 16.127716389946315
Epoch 18: loss 16.127499335745107
Epoch 19: loss 16.12737334707509
Epoch 20: loss 16.1275593602139
Epoch 21: loss 16.127396314040475
Epoch 22: loss 16.12743226341579
Epoch 23: loss 16.126965311299198
Epoch 24: loss 16.1273221347643
Epoch 25: loss 16.12724063500114
Epoch 26: loss 16.12706432238869
Epoch 27: loss 16.127014332232267
Epoch 28: loss 16.126988809005073
Epoch 29: loss 16.127017670092375
Epoch 30: loss 16.1267421680948
Epoch 31: loss 16.126665860673654
Epoch 32: loss 16.12643228302831
Epoch 33: loss 16.126753443220387
Epoch 34: loss 16.126458080955175
Epoch 35: loss 16.126330170424087
Epoch 36: loss 16.126125375084253
Epoch 37: loss 16.126199712960616
Epoch 38: loss 16.12605285437211
Epoch 39: loss 16.126436215898266
Epoch 40: loss 16.126087363906528
Epoch 41: loss 16.12591999821041
Epoch 42: loss 16.125991966413416
Epoch 43: loss 16.125910415856733
Epoch 44: loss 16.125817677249078
Epoch 45: loss 16.125271445771922
Epoch 46: loss 16.12533577317777
Epoch 47: loss 16.125390682013137
Epoch 48: loss 16.125509901668714
Epoch 49: loss 16.12546488409457
-----------Time: 0:01:59.961782, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016713619232178-------------


Epoch 0: loss 16.129034362668577
Epoch 1: loss 16.12913154311802
Epoch 2: loss 16.129085549064303
Epoch 3: loss 16.128620917900747
Epoch 4: loss 16.12875852584839
Epoch 5: loss 16.128862270064975
Epoch 6: loss 16.12861085974652
Epoch 7: loss 16.12880630389504
Epoch 8: loss 16.128567119266677
Epoch 9: loss 16.12817526070968
Epoch 10: loss 16.128330234859302
Epoch 11: loss 16.12818648711495
Epoch 12: loss 16.128302608365598
Epoch 13: loss 16.12857236136561
Epoch 14: loss 16.12822434798531
Epoch 15: loss 16.127691745758057
Epoch 16: loss 16.12785122187241
Epoch 17: loss 16.127717868141506
Epoch 18: loss 16.12749088847119
Epoch 19: loss 16.127598826781565
Epoch 20: loss 16.12729214170705
Epoch 21: loss 16.12778859138489
Epoch 22: loss 16.12734390652698
Epoch 23: loss 16.127360464179
Epoch 24: loss 16.12762695291768
Epoch 25: loss 16.127542504020358
Epoch 26: loss 16.127475854624873
Epoch 27: loss 16.127112687152366
Epoch 28: loss 16.12708520370981
Epoch 29: loss 16.12684972597205
Epoch 30: loss 16.126667523384093
Epoch 31: loss 16.126850592571756
Epoch 32: loss 16.126708606015082
Epoch 33: loss 16.126323362018752
Epoch 34: loss 16.126676965796428
Epoch 35: loss 16.126420179657313
Epoch 36: loss 16.126303802365843
Epoch 37: loss 16.126636380734652
Epoch 38: loss 16.126266559310583
Epoch 39: loss 16.12610726978468
Epoch 40: loss 16.126140591372614
Epoch 41: loss 16.125976575975834
Epoch 42: loss 16.126023910356604
Epoch 43: loss 16.12590811563575
Epoch 44: loss 16.12612921776979
Epoch 45: loss 16.12583381196727
Epoch 46: loss 16.12577830604885
Epoch 47: loss 16.125455883274906
Epoch 48: loss 16.125476139524707
Epoch 49: loss 16.12526426004327
-----------Time: 0:02:45.508395, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.01671838760376-------------


Epoch 0: loss 16.12924011375593
Epoch 1: loss 16.128896365994994
Epoch 2: loss 16.128894017053685
Epoch 3: loss 16.128687952912372
Epoch 4: loss 16.12905251254206
Epoch 5: loss 16.12891273083894
Epoch 6: loss 16.128525359734244
Epoch 7: loss 16.128622133835503
Epoch 8: loss 16.12841394154922
Epoch 9: loss 16.128643923220427
Epoch 10: loss 16.128253537675608
Epoch 11: loss 16.128206436530405
Epoch 12: loss 16.12808972130651
Epoch 13: loss 16.128082008983778
Epoch 14: loss 16.128108101305756
Epoch 15: loss 16.12801850464033
Epoch 16: loss 16.127879832101904
Epoch 17: loss 16.128017136325006
Epoch 18: loss 16.127332248895065
Epoch 19: loss 16.127713841977325
Epoch 20: loss 16.127296147139177
Epoch 21: loss 16.127658250020897
Epoch 22: loss 16.127534934748773
Epoch 23: loss 16.127039663687995
Epoch 24: loss 16.127370965999106
Epoch 25: loss 16.12687132669532
Epoch 26: loss 16.12680255641108
Epoch 27: loss 16.127107795425083
Epoch 28: loss 16.127276965846185
Epoch 29: loss 16.126711629784626
Epoch 30: loss 16.126820816164432
Epoch 31: loss 16.12667661231497
Epoch 32: loss 16.126864186577176
Epoch 33: loss 16.12666335520537
Epoch 34: loss 16.126629667696747
Epoch 35: loss 16.126643721953684
Epoch 36: loss 16.12632178534632
Epoch 37: loss 16.12628573956697
Epoch 38: loss 16.12650518313698
Epoch 39: loss 16.126373073329095
Epoch 40: loss 16.125751152245893
Epoch 41: loss 16.12609285582667
Epoch 42: loss 16.126153493964154
Epoch 43: loss 16.1259376567343
Epoch 44: loss 16.126081359904745
Epoch 45: loss 16.125501159999683
Epoch 46: loss 16.125820639859075
Epoch 47: loss 16.125741079579228
Epoch 48: loss 16.125466957299604
Epoch 49: loss 16.125409214392953
-----------Time: 0:03:04.713566, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016716003417969-------------


Epoch 0: loss 16.129170376321543
Epoch 1: loss 16.129262360282567
Epoch 2: loss 16.12891428159631
Epoch 3: loss 16.128765750967936
Epoch 4: loss 16.12888130830682
Epoch 5: loss 16.12898928600809
Epoch 6: loss 16.128709909190302
Epoch 7: loss 16.128185085628342
Epoch 8: loss 16.128538533915645
Epoch 9: loss 16.12848180376965
Epoch 10: loss 16.128328906971475
Epoch 11: loss 16.128411664133488
Epoch 12: loss 16.127748384683027
Epoch 13: loss 16.128093095447706
Epoch 14: loss 16.128240193491397
Epoch 15: loss 16.128145253139994
Epoch 16: loss 16.127847443456236
Epoch 17: loss 16.128188736542413
Epoch 18: loss 16.12768906717715
Epoch 19: loss 16.127827400746554
Epoch 20: loss 16.12762565509133
Epoch 21: loss 16.12761797386667
Epoch 22: loss 16.127584080074143
Epoch 23: loss 16.127455136050347
Epoch 24: loss 16.12727176832116
Epoch 25: loss 16.127352458497754
Epoch 26: loss 16.126995852719183
Epoch 27: loss 16.12724882727084
Epoch 28: loss 16.127184285288273
Epoch 29: loss 16.127066484741544
Epoch 30: loss 16.126800316313037
Epoch 31: loss 16.126588541528452
Epoch 32: loss 16.126679001683772
Epoch 33: loss 16.12675963277402
Epoch 34: loss 16.126483618694802
Epoch 35: loss 16.126341765859852
Epoch 36: loss 16.126481290485547
Epoch 37: loss 16.126542288324107
Epoch 38: loss 16.126453996741255
Epoch 39: loss 16.12677532175313
Epoch 40: loss 16.12596721338189
Epoch 41: loss 16.12632046782452
Epoch 42: loss 16.12588852695797
Epoch 43: loss 16.126129574361055
Epoch 44: loss 16.12565050539763
Epoch 45: loss 16.12557949812516
Epoch 46: loss 16.125625313883244
Epoch 47: loss 16.125406065194504
Epoch 48: loss 16.125695930356564
Epoch 49: loss 16.125495187095975
-----------Time: 0:03:38.201151, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.0167155265808105-------------


Epoch 0: loss 16.12888817165209
Epoch 1: loss 16.128189734790634
Epoch 2: loss 16.127729646019315
Epoch 3: loss 16.126569992563
Epoch 4: loss 16.125609807346176
Epoch 5: loss 16.12508325887763
Epoch 6: loss 16.124411922952405
Epoch 7: loss 16.123637297879096
Epoch 8: loss 16.122475759879404
Epoch 9: loss 16.121884783454565
Epoch 10: loss 16.121268130385356
Epoch 11: loss 16.120696898128674
Epoch 12: loss 16.12004226912623
Epoch 13: loss 16.119111433236494
Epoch 14: loss 16.118154611794846
Epoch 15: loss 16.117529706332995
Epoch 16: loss 16.1165292667306
Epoch 17: loss 16.1159078411434
Epoch 18: loss 16.115266259856845
Epoch 19: loss 16.114368442867114
Epoch 20: loss 16.113640718874723
Epoch 21: loss 16.113219935997673
Epoch 22: loss 16.112143202449964
Epoch 23: loss 16.11112932951554
Epoch 24: loss 16.110582271866175
Epoch 25: loss 16.10974072995393
Epoch 26: loss 16.109359924689585
Epoch 27: loss 16.108683957224308
Epoch 28: loss 16.107651262697967
Epoch 29: loss 16.106852176915044
Epoch 30: loss 16.105778908729555
Epoch 31: loss 16.105158454438914
Epoch 32: loss 16.10444638210794
Epoch 33: loss 16.104056519010793
Epoch 34: loss 16.1029968375745
Epoch 35: loss 16.102149933317435
Epoch 36: loss 16.101603800317516
Epoch 37: loss 16.100671204276708
Epoch 38: loss 16.100476641240327
Epoch 39: loss 16.099092490776727
Epoch 40: loss 16.09851647667263
Epoch 41: loss 16.09734378172004
Epoch 42: loss 16.09706857411758
Epoch 43: loss 16.096367630751235
Epoch 44: loss 16.09575873457867
Epoch 45: loss 16.094597437070764
Epoch 46: loss 16.093757971473362
Epoch 47: loss 16.093145060539246
Epoch 48: loss 16.09244763021884
Epoch 49: loss 16.091595556425013
-----------Time: 0:02:32.638492, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012513637542725-------------


Epoch 0: loss 16.128939225362696
Epoch 1: loss 16.128166110619254
Epoch 2: loss 16.127552894923998
Epoch 3: loss 16.126177631253782
Epoch 4: loss 16.12560523074606
Epoch 5: loss 16.124908020185387
Epoch 6: loss 16.12426302847655
Epoch 7: loss 16.123443201313847
Epoch 8: loss 16.122654593509175
Epoch 9: loss 16.121918680356895
Epoch 10: loss 16.12115368635758
Epoch 11: loss 16.12010167474332
Epoch 12: loss 16.119943114985592
Epoch 13: loss 16.118340603165006
Epoch 14: loss 16.117859443374304
Epoch 15: loss 16.11698717656343
Epoch 16: loss 16.116758798516315
Epoch 17: loss 16.115847358496293
Epoch 18: loss 16.114908849674723
Epoch 19: loss 16.114273750263713
Epoch 20: loss 16.11360151042109
Epoch 21: loss 16.112960311640865
Epoch 22: loss 16.1119612434636
Epoch 23: loss 16.11110310450844
Epoch 24: loss 16.110372990110648
Epoch 25: loss 16.110065708989683
Epoch 26: loss 16.10882633872654
Epoch 27: loss 16.10803204619366
Epoch 28: loss 16.107189445910247
Epoch 29: loss 16.106620095087134
Epoch 30: loss 16.105946593699247
Epoch 31: loss 16.10531226552051
Epoch 32: loss 16.10411153772603
Epoch 33: loss 16.103570786766383
Epoch 34: loss 16.10287022072336
Epoch 35: loss 16.102535144142482
Epoch 36: loss 16.10152030405791
Epoch 37: loss 16.10064735516258
Epoch 38: loss 16.09950525138689
Epoch 39: loss 16.09924483195595
Epoch 40: loss 16.09837538159412
Epoch 41: loss 16.097336796055668
Epoch 42: loss 16.09670540975488
Epoch 43: loss 16.09592504190362
Epoch 44: loss 16.09515227234882
Epoch 45: loss 16.094134017695552
Epoch 46: loss 16.093511621848396
Epoch 47: loss 16.092934696570687
Epoch 48: loss 16.091924358450846
Epoch 49: loss 16.09160084206125
-----------Time: 0:02:58.372326, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012455463409424-------------


Epoch 0: loss 16.12870777171591
Epoch 1: loss 16.12767864206563
Epoch 2: loss 16.127268738332003
Epoch 3: loss 16.126696227944414
Epoch 4: loss 16.125727891921997
Epoch 5: loss 16.124779173602228
Epoch 6: loss 16.12393786077914
Epoch 7: loss 16.12335473143536
Epoch 8: loss 16.122937122635218
Epoch 9: loss 16.121912174639494
Epoch 10: loss 16.121076678193134
Epoch 11: loss 16.12023640508237
Epoch 12: loss 16.11974676070006
Epoch 13: loss 16.118840909004213
Epoch 14: loss 16.11786162853241
Epoch 15: loss 16.11751109206158
Epoch 16: loss 16.116549397551495
Epoch 17: loss 16.11617383853249
Epoch 18: loss 16.115009169993193
Epoch 19: loss 16.11448624652365
Epoch 20: loss 16.113366317749023
Epoch 21: loss 16.112730650279833
Epoch 22: loss 16.11211604035419
Epoch 23: loss 16.111156079043514
Epoch 24: loss 16.110579225291378
Epoch 25: loss 16.10990356673365
Epoch 26: loss 16.108707312915637
Epoch 27: loss 16.10814033280248
Epoch 28: loss 16.10737604369288
Epoch 29: loss 16.106528351617897
Epoch 30: loss 16.105830512876096
Epoch 31: loss 16.10531972594883
Epoch 32: loss 16.10455338747605
Epoch 33: loss 16.10359884759654
Epoch 34: loss 16.10294874440069
Epoch 35: loss 16.10226305982341
Epoch 36: loss 16.10131865895313
Epoch 37: loss 16.100556273045747
Epoch 38: loss 16.099853994535362
Epoch 39: loss 16.099311553913616
Epoch 40: loss 16.098111835769984
Epoch 41: loss 16.097562566010847
Epoch 42: loss 16.096447714515353
Epoch 43: loss 16.0961092710495
Epoch 44: loss 16.095321077885835
Epoch 45: loss 16.09390844469485
Epoch 46: loss 16.093848301016767
Epoch 47: loss 16.092865967750548
Epoch 48: loss 16.092197206745976
Epoch 49: loss 16.091561204454173
-----------Time: 0:03:21.529433, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012455463409424-------------


Epoch 0: loss 16.128983265420665
Epoch 1: loss 16.127818473525668
Epoch 2: loss 16.127361730907275
Epoch 3: loss 16.126869074158048
Epoch 4: loss 16.125659969578617
Epoch 5: loss 16.125010270657747
Epoch 6: loss 16.124275621123935
Epoch 7: loss 16.123348613407302
Epoch 8: loss 16.122726353355077
Epoch 9: loss 16.121843079898667
Epoch 10: loss 16.12123375457266
Epoch 11: loss 16.120722584102463
Epoch 12: loss 16.119330188502435
Epoch 13: loss 16.11904540165611
Epoch 14: loss 16.117909507129504
Epoch 15: loss 16.117014689030853
Epoch 16: loss 16.116425041530444
Epoch 17: loss 16.11588995664016
Epoch 18: loss 16.11517411003942
Epoch 19: loss 16.114202048467554
Epoch 20: loss 16.113667112848034
Epoch 21: loss 16.11275631344837
Epoch 22: loss 16.111955230132395
Epoch 23: loss 16.111403951437577
Epoch 24: loss 16.110344494944034
Epoch 25: loss 16.109550878275996
Epoch 26: loss 16.109141437903695
Epoch 27: loss 16.108544128874072
Epoch 28: loss 16.107304906845094
Epoch 29: loss 16.106784151948016
Epoch 30: loss 16.10615019798279
Epoch 31: loss 16.104841767186702
Epoch 32: loss 16.104310862914375
Epoch 33: loss 16.10347306313722
Epoch 34: loss 16.102797751841337
Epoch 35: loss 16.10221567672232
Epoch 36: loss 16.101194901051727
Epoch 37: loss 16.100362697891565
Epoch 38: loss 16.099731871356134
Epoch 39: loss 16.098804574427398
Epoch 40: loss 16.098452759825665
Epoch 41: loss 16.09764650386313
Epoch 42: loss 16.096573345557502
Epoch 43: loss 16.09606996100882
Epoch 44: loss 16.094916404848515
Epoch 45: loss 16.094189957950427
Epoch 46: loss 16.09370371984399
Epoch 47: loss 16.092905740115953
Epoch 48: loss 16.09210953194162
Epoch 49: loss 16.091319761068924
-----------Time: 0:02:43.694845, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012457370758057-------------


Epoch 0: loss 16.129026436805724
Epoch 1: loss 16.12792564060377
Epoch 2: loss 16.12734691163768
Epoch 3: loss 16.126454319124637
Epoch 4: loss 16.125640960361647
Epoch 5: loss 16.12505993635758
Epoch 6: loss 16.12451495087665
Epoch 7: loss 16.12342130308566
Epoch 8: loss 16.122663632683132
Epoch 9: loss 16.122233362819838
Epoch 10: loss 16.120914193858273
Epoch 11: loss 16.120342115733933
Epoch 12: loss 16.119511412537616
Epoch 13: loss 16.118899944554204
Epoch 14: loss 16.118100916821025
Epoch 15: loss 16.117123132166654
Epoch 16: loss 16.116575059683427
Epoch 17: loss 16.11578256772912
Epoch 18: loss 16.114808351060617
Epoch 19: loss 16.11451776649641
Epoch 20: loss 16.11360337215921
Epoch 21: loss 16.112723667725273
Epoch 22: loss 16.112049424129985
Epoch 23: loss 16.11156569874805
Epoch 24: loss 16.110440032378488
Epoch 25: loss 16.10950092854707
Epoch 26: loss 16.109011545388594
Epoch 27: loss 16.10820199821306
Epoch 28: loss 16.107518882336823
Epoch 29: loss 16.106740860317064
Epoch 30: loss 16.105862610236457
Epoch 31: loss 16.105208857163138
Epoch 32: loss 16.10457368415335
Epoch 33: loss 16.1034457994544
Epoch 34: loss 16.102896461279496
Epoch 35: loss 16.10224177941032
Epoch 36: loss 16.101002901533377
Epoch 37: loss 16.100352816996367
Epoch 38: loss 16.09953035271686
Epoch 39: loss 16.09904114474421
Epoch 40: loss 16.098328667101654
Epoch 41: loss 16.097310713063116
Epoch 42: loss 16.09664321878682
Epoch 43: loss 16.095819515767303
Epoch 44: loss 16.095422656639762
Epoch 45: loss 16.094431104867354
Epoch 46: loss 16.093400451411373
Epoch 47: loss 16.09284847093665
Epoch 48: loss 16.092017160291256
Epoch 49: loss 16.091484648248425
-----------Time: 0:03:27.059197, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.01245641708374-------------


Epoch 0: loss 16.125637303227965
Epoch 1: loss 16.11796873756077
Epoch 2: loss 16.1104269048442
Epoch 3: loss 16.10216248450072
Epoch 4: loss 16.094410804043644
Epoch 5: loss 16.087553153867308
Epoch 6: loss 16.079633559351382
Epoch 7: loss 16.07221780548925
Epoch 8: loss 16.064465322701828
Epoch 9: loss 16.056908643763997
Epoch 10: loss 16.049052220842114
Epoch 11: loss 16.041619461515676
Epoch 12: loss 16.03379047538923
Epoch 13: loss 16.02598390786544
Epoch 14: loss 16.018520673461584
Epoch 15: loss 16.011374017466668
Epoch 16: loss 16.003120496998662
Epoch 17: loss 15.995704589719358
Epoch 18: loss 15.98836469442948
Epoch 19: loss 15.980468822562177
Epoch 20: loss 15.97315299199975
Epoch 21: loss 15.965139410806739
Epoch 22: loss 15.95767224042312
Epoch 23: loss 15.950257572920426
Epoch 24: loss 15.94223058431045
Epoch 25: loss 15.935303229871003
Epoch 26: loss 15.927471566200257
Epoch 27: loss 15.91979299524556
Epoch 28: loss 15.912173878628275
Epoch 29: loss 15.904665236887725
Epoch 30: loss 15.897232624758844
Epoch 31: loss 15.889291124758513
Epoch 32: loss 15.881904623819434
Epoch 33: loss 15.874288918661035
Epoch 34: loss 15.86639186506686
Epoch 35: loss 15.85893826795661
Epoch 36: loss 15.851393590802731
Epoch 37: loss 15.84389635272648
Epoch 38: loss 15.836367300282355
Epoch 39: loss 15.828759589402573
Epoch 40: loss 15.821400937826738
Epoch 41: loss 15.814103821049565
Epoch 42: loss 15.806392831387727
Epoch 43: loss 15.798887487079787
Epoch 44: loss 15.791012033172276
Epoch 45: loss 15.783838625576186
Epoch 46: loss 15.776284142162488
Epoch 47: loss 15.768592733922212
Epoch 48: loss 15.760940690662549
Epoch 49: loss 15.75361214513364
-----------Time: 0:02:06.314822, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9699385166168213-------------


Epoch 0: loss 16.125672505212865
Epoch 1: loss 16.11761700174083
Epoch 2: loss 16.110198023008262
Epoch 3: loss 16.10240071752797
Epoch 4: loss 16.094949806254842
Epoch 5: loss 16.08729381042978
Epoch 6: loss 16.079726429607557
Epoch 7: loss 16.071927770324375
Epoch 8: loss 16.064244029832924
Epoch 9: loss 16.056782487164373
Epoch 10: loss 16.048697047648222
Epoch 11: loss 16.041541138939234
Epoch 12: loss 16.033520375127377
Epoch 13: loss 16.026576899445576
Epoch 14: loss 16.018556788693303
Epoch 15: loss 16.010707201128422
Epoch 16: loss 16.00332448275193
Epoch 17: loss 15.99559293207915
Epoch 18: loss 15.98831389364989
Epoch 19: loss 15.980569661181907
Epoch 20: loss 15.973020840727765
Epoch 21: loss 15.965597623327504
Epoch 22: loss 15.958019917944203
Epoch 23: loss 15.949838634159255
Epoch 24: loss 15.942447428081346
Epoch 25: loss 15.934900914067807
Epoch 26: loss 15.927469684766686
Epoch 27: loss 15.919747894743214
Epoch 28: loss 15.911840988242108
Epoch 29: loss 15.90465898928435
Epoch 30: loss 15.896876980947411
Epoch 31: loss 15.889173750255418
Epoch 32: loss 15.881768864134084
Epoch 33: loss 15.874127437757409
Epoch 34: loss 15.866074039625085
Epoch 35: loss 15.85906392906023
Epoch 36: loss 15.85187009624813
Epoch 37: loss 15.843422395250071
Epoch 38: loss 15.835960369524749
Epoch 39: loss 15.828465500085251
Epoch 40: loss 15.821076761121335
Epoch 41: loss 15.813429911240288
Epoch 42: loss 15.805970112137173
Epoch 43: loss 15.798230825299802
Epoch 44: loss 15.79059550140215
Epoch 45: loss 15.783018206513447
Epoch 46: loss 15.775470287903495
Epoch 47: loss 15.767576445703922
Epoch 48: loss 15.760320373203443
Epoch 49: loss 15.752626737304356
-----------Time: 0:02:30.029541, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9698092937469482-------------


Epoch 0: loss 16.125358349344005
Epoch 1: loss 16.11785463353862
Epoch 2: loss 16.10992177258367
Epoch 3: loss 16.102684033435324
Epoch 4: loss 16.0947763774706
Epoch 5: loss 16.087084987889167
Epoch 6: loss 16.07957091746123
Epoch 7: loss 16.07191242031429
Epoch 8: loss 16.064287588907323
Epoch 9: loss 16.056493426405865
Epoch 10: loss 16.04896861159283
Epoch 11: loss 16.04120116752127
Epoch 12: loss 16.03385569634645
Epoch 13: loss 16.026480659194615
Epoch 14: loss 16.01847192204517
Epoch 15: loss 16.01094747522603
Epoch 16: loss 16.003141704849575
Epoch 17: loss 15.995551047117814
Epoch 18: loss 15.98829391417296
Epoch 19: loss 15.980454931051835
Epoch 20: loss 15.97270096488621
Epoch 21: loss 15.965417746875596
Epoch 22: loss 15.957328313329946
Epoch 23: loss 15.950106780425362
Epoch 24: loss 15.942326263759448
Epoch 25: loss 15.935052799141925
Epoch 26: loss 15.926894749765811
Epoch 27: loss 15.919051741517109
Epoch 28: loss 15.911451658995254
Epoch 29: loss 15.904361374481864
Epoch 30: loss 15.896430637525475
Epoch 31: loss 15.88854085362476
Epoch 32: loss 15.881055192325427
Epoch 33: loss 15.873318936513817
Epoch 34: loss 15.865353959539663
Epoch 35: loss 15.85807275046473
Epoch 36: loss 15.850048662268597
Epoch 37: loss 15.842022160861804
Epoch 38: loss 15.834180718919505
Epoch 39: loss 15.826347890107527
Epoch 40: loss 15.818833351135254
Epoch 41: loss 15.810793094013048
Epoch 42: loss 15.80290635254072
Epoch 43: loss 15.794770212795424
Epoch 44: loss 15.787121906487839
Epoch 45: loss 15.779234498480092
Epoch 46: loss 15.770806761409926
Epoch 47: loss 15.762930376633355
Epoch 48: loss 15.75474762501924
Epoch 49: loss 15.746619964682537
-----------Time: 0:03:02.059883, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9690001010894775-------------


Epoch 0: loss 16.125516497570537
Epoch 1: loss 16.117885206056677
Epoch 2: loss 16.11004234500553
Epoch 3: loss 16.10247223895529
Epoch 4: loss 16.094823352150296
Epoch 5: loss 16.087204595234084
Epoch 6: loss 16.079716488589412
Epoch 7: loss 16.07199708171513
Epoch 8: loss 16.06459182034368
Epoch 9: loss 16.05657695998316
Epoch 10: loss 16.04921184622723
Epoch 11: loss 16.04141237735748
Epoch 12: loss 16.033962613603343
Epoch 13: loss 16.02616054078807
Epoch 14: loss 16.01854999791021
Epoch 15: loss 16.011094220824862
Epoch 16: loss 16.003405040243397
Epoch 17: loss 15.995637636599334
Epoch 18: loss 15.98754678187163
Epoch 19: loss 15.980438801516657
Epoch 20: loss 15.97272671720256
Epoch 21: loss 15.964913771463477
Epoch 22: loss 15.95727638265361
Epoch 23: loss 15.949783798922663
Epoch 24: loss 15.941701439152594
Epoch 25: loss 15.933920506809068
Epoch 26: loss 15.926222507849984
Epoch 27: loss 15.918337334757267
Epoch 28: loss 15.91077535048775
Epoch 29: loss 15.902708026637201
Epoch 30: loss 15.89445421177408
Epoch 31: loss 15.886825215298197
Epoch 32: loss 15.87862532346145
Epoch 33: loss 15.870337834565536
Epoch 34: loss 15.862152350467184
Epoch 35: loss 15.85358603415282
Epoch 36: loss 15.845046992923903
Epoch 37: loss 15.836616583492445
Epoch 38: loss 15.827727711719016
Epoch 39: loss 15.81914183886155
Epoch 40: loss 15.810356798379317
Epoch 41: loss 15.800951247629913
Epoch 42: loss 15.79170028126758
Epoch 43: loss 15.782139946066815
Epoch 44: loss 15.772370077216108
Epoch 45: loss 15.762368849049444
Epoch 46: loss 15.752319992106894
Epoch 47: loss 15.741721019537552
Epoch 48: loss 15.731164263642352
Epoch 49: loss 15.719952981368355
-----------Time: 0:03:29.309723, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.965505361557007-------------


Epoch 0: loss 16.125436026117075
Epoch 1: loss 16.118000524976978
Epoch 2: loss 16.110165046608966
Epoch 3: loss 16.102387710239576
Epoch 4: loss 16.094352254660233
Epoch 5: loss 16.087401640933493
Epoch 6: loss 16.079629958194236
Epoch 7: loss 16.071816344883132
Epoch 8: loss 16.06395868840425
Epoch 9: loss 16.05635387690171
Epoch 10: loss 16.049228427721108
Epoch 11: loss 16.04126996786698
Epoch 12: loss 16.033746835459834
Epoch 13: loss 16.025628648633543
Epoch 14: loss 16.01810456255208
Epoch 15: loss 16.01064031953397
Epoch 16: loss 16.00295242537623
Epoch 17: loss 15.995186780846637
Epoch 18: loss 15.987123418890912
Epoch 19: loss 15.97964155363
Epoch 20: loss 15.97198934658714
Epoch 21: loss 15.9637633147447
Epoch 22: loss 15.955655242049176
Epoch 23: loss 15.947521758079528
Epoch 24: loss 15.939311787356502
Epoch 25: loss 15.931063451974289
Epoch 26: loss 15.92234972041586
Epoch 27: loss 15.913854040270266
Epoch 28: loss 15.904976009285969
Epoch 29: loss 15.895751173599907
Epoch 30: loss 15.886818586225095
Epoch 31: loss 15.87717635527901
Epoch 32: loss 15.867345228402511
Epoch 33: loss 15.857668994820637
Epoch 34: loss 15.84679931246716
Epoch 35: loss 15.836191200173419
Epoch 36: loss 15.824954296194989
Epoch 37: loss 15.813335988832558
Epoch 38: loss 15.801889979321023
Epoch 39: loss 15.789222112945888
Epoch 40: loss 15.775873855922534
Epoch 41: loss 15.763031463001086
Epoch 42: loss 15.749481705997301
Epoch 43: loss 15.735251455721647
Epoch 44: loss 15.720351202591607
Epoch 45: loss 15.70545321651127
Epoch 46: loss 15.689585460787233
Epoch 47: loss 15.673575062337129
Epoch 48: loss 15.656132531166076
Epoch 49: loss 15.639588458641716
-----------Time: 0:04:17.171217, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.954923391342163-------------


Epoch 0: loss 16.09141445989194
Epoch 1: loss 16.015602426943573
Epoch 2: loss 15.939459648339644
Epoch 3: loss 15.863917075032774
Epoch 4: loss 15.78864012075507
Epoch 5: loss 15.713477457087972
Epoch 6: loss 15.638131798868594
Epoch 7: loss 15.563131408069445
Epoch 8: loss 15.488523456324701
Epoch 9: loss 15.413846611976624
Epoch 10: loss 15.339470403090766
Epoch 11: loss 15.265086951463118
Epoch 12: loss 15.190967262309531
Epoch 13: loss 15.116862341632014
Epoch 14: loss 15.042723091788915
Epoch 15: loss 14.969343813605931
Epoch 16: loss 14.895089770400006
Epoch 17: loss 14.821284467241039
Epoch 18: loss 14.747302043956259
Epoch 19: loss 14.673332134537075
Epoch 20: loss 14.598989100041598
Epoch 21: loss 14.52418554243834
Epoch 22: loss 14.449109395690586
Epoch 23: loss 14.373625977142998
Epoch 24: loss 14.298123988897904
Epoch 25: loss 14.221364810155785
Epoch 26: loss 14.144060842887216
Epoch 27: loss 14.065671338205751
Epoch 28: loss 13.986638778188954
Epoch 29: loss 13.905579356525255
Epoch 30: loss 13.824154571864916
Epoch 31: loss 13.740752861810767
Epoch 32: loss 13.655735128858815
Epoch 33: loss 13.569778319027114
Epoch 34: loss 13.480692983710247
Epoch 35: loss 13.390415770074595
Epoch 36: loss 13.297796957389169
Epoch 37: loss 13.202895984442337
Epoch 38: loss 13.105800228533537
Epoch 39: loss 13.006552169633949
Epoch 40: loss 12.903955375629923
Epoch 41: loss 12.799275216848955
Epoch 42: loss 12.691692706812983
Epoch 43: loss 12.581527437334476
Epoch 44: loss 12.468747487275497
Epoch 45: loss 12.352454826106197
Epoch 46: loss 12.233692280105922
Epoch 47: loss 12.112092177764229
Epoch 48: loss 11.987597715336344
Epoch 49: loss 11.859878718334695
-----------Time: 0:01:58.419490, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 20, rmse: 3.438387632369995-------------


Epoch 0: loss 16.09128793011541
Epoch 1: loss 16.015557765960693
Epoch 2: loss 15.939143273104792
Epoch 3: loss 15.863441581311433
Epoch 4: loss 15.788046295746513
Epoch 5: loss 15.71233440482098
Epoch 6: loss 15.636319459002952
Epoch 7: loss 15.559992867967356
Epoch 8: loss 15.48314068110093
Epoch 9: loss 15.404617733540743
Epoch 10: loss 15.323818442095881
Epoch 11: loss 15.23931003342504
Epoch 12: loss 15.150095591337784
Epoch 13: loss 15.05457238321719
Epoch 14: loss 14.951055532953013
Epoch 15: loss 14.838350100102632
Epoch 16: loss 14.714430999755859
Epoch 17: loss 14.579346021361973
Epoch 18: loss 14.432324869736382
Epoch 19: loss 14.272521094653918
Epoch 20: loss 14.099612839325614
Epoch 21: loss 13.914152199289074
Epoch 22: loss 13.715869771915933
Epoch 23: loss 13.504633993687838
Epoch 24: loss 13.280925800489342
Epoch 25: loss 13.046070076071699
Epoch 26: loss 12.800117749753205
Epoch 27: loss 12.542482451770617
Epoch 28: loss 12.274988448101542
Epoch 29: loss 11.997638788430587
Epoch 30: loss 11.711166514521059
Epoch 31: loss 11.416177057183306
Epoch 32: loss 11.113586702554123
Epoch 33: loss 10.804058139220528
Epoch 34: loss 10.487875784998355
Epoch 35: loss 10.16651582095934
Epoch 36: loss 9.83987893436266
Epoch 37: loss 9.50913305800894
Epoch 38: loss 9.175009180151898
Epoch 39: loss 8.838571647975757
Epoch 40: loss 8.500037781051967
Epoch 41: loss 8.160681380914605
Epoch 42: loss 7.8211011855498604
Epoch 43: loss 7.482446827577508
Epoch 44: loss 7.145327695038008
Epoch 45: loss 6.810656794776087
Epoch 46: loss 6.479450052199156
Epoch 47: loss 6.152503863106603
Epoch 48: loss 5.830772280693054
Epoch 49: loss 5.515151332253995
-----------Time: 0:02:06.050170, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 50, rmse: 2.3238229751586914-------------


Epoch 0: loss 16.091480742330138
Epoch 1: loss 16.01495694388514
Epoch 2: loss 15.938883370938509
Epoch 3: loss 15.862502236988233
Epoch 4: loss 15.785551599834276
Epoch 5: loss 15.70461512648541
Epoch 6: loss 15.615433154935422
Epoch 7: loss 15.510361996940944
Epoch 8: loss 15.382983666917552
Epoch 9: loss 15.22619250546331
Epoch 10: loss 15.037682979003243
Epoch 11: loss 14.816455050136732
Epoch 12: loss 14.563013949601547
Epoch 13: loss 14.27850934215214
Epoch 14: loss 13.965701912797016
Epoch 15: loss 13.625813541205034
Epoch 16: loss 13.261610066372416
Epoch 17: loss 12.874555045625439
Epoch 18: loss 12.46685603286909
Epoch 19: loss 12.040584479207578
Epoch 20: loss 11.598236025934634
Epoch 21: loss 11.141151674934056
Epoch 22: loss 10.67151692017265
Epoch 23: loss 10.192276009269383
Epoch 24: loss 9.704494027469469
Epoch 25: loss 9.211880437187526
Epoch 26: loss 8.715756131255109
Epoch 27: loss 8.21858596853588
Epoch 28: loss 7.722891616302988
Epoch 29: loss 7.2313485933386765
Epoch 30: loss 6.746121186277141
Epoch 31: loss 6.270157386945642
Epoch 32: loss 5.805922529490098
Epoch 33: loss 5.355819309794384
Epoch 34: loss 4.922286710531815
Epoch 35: loss 4.508235246720521
Epoch 36: loss 4.1151589600936225
Epoch 37: loss 3.746055445463761
Epoch 38: loss 3.402315800345462
Epoch 39: loss 3.086043286323547
Epoch 40: loss 2.798191402010296
Epoch 41: loss 2.5396832263988
Epoch 42: loss 2.3111090525336886
Epoch 43: loss 2.111458901607472
Epoch 44: loss 1.9399566738501839
Epoch 45: loss 1.7946747970321903
Epoch 46: loss 1.673080506272938
Epoch 47: loss 1.5725571274757386
Epoch 48: loss 1.4898129233847495
Epoch 49: loss 1.4220639765262604
-----------Time: 0:02:44.824660, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.1905488967895508-------------


Epoch 0: loss 16.091454866658086
Epoch 1: loss 16.014790963089986
Epoch 2: loss 15.93796277564505
Epoch 3: loss 15.85779856806216
Epoch 4: loss 15.763245830328568
Epoch 5: loss 15.63459436478822
Epoch 6: loss 15.452755474007647
Epoch 7: loss 15.211986553150675
Epoch 8: loss 14.914697066597316
Epoch 9: loss 14.566372845483864
Epoch 10: loss 14.171551395499188
Epoch 11: loss 13.73560886072076
Epoch 12: loss 13.263361909078515
Epoch 13: loss 12.758753793135932
Epoch 14: loss 12.22506922016973
Epoch 15: loss 11.667342960316201
Epoch 16: loss 11.089247195617013
Epoch 17: loss 10.49438972265824
Epoch 18: loss 9.887593528498774
Epoch 19: loss 9.272583269036334
Epoch 20: loss 8.653406686368196
Epoch 21: loss 8.035071415486543
Epoch 22: loss 7.422193175295125
Epoch 23: loss 6.819693907447483
Epoch 24: loss 6.231376457214355
Epoch 25: loss 5.662489356165347
Epoch 26: loss 5.1174549823221955
Epoch 27: loss 4.600945725129998
Epoch 28: loss 4.116617043381152
Epoch 29: loss 3.6687975689120913
Epoch 30: loss 3.2605714074943375
Epoch 31: loss 2.894661903899649
Epoch 32: loss 2.5725674719914147
Epoch 33: loss 2.2946700916342113
Epoch 34: loss 2.059799025369727
Epoch 35: loss 1.8654964980871782
Epoch 36: loss 1.707811533368152
Epoch 37: loss 1.581992473550465
Epoch 38: loss 1.482449189865071
Epoch 39: loss 1.4042167526224385
Epoch 40: loss 1.3422800648471584
Epoch 41: loss 1.2929247720086057
Epoch 42: loss 1.2527406419100968
Epoch 43: loss 1.2194813987483148
Epoch 44: loss 1.191395673803661
Epoch 45: loss 1.167320675176123
Epoch 46: loss 1.1465229592893436
Epoch 47: loss 1.128323302903901
Epoch 48: loss 1.1124068505090217
Epoch 49: loss 1.0982800405958424
-----------Time: 0:03:06.075831, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.054650902748108-------------


Epoch 0: loss 16.091089209266332
Epoch 1: loss 16.015134317978568
Epoch 2: loss 15.935655336794646
Epoch 3: loss 15.840094738421232
Epoch 4: loss 15.689998055540997
Epoch 5: loss 15.453827201801797
Epoch 6: loss 15.126812256937441
Epoch 7: loss 14.722770290789397
Epoch 8: loss 14.255101895332336
Epoch 9: loss 13.732838900192924
Epoch 10: loss 13.163356529111448
Epoch 11: loss 12.552646408910336
Epoch 12: loss 11.908051307305046
Epoch 13: loss 11.234542671493863
Epoch 14: loss 10.539308070099873
Epoch 15: loss 9.828546213067096
Epoch 16: loss 9.109022976004558
Epoch 17: loss 8.387582184957422
Epoch 18: loss 7.671245638702227
Epoch 19: loss 6.966839092710744
Epoch 20: loss 6.281955588382224
Epoch 21: loss 5.623707864595496
Epoch 22: loss 4.998747706413269
Epoch 23: loss 4.4137047697668494
Epoch 24: loss 3.8744156469469484
Epoch 25: loss 3.387055480739345
Epoch 26: loss 2.9550851039264514
Epoch 27: loss 2.5812788305075274
Epoch 28: loss 2.265674850733384
Epoch 29: loss 2.0063430318365927
Epoch 30: loss 1.7990244580351789
Epoch 31: loss 1.6369380769522295
Epoch 32: loss 1.512283520594887
Epoch 33: loss 1.4172506732785184
Epoch 34: loss 1.344588589279548
Epoch 35: loss 1.28843537348768
Epoch 36: loss 1.2439216717429784
Epoch 37: loss 1.2078854607499163
Epoch 38: loss 1.1778917294481526
Epoch 39: loss 1.1526591219331908
Epoch 40: loss 1.1311256767615028
Epoch 41: loss 1.1125749135794847
Epoch 42: loss 1.0966260065203128
Epoch 43: loss 1.0826928484051124
Epoch 44: loss 1.0705922328907511
Epoch 45: loss 1.060111503691777
Epoch 46: loss 1.0509215265512466
Epoch 47: loss 1.042927334425242
Epoch 48: loss 1.035941726617191
Epoch 49: loss 1.0297112867236138
-----------Time: 0:03:46.291417, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0230028629302979-------------


Epoch 0: loss 15.75451917440995
Epoch 1: loss 15.015567448864813
Epoch 2: loss 14.274992904455765
Epoch 3: loss 13.404849026514137
Epoch 4: loss 12.125013471686321
Epoch 5: loss 10.333382049850796
Epoch 6: loss 8.221268902135932
Epoch 7: loss 6.089176647041155
Epoch 8: loss 4.225022264667179
Epoch 9: loss 2.820482133264127
Epoch 10: loss 1.9303062896365706
Epoch 11: loss 1.4626917665419372
Epoch 12: loss 1.2473644094622653
Epoch 13: loss 1.145220345582651
Epoch 14: loss 1.0888345069211463
Epoch 15: loss 1.0537240492260975
Epoch 16: loss 1.0309525110151456
Epoch 17: loss 1.0158028009793032
Epoch 18: loss 1.0056634459158649
Epoch 19: loss 0.9986996169971383
Epoch 20: loss 0.9938519221285115
Epoch 21: loss 0.99051220436459
Epoch 22: loss 0.9880258194778276
Epoch 23: loss 0.9863221331135087
Epoch 24: loss 0.9849032181760539
Epoch 25: loss 0.9839646606989528
Epoch 26: loss 0.9830827689689139
Epoch 27: loss 0.9825000271201134
Epoch 28: loss 0.9820209157207738
Epoch 29: loss 0.9815346329756405
Epoch 30: loss 0.9811266935389975
Epoch 31: loss 0.9807933681684992
Epoch 32: loss 0.9805076622444651
Epoch 33: loss 0.9801473506118941
Epoch 34: loss 0.979906344284182
Epoch 35: loss 0.9796412860569746
Epoch 36: loss 0.9793698353611905
Epoch 37: loss 0.9791198583400768
Epoch 38: loss 0.9788808362639468
Epoch 39: loss 0.9785681476411612
Epoch 40: loss 0.9783162085258442
Epoch 41: loss 0.9780135728094889
Epoch 42: loss 0.9778075215609178
Epoch 43: loss 0.9774992875430895
Epoch 44: loss 0.9772136508770611
Epoch 45: loss 0.9769599313321321
Epoch 46: loss 0.9766491846545883
Epoch 47: loss 0.9762172209827796
Epoch 48: loss 0.9759632811598156
Epoch 49: loss 0.975614272835462
-----------Time: 0:02:33.959013, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.9992852807044983-------------


Epoch 0: loss 15.752592697350876
Epoch 1: loss 14.804703155807827
Epoch 2: loss 12.139402280683102
Epoch 3: loss 7.912109140727831
Epoch 4: loss 4.306431374601696
Epoch 5: loss 2.245762714743614
Epoch 6: loss 1.4399993887414102
Epoch 7: loss 1.1858963111172551
Epoch 8: loss 1.0895895021117252
Epoch 9: loss 1.0427342625415843
Epoch 10: loss 1.017822933132234
Epoch 11: loss 1.0041450266604839
Epoch 12: loss 0.9964976284814917
Epoch 13: loss 0.9921214228091033
Epoch 14: loss 0.9893816192512926
Epoch 15: loss 0.9877394746827043
Epoch 16: loss 0.9866247608609822
Epoch 17: loss 0.9860108852386474
Epoch 18: loss 0.9854492561324782
Epoch 19: loss 0.9850853431483974
Epoch 20: loss 0.984866791639639
Epoch 21: loss 0.9845892341240593
Epoch 22: loss 0.9842965031447618
Epoch 23: loss 0.9841426379654719
Epoch 24: loss 0.9839557660014733
Epoch 25: loss 0.9837973090617553
Epoch 26: loss 0.9835633973712506
Epoch 27: loss 0.9833398489848427
Epoch 28: loss 0.983181668234908
Epoch 29: loss 0.983009825322939
Epoch 30: loss 0.982772264532421
Epoch 31: loss 0.9826311863634898
Epoch 32: loss 0.9824306598176127
Epoch 33: loss 0.9821224791848141
Epoch 34: loss 0.981949757039547
Epoch 35: loss 0.9816931573593098
Epoch 36: loss 0.981395801383516
Epoch 37: loss 0.9812652584003365
Epoch 38: loss 0.980904821144498
Epoch 39: loss 0.9806124964485998
Epoch 40: loss 0.980256768024486
Epoch 41: loss 0.9798699450881585
Epoch 42: loss 0.9795945333397906
Epoch 43: loss 0.979081863294477
Epoch 44: loss 0.9787647982654364
Epoch 45: loss 0.9783041493400283
Epoch 46: loss 0.9778820717464323
Epoch 47: loss 0.9774781624260156
Epoch 48: loss 0.9769376785210941
Epoch 49: loss 0.976327998806601
-----------Time: 0:03:04.952108, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9987084865570068-------------


Epoch 0: loss 15.70358930774357
Epoch 1: loss 12.34924777901691
Epoch 2: loss 5.94738825922427
Epoch 3: loss 2.312374108900195
Epoch 4: loss 1.323126158506974
Epoch 5: loss 1.1120819886093554
Epoch 6: loss 1.0423050805926324
Epoch 7: loss 1.0131330819233604
Epoch 8: loss 1.0003011875178502
Epoch 9: loss 0.9943087874547295
Epoch 10: loss 0.9913331658295963
Epoch 11: loss 0.9897751688309337
Epoch 12: loss 0.9888540083947389
Epoch 13: loss 0.9883248561102411
Epoch 14: loss 0.98806684166193
Epoch 15: loss 0.9877921181528465
Epoch 16: loss 0.9876194618966269
Epoch 17: loss 0.9874305239838103
Epoch 18: loss 0.9873270429994749
Epoch 19: loss 0.9872032314538955
Epoch 20: loss 0.9870561437114426
Epoch 21: loss 0.9868854702166889
Epoch 22: loss 0.9868010255305664
Epoch 23: loss 0.9867125090049661
Epoch 24: loss 0.9865250701489656
Epoch 25: loss 0.9864042200472044
Epoch 26: loss 0.9862975127023199
Epoch 27: loss 0.9860575932523479
Epoch 28: loss 0.9859945437182551
Epoch 29: loss 0.9857425741527391
Epoch 30: loss 0.9855636583722156
Epoch 31: loss 0.9854242387025253
Epoch 32: loss 0.9852016312920528
Epoch 33: loss 0.9850897411937299
Epoch 34: loss 0.9847737323330796
Epoch 35: loss 0.9845951513751693
Epoch 36: loss 0.9842764822037323
Epoch 37: loss 0.9840316229540369
Epoch 38: loss 0.9837016378407893
Epoch 39: loss 0.9834133220755535
Epoch 40: loss 0.9831356011007143
Epoch 41: loss 0.9827368451849274
Epoch 42: loss 0.9822437986083653
Epoch 43: loss 0.981896433558153
Epoch 44: loss 0.9814068314173947
Epoch 45: loss 0.9809479167927866
Epoch 46: loss 0.9804052978754043
Epoch 47: loss 0.9797704778287721
Epoch 48: loss 0.9791344928352729
Epoch 49: loss 0.9785198134572609
-----------Time: 0:02:59.929960, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9989556074142456-------------


Epoch 0: loss 15.480688067104506
Epoch 1: loss 9.475723357304283
Epoch 2: loss 3.107643455007802
Epoch 3: loss 1.3683647861947184
Epoch 4: loss 1.1015575323415838
Epoch 5: loss 1.0319032255074252
Epoch 6: loss 1.0070264178125754
Epoch 7: loss 0.9973693160259205
Epoch 8: loss 0.9932777442361997
Epoch 9: loss 0.9914169282368992
Epoch 10: loss 0.9906456058440001
Epoch 11: loss 0.9901120081543923
Epoch 12: loss 0.9897161836857381
Epoch 13: loss 0.9894629438286242
Epoch 14: loss 0.9894359490145808
Epoch 15: loss 0.9893015401518863
Epoch 16: loss 0.9891594766922619
Epoch 17: loss 0.9890996282515319
Epoch 18: loss 0.9889743768650553
Epoch 19: loss 0.9888534687135531
Epoch 20: loss 0.9886562536592068
Epoch 21: loss 0.9886896799439969
Epoch 22: loss 0.9883326097674991
Epoch 23: loss 0.9882560021851374
Epoch 24: loss 0.9881129351647003
Epoch 25: loss 0.9878688612709875
Epoch 26: loss 0.987760779455952
Epoch 27: loss 0.9874895607647689
Epoch 28: loss 0.9872403872401818
Epoch 29: loss 0.9870495664036792
Epoch 30: loss 0.9867901984116305
Epoch 31: loss 0.9866121279156727
Epoch 32: loss 0.9861637263194375
Epoch 33: loss 0.9858140907857729
Epoch 34: loss 0.9855139131779256
Epoch 35: loss 0.985029150934323
Epoch 36: loss 0.9845301313244779
Epoch 37: loss 0.9840558745938799
Epoch 38: loss 0.9835586316559626
Epoch 39: loss 0.982918296430422
Epoch 40: loss 0.9822057596367338
Epoch 41: loss 0.9814716026186943
Epoch 42: loss 0.980682814380397
Epoch 43: loss 0.9797837973936744
Epoch 44: loss 0.9787261386928351
Epoch 45: loss 0.9775772213935852
Epoch 46: loss 0.9763752765629603
Epoch 47: loss 0.9750590949602749
Epoch 48: loss 0.9737085036609484
Epoch 49: loss 0.9720899513234262
-----------Time: 0:02:40.190951, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9972290396690369-------------


Epoch 0: loss 15.081282089067543
Epoch 1: loss 7.207101307226264
Epoch 2: loss 1.9765796021274897
Epoch 3: loss 1.1669382749692254
Epoch 4: loss 1.0452649204627327
Epoch 5: loss 1.0103444644938344
Epoch 6: loss 0.998673282434111
Epoch 7: loss 0.9943822318445081
Epoch 8: loss 0.9926382960184761
Epoch 9: loss 0.9918314847609271
Epoch 10: loss 0.9914072172797245
Epoch 11: loss 0.9912284370349801
Epoch 12: loss 0.9909482359886169
Epoch 13: loss 0.9908096969127655
Epoch 14: loss 0.9907115363556406
Epoch 15: loss 0.9906091049313546
Epoch 16: loss 0.9904893507128176
Epoch 17: loss 0.9904611502004707
Epoch 18: loss 0.9902869773947675
Epoch 19: loss 0.9900351470579272
Epoch 20: loss 0.9899120629481647
Epoch 21: loss 0.9898991681311442
Epoch 22: loss 0.9896907457191011
Epoch 23: loss 0.9895132537769235
Epoch 24: loss 0.9893173034424367
Epoch 25: loss 0.9890153620554053
Epoch 26: loss 0.9888527180189671
Epoch 27: loss 0.9886965330528177
Epoch 28: loss 0.9883444469260133
Epoch 29: loss 0.9879986982630646
Epoch 30: loss 0.9876908892522688
Epoch 31: loss 0.9873303107593371
Epoch 32: loss 0.9869273667750151
Epoch 33: loss 0.9864452046544655
Epoch 34: loss 0.9859062828447508
Epoch 35: loss 0.985243101482806
Epoch 36: loss 0.984617197384005
Epoch 37: loss 0.9838820383600567
Epoch 38: loss 0.9831123602779015
Epoch 39: loss 0.9821693927697513
Epoch 40: loss 0.981182715361533
Epoch 41: loss 0.980021310241326
Epoch 42: loss 0.9787665336676266
Epoch 43: loss 0.9772980057674906
Epoch 44: loss 0.9757113494302916
Epoch 45: loss 0.9739588076653688
Epoch 46: loss 0.9719749310094378
Epoch 47: loss 0.9697742561283319
Epoch 48: loss 0.9674796664844388
Epoch 49: loss 0.9648834696282511
-----------Time: 0:03:34.276503, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9958152174949646-------------


Epoch 0: loss 10.327649880492169
Epoch 1: loss 1.1335313299427863
Epoch 2: loss 1.0086090371012688
Epoch 3: loss 1.0037611406782398
Epoch 4: loss 1.0008243663155514
Epoch 5: loss 0.997384690720102
Epoch 6: loss 0.9922446467954179
Epoch 7: loss 0.985108485493971
Epoch 8: loss 0.9761706496062486
Epoch 9: loss 0.9653786381301672
Epoch 10: loss 0.9536337640622388
Epoch 11: loss 0.9404106995981673
Epoch 12: loss 0.9266946979190992
Epoch 13: loss 0.9122272824463638
Epoch 14: loss 0.8971669889014701
Epoch 15: loss 0.8822728111692096
Epoch 16: loss 0.8674486288557882
Epoch 17: loss 0.8528650826734046
Epoch 18: loss 0.8390211589958356
Epoch 19: loss 0.8257455981296041
Epoch 20: loss 0.8135081197256627
Epoch 21: loss 0.8019534942248593
Epoch 22: loss 0.7915180223791496
Epoch 23: loss 0.7819649856375611
Epoch 24: loss 0.7733858028183813
Epoch 25: loss 0.7657690354663392
Epoch 26: loss 0.7589047564760498
Epoch 27: loss 0.7529120319563409
Epoch 28: loss 0.7475606671493986
Epoch 29: loss 0.7426082155626753
Epoch 30: loss 0.738324743250142
Epoch 31: loss 0.7344083246329557
Epoch 32: loss 0.7309214752005494
Epoch 33: loss 0.7277410317374312
Epoch 34: loss 0.7247852249637894
Epoch 35: loss 0.721928199607393
Epoch 36: loss 0.7194960803441379
Epoch 37: loss 0.7170921368443448
Epoch 38: loss 0.7148380885305612
Epoch 39: loss 0.7127599723961042
Epoch 40: loss 0.710827144138191
Epoch 41: loss 0.7090353843310605
Epoch 42: loss 0.7073332918079003
Epoch 43: loss 0.7055675541577132
Epoch 44: loss 0.7040739083419676
Epoch 45: loss 0.7026552324709685
Epoch 46: loss 0.7011401374054992
Epoch 47: loss 0.6998162268944409
Epoch 48: loss 0.6985194201702657
Epoch 49: loss 0.6972446523282839
-----------Time: 0:02:13.863109, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0994234085083008-------------


Epoch 0: loss 6.376756251247033
Epoch 1: loss 1.0266497106007908
Epoch 2: loss 1.0159696025045022
Epoch 3: loss 1.0080598025866176
Epoch 4: loss 0.9967739756340566
Epoch 5: loss 0.9849915697522785
Epoch 6: loss 0.9724366038389828
Epoch 7: loss 0.958084071200827
Epoch 8: loss 0.9404907049692195
Epoch 9: loss 0.9196199380185293
Epoch 10: loss 0.8946816066037053
Epoch 11: loss 0.8658334334259448
Epoch 12: loss 0.8324093391713889
Epoch 13: loss 0.7952455894454665
Epoch 14: loss 0.7560669747383698
Epoch 15: loss 0.7162245299505151
Epoch 16: loss 0.6777982393036718
Epoch 17: loss 0.642639290055503
Epoch 18: loss 0.6114391306172247
Epoch 19: loss 0.5846383999547232
Epoch 20: loss 0.5618451750148897
Epoch 21: loss 0.5422812570372354
Epoch 22: loss 0.5259075969133689
Epoch 23: loss 0.5117917708080748
Epoch 24: loss 0.49957919878804163
Epoch 25: loss 0.48903019126990566
Epoch 26: loss 0.47969572369171226
Epoch 27: loss 0.47145973690178083
Epoch 28: loss 0.46392988572302074
Epoch 29: loss 0.457269079302964
Epoch 30: loss 0.45134369487995685
Epoch 31: loss 0.4457583218812943
Epoch 32: loss 0.44069144573548563
Epoch 33: loss 0.435907330720321
Epoch 34: loss 0.43164952387628347
Epoch 35: loss 0.4276695272196894
Epoch 36: loss 0.4237427206467027
Epoch 37: loss 0.42027000843182855
Epoch 38: loss 0.41699039372413055
Epoch 39: loss 0.4138925305851128
Epoch 40: loss 0.41099233050709183
Epoch 41: loss 0.40814370292684304
Epoch 42: loss 0.405636061663213
Epoch 43: loss 0.40315360439860304
Epoch 44: loss 0.40068120136857033
Epoch 45: loss 0.39851236498874165
Epoch 46: loss 0.39632360838029695
Epoch 47: loss 0.3943964221879192
Epoch 48: loss 0.39232825630384943
Epoch 49: loss 0.3905176632754181
-----------Time: 0:02:33.968810, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3143025636672974-------------


Epoch 0: loss 4.825042705367441
Epoch 1: loss 1.0358666569642399
Epoch 2: loss 1.0318001037058624
Epoch 3: loss 1.025328577277453
Epoch 4: loss 1.0123060091034226
Epoch 5: loss 0.994250463143639
Epoch 6: loss 0.9705524367482766
Epoch 7: loss 0.936206361003544
Epoch 8: loss 0.8897026733859725
Epoch 9: loss 0.8304674994038499
Epoch 10: loss 0.7602069519136263
Epoch 11: loss 0.6826690765826599
Epoch 12: loss 0.6043174317349558
Epoch 13: loss 0.5309970839515976
Epoch 14: loss 0.4667796807444614
Epoch 15: loss 0.4126852852170882
Epoch 16: loss 0.3686891955525979
Epoch 17: loss 0.33298910943710286
Epoch 18: loss 0.30389296417327033
Epoch 19: loss 0.28023693994011567
Epoch 20: loss 0.26046587983551234
Epoch 21: loss 0.24407480534004128
Epoch 22: loss 0.22998088614448256
Epoch 23: loss 0.2178914975374937
Epoch 24: loss 0.20740571405900562
Epoch 25: loss 0.19813514917117098
Epoch 26: loss 0.18994761000832786
Epoch 27: loss 0.1826446609004684
Epoch 28: loss 0.17603407316719707
Epoch 29: loss 0.17014644615676092
Epoch 30: loss 0.16477944382020962
Epoch 31: loss 0.15988360521909983
Epoch 32: loss 0.15529182022842375
Epoch 33: loss 0.15111148781750514
Epoch 34: loss 0.14728048523968976
Epoch 35: loss 0.14373840085514214
Epoch 36: loss 0.1404002564837751
Epoch 37: loss 0.1373585079918089
Epoch 38: loss 0.1343666713927751
Epoch 39: loss 0.13168300663971383
Epoch 40: loss 0.1290931881688859
Epoch 41: loss 0.12673144532448571
Epoch 42: loss 0.12443649861151758
Epoch 43: loss 0.12229257159582947
Epoch 44: loss 0.12020290128724731
Epoch 45: loss 0.11832691452101521
Epoch 46: loss 0.1164394401985666
Epoch 47: loss 0.11469106535710719
Epoch 48: loss 0.1129739593471522
Epoch 49: loss 0.1115257496095222
-----------Time: 0:03:03.889697, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5141593217849731-------------


Epoch 0: loss 4.201588910040648
Epoch 1: loss 1.042097653189431
Epoch 2: loss 1.0262575599162476
Epoch 3: loss 1.0041542930447538
Epoch 4: loss 0.9808043725464655
Epoch 5: loss 0.9508774750906488
Epoch 6: loss 0.9064542925228243
Epoch 7: loss 0.8444366490711337
Epoch 8: loss 0.7630330190062523
Epoch 9: loss 0.6645391930704532
Epoch 10: loss 0.5608161373954752
Epoch 11: loss 0.46174400779216185
Epoch 12: loss 0.37637271910257963
Epoch 13: loss 0.3075756537201612
Epoch 14: loss 0.2532983131706715
Epoch 15: loss 0.21158364049118497
Epoch 16: loss 0.1795121040519165
Epoch 17: loss 0.1545881162195102
Epoch 18: loss 0.134923116368768
Epoch 19: loss 0.11926732192868772
Epoch 20: loss 0.10641417087096235
Epoch 21: loss 0.09595361584392578
Epoch 22: loss 0.08705609365082953
Epoch 23: loss 0.07969002209036895
Epoch 24: loss 0.07332936254451457
Epoch 25: loss 0.06778901440455862
Epoch 26: loss 0.06305211703090564
Epoch 27: loss 0.058876837057101984
Epoch 28: loss 0.055151340272277596
Epoch 29: loss 0.051979718920167374
Epoch 30: loss 0.04907583689479076
Epoch 31: loss 0.04645608721867851
Epoch 32: loss 0.044122653912104993
Epoch 33: loss 0.04201110503838762
Epoch 34: loss 0.040151257104361834
Epoch 35: loss 0.03837262321182567
Epoch 36: loss 0.03685317270498237
Epoch 37: loss 0.03533289481926224
Epoch 38: loss 0.034077034831937886
Epoch 39: loss 0.03290124008553508
Epoch 40: loss 0.03176626271325285
Epoch 41: loss 0.030722960475427302
Epoch 42: loss 0.02974690161847874
Epoch 43: loss 0.02885972089741541
Epoch 44: loss 0.02804912851880426
Epoch 45: loss 0.027313176943394153
Epoch 46: loss 0.026587711875934315
Epoch 47: loss 0.025948334246387948
Epoch 48: loss 0.025291544482435868
Epoch 49: loss 0.024642369051909318
-----------Time: 0:03:35.061549, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.001, embedding_dim: 150, rmse: 1.4574958086013794-------------


Epoch 0: loss 3.870948271971682
Epoch 1: loss 1.0513843991186307
Epoch 2: loss 1.0363075387218723
Epoch 3: loss 1.0147314880853113
Epoch 4: loss 0.9858764660747155
Epoch 5: loss 0.938618039307387
Epoch 6: loss 0.8650341799725657
Epoch 7: loss 0.7605673594967178
Epoch 8: loss 0.6325174610899842
Epoch 9: loss 0.4999968082684538
Epoch 10: loss 0.38251574677617656
Epoch 11: loss 0.28871187183844005
Epoch 12: loss 0.21828510965342107
Epoch 13: loss 0.16645854240202385
Epoch 14: loss 0.1294124862584083
Epoch 15: loss 0.10265872327853805
Epoch 16: loss 0.08330022811322757
Epoch 17: loss 0.0688456249990217
Epoch 18: loss 0.057859305142546474
Epoch 19: loss 0.049484241923884204
Epoch 20: loss 0.043133456513042685
Epoch 21: loss 0.038046735225488314
Epoch 22: loss 0.03402020829203336
Epoch 23: loss 0.030692066669302142
Epoch 24: loss 0.028124828102148097
Epoch 25: loss 0.02601322117868973
Epoch 26: loss 0.024306238031662677
Epoch 27: loss 0.022921176414693826
Epoch 28: loss 0.02171576849144438
Epoch 29: loss 0.020760096585297066
Epoch 30: loss 0.01998006943208368
Epoch 31: loss 0.01930231537933097
Epoch 32: loss 0.018772814999860915
Epoch 33: loss 0.01828824556128972
Epoch 34: loss 0.01784392583750836
Epoch 35: loss 0.017471042036524286
Epoch 36: loss 0.017146789244092676
Epoch 37: loss 0.016873479571760348
Epoch 38: loss 0.016629024217431634
Epoch 39: loss 0.016346738470272848
Epoch 40: loss 0.016138194152154027
Epoch 41: loss 0.015949849975700287
Epoch 42: loss 0.01581015425997422
Epoch 43: loss 0.01558439393284852
Epoch 44: loss 0.015388467522216556
Epoch 45: loss 0.015280815150143335
Epoch 46: loss 0.015185018946700123
Epoch 47: loss 0.014998926090724443
Epoch 48: loss 0.014802370320641153
Epoch 49: loss 0.014711702475324273
-----------Time: 0:04:07.188798, Loss: regression, n_iter: 50, l2: 1e-08, batch_size: 1024, learning_rate: 0.001, embedding_dim: 200, rmse: 1.281550407409668-------------


Epoch 0: loss 16.12952209762905
Epoch 1: loss 16.129149515732475
Epoch 2: loss 16.13000893385514
Epoch 3: loss 16.129045975726584
Epoch 4: loss 16.129317412169083
Epoch 5: loss 16.129070259177166
Epoch 6: loss 16.129517771886743
Epoch 7: loss 16.128940239159956
Epoch 8: loss 16.129297379825427
Epoch 9: loss 16.129464024046193
Epoch 10: loss 16.128988033792247
Epoch 11: loss 16.128993274854576
Epoch 12: loss 16.12953886260157
Epoch 13: loss 16.129104829871135
Epoch 14: loss 16.129433859949525
Epoch 15: loss 16.129352855682374
Epoch 16: loss 16.129245456405307
Epoch 17: loss 16.129471039772035
Epoch 18: loss 16.129139382942864
Epoch 19: loss 16.12943076880082
Epoch 20: loss 16.12891032903091
Epoch 21: loss 16.12884921716607
Epoch 22: loss 16.129190297748732
Epoch 23: loss 16.1290761180546
Epoch 24: loss 16.12924306185349
Epoch 25: loss 16.129076521292976
Epoch 26: loss 16.128851114148677
Epoch 27: loss 16.12924828218377
Epoch 28: loss 16.12926124593486
Epoch 29: loss 16.128934862302696
Epoch 30: loss 16.12939105759496
Epoch 31: loss 16.129181476261305
Epoch 32: loss 16.129299780596856
Epoch 33: loss 16.12919748658719
Epoch 34: loss 16.129384112358093
Epoch 35: loss 16.12913135030995
Epoch 36: loss 16.129274789146756
Epoch 37: loss 16.12908234907233
Epoch 38: loss 16.12898271602133
Epoch 39: loss 16.129200090532716
Epoch 40: loss 16.129174542427062
Epoch 41: loss 16.129301261901855
Epoch 42: loss 16.12951119982678
Epoch 43: loss 16.129037078567173
Epoch 44: loss 16.129279642519744
Epoch 45: loss 16.129494392353557
Epoch 46: loss 16.129154066417527
Epoch 47: loss 16.129656579183496
Epoch 48: loss 16.12901972273122
Epoch 49: loss 16.129167469688085
-----------Time: 0:01:47.384229, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.017200469970703-------------


Epoch 0: loss 16.129367966237275
Epoch 1: loss 16.129040468257408
Epoch 2: loss 16.129076239337092
Epoch 3: loss 16.12902535251949
Epoch 4: loss 16.129551663606062
Epoch 5: loss 16.129059179969456
Epoch 6: loss 16.129042004502338
Epoch 7: loss 16.129447321269822
Epoch 8: loss 16.12912584491398
Epoch 9: loss 16.129094561286596
Epoch 10: loss 16.128883842799976
Epoch 11: loss 16.129300048040307
Epoch 12: loss 16.129339163199717
Epoch 13: loss 16.129193656340888
Epoch 14: loss 16.12907481400863
Epoch 15: loss 16.128889218620632
Epoch 16: loss 16.129295722298
Epoch 17: loss 16.128972967811254
Epoch 18: loss 16.12883022868115
Epoch 19: loss 16.12940369585286
Epoch 20: loss 16.129411400919377
Epoch 21: loss 16.12892514000768
Epoch 22: loss 16.129417283638663
Epoch 23: loss 16.129303229373434
Epoch 24: loss 16.12892633313718
Epoch 25: loss 16.129516730101212
Epoch 26: loss 16.129385054629783
Epoch 27: loss 16.128930473327635
Epoch 28: loss 16.129317017223524
Epoch 29: loss 16.128999244648476
Epoch 30: loss 16.12908351317696
Epoch 31: loss 16.128889198925183
Epoch 32: loss 16.129067506997483
Epoch 33: loss 16.12916883800341
Epoch 34: loss 16.12935397313989
Epoch 35: loss 16.129413826569266
Epoch 36: loss 16.12948047078174
Epoch 37: loss 16.129102424953295
Epoch 38: loss 16.129104767674985
Epoch 39: loss 16.129133707544078
Epoch 40: loss 16.129424220582713
Epoch 41: loss 16.12897831460704
Epoch 42: loss 16.129504191357157
Epoch 43: loss 16.129156939879707
Epoch 44: loss 16.129068003530087
Epoch 45: loss 16.12917733192444
Epoch 46: loss 16.129516848273898
Epoch 47: loss 16.129159304370052
Epoch 48: loss 16.129239172520844
Epoch 49: loss 16.129627006986865
-----------Time: 0:02:18.976456, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129021385441657
Epoch 1: loss 16.129471790272255
Epoch 2: loss 16.12907000417295
Epoch 3: loss 16.12918120259824
Epoch 4: loss 16.128926458566085
Epoch 5: loss 16.1294440590817
Epoch 6: loss 16.12901647505553
Epoch 7: loss 16.128994952077452
Epoch 8: loss 16.129396764091823
Epoch 9: loss 16.129301145802373
Epoch 10: loss 16.129107308387756
Epoch 11: loss 16.129268162146857
Epoch 12: loss 16.12921245512755
Epoch 13: loss 16.128770262262094
Epoch 14: loss 16.129036561302517
Epoch 15: loss 16.129382923375005
Epoch 16: loss 16.12904834125353
Epoch 17: loss 16.1289076494134
Epoch 18: loss 16.12882574122885
Epoch 19: loss 16.12919786805692
Epoch 20: loss 16.129139059522878
Epoch 21: loss 16.12942183225051
Epoch 22: loss 16.129062259715536
Epoch 23: loss 16.12920518439749
Epoch 24: loss 16.129121765883074
Epoch 25: loss 16.129116955010787
Epoch 26: loss 16.12916218094204
Epoch 27: loss 16.12939878857654
Epoch 28: loss 16.129344339992688
Epoch 29: loss 16.129674278134885
Epoch 30: loss 16.129286158603172
Epoch 31: loss 16.128830868264902
Epoch 32: loss 16.1290427705516
Epoch 33: loss 16.12929389373116
Epoch 34: loss 16.129138149385867
Epoch 35: loss 16.129247548269188
Epoch 36: loss 16.12925992012024
Epoch 37: loss 16.129111720168073
Epoch 38: loss 16.12915448313174
Epoch 39: loss 16.128968535298885
Epoch 40: loss 16.12922317567079
Epoch 41: loss 16.128993426198544
Epoch 42: loss 16.129323805933414
Epoch 43: loss 16.12915517869203
Epoch 44: loss 16.129223430675008
Epoch 45: loss 16.129364090380463
Epoch 46: loss 16.129285932623823
Epoch 47: loss 16.129095327335854
Epoch 48: loss 16.1290970740111
Epoch 49: loss 16.129149727199387
-----------Time: 0:02:43.396921, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017185688018799-------------


Epoch 0: loss 16.129154858381852
Epoch 1: loss 16.129043583247974
Epoch 2: loss 16.12920661698217
Epoch 3: loss 16.129443637184476
Epoch 4: loss 16.12925183047419
Epoch 5: loss 16.129301415319027
Epoch 6: loss 16.129390303984934
Epoch 7: loss 16.12917123566503
Epoch 8: loss 16.129161731056545
Epoch 9: loss 16.129156621642736
Epoch 10: loss 16.12914257982503
Epoch 11: loss 16.129509472846983
Epoch 12: loss 16.129135786968728
Epoch 13: loss 16.12934217763984
Epoch 14: loss 16.128875596626944
Epoch 15: loss 16.129153010119563
Epoch 16: loss 16.12931141542352
Epoch 17: loss 16.12947012341541
Epoch 18: loss 16.128994627620862
Epoch 19: loss 16.129151081002277
Epoch 20: loss 16.129048607660376
Epoch 21: loss 16.129042802686275
Epoch 22: loss 16.129246982284215
Epoch 23: loss 16.12941978496054
Epoch 24: loss 16.129075273223545
Epoch 25: loss 16.129352845316347
Epoch 26: loss 16.12956228774527
Epoch 27: loss 16.129117683742358
Epoch 28: loss 16.129211863227514
Epoch 29: loss 16.12906841195148
Epoch 30: loss 16.129286650989368
Epoch 31: loss 16.12926521093949
Epoch 32: loss 16.12897340733072
Epoch 33: loss 16.129233227605404
Epoch 34: loss 16.129284698030222
Epoch 35: loss 16.129322098649066
Epoch 36: loss 16.1292823418327
Epoch 37: loss 16.12926516947539
Epoch 38: loss 16.128975146749745
Epoch 39: loss 16.129286353484446
Epoch 40: loss 16.129255088515905
Epoch 41: loss 16.12885949715324
Epoch 42: loss 16.12895695126575
Epoch 43: loss 16.129067621023758
Epoch 44: loss 16.12915690774503
Epoch 45: loss 16.128889067276663
Epoch 46: loss 16.128993117290996
Epoch 47: loss 16.129170974441198
Epoch 48: loss 16.128855309279068
Epoch 49: loss 16.129141454074695
-----------Time: 0:03:06.166936, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017185211181641-------------


Epoch 0: loss 16.12881646467292
Epoch 1: loss 16.12936220894689
Epoch 2: loss 16.128869268168575
Epoch 3: loss 16.129079088957415
Epoch 4: loss 16.129316107086513
Epoch 5: loss 16.12944522525953
Epoch 6: loss 16.129135780749113
Epoch 7: loss 16.128994207796843
Epoch 8: loss 16.129376315033955
Epoch 9: loss 16.129183633431143
Epoch 10: loss 16.12935088717419
Epoch 11: loss 16.129210578876993
Epoch 12: loss 16.129095269286115
Epoch 13: loss 16.129051715394723
Epoch 14: loss 16.12933598290319
Epoch 15: loss 16.129324127280192
Epoch 16: loss 16.129314572914787
Epoch 17: loss 16.129361974674723
Epoch 18: loss 16.129550992924234
Epoch 19: loss 16.128951245805492
Epoch 20: loss 16.129264854348225
Epoch 21: loss 16.128939846287604
Epoch 22: loss 16.12895664650461
Epoch 23: loss 16.128960242478744
Epoch 24: loss 16.129261695820354
Epoch 25: loss 16.128948958023734
Epoch 26: loss 16.129310344613117
Epoch 27: loss 16.129202554536903
Epoch 28: loss 16.129263080721316
Epoch 29: loss 16.129079010175623
Epoch 30: loss 16.129385743970456
Epoch 31: loss 16.129185473400614
Epoch 32: loss 16.128902176152106
Epoch 33: loss 16.129175298110297
Epoch 34: loss 16.12868793114372
Epoch 35: loss 16.129232721743378
Epoch 36: loss 16.129338651118072
Epoch 37: loss 16.129163390657176
Epoch 38: loss 16.12906001754429
Epoch 39: loss 16.129429197311403
Epoch 40: loss 16.12892664100813
Epoch 41: loss 16.12956089351488
Epoch 42: loss 16.129532701036204
Epoch 43: loss 16.12917952330216
Epoch 44: loss 16.12917009954867
Epoch 45: loss 16.12899903525477
Epoch 46: loss 16.12927216757899
Epoch 47: loss 16.12918301872585
Epoch 48: loss 16.1291769867358
Epoch 49: loss 16.129228306853253
-----------Time: 0:03:59.192033, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017183303833008-------------


Epoch 0: loss 16.129491938715397
Epoch 1: loss 16.129303834749305
Epoch 2: loss 16.129553343938746
Epoch 3: loss 16.129020759333734
Epoch 4: loss 16.129147151242133
Epoch 5: loss 16.129643708726633
Epoch 6: loss 16.12932960157809
Epoch 7: loss 16.129046490918036
Epoch 8: loss 16.12941098627837
Epoch 9: loss 16.12956572926563
Epoch 10: loss 16.129212506957675
Epoch 11: loss 16.12915237571882
Epoch 12: loss 16.128841370085013
Epoch 13: loss 16.12917343844538
Epoch 14: loss 16.129346953267635
Epoch 15: loss 16.128976764886275
Epoch 16: loss 16.129211823836616
Epoch 17: loss 16.129146499219146
Epoch 18: loss 16.12927773828092
Epoch 19: loss 16.12915163351142
Epoch 20: loss 16.129106305993123
Epoch 21: loss 16.129048623209414
Epoch 22: loss 16.129347190649614
Epoch 23: loss 16.129064341213393
Epoch 24: loss 16.12901677256045
Epoch 25: loss 16.129204808110774
Epoch 26: loss 16.129190242808797
Epoch 27: loss 16.128923319733662
Epoch 28: loss 16.129146749040356
Epoch 29: loss 16.12883124144181
Epoch 30: loss 16.129108427918474
Epoch 31: loss 16.129088686860126
Epoch 32: loss 16.12905953967053
Epoch 33: loss 16.128992272459943
Epoch 34: loss 16.128901001681452
Epoch 35: loss 16.128982113755267
Epoch 36: loss 16.128935548533565
Epoch 37: loss 16.129065621417503
Epoch 38: loss 16.128947311898937
Epoch 39: loss 16.12896983312524
Epoch 40: loss 16.129182790673298
Epoch 41: loss 16.12878690180571
Epoch 42: loss 16.129205517146897
Epoch 43: loss 16.128863641490106
Epoch 44: loss 16.128980201223623
Epoch 45: loss 16.12899628618489
Epoch 46: loss 16.128753078502157
Epoch 47: loss 16.129032879290374
Epoch 48: loss 16.128717031686204
Epoch 49: loss 16.128916006502898
-----------Time: 0:02:39.344461, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017151832580566-------------


Epoch 0: loss 16.12891797086467
Epoch 1: loss 16.12944124781567
Epoch 2: loss 16.129222008456356
Epoch 3: loss 16.129290255256322
Epoch 4: loss 16.12900218030681
Epoch 5: loss 16.129321830169015
Epoch 6: loss 16.129256077434707
Epoch 7: loss 16.129203025154446
Epoch 8: loss 16.129431360700856
Epoch 9: loss 16.12905618833459
Epoch 10: loss 16.12905832995539
Epoch 11: loss 16.12938323953877
Epoch 12: loss 16.128776549256365
Epoch 13: loss 16.128799521404765
Epoch 14: loss 16.12929083471713
Epoch 15: loss 16.12893596732098
Epoch 16: loss 16.129041463395822
Epoch 17: loss 16.129128938135892
Epoch 18: loss 16.128865563351177
Epoch 19: loss 16.129002273601035
Epoch 20: loss 16.128705849854843
Epoch 21: loss 16.129542008690212
Epoch 22: loss 16.12897822753243
Epoch 23: loss 16.129112602316816
Epoch 24: loss 16.129109159759853
Epoch 25: loss 16.12908431654391
Epoch 26: loss 16.129100898037787
Epoch 27: loss 16.12897333684175
Epoch 28: loss 16.129217828875003
Epoch 29: loss 16.129090587989143
Epoch 30: loss 16.129330126098964
Epoch 31: loss 16.129099320328756
Epoch 32: loss 16.12870647492616
Epoch 33: loss 16.12900668123494
Epoch 34: loss 16.128829924956612
Epoch 35: loss 16.1290880172149
Epoch 36: loss 16.129127900496773
Epoch 37: loss 16.128695346998132
Epoch 38: loss 16.12897822027621
Epoch 39: loss 16.128830639175746
Epoch 40: loss 16.129210173565408
Epoch 41: loss 16.12875081559886
Epoch 42: loss 16.128989499548208
Epoch 43: loss 16.12886115571727
Epoch 44: loss 16.128733394456948
Epoch 45: loss 16.12875168427177
Epoch 46: loss 16.128810053286344
Epoch 47: loss 16.12887951809427
Epoch 48: loss 16.12847316576087
Epoch 49: loss 16.128847189571548
-----------Time: 0:03:06.591892, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017143249511719-------------


Epoch 0: loss 16.129153003899948
Epoch 1: loss 16.129179298359414
Epoch 2: loss 16.129061241771865
Epoch 3: loss 16.128922604477925
Epoch 4: loss 16.129366960732835
Epoch 5: loss 16.128975518890048
Epoch 6: loss 16.12934276746667
Epoch 7: loss 16.129486793020497
Epoch 8: loss 16.12935942463253
Epoch 9: loss 16.1291942368383
Epoch 10: loss 16.12927325497503
Epoch 11: loss 16.128844096349635
Epoch 12: loss 16.12904625457266
Epoch 13: loss 16.12897344775822
Epoch 14: loss 16.129119053094282
Epoch 15: loss 16.12918458606886
Epoch 16: loss 16.129034122176794
Epoch 17: loss 16.128839899146037
Epoch 18: loss 16.129091575871342
Epoch 19: loss 16.128690601431806
Epoch 20: loss 16.128802698591482
Epoch 21: loss 16.12937222460042
Epoch 22: loss 16.12864017382912
Epoch 23: loss 16.12896309002586
Epoch 24: loss 16.128859805024188
Epoch 25: loss 16.129319186832593
Epoch 26: loss 16.128887716583584
Epoch 27: loss 16.12897985810819
Epoch 28: loss 16.128756956432177
Epoch 29: loss 16.129098576048147
Epoch 30: loss 16.129085443330847
Epoch 31: loss 16.12928857181383
Epoch 32: loss 16.12879242171412
Epoch 33: loss 16.129121634234554
Epoch 34: loss 16.1291355278181
Epoch 35: loss 16.128884723912115
Epoch 36: loss 16.128500899024633
Epoch 37: loss 16.12902848098589
Epoch 38: loss 16.129021366782812
Epoch 39: loss 16.128812745343083
Epoch 40: loss 16.129215474750684
Epoch 41: loss 16.12866656365602
Epoch 42: loss 16.128663304577703
Epoch 43: loss 16.128787092540573
Epoch 44: loss 16.128968875304512
Epoch 45: loss 16.129185976152836
Epoch 46: loss 16.128640531456988
Epoch 47: loss 16.128740554270536
Epoch 48: loss 16.128814606044603
Epoch 49: loss 16.128583252948264
-----------Time: 0:02:34.829592, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.017143249511719-------------


Epoch 0: loss 16.129168033599854
Epoch 1: loss 16.129054658309272
Epoch 2: loss 16.129404888982357
Epoch 3: loss 16.129211698407712
Epoch 4: loss 16.12905036366504
Epoch 5: loss 16.129042739453524
Epoch 6: loss 16.1292193516441
Epoch 7: loss 16.129276328501494
Epoch 8: loss 16.12909964063893
Epoch 9: loss 16.12896193317745
Epoch 10: loss 16.12918125235516
Epoch 11: loss 16.12894327847854
Epoch 12: loss 16.12912338712941
Epoch 13: loss 16.12914763637211
Epoch 14: loss 16.128831024791882
Epoch 15: loss 16.129059159237407
Epoch 16: loss 16.12917681258658
Epoch 17: loss 16.12913213190825
Epoch 18: loss 16.129079897507378
Epoch 19: loss 16.129099482038747
Epoch 20: loss 16.129327339711395
Epoch 21: loss 16.12885492366293
Epoch 22: loss 16.12882480621338
Epoch 23: loss 16.128759968799095
Epoch 24: loss 16.129563147088756
Epoch 25: loss 16.12899380663167
Epoch 26: loss 16.12939402953438
Epoch 27: loss 16.128942414988643
Epoch 28: loss 16.129121387523153
Epoch 29: loss 16.128863732711128
Epoch 30: loss 16.12891710737477
Epoch 31: loss 16.12882884170698
Epoch 32: loss 16.128653178007706
Epoch 33: loss 16.12898257193358
Epoch 34: loss 16.12863799592723
Epoch 35: loss 16.12856052647466
Epoch 36: loss 16.12909199880517
Epoch 37: loss 16.128722111038538
Epoch 38: loss 16.12904257670693
Epoch 39: loss 16.12890305208123
Epoch 40: loss 16.128814595678577
Epoch 41: loss 16.12900035277657
Epoch 42: loss 16.129283693562382
Epoch 43: loss 16.128795133466305
Epoch 44: loss 16.128795005964196
Epoch 45: loss 16.128732875119084
Epoch 46: loss 16.12866314597752
Epoch 47: loss 16.128803849220276
Epoch 48: loss 16.1290100408637
Epoch 49: loss 16.128844187570653
-----------Time: 0:02:50.681119, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017140865325928-------------


Epoch 0: loss 16.128983192858488
Epoch 1: loss 16.129239573686018
Epoch 2: loss 16.129149094871853
Epoch 3: loss 16.129189377245694
Epoch 4: loss 16.129090181640958
Epoch 5: loss 16.12916865452476
Epoch 6: loss 16.12914239116337
Epoch 7: loss 16.129511122081592
Epoch 8: loss 16.129129408753435
Epoch 9: loss 16.128714886955592
Epoch 10: loss 16.129165284529975
Epoch 11: loss 16.128959225571673
Epoch 12: loss 16.129103174416915
Epoch 13: loss 16.129035695739415
Epoch 14: loss 16.128789093183435
Epoch 15: loss 16.12899963233782
Epoch 16: loss 16.129096640711246
Epoch 17: loss 16.12911085564157
Epoch 18: loss 16.12903765388157
Epoch 19: loss 16.129093390962353
Epoch 20: loss 16.128933013003806
Epoch 21: loss 16.129325627244036
Epoch 22: loss 16.128951498736505
Epoch 23: loss 16.128941757782645
Epoch 24: loss 16.129107406864996
Epoch 25: loss 16.128751837688945
Epoch 26: loss 16.128973253913546
Epoch 27: loss 16.12922105478204
Epoch 28: loss 16.129040817592454
Epoch 29: loss 16.128890383761863
Epoch 30: loss 16.12877585576928
Epoch 31: loss 16.1290901173716
Epoch 32: loss 16.129073647830797
Epoch 33: loss 16.128709271679753
Epoch 34: loss 16.128950448658156
Epoch 35: loss 16.12873116887134
Epoch 36: loss 16.12906819011854
Epoch 37: loss 16.128891484633737
Epoch 38: loss 16.129205623916956
Epoch 39: loss 16.12886519535728
Epoch 40: loss 16.129016938416854
Epoch 41: loss 16.128997247115425
Epoch 42: loss 16.128720211982728
Epoch 43: loss 16.128883736029916
Epoch 44: loss 16.12883673336195
Epoch 45: loss 16.12845174022343
Epoch 46: loss 16.128972103284752
Epoch 47: loss 16.128953515965005
Epoch 48: loss 16.128749536431354
Epoch 49: loss 16.12900391454282
-----------Time: 0:03:37.413702, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017141819000244-------------


Epoch 0: loss 16.128892753435217
Epoch 1: loss 16.129073646794193
Epoch 2: loss 16.128843550060108
Epoch 3: loss 16.12895998125491
Epoch 4: loss 16.128748774528503
Epoch 5: loss 16.128867712228196
Epoch 6: loss 16.128544886215874
Epoch 7: loss 16.12853263979373
Epoch 8: loss 16.128684787128282
Epoch 9: loss 16.12856753287108
Epoch 10: loss 16.12847775583682
Epoch 11: loss 16.128264627249344
Epoch 12: loss 16.12826890219813
Epoch 13: loss 16.12813426826311
Epoch 14: loss 16.128152520760246
Epoch 15: loss 16.127519692545352
Epoch 16: loss 16.12768790929214
Epoch 17: loss 16.127929915552553
Epoch 18: loss 16.127861497713173
Epoch 19: loss 16.128105494250423
Epoch 20: loss 16.1274730247
Epoch 21: loss 16.1277704415114
Epoch 22: loss 16.127248319335607
Epoch 23: loss 16.12712693007096
Epoch 24: loss 16.127011132240295
Epoch 25: loss 16.127258406514706
Epoch 26: loss 16.12719555626745
Epoch 27: loss 16.12705819606781
Epoch 28: loss 16.127128110761227
Epoch 29: loss 16.126838769083438
Epoch 30: loss 16.126562967507734
Epoch 31: loss 16.126749521753062
Epoch 32: loss 16.12649647982224
Epoch 33: loss 16.12644493683525
Epoch 34: loss 16.126381809815115
Epoch 35: loss 16.126668494680654
Epoch 36: loss 16.126433170360066
Epoch 37: loss 16.12585833279983
Epoch 38: loss 16.126366981216098
Epoch 39: loss 16.126169698134714
Epoch 40: loss 16.126336723825204
Epoch 41: loss 16.12599135067152
Epoch 42: loss 16.125837259707243
Epoch 43: loss 16.125605458798617
Epoch 44: loss 16.125672537347544
Epoch 45: loss 16.125832514140917
Epoch 46: loss 16.125577697546586
Epoch 47: loss 16.125512801045957
Epoch 48: loss 16.1255518073621
Epoch 49: loss 16.125591740400896
-----------Time: 0:02:15.423712, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.016712665557861-------------


Epoch 0: loss 16.129346767715784
Epoch 1: loss 16.129055213928222
Epoch 2: loss 16.129132409717727
Epoch 3: loss 16.129023674260015
Epoch 4: loss 16.128661020942356
Epoch 5: loss 16.12886757539666
Epoch 6: loss 16.128698280583258
Epoch 7: loss 16.12875352838765
Epoch 8: loss 16.12834596633911
Epoch 9: loss 16.128510710467463
Epoch 10: loss 16.128456147857335
Epoch 11: loss 16.128627413252126
Epoch 12: loss 16.128128142978834
Epoch 13: loss 16.12820364703303
Epoch 14: loss 16.12814089940942
Epoch 15: loss 16.127950921265974
Epoch 16: loss 16.12808170836905
Epoch 17: loss 16.127772039952486
Epoch 18: loss 16.127883402160975
Epoch 19: loss 16.127777016681172
Epoch 20: loss 16.127552958156752
Epoch 21: loss 16.127731135617132
Epoch 22: loss 16.12749996807264
Epoch 23: loss 16.127113940404808
Epoch 24: loss 16.127326041719186
Epoch 25: loss 16.12701943853627
Epoch 26: loss 16.127154074544492
Epoch 27: loss 16.126969356122224
Epoch 28: loss 16.127010558999103
Epoch 29: loss 16.126870448692983
Epoch 30: loss 16.126724636036418
Epoch 31: loss 16.12693683893784
Epoch 32: loss 16.126278669937797
Epoch 33: loss 16.126641582406087
Epoch 34: loss 16.126507304025733
Epoch 35: loss 16.126657858102217
Epoch 36: loss 16.126205530373948
Epoch 37: loss 16.126397038542706
Epoch 38: loss 16.12607619036799
Epoch 39: loss 16.126241213342418
Epoch 40: loss 16.126122734857642
Epoch 41: loss 16.12595340790956
Epoch 42: loss 16.126057564693948
Epoch 43: loss 16.125931561511496
Epoch 44: loss 16.125926981801573
Epoch 45: loss 16.125913201207698
Epoch 46: loss 16.125591608752377
Epoch 47: loss 16.125670834209608
Epoch 48: loss 16.125697721605714
Epoch 49: loss 16.125274955708047
-----------Time: 0:02:43.097617, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016709327697754-------------


Epoch 0: loss 16.129322060294772
Epoch 1: loss 16.129123450362165
Epoch 2: loss 16.128751258228135
Epoch 3: loss 16.128942147545192
Epoch 4: loss 16.12945072028948
Epoch 5: loss 16.12882330313973
Epoch 6: loss 16.128761946636697
Epoch 7: loss 16.128583798201188
Epoch 8: loss 16.128685416346013
Epoch 9: loss 16.128447375090225
Epoch 10: loss 16.128312459199325
Epoch 11: loss 16.128423488658406
Epoch 12: loss 16.12821159466453
Epoch 13: loss 16.12836088615915
Epoch 14: loss 16.127844133584397
Epoch 15: loss 16.128046958342843
Epoch 16: loss 16.12772836788841
Epoch 17: loss 16.1279878616333
Epoch 18: loss 16.12781628422115
Epoch 19: loss 16.127394383886585
Epoch 20: loss 16.12721419438072
Epoch 21: loss 16.127564247794773
Epoch 22: loss 16.1275858319324
Epoch 23: loss 16.127631908914317
Epoch 24: loss 16.126969397586326
Epoch 25: loss 16.12727072653563
Epoch 26: loss 16.126982810186302
Epoch 27: loss 16.12694642543793
Epoch 28: loss 16.12715249268905
Epoch 29: loss 16.126767193752787
Epoch 30: loss 16.126885739616725
Epoch 31: loss 16.126947521126787
Epoch 32: loss 16.126752885528234
Epoch 33: loss 16.126743375736734
Epoch 34: loss 16.12679436310478
Epoch 35: loss 16.126154076534768
Epoch 36: loss 16.1260995802672
Epoch 37: loss 16.126394877226456
Epoch 38: loss 16.126132953685264
Epoch 39: loss 16.12597567413164
Epoch 40: loss 16.12599651398866
Epoch 41: loss 16.125820948766627
Epoch 42: loss 16.125794743454975
Epoch 43: loss 16.125553257569024
Epoch 44: loss 16.12582924676978
Epoch 45: loss 16.12586192773736
Epoch 46: loss 16.125591303991236
Epoch 47: loss 16.12585417913354
Epoch 48: loss 16.125695934502975
Epoch 49: loss 16.125265888545822
-----------Time: 0:03:07.187173, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016714096069336-------------


Epoch 0: loss 16.128977080013442
Epoch 1: loss 16.1291536828746
Epoch 2: loss 16.12903153999992
Epoch 3: loss 16.12875882749972
Epoch 4: loss 16.12866955840069
Epoch 5: loss 16.128510146555694
Epoch 6: loss 16.128664733015974
Epoch 7: loss 16.12840063260949
Epoch 8: loss 16.1285129070282
Epoch 9: loss 16.128159287701482
Epoch 10: loss 16.128312216634335
Epoch 11: loss 16.128283758785415
Epoch 12: loss 16.128330404862112
Epoch 13: loss 16.128209302736366
Epoch 14: loss 16.128116409674934
Epoch 15: loss 16.12797152477762
Epoch 16: loss 16.12821350719618
Epoch 17: loss 16.12780968417292
Epoch 18: loss 16.127976741998094
Epoch 19: loss 16.1275304991266
Epoch 20: loss 16.12775061130524
Epoch 21: loss 16.127718815596207
Epoch 22: loss 16.127159294874772
Epoch 23: loss 16.12735860866049
Epoch 24: loss 16.1270523040191
Epoch 25: loss 16.12703594021175
Epoch 26: loss 16.126890477926835
Epoch 27: loss 16.127294108142024
Epoch 28: loss 16.12712714879409
Epoch 29: loss 16.127001585131108
Epoch 30: loss 16.126889220527982
Epoch 31: loss 16.126415606167004
Epoch 32: loss 16.126698273161182
Epoch 33: loss 16.12668375968933
Epoch 34: loss 16.126353004704352
Epoch 35: loss 16.126325723399287
Epoch 36: loss 16.126167636332305
Epoch 37: loss 16.126317990344504
Epoch 38: loss 16.126507959158523
Epoch 39: loss 16.126278709328695
Epoch 40: loss 16.12575696862262
Epoch 41: loss 16.12606687753097
Epoch 42: loss 16.125992694108383
Epoch 43: loss 16.125801695948063
Epoch 44: loss 16.12583638481472
Epoch 45: loss 16.12583741934403
Epoch 46: loss 16.12540661252063
Epoch 47: loss 16.125347623617753
Epoch 48: loss 16.125613610640816
Epoch 49: loss 16.125480283861574
-----------Time: 0:03:41.986483, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016715049743652-------------


Epoch 0: loss 16.129229958161062
Epoch 1: loss 16.129106628376505
Epoch 2: loss 16.129073761857075
Epoch 3: loss 16.128730235929073
Epoch 4: loss 16.12885872695757
Epoch 5: loss 16.128798862125564
Epoch 6: loss 16.12827516016753
Epoch 7: loss 16.128692840493244
Epoch 8: loss 16.128450389530347
Epoch 9: loss 16.128346745864206
Epoch 10: loss 16.1284471584403
Epoch 11: loss 16.128152478259544
Epoch 12: loss 16.128459911761077
Epoch 13: loss 16.128146732371786
Epoch 14: loss 16.128432628382807
Epoch 15: loss 16.128105112780695
Epoch 16: loss 16.128046317722486
Epoch 17: loss 16.128008020442465
Epoch 18: loss 16.12786502112513
Epoch 19: loss 16.127688241004943
Epoch 20: loss 16.12788756619329
Epoch 21: loss 16.12768576974454
Epoch 22: loss 16.127643303249194
Epoch 23: loss 16.12739776943041
Epoch 24: loss 16.127203512191773
Epoch 25: loss 16.127260163555974
Epoch 26: loss 16.12736041960509
Epoch 27: loss 16.127343793537307
Epoch 28: loss 16.126956535422284
Epoch 29: loss 16.126823540355847
Epoch 30: loss 16.12685730146325
Epoch 31: loss 16.12659875413646
Epoch 32: loss 16.126609505777775
Epoch 33: loss 16.126508990578028
Epoch 34: loss 16.126839318482773
Epoch 35: loss 16.126501335268436
Epoch 36: loss 16.126404168294822
Epoch 37: loss 16.12609090079432
Epoch 38: loss 16.12612893892371
Epoch 39: loss 16.12602098091789
Epoch 40: loss 16.126418924331666
Epoch 41: loss 16.125975041804107
Epoch 42: loss 16.125729455118595
Epoch 43: loss 16.126039201280346
Epoch 44: loss 16.126081758996715
Epoch 45: loss 16.125627821424732
Epoch 46: loss 16.12557106743688
Epoch 47: loss 16.12546091908994
Epoch 48: loss 16.125397638652636
Epoch 49: loss 16.125362636732017
-----------Time: 0:03:36.273759, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.016715049743652-------------


Epoch 0: loss 16.12909210972164
Epoch 1: loss 16.128049918879633
Epoch 2: loss 16.12759471250617
Epoch 3: loss 16.126492759455804
Epoch 4: loss 16.126005333402883
Epoch 5: loss 16.125250493961833
Epoch 6: loss 16.124125537664995
Epoch 7: loss 16.123541579039202
Epoch 8: loss 16.122746968269347
Epoch 9: loss 16.121717493430427
Epoch 10: loss 16.121030177240787
Epoch 11: loss 16.120464160131373
Epoch 12: loss 16.119873860607978
Epoch 13: loss 16.118848812061806
Epoch 14: loss 16.118475849732107
Epoch 15: loss 16.11754935202391
Epoch 16: loss 16.116586381456127
Epoch 17: loss 16.116156619528066
Epoch 18: loss 16.11568112787993
Epoch 19: loss 16.11412776760433
Epoch 20: loss 16.113695964605913
Epoch 21: loss 16.11310387072356
Epoch 22: loss 16.11208717201067
Epoch 23: loss 16.111202932440715
Epoch 24: loss 16.110442793887593
Epoch 25: loss 16.109426122126372
Epoch 26: loss 16.10934554597606
Epoch 27: loss 16.10840001002602
Epoch 28: loss 16.10751590521439
Epoch 29: loss 16.10699882403664
Epoch 30: loss 16.10607071441153
Epoch 31: loss 16.105027945145316
Epoch 32: loss 16.10461579302083
Epoch 33: loss 16.103698694187663
Epoch 34: loss 16.102704948964327
Epoch 35: loss 16.102274617941482
Epoch 36: loss 16.10144560544387
Epoch 37: loss 16.100809135644333
Epoch 38: loss 16.100298832810445
Epoch 39: loss 16.09888948564944
Epoch 40: loss 16.09830087682475
Epoch 41: loss 16.097975204301918
Epoch 42: loss 16.096952262132064
Epoch 43: loss 16.096476225231005
Epoch 44: loss 16.09516442340353
Epoch 45: loss 16.094765356312628
Epoch 46: loss 16.09377195627793
Epoch 47: loss 16.092944399170253
Epoch 48: loss 16.09229138934094
Epoch 49: loss 16.091543584284576
-----------Time: 0:01:50.111325, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012475967407227-------------


Epoch 0: loss 16.128580786870874
Epoch 1: loss 16.127994314483974
Epoch 2: loss 16.127135400150134
Epoch 3: loss 16.126790564993154
Epoch 4: loss 16.12550085109213
Epoch 5: loss 16.124908225432687
Epoch 6: loss 16.124446791151296
Epoch 7: loss 16.123649624119633
Epoch 8: loss 16.122823694477912
Epoch 9: loss 16.12200664644656
Epoch 10: loss 16.121118525836778
Epoch 11: loss 16.12005558946858
Epoch 12: loss 16.11940607195315
Epoch 13: loss 16.118937628165536
Epoch 14: loss 16.117966472584268
Epoch 15: loss 16.11718613272128
Epoch 16: loss 16.116762651567875
Epoch 17: loss 16.1159575337949
Epoch 18: loss 16.115198987463245
Epoch 19: loss 16.11390470214512
Epoch 20: loss 16.113447399761366
Epoch 21: loss 16.112795273117396
Epoch 22: loss 16.111999538670414
Epoch 23: loss 16.110962821089704
Epoch 24: loss 16.110388901959293
Epoch 25: loss 16.109735768774282
Epoch 26: loss 16.109061559386877
Epoch 27: loss 16.10815459023351
Epoch 28: loss 16.107232373693716
Epoch 29: loss 16.10669383691705
Epoch 30: loss 16.105767372380132
Epoch 31: loss 16.105172192532084
Epoch 32: loss 16.104293880255327
Epoch 33: loss 16.1036234088566
Epoch 34: loss 16.10303835765175
Epoch 35: loss 16.102017389173092
Epoch 36: loss 16.101259048088735
Epoch 37: loss 16.100650799792746
Epoch 38: loss 16.099869076065396
Epoch 39: loss 16.098893984504368
Epoch 40: loss 16.098552858311198
Epoch 41: loss 16.097552741092183
Epoch 42: loss 16.096949203118033
Epoch 43: loss 16.0960568199987
Epoch 44: loss 16.095260782863782
Epoch 45: loss 16.094401907920837
Epoch 46: loss 16.093378433973893
Epoch 47: loss 16.093165317825648
Epoch 48: loss 16.09248029978379
Epoch 49: loss 16.09110763383948
-----------Time: 0:02:23.385263, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.01246452331543-------------


Epoch 0: loss 16.12890523723934
Epoch 1: loss 16.12818056396816
Epoch 2: loss 16.127421811352605
Epoch 3: loss 16.12660481618798
Epoch 4: loss 16.12592261666837
Epoch 5: loss 16.124563533326853
Epoch 6: loss 16.124077105522154
Epoch 7: loss 16.123365132705025
Epoch 8: loss 16.122980481645335
Epoch 9: loss 16.121832530394844
Epoch 10: loss 16.121114978582963
Epoch 11: loss 16.120499120587887
Epoch 12: loss 16.119362437206767
Epoch 13: loss 16.11891263982524
Epoch 14: loss 16.11824824395387
Epoch 15: loss 16.117182516015095
Epoch 16: loss 16.116421012256456
Epoch 17: loss 16.115885712789453
Epoch 18: loss 16.115164724640223
Epoch 19: loss 16.114222107762874
Epoch 20: loss 16.113320013751153
Epoch 21: loss 16.112971028037695
Epoch 22: loss 16.112015894184943
Epoch 23: loss 16.111263833875242
Epoch 24: loss 16.110366722811822
Epoch 25: loss 16.10992207630821
Epoch 26: loss 16.10869505301766
Epoch 27: loss 16.108010721206664
Epoch 28: loss 16.107290678438932
Epoch 29: loss 16.106706916767617
Epoch 30: loss 16.105734705924988
Epoch 31: loss 16.105140442433566
Epoch 32: loss 16.104134664328203
Epoch 33: loss 16.103493852200714
Epoch 34: loss 16.102936174558558
Epoch 35: loss 16.1020370721817
Epoch 36: loss 16.101436121567435
Epoch 37: loss 16.100310490442357
Epoch 38: loss 16.099981154566226
Epoch 39: loss 16.098912451578222
Epoch 40: loss 16.09813368320465
Epoch 41: loss 16.097774205000505
Epoch 42: loss 16.096980587295864
Epoch 43: loss 16.095793195392776
Epoch 44: loss 16.095266009413677
Epoch 45: loss 16.093987213010372
Epoch 46: loss 16.093705520422564
Epoch 47: loss 16.09287229102591
Epoch 48: loss 16.09219825371452
Epoch 49: loss 16.091628173123237
-----------Time: 0:02:46.427765, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012455940246582-------------


Epoch 0: loss 16.128522864631986
Epoch 1: loss 16.12805473078852
Epoch 2: loss 16.127181817137675
Epoch 3: loss 16.126205030731533
Epoch 4: loss 16.125833369337993
Epoch 5: loss 16.124828840338665
Epoch 6: loss 16.124119260000146
Epoch 7: loss 16.12341525140016
Epoch 8: loss 16.122294265290964
Epoch 9: loss 16.121679822258326
Epoch 10: loss 16.121291946328206
Epoch 11: loss 16.120357850323554
Epoch 12: loss 16.11973319882932
Epoch 13: loss 16.119460014674974
Epoch 14: loss 16.118054607640143
Epoch 15: loss 16.117353855008663
Epoch 16: loss 16.116936802864075
Epoch 17: loss 16.116361399318862
Epoch 18: loss 16.115155746625817
Epoch 19: loss 16.114563859027363
Epoch 20: loss 16.11352075908495
Epoch 21: loss 16.112943266785663
Epoch 22: loss 16.112065634520157
Epoch 23: loss 16.111339847937874
Epoch 24: loss 16.110449541133384
Epoch 25: loss 16.10967343268187
Epoch 26: loss 16.10899810480035
Epoch 27: loss 16.108291931774307
Epoch 28: loss 16.107336358402087
Epoch 29: loss 16.106823031798655
Epoch 30: loss 16.106053401076274
Epoch 31: loss 16.10506392872852
Epoch 32: loss 16.104227039088375
Epoch 33: loss 16.103822807643724
Epoch 34: loss 16.10271684916123
Epoch 35: loss 16.10192916807921
Epoch 36: loss 16.101653757302657
Epoch 37: loss 16.10047490907752
Epoch 38: loss 16.100051025722337
Epoch 39: loss 16.099283184175906
Epoch 40: loss 16.098238676527274
Epoch 41: loss 16.097979733218317
Epoch 42: loss 16.096768478725267
Epoch 43: loss 16.09611778881239
Epoch 44: loss 16.095118454228277
Epoch 45: loss 16.09429424016372
Epoch 46: loss 16.093447749511057
Epoch 47: loss 16.093228834608325
Epoch 48: loss 16.09198403980421
Epoch 49: loss 16.091242376617764
-----------Time: 0:03:13.682652, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012458324432373-------------


Epoch 0: loss 16.128757251863895
Epoch 1: loss 16.127970431161962
Epoch 2: loss 16.12702387208524
Epoch 3: loss 16.12636741347935
Epoch 4: loss 16.125626985923105
Epoch 5: loss 16.125163900333902
Epoch 6: loss 16.124168303738468
Epoch 7: loss 16.123588413777558
Epoch 8: loss 16.122722745978315
Epoch 9: loss 16.121955588589543
Epoch 10: loss 16.12135961677717
Epoch 11: loss 16.12039915997049
Epoch 12: loss 16.1197397781455
Epoch 13: loss 16.118919332131096
Epoch 14: loss 16.11832610524219
Epoch 15: loss 16.11724054087763
Epoch 16: loss 16.116296617881112
Epoch 17: loss 16.115768118526624
Epoch 18: loss 16.115120613056682
Epoch 19: loss 16.114373947226483
Epoch 20: loss 16.113796154312464
Epoch 21: loss 16.112998349770255
Epoch 22: loss 16.11174041395602
Epoch 23: loss 16.111251063968826
Epoch 24: loss 16.11026460191478
Epoch 25: loss 16.109545571907706
Epoch 26: loss 16.10889612177144
Epoch 27: loss 16.10815947263137
Epoch 28: loss 16.1071426982465
Epoch 29: loss 16.1065944723461
Epoch 30: loss 16.10598855951558
Epoch 31: loss 16.105243273403335
Epoch 32: loss 16.10424525012141
Epoch 33: loss 16.103716752840125
Epoch 34: loss 16.102896148225536
Epoch 35: loss 16.1020449348118
Epoch 36: loss 16.101460266113282
Epoch 37: loss 16.100464326402417
Epoch 38: loss 16.099494757859603
Epoch 39: loss 16.09897824784984
Epoch 40: loss 16.09831684672314
Epoch 41: loss 16.097061525220457
Epoch 42: loss 16.09689282334369
Epoch 43: loss 16.09581654590109
Epoch 44: loss 16.095398969235628
Epoch 45: loss 16.09418438206548
Epoch 46: loss 16.093602508047354
Epoch 47: loss 16.092776618833128
Epoch 48: loss 16.09205745821414
Epoch 49: loss 16.09133966591047
-----------Time: 0:03:59.042668, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012455940246582-------------


Epoch 0: loss 16.125882141486457
Epoch 1: loss 16.1177621571914
Epoch 2: loss 16.110104320360268
Epoch 3: loss 16.102434101312056
Epoch 4: loss 16.094579241586768
Epoch 5: loss 16.087426970316017
Epoch 6: loss 16.079731036269145
Epoch 7: loss 16.072093594592552
Epoch 8: loss 16.064256522966467
Epoch 9: loss 16.05687164431033
Epoch 10: loss 16.049132039235985
Epoch 11: loss 16.041732321614806
Epoch 12: loss 16.033949412470278
Epoch 13: loss 16.02616784779922
Epoch 14: loss 16.018711837478307
Epoch 15: loss 16.0111516278723
Epoch 16: loss 16.00344251446102
Epoch 17: loss 15.995789693749469
Epoch 18: loss 15.988247594626053
Epoch 19: loss 15.980395591777304
Epoch 20: loss 15.972953381745711
Epoch 21: loss 15.965416735151539
Epoch 22: loss 15.957907165651736
Epoch 23: loss 15.950514777846958
Epoch 24: loss 15.9427670147108
Epoch 25: loss 15.935074908837029
Epoch 26: loss 15.927741467434426
Epoch 27: loss 15.920087092855702
Epoch 28: loss 15.912199607102767
Epoch 29: loss 15.904956788602084
Epoch 30: loss 15.897262357628863
Epoch 31: loss 15.890018235082211
Epoch 32: loss 15.882015582789546
Epoch 33: loss 15.874688127766484
Epoch 34: loss 15.866711389500162
Epoch 35: loss 15.859102991352911
Epoch 36: loss 15.851968235554903
Epoch 37: loss 15.844125126755756
Epoch 38: loss 15.836818541651187
Epoch 39: loss 15.828974057280499
Epoch 40: loss 15.821519631924836
Epoch 41: loss 15.814014081332994
Epoch 42: loss 15.80630317563596
Epoch 43: loss 15.798941240103348
Epoch 44: loss 15.79082278687021
Epoch 45: loss 15.783674768779589
Epoch 46: loss 15.776552336112312
Epoch 47: loss 15.768264208669247
Epoch 48: loss 15.761184113958608
Epoch 49: loss 15.753386662317359
-----------Time: 0:02:50.444717, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.9699788093566895-------------


Epoch 0: loss 16.125582438966504
Epoch 1: loss 16.117671687706657
Epoch 2: loss 16.11000220257303
Epoch 3: loss 16.102854770162832
Epoch 4: loss 16.094447144218112
Epoch 5: loss 16.087170763637708
Epoch 6: loss 16.07981854832691
Epoch 7: loss 16.071835185133892
Epoch 8: loss 16.064230534304745
Epoch 9: loss 16.056402582707612
Epoch 10: loss 16.049035275500753
Epoch 11: loss 16.041336822509766
Epoch 12: loss 16.033884799998738
Epoch 13: loss 16.026387120329815
Epoch 14: loss 16.018607675510903
Epoch 15: loss 16.011060260689778
Epoch 16: loss 16.00330941055132
Epoch 17: loss 15.995907031971475
Epoch 18: loss 15.987917252208875
Epoch 19: loss 15.98081171616264
Epoch 20: loss 15.973049207355665
Epoch 21: loss 15.965084072817927
Epoch 22: loss 15.957606951050137
Epoch 23: loss 15.950135082783907
Epoch 24: loss 15.94230246854865
Epoch 25: loss 15.935115527070087
Epoch 26: loss 15.927168382769045
Epoch 27: loss 15.919546579278034
Epoch 28: loss 15.912134630783745
Epoch 29: loss 15.904715649977975
Epoch 30: loss 15.8969563525656
Epoch 31: loss 15.889431509764298
Epoch 32: loss 15.881698678887409
Epoch 33: loss 15.874124153800633
Epoch 34: loss 15.867099161770033
Epoch 35: loss 15.85865509821021
Epoch 36: loss 15.85124074065167
Epoch 37: loss 15.843817236112512
Epoch 38: loss 15.836116845711418
Epoch 39: loss 15.82836247941722
Epoch 40: loss 15.821088388691777
Epoch 41: loss 15.813467290090477
Epoch 42: loss 15.805859382256218
Epoch 43: loss 15.797928989451865
Epoch 44: loss 15.790676523291546
Epoch 45: loss 15.782936370891074
Epoch 46: loss 15.775562323694643
Epoch 47: loss 15.767826931372932
Epoch 48: loss 15.760391496575396
Epoch 49: loss 15.752280690359033
-----------Time: 0:02:44.143657, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9698243141174316-------------


Epoch 0: loss 16.125206195789836
Epoch 1: loss 16.117891047311865
Epoch 2: loss 16.110116013236667
Epoch 3: loss 16.10215258287347
Epoch 4: loss 16.09476267358531
Epoch 5: loss 16.08721770203632
Epoch 6: loss 16.079463620807815
Epoch 7: loss 16.072080270103786
Epoch 8: loss 16.064373540878297
Epoch 9: loss 16.056465117827706
Epoch 10: loss 16.04898610426032
Epoch 11: loss 16.041455814112787
Epoch 12: loss 16.033825365356776
Epoch 13: loss 16.025966714776082
Epoch 14: loss 16.01839256286621
Epoch 15: loss 16.010585223073544
Epoch 16: loss 16.003258420073468
Epoch 17: loss 15.99572026003962
Epoch 18: loss 15.988339655295663
Epoch 19: loss 15.980369029874387
Epoch 20: loss 15.972810384501582
Epoch 21: loss 15.965210571496383
Epoch 22: loss 15.95790284405584
Epoch 23: loss 15.94996132850647
Epoch 24: loss 15.942308780421381
Epoch 25: loss 15.934583527108897
Epoch 26: loss 15.927063602986543
Epoch 27: loss 15.91963649418043
Epoch 28: loss 15.911881971359254
Epoch 29: loss 15.904189875851507
Epoch 30: loss 15.896760300968005
Epoch 31: loss 15.889130890887717
Epoch 32: loss 15.881100849483325
Epoch 33: loss 15.873621912624525
Epoch 34: loss 15.865905793853429
Epoch 35: loss 15.857768001763716
Epoch 36: loss 15.850529960964037
Epoch 37: loss 15.842811485995417
Epoch 38: loss 15.83528807370559
Epoch 39: loss 15.827585768699645
Epoch 40: loss 15.819515719621078
Epoch 41: loss 15.811509000736734
Epoch 42: loss 15.803943817511849
Epoch 43: loss 15.795945324068484
Epoch 44: loss 15.788619965055714
Epoch 45: loss 15.780319479237432
Epoch 46: loss 15.771971406107363
Epoch 47: loss 15.76435545631077
Epoch 48: loss 15.75649952992149
Epoch 49: loss 15.748656649174897
-----------Time: 0:02:24.143373, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.969263792037964-------------


Epoch 0: loss 16.125662371386653
Epoch 1: loss 16.117592592861342
Epoch 2: loss 16.110047045997952
Epoch 3: loss 16.102647030871847
Epoch 4: loss 16.094793475192525
Epoch 5: loss 16.08735961706742
Epoch 6: loss 16.07942512450011
Epoch 7: loss 16.071614145196
Epoch 8: loss 16.064221908735192
Epoch 9: loss 16.056858380981115
Epoch 10: loss 16.049268246733625
Epoch 11: loss 16.04125364448713
Epoch 12: loss 16.033892773545308
Epoch 13: loss 16.02622352268385
Epoch 14: loss 16.018545499055282
Epoch 15: loss 16.010945432082465
Epoch 16: loss 16.003233427586764
Epoch 17: loss 15.99528056849604
Epoch 18: loss 15.987599847627722
Epoch 19: loss 15.980286024964373
Epoch 20: loss 15.972458857038747
Epoch 21: loss 15.9646304047626
Epoch 22: loss 15.957198108797488
Epoch 23: loss 15.948875242730846
Epoch 24: loss 15.941302025836446
Epoch 25: loss 15.93345400976098
Epoch 26: loss 15.92576843344647
Epoch 27: loss 15.918064781893854
Epoch 28: loss 15.910035890081655
Epoch 29: loss 15.90219003221263
Epoch 30: loss 15.893797145719114
Epoch 31: loss 15.886228549998739
Epoch 32: loss 15.877706253010293
Epoch 33: loss 15.869376384693643
Epoch 34: loss 15.861020669729813
Epoch 35: loss 15.852589581323707
Epoch 36: loss 15.84399851716083
Epoch 37: loss 15.835323394899783
Epoch 38: loss 15.826259382911351
Epoch 39: loss 15.817912484251934
Epoch 40: loss 15.808225599579188
Epoch 41: loss 15.798643054132876
Epoch 42: loss 15.789071719542793
Epoch 43: loss 15.779607443187547
Epoch 44: loss 15.769404935836793
Epoch 45: loss 15.759056046734685
Epoch 46: loss 15.748969320628953
Epoch 47: loss 15.73806586265564
Epoch 48: loss 15.727176817603734
Epoch 49: loss 15.715612441560497
-----------Time: 0:02:57.413205, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.964940309524536-------------


Epoch 0: loss 16.12544056850931
Epoch 1: loss 16.117595706815305
Epoch 2: loss 16.109980204831
Epoch 3: loss 16.10249307881231
Epoch 4: loss 16.094796274019323
Epoch 5: loss 16.087173285691634
Epoch 6: loss 16.07947417964106
Epoch 7: loss 16.071933570115462
Epoch 8: loss 16.064127794555997
Epoch 9: loss 16.056599651212277
Epoch 10: loss 16.04913586118947
Epoch 11: loss 16.041400122642518
Epoch 12: loss 16.033610990773077
Epoch 13: loss 16.02603499889374
Epoch 14: loss 16.018309816070225
Epoch 15: loss 16.010886680561562
Epoch 16: loss 16.003033953127655
Epoch 17: loss 15.995431365137515
Epoch 18: loss 15.987312230856523
Epoch 19: loss 15.979447872742362
Epoch 20: loss 15.971698206403982
Epoch 21: loss 15.964147009020266
Epoch 22: loss 15.955971824604532
Epoch 23: loss 15.947810756641886
Epoch 24: loss 15.939812887233236
Epoch 25: loss 15.931286947623542
Epoch 26: loss 15.923214559969695
Epoch 27: loss 15.914837735632192
Epoch 28: loss 15.90563956136289
Epoch 29: loss 15.897186932356462
Epoch 30: loss 15.887462964265243
Epoch 31: loss 15.878113066631816
Epoch 32: loss 15.86877824741861
Epoch 33: loss 15.858843936090883
Epoch 34: loss 15.848546411680138
Epoch 35: loss 15.838482345705447
Epoch 36: loss 15.827010165090146
Epoch 37: loss 15.815886417679165
Epoch 38: loss 15.8046494172967
Epoch 39: loss 15.79199574408324
Epoch 40: loss 15.779883412692858
Epoch 41: loss 15.76742281810097
Epoch 42: loss 15.754074004422064
Epoch 43: loss 15.740238493421803
Epoch 44: loss 15.725910786960435
Epoch 45: loss 15.711285048982372
Epoch 46: loss 15.695843206281248
Epoch 47: loss 15.68069454068723
Epoch 48: loss 15.664104116481283
Epoch 49: loss 15.647648918110391
-----------Time: 0:03:35.238870, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.9560043811798096-------------


Epoch 0: loss 16.091961418027463
Epoch 1: loss 16.015677012567934
Epoch 2: loss 15.93931533150051
Epoch 3: loss 15.863997564108475
Epoch 4: loss 15.788565847148066
Epoch 5: loss 15.713389871431433
Epoch 6: loss 15.638121321926945
Epoch 7: loss 15.563289845508077
Epoch 8: loss 15.48868894058725
Epoch 9: loss 15.41411258448725
Epoch 10: loss 15.339666534506756
Epoch 11: loss 15.265636734340502
Epoch 12: loss 15.191288658846979
Epoch 13: loss 15.117576096368873
Epoch 14: loss 15.04396251699199
Epoch 15: loss 14.969786003361577
Epoch 16: loss 14.895910009093907
Epoch 17: loss 14.822078065250231
Epoch 18: loss 14.74879543988601
Epoch 19: loss 14.674950719916302
Epoch 20: loss 14.600467837375144
Epoch 21: loss 14.526639638776365
Epoch 22: loss 14.45199875831604
Epoch 23: loss 14.377428240361422
Epoch 24: loss 14.302154675773952
Epoch 25: loss 14.22594167875207
Epoch 26: loss 14.149574570033861
Epoch 27: loss 14.072052878918855
Epoch 28: loss 13.99333620486052
Epoch 29: loss 13.913195604863374
Epoch 30: loss 13.831862179092738
Epoch 31: loss 13.749365500781847
Epoch 32: loss 13.664612107691557
Epoch 33: loss 13.57832288949386
Epoch 34: loss 13.489764303746432
Epoch 35: loss 13.399628583244656
Epoch 36: loss 13.306660903018454
Epoch 37: loss 13.211164938885233
Epoch 38: loss 13.113068759959678
Epoch 39: loss 13.01244197410086
Epoch 40: loss 12.908582944455354
Epoch 41: loss 12.802114811150924
Epoch 42: loss 12.692535486428634
Epoch 43: loss 12.579880625268688
Epoch 44: loss 12.463609320184458
Epoch 45: loss 12.344192217743915
Epoch 46: loss 12.221675437429678
Epoch 47: loss 12.095518321576327
Epoch 48: loss 11.966560094252877
Epoch 49: loss 11.833924219919288
-----------Time: 0:02:19.936494, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 20, rmse: 3.4341108798980713-------------


Epoch 0: loss 16.091538826279017
Epoch 1: loss 16.015138646830682
Epoch 2: loss 15.939162166222282
Epoch 3: loss 15.86317675528319
Epoch 4: loss 15.787886267123016
Epoch 5: loss 15.712418793595356
Epoch 6: loss 15.636644389318382
Epoch 7: loss 15.560457237907078
Epoch 8: loss 15.48395207239234
Epoch 9: loss 15.406045137281003
Epoch 10: loss 15.325400772302046
Epoch 11: loss 15.242138555775519
Epoch 12: loss 15.1541187307109
Epoch 13: loss 15.059416337635206
Epoch 14: loss 14.957533474590468
Epoch 15: loss 14.847230297586192
Epoch 16: loss 14.72662165372268
Epoch 17: loss 14.595578878858815
Epoch 18: loss 14.452219220866327
Epoch 19: loss 14.297969258349875
Epoch 20: loss 14.130573616857115
Epoch 21: loss 13.951522137807762
Epoch 22: loss 13.759563894893812
Epoch 23: loss 13.555507525153782
Epoch 24: loss 13.33930142444113
Epoch 25: loss 13.110633290332297
Epoch 26: loss 12.870715950882953
Epoch 27: loss 12.620456370063451
Epoch 28: loss 12.35938084229179
Epoch 29: loss 12.08850905791573
Epoch 30: loss 11.80838537838148
Epoch 31: loss 11.519997380090796
Epoch 32: loss 11.223376571613809
Epoch 33: loss 10.920025177623915
Epoch 34: loss 10.609160575659379
Epoch 35: loss 10.292977385935576
Epoch 36: loss 9.971650665739308
Epoch 37: loss 9.645291187452234
Epoch 38: loss 9.31540792506674
Epoch 39: loss 8.982573397263236
Epoch 40: loss 8.64783432898314
Epoch 41: loss 8.31146512394366
Epoch 42: loss 7.974263059574625
Epoch 43: loss 7.637470111639604
Epoch 44: loss 7.301746915734332
Epoch 45: loss 6.96775611690853
Epoch 46: loss 6.636687162648077
Epoch 47: loss 6.309582610752272
Epoch 48: loss 5.9868066979491195
Epoch 49: loss 5.669785079230433
-----------Time: 0:02:47.957902, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 50, rmse: 2.356551170349121-------------


Epoch 0: loss 16.09145173508188
Epoch 1: loss 16.015018710882767
Epoch 2: loss 15.939097823267398
Epoch 3: loss 15.862117200312406
Epoch 4: loss 15.784395808758942
Epoch 5: loss 15.702099751389545
Epoch 6: loss 15.610483895177426
Epoch 7: loss 15.501655716481416
Epoch 8: loss 15.36724322775136
Epoch 9: loss 15.202033900177996
Epoch 10: loss 15.003390925863515
Epoch 11: loss 14.772562857296156
Epoch 12: loss 14.510180479547252
Epoch 13: loss 14.217935520669688
Epoch 14: loss 13.897977320007655
Epoch 15: loss 13.552297823325448
Epoch 16: loss 13.18226000537043
Epoch 17: loss 12.790101829819058
Epoch 18: loss 12.377528631168863
Epoch 19: loss 11.947382893769637
Epoch 20: loss 11.501252048948537
Epoch 21: loss 11.040723161075427
Epoch 22: loss 10.568513513647991
Epoch 23: loss 10.08669943705849
Epoch 24: loss 9.597233766058217
Epoch 25: loss 9.102780954734138
Epoch 26: loss 8.605895663344342
Epoch 27: loss 8.10864977059157
Epoch 28: loss 7.613080489117166
Epoch 29: loss 7.122793982858243
Epoch 30: loss 6.638981518019801
Epoch 31: loss 6.165616208055745
Epoch 32: loss 5.703893522594286
Epoch 33: loss 5.257211515696152
Epoch 34: loss 4.8276750906654025
Epoch 35: loss 4.4175796425860865
Epoch 36: loss 4.029693580451219
Epoch 37: loss 3.665877850677656
Epoch 38: loss 3.328171606167503
Epoch 39: loss 3.0182640384072843
Epoch 40: loss 2.7369936486949094
Epoch 41: loss 2.4854176832282024
Epoch 42: loss 2.263299348172934
Epoch 43: loss 2.070181814224824
Epoch 44: loss 1.9048869989488435
Epoch 45: loss 1.7651138274565987
Epoch 46: loss 1.6485802824082583
Epoch 47: loss 1.5522731197916944
Epoch 48: loss 1.4732062861971233
Epoch 49: loss 1.4085021421961161
-----------Time: 0:03:14.177289, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 100, rmse: 1.1852697134017944-------------


Epoch 0: loss 16.091199464383333
Epoch 1: loss 16.01520465353261
Epoch 2: loss 15.938115144812542
Epoch 3: loss 15.856557618016781
Epoch 4: loss 15.75896798113118
Epoch 5: loss 15.624189293902853
Epoch 6: loss 15.434899652522542
Epoch 7: loss 15.185346439610356
Epoch 8: loss 14.880548871081809
Epoch 9: loss 14.526322400051615
Epoch 10: loss 14.126526826360951
Epoch 11: loss 13.686039741142936
Epoch 12: loss 13.209427424099134
Epoch 13: loss 12.700822476718736
Epoch 14: loss 12.164102822801341
Epoch 15: loss 11.603937121059584
Epoch 16: loss 11.022885652210402
Epoch 17: loss 10.426444571951162
Epoch 18: loss 9.817612693620765
Epoch 19: loss 9.201902457942133
Epoch 20: loss 8.582582944372426
Epoch 21: loss 7.964873434668002
Epoch 22: loss 7.3527965618216475
Epoch 23: loss 6.751364932371223
Epoch 24: loss 6.165258307560631
Epoch 25: loss 5.5987896224726805
Epoch 26: loss 5.05715881637905
Epoch 27: loss 4.54384201194929
Epoch 28: loss 4.063438456991444
Epoch 29: loss 3.6201279352540556
Epoch 30: loss 3.2165903521620707
Epoch 31: loss 2.855520298688308
Epoch 32: loss 2.538356732285541
Epoch 33: loss 2.2656685199426567
Epoch 34: loss 2.0355917040420617
Epoch 35: loss 1.8457082845594572
Epoch 36: loss 1.6919028080028036
Epoch 37: loss 1.5692417003538297
Epoch 38: loss 1.4724661707878113
Epoch 39: loss 1.3963056732778965
Epoch 40: loss 1.3360291021025699
Epoch 41: loss 1.2878736782333124
Epoch 42: loss 1.2485668708448825
Epoch 43: loss 1.2159523796776066
Epoch 44: loss 1.188407736239226
Epoch 45: loss 1.1648123058287994
Epoch 46: loss 1.1442924042758735
Epoch 47: loss 1.126411730569342
Epoch 48: loss 1.1106798630045809
Epoch 49: loss 1.0968212164614513
-----------Time: 0:03:41.614471, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 150, rmse: 1.053977131843567-------------


Epoch 0: loss 16.09097333576368
Epoch 1: loss 16.014803441711095
Epoch 2: loss 15.93483658666196
Epoch 3: loss 15.835164610199307
Epoch 4: loss 15.676425642552584
Epoch 5: loss 15.42803913510364
Epoch 6: loss 15.092549354097118
Epoch 7: loss 14.682341899042544
Epoch 8: loss 14.209390481658604
Epoch 9: loss 13.681553888320924
Epoch 10: loss 13.107024616780489
Epoch 11: loss 12.492156850773355
Epoch 12: loss 11.843620027666507
Epoch 13: loss 11.167350920386937
Epoch 14: loss 10.470060043749601
Epoch 15: loss 9.757488861291305
Epoch 16: loss 9.037131362376005
Epoch 17: loss 8.31588495866112
Epoch 18: loss 7.600117540359497
Epoch 19: loss 6.897367252474246
Epoch 20: loss 6.21483283146568
Epoch 21: loss 5.559395710800005
Epoch 22: loss 4.937895953136942
Epoch 23: loss 4.357031403935474
Epoch 24: loss 3.8230412822702657
Epoch 25: loss 3.340794212403505
Epoch 26: loss 2.9144753251386724
Epoch 27: loss 2.54656750637552
Epoch 28: loss 2.23681115080481
Epoch 29: loss 1.9831012962952903
Epoch 30: loss 1.780657670420149
Epoch 31: loss 1.6227471606886905
Epoch 32: loss 1.5015016522096551
Epoch 33: loss 1.4090675715519034
Epoch 34: loss 1.3383790114651555
Epoch 35: loss 1.2834898575492528
Epoch 36: loss 1.2400247568669527
Epoch 37: loss 1.2046665515588677
Epoch 38: loss 1.1751697187838348
Epoch 39: loss 1.1503877014569615
Epoch 40: loss 1.1290988106442534
Epoch 41: loss 1.110962208263252
Epoch 42: loss 1.0951861885578735
Epoch 43: loss 1.0814919212590093
Epoch 44: loss 1.069552562482979
Epoch 45: loss 1.0592123827856519
Epoch 46: loss 1.050107385088568
Epoch 47: loss 1.0422606310766676
Epoch 48: loss 1.035276429290357
Epoch 49: loss 1.02918667100046
-----------Time: 0:03:07.176837, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 200, rmse: 1.0227628946304321-------------


Epoch 0: loss 15.754655500080274
Epoch 1: loss 15.016058678212373
Epoch 2: loss 14.279120861965676
Epoch 3: loss 13.43022879206616
Epoch 4: loss 12.201972060618193
Epoch 5: loss 10.440880924722423
Epoch 6: loss 8.318771328615105
Epoch 7: loss 6.168075571371161
Epoch 8: loss 4.283957539693169
Epoch 9: loss 2.858794177874275
Epoch 10: loss 1.9506140134904695
Epoch 11: loss 1.4712793378726297
Epoch 12: loss 1.2504254723372668
Epoch 13: loss 1.1465288842501848
Epoch 14: loss 1.089366582489532
Epoch 15: loss 1.0540137054479641
Epoch 16: loss 1.0310510641854742
Epoch 17: loss 1.0158837705202726
Epoch 18: loss 1.0055581225001293
Epoch 19: loss 0.9986285496665084
Epoch 20: loss 0.9937871510567873
Epoch 21: loss 0.9903668376414673
Epoch 22: loss 0.9879616226191106
Epoch 23: loss 0.9861303044402081
Epoch 24: loss 0.9848214501272077
Epoch 25: loss 0.983843307002731
Epoch 26: loss 0.9830036441269128
Epoch 27: loss 0.9824240321698396
Epoch 28: loss 0.9818990260362626
Epoch 29: loss 0.9814401712106622
Epoch 30: loss 0.9810550576318865
Epoch 31: loss 0.9807375942235408
Epoch 32: loss 0.98040272599977
Epoch 33: loss 0.9801286451194597
Epoch 34: loss 0.9797762023366016
Epoch 35: loss 0.9795308960520703
Epoch 36: loss 0.9793148665972378
Epoch 37: loss 0.9790200922800147
Epoch 38: loss 0.9787719443440437
Epoch 39: loss 0.978576839034972
Epoch 40: loss 0.978288426865702
Epoch 41: loss 0.9779685773927232
Epoch 42: loss 0.9778127661865691
Epoch 43: loss 0.9774153268855551
Epoch 44: loss 0.9772293611065201
Epoch 45: loss 0.9769226218695226
Epoch 46: loss 0.9766554322579633
Epoch 47: loss 0.9764090148003205
Epoch 48: loss 0.9760319811494454
Epoch 49: loss 0.9757444282588752
-----------Time: 0:01:52.926791, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 20, rmse: 0.99944007396698-------------


Epoch 0: loss 15.751707635755125
Epoch 1: loss 14.738797431406768
Epoch 2: loss 11.910745457980944
Epoch 3: loss 7.644787142587744
Epoch 4: loss 4.116650486769884
Epoch 5: loss 2.1582829563514045
Epoch 6: loss 1.4112699763930363
Epoch 7: loss 1.1759689927101136
Epoch 8: loss 1.0850248756616012
Epoch 9: loss 1.0402007202091423
Epoch 10: loss 1.016385954812817
Epoch 11: loss 1.0034142835632613
Epoch 12: loss 0.9960621766422106
Epoch 13: loss 0.9918423474482868
Epoch 14: loss 0.9892598586237948
Epoch 15: loss 0.9877439307777778
Epoch 16: loss 0.9867398460274157
Epoch 17: loss 0.9860633143264315
Epoch 18: loss 0.985500751759695
Epoch 19: loss 0.9852262241684873
Epoch 20: loss 0.9849458849300509
Epoch 21: loss 0.9846490391570589
Epoch 22: loss 0.9844341895502546
Epoch 23: loss 0.9843010294696559
Epoch 24: loss 0.9840369034720504
Epoch 25: loss 0.9839415412234224
Epoch 26: loss 0.9838068071266879
Epoch 27: loss 0.9836259567867155
Epoch 28: loss 0.9834623062092325
Epoch 29: loss 0.9832053322506987
Epoch 30: loss 0.983171683161155
Epoch 31: loss 0.9830297390403955
Epoch 32: loss 0.9827064803760984
Epoch 33: loss 0.9824967874133068
Epoch 34: loss 0.9823924179958261
Epoch 35: loss 0.9821323749811753
Epoch 36: loss 0.9819881130171859
Epoch 37: loss 0.9817270477828772
Epoch 38: loss 0.9814677618120028
Epoch 39: loss 0.9813062803252883
Epoch 40: loss 0.9809332219802815
Epoch 41: loss 0.9807089607352796
Epoch 42: loss 0.9803963731164518
Epoch 43: loss 0.9801335571252782
Epoch 44: loss 0.9798333844412928
Epoch 45: loss 0.9795295682938202
Epoch 46: loss 0.9791143149137497
Epoch 47: loss 0.9787464375729146
Epoch 48: loss 0.9783382146902706
Epoch 49: loss 0.9779444255906603
-----------Time: 0:02:27.730528, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 50, rmse: 0.9991170167922974-------------


Epoch 0: loss 15.718497038924175
Epoch 1: loss 12.643803172526152
Epoch 2: loss 6.254536320333895
Epoch 3: loss 2.4192448787067247
Epoch 4: loss 1.3435657603585203
Epoch 5: loss 1.1174809898371283
Epoch 6: loss 1.0444937886751215
Epoch 7: loss 1.0141942010625549
Epoch 8: loss 1.0006943806358006
Epoch 9: loss 0.994498637707337
Epoch 10: loss 0.9913493874280349
Epoch 11: loss 0.9896593484541644
Epoch 12: loss 0.988822981585627
Epoch 13: loss 0.988232837487822
Epoch 14: loss 0.9880121922363405
Epoch 15: loss 0.9876363090198973
Epoch 16: loss 0.9874508350439694
Epoch 17: loss 0.9872814956566561
Epoch 18: loss 0.9871825698277225
Epoch 19: loss 0.9869972745361535
Epoch 20: loss 0.9867953211069107
Epoch 21: loss 0.9866604911244434
Epoch 22: loss 0.9865641699536987
Epoch 23: loss 0.9863414417142453
Epoch 24: loss 0.9861489152130873
Epoch 25: loss 0.9859699747484664
Epoch 26: loss 0.9858005142730215
Epoch 27: loss 0.9855145079286202
Epoch 28: loss 0.9852860398266626
Epoch 29: loss 0.9850376688915751
Epoch 30: loss 0.9847771814335947
Epoch 31: loss 0.9844852965163148
Epoch 32: loss 0.984220499318579
Epoch 33: loss 0.9838782644790152
Epoch 34: loss 0.9834892930543941
Epoch 35: loss 0.9831193620743959
Epoch 36: loss 0.9826267732874207
Epoch 37: loss 0.982155233103296
Epoch 38: loss 0.9815610424979874
Epoch 39: loss 0.981066578173119
Epoch 40: loss 0.9804522606341736
Epoch 41: loss 0.9797438030657561
Epoch 42: loss 0.9790213336115298
Epoch 43: loss 0.9783108140463415
Epoch 44: loss 0.9774473715087642
Epoch 45: loss 0.9764997536721437
Epoch 46: loss 0.9754249982859777
Epoch 47: loss 0.9744590710038724
Epoch 48: loss 0.973262278476487
Epoch 49: loss 0.9720884117743244
-----------Time: 0:02:46.522657, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 100, rmse: 0.9971582889556885-------------


Epoch 0: loss 15.441118312918622
Epoch 1: loss 9.292970484754314
Epoch 2: loss 3.0442556961722995
Epoch 3: loss 1.3599405217429865
Epoch 4: loss 1.099959156435469
Epoch 5: loss 1.031327157046484
Epoch 6: loss 1.0067947959770327
Epoch 7: loss 0.9972671935091848
Epoch 8: loss 0.9932822703667309
Epoch 9: loss 0.9914965926952984
Epoch 10: loss 0.9905960912289826
Epoch 11: loss 0.9900815079393595
Epoch 12: loss 0.9897991366360499
Epoch 13: loss 0.9895736759123595
Epoch 14: loss 0.9894435565108838
Epoch 15: loss 0.9893068430216416
Epoch 16: loss 0.9891755964445031
Epoch 17: loss 0.9890318157880202
Epoch 18: loss 0.9890781619626543
Epoch 19: loss 0.9888751913671908
Epoch 20: loss 0.9888133014673772
Epoch 21: loss 0.9885617039125899
Epoch 22: loss 0.9884729356221531
Epoch 23: loss 0.9883408753120381
Epoch 24: loss 0.988191001570743
Epoch 25: loss 0.9879710862817972
Epoch 26: loss 0.9877810646010482
Epoch 27: loss 0.9875697334823401
Epoch 28: loss 0.9874365966605103
Epoch 29: loss 0.9871882857187935
Epoch 30: loss 0.9869861907285192
Epoch 31: loss 0.9867130199852197
Epoch 32: loss 0.9864298428530278
Epoch 33: loss 0.9860784110167752
Epoch 34: loss 0.9858057311695555
Epoch 35: loss 0.9854143630551255
Epoch 36: loss 0.9849105639950089
Epoch 37: loss 0.9844786240354828
Epoch 38: loss 0.9839905629339425
Epoch 39: loss 0.9834962951100391
Epoch 40: loss 0.9828418254204419
Epoch 41: loss 0.9822322156766187
Epoch 42: loss 0.9815018383705097
Epoch 43: loss 0.980566661707733
Epoch 44: loss 0.9797642763542093
Epoch 45: loss 0.978741815103137
Epoch 46: loss 0.9775272517100625
Epoch 47: loss 0.9764366556768832
Epoch 48: loss 0.9750605793750804
Epoch 49: loss 0.9736236771811609
-----------Time: 0:03:17.931028, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 150, rmse: 0.9975128769874573-------------


Epoch 0: loss 15.046869611740112
Epoch 1: loss 7.123566634240357
Epoch 2: loss 1.9620730436366538
Epoch 3: loss 1.165477662112402
Epoch 4: loss 1.0450039962063664
Epoch 5: loss 1.0103258070090542
Epoch 6: loss 0.9986029064525729
Epoch 7: loss 0.9943530404697294
Epoch 8: loss 0.9926583777951158
Epoch 9: loss 0.9917834523579349
Epoch 10: loss 0.9914593727692314
Epoch 11: loss 0.9910696555738864
Epoch 12: loss 0.9910844850151436
Epoch 13: loss 0.990881236091904
Epoch 14: loss 0.9907918504398802
Epoch 15: loss 0.9906159750793291
Epoch 16: loss 0.9905177240138469
Epoch 17: loss 0.9903777638207312
Epoch 18: loss 0.9902858844917753
Epoch 19: loss 0.990183583225893
Epoch 20: loss 0.9899959448239077
Epoch 21: loss 0.9899765556273253
Epoch 22: loss 0.9897743138930072
Epoch 23: loss 0.9895633799226388
Epoch 24: loss 0.9893936871186547
Epoch 25: loss 0.9893070601250814
Epoch 26: loss 0.9889991845773614
Epoch 27: loss 0.988804217784301
Epoch 28: loss 0.9886750512149023
Epoch 29: loss 0.988314045317795
Epoch 30: loss 0.988095797598362
Epoch 31: loss 0.9877325119531672
Epoch 32: loss 0.9873613960069159
Epoch 33: loss 0.9869043086533961
Epoch 34: loss 0.986470790790475
Epoch 35: loss 0.9860235784364784
Epoch 36: loss 0.9854299829058025
Epoch 37: loss 0.984906284964603
Epoch 38: loss 0.9841676069342572
Epoch 39: loss 0.9833407184352045
Epoch 40: loss 0.9825522780418396
Epoch 41: loss 0.9814892488977184
Epoch 42: loss 0.9805734890958537
Epoch 43: loss 0.9792961918789408
Epoch 44: loss 0.9779327185257621
Epoch 45: loss 0.9764299640189047
Epoch 46: loss 0.9747919630745183
Epoch 47: loss 0.9728918326937634
Epoch 48: loss 0.9709455161638881
Epoch 49: loss 0.9686694055795669
-----------Time: 0:04:04.378633, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.0001, embedding_dim: 200, rmse: 0.9968640804290771-------------


Epoch 0: loss 10.284679445753927
Epoch 1: loss 1.1385597248440202
Epoch 2: loss 1.0076131956084915
Epoch 3: loss 1.0000310338061789
Epoch 4: loss 0.9928557384921157
Epoch 5: loss 0.9832662192375764
Epoch 6: loss 0.9729180196057196
Epoch 7: loss 0.9617551335174105
Epoch 8: loss 0.9514079125679058
Epoch 9: loss 0.9410289448888406
Epoch 10: loss 0.930801258203776
Epoch 11: loss 0.9204269224534865
Epoch 12: loss 0.9100876687013585
Epoch 13: loss 0.8986087799072265
Epoch 14: loss 0.8868615778891936
Epoch 15: loss 0.8742507685137831
Epoch 16: loss 0.8610832097737685
Epoch 17: loss 0.8477585894906002
Epoch 18: loss 0.8346145685600198
Epoch 19: loss 0.8219240706899892
Epoch 20: loss 0.8100067316837932
Epoch 21: loss 0.7987965498929438
Epoch 22: loss 0.7887399024289588
Epoch 23: loss 0.7795486509799957
Epoch 24: loss 0.7711663047256677
Epoch 25: loss 0.7638772378797116
Epoch 26: loss 0.7571413540969725
Epoch 27: loss 0.7511802038420802
Epoch 28: loss 0.7460198630457339
Epoch 29: loss 0.7410617987746778
Epoch 30: loss 0.7370455261805783
Epoch 31: loss 0.7331217693245929
Epoch 32: loss 0.7295591706814973
Epoch 33: loss 0.7263139369047207
Epoch 34: loss 0.7235963499416476
Epoch 35: loss 0.7208269447088241
Epoch 36: loss 0.7182863977940186
Epoch 37: loss 0.7160017485851827
Epoch 38: loss 0.7137593474725018
Epoch 39: loss 0.7117277724587399
Epoch 40: loss 0.7098703241866567
Epoch 41: loss 0.708068204768326
Epoch 42: loss 0.7064175373834113
Epoch 43: loss 0.7047296619933584
Epoch 44: loss 0.7032034662106763
Epoch 45: loss 0.7018189134805098
Epoch 46: loss 0.7003469422459603
Epoch 47: loss 0.6991195506375769
Epoch 48: loss 0.6975908896197444
Epoch 49: loss 0.6964250491365143
-----------Time: 0:02:52.079713, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.001, embedding_dim: 20, rmse: 1.0975816249847412-------------


Epoch 0: loss 6.372207681640335
Epoch 1: loss 1.026961532170358
Epoch 2: loss 1.0165553297685541
Epoch 3: loss 1.0085990517683652
Epoch 4: loss 0.9981992647699688
Epoch 5: loss 0.988983609235805
Epoch 6: loss 0.9792866291559261
Epoch 7: loss 0.9676076539832613
Epoch 8: loss 0.9517874042625013
Epoch 9: loss 0.9303751722626065
Epoch 10: loss 0.9044848109069078
Epoch 11: loss 0.8731060488716416
Epoch 12: loss 0.8369326694504075
Epoch 13: loss 0.7972384954924169
Epoch 14: loss 0.7547146756363952
Epoch 15: loss 0.7129768777800642
Epoch 16: loss 0.6732440942007563
Epoch 17: loss 0.6376684188842774
Epoch 18: loss 0.6064608353635539
Epoch 19: loss 0.579877014749724
Epoch 20: loss 0.5576468970140686
Epoch 21: loss 0.5388845831479715
Epoch 22: loss 0.5230546114561351
Epoch 23: loss 0.5093144878097202
Epoch 24: loss 0.4976047267084536
Epoch 25: loss 0.4873598039474176
Epoch 26: loss 0.4782780371282412
Epoch 27: loss 0.47028279553936875
Epoch 28: loss 0.4630510951189891
Epoch 29: loss 0.4564636122273362
Epoch 30: loss 0.4506417641173238
Epoch 31: loss 0.44520142745712526
Epoch 32: loss 0.4401850041811881
Epoch 33: loss 0.4355718146847642
Epoch 34: loss 0.4312674425866293
Epoch 35: loss 0.4272652986904849
Epoch 36: loss 0.42359979706613915
Epoch 37: loss 0.42009812664726504
Epoch 38: loss 0.41677107001128405
Epoch 39: loss 0.4136953768198905
Epoch 40: loss 0.4110064790624639
Epoch 41: loss 0.4080455837690312
Epoch 42: loss 0.40547712736803554
Epoch 43: loss 0.4030281462423179
Epoch 44: loss 0.40071272817642795
Epoch 45: loss 0.39849325029746346
Epoch 46: loss 0.39634058002544487
Epoch 47: loss 0.3942256630762764
Epoch 48: loss 0.392303429543972
Epoch 49: loss 0.39055400063162266
-----------Time: 0:02:23.028623, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.001, embedding_dim: 50, rmse: 1.3120009899139404-------------


Epoch 0: loss 4.833220984883931
Epoch 1: loss 1.034467040391072
Epoch 2: loss 1.0266970382436462
Epoch 3: loss 1.0142736760170563
Epoch 4: loss 1.0003418106099833
Epoch 5: loss 0.9836701243467952
Epoch 6: loss 0.9592120407068211
Epoch 7: loss 0.9236177529977716
Epoch 8: loss 0.8770671543867692
Epoch 9: loss 0.818647266211717
Epoch 10: loss 0.7507747873663903
Epoch 11: loss 0.6760710827034453
Epoch 12: loss 0.6003350687739641
Epoch 13: loss 0.5287421959897746
Epoch 14: loss 0.46556106391160385
Epoch 15: loss 0.412284973555285
Epoch 16: loss 0.36848998361307644
Epoch 17: loss 0.33291692338559936
Epoch 18: loss 0.30407930176867093
Epoch 19: loss 0.28024147818593875
Epoch 20: loss 0.26057358936447167
Epoch 21: loss 0.24423046460294207
Epoch 22: loss 0.2303068129102821
Epoch 23: loss 0.21813320788028448
Epoch 24: loss 0.20759233605602515
Epoch 25: loss 0.1983583867225958
Epoch 26: loss 0.1901412469699331
Epoch 27: loss 0.18284399425205977
Epoch 28: loss 0.1763134134447445
Epoch 29: loss 0.17038450152005838
Epoch 30: loss 0.16503265548659407
Epoch 31: loss 0.16003807905413533
Epoch 32: loss 0.15557974592337143
Epoch 33: loss 0.15144363338046749
Epoch 34: loss 0.147649826174197
Epoch 35: loss 0.14404492691645157
Epoch 36: loss 0.14083373530403429
Epoch 37: loss 0.13766400244734858
Epoch 38: loss 0.13484465703368187
Epoch 39: loss 0.13211981859867988
Epoch 40: loss 0.12949050170086
Epoch 41: loss 0.1271111964693536
Epoch 42: loss 0.12492211934826944
Epoch 43: loss 0.12283039687444335
Epoch 44: loss 0.12075746784229642
Epoch 45: loss 0.11880708812209576
Epoch 46: loss 0.11709028228955425
Epoch 47: loss 0.11530337345826885
Epoch 48: loss 0.11374019939128471
Epoch 49: loss 0.11211988390625818
-----------Time: 0:02:26.508513, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.001, embedding_dim: 100, rmse: 1.5038191080093384-------------


Epoch 0: loss 4.2082038614413015
Epoch 1: loss 1.0409554475675458
Epoch 2: loss 1.0229559558241264
Epoch 3: loss 0.9983487041100212
Epoch 4: loss 0.9772802921740905
Epoch 5: loss 0.9504144465793734
Epoch 6: loss 0.9099226349073908
Epoch 7: loss 0.848339590365472
Epoch 8: loss 0.7651405535314394
Epoch 9: loss 0.6654003073987753
Epoch 10: loss 0.5603138476286246
Epoch 11: loss 0.46121934295996375
Epoch 12: loss 0.3760749929948993
Epoch 13: loss 0.30710341522227164
Epoch 14: loss 0.2529813387957604
Epoch 15: loss 0.2114145286543214
Epoch 16: loss 0.17939212351389552
Epoch 17: loss 0.15454221082770306
Epoch 18: loss 0.13494237988539365
Epoch 19: loss 0.11938314744797737
Epoch 20: loss 0.10655507899658835
Epoch 21: loss 0.09604495549574495
Epoch 22: loss 0.08730136129762167
Epoch 23: loss 0.0798299042062591
Epoch 24: loss 0.07338176470897768
Epoch 25: loss 0.067955933543651
Epoch 26: loss 0.06324757495895028
Epoch 27: loss 0.05895559879667733
Epoch 28: loss 0.05528809991867646
Epoch 29: loss 0.05203073351927426
Epoch 30: loss 0.04912130221238603
Epoch 31: loss 0.04650551991780167
Epoch 32: loss 0.044215369062579196
Epoch 33: loss 0.04219766723437478
Epoch 34: loss 0.040225999828670984
Epoch 35: loss 0.03857726598077494
Epoch 36: loss 0.03700497698889155
Epoch 37: loss 0.03554263007219719
Epoch 38: loss 0.03425624502579803
Epoch 39: loss 0.03314463068002268
Epoch 40: loss 0.03202166268399552
Epoch 41: loss 0.03099023517140228
Epoch 42: loss 0.030027717021901323
Epoch 43: loss 0.029087137092795708
Epoch 44: loss 0.02840145132706865
Epoch 45: loss 0.027578465067579046
Epoch 46: loss 0.026880242153192343
Epoch 47: loss 0.02619185729721642
Epoch 48: loss 0.02565960866480094
Epoch 49: loss 0.02505600612975009
-----------Time: 0:02:59.542364, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.001, embedding_dim: 150, rmse: 1.447346806526184-------------


Epoch 0: loss 3.8692967344237412
Epoch 1: loss 1.050825453063716
Epoch 2: loss 1.0361757680773735
Epoch 3: loss 1.0147694614918337
Epoch 4: loss 0.9837919758065887
Epoch 5: loss 0.93435341845388
Epoch 6: loss 0.8598043117186297
Epoch 7: loss 0.7572194722683533
Epoch 8: loss 0.6325160321334135
Epoch 9: loss 0.5031234368034031
Epoch 10: loss 0.38631101001216017
Epoch 11: loss 0.2923629315165074
Epoch 12: loss 0.22116789246058982
Epoch 13: loss 0.16883451287513193
Epoch 14: loss 0.13124529507173144
Epoch 15: loss 0.10410321942814019
Epoch 16: loss 0.08438262362438051
Epoch 17: loss 0.06985328780973087
Epoch 18: loss 0.05877750905473595
Epoch 19: loss 0.05024699937633198
Epoch 20: loss 0.043660910646228686
Epoch 21: loss 0.03857170163410837
Epoch 22: loss 0.03451793935838277
Epoch 23: loss 0.03127119154871806
Epoch 24: loss 0.02863007101797215
Epoch 25: loss 0.026506999418463396
Epoch 26: loss 0.02462908066606716
Epoch 27: loss 0.023302515328902266
Epoch 28: loss 0.022124220675829313
Epoch 29: loss 0.0212067449607117
Epoch 30: loss 0.020381613901775817
Epoch 31: loss 0.019711557974148055
Epoch 32: loss 0.01911744094737198
Epoch 33: loss 0.01859347662564529
Epoch 34: loss 0.01821365832634594
Epoch 35: loss 0.01788820709688994
Epoch 36: loss 0.01757414140029932
Epoch 37: loss 0.01722147329001809
Epoch 38: loss 0.016970033471917976
Epoch 39: loss 0.01677131183323977
Epoch 40: loss 0.016598945754863644
Epoch 41: loss 0.01640251190300383
Epoch 42: loss 0.01621664032218573
Epoch 43: loss 0.016021766531807573
Epoch 44: loss 0.015914321302071863
Epoch 45: loss 0.01578829745033189
Epoch 46: loss 0.01558383006359572
Epoch 47: loss 0.015494720445459952
Epoch 48: loss 0.015384422677933522
Epoch 49: loss 0.01527357984273492
-----------Time: 0:03:33.526469, Loss: regression, n_iter: 50, l2: 1e-07, batch_size: 1024, learning_rate: 0.001, embedding_dim: 200, rmse: 1.2760121822357178-------------


Epoch 0: loss 16.12945701557657
Epoch 1: loss 16.129308915138246
Epoch 2: loss 16.129298495209735
Epoch 3: loss 16.129404344766037
Epoch 4: loss 16.12933603266011
Epoch 5: loss 16.12951113763063
Epoch 6: loss 16.12943816806959
Epoch 7: loss 16.129292123214057
Epoch 8: loss 16.129366371942602
Epoch 9: loss 16.129050647694132
Epoch 10: loss 16.129374980926514
Epoch 11: loss 16.129570313121963
Epoch 12: loss 16.129299722547117
Epoch 13: loss 16.12964873832205
Epoch 14: loss 16.129665476342907
Epoch 15: loss 16.12933634467747
Epoch 16: loss 16.129504786367
Epoch 17: loss 16.129384471022565
Epoch 18: loss 16.129796204359636
Epoch 19: loss 16.129437680866406
Epoch 20: loss 16.12928562371627
Epoch 21: loss 16.12970848498137
Epoch 22: loss 16.1295029743858
Epoch 23: loss 16.129252684634665
Epoch 24: loss 16.129573256036508
Epoch 25: loss 16.12941957142042
Epoch 26: loss 16.128926668996396
Epoch 27: loss 16.128788964644723
Epoch 28: loss 16.12907268171725
Epoch 29: loss 16.129395359495412
Epoch 30: loss 16.129834670605867
Epoch 31: loss 16.1293350395949
Epoch 32: loss 16.129502983715224
Epoch 33: loss 16.1296349245569
Epoch 34: loss 16.129283689415974
Epoch 35: loss 16.129548920755802
Epoch 36: loss 16.129274425299272
Epoch 37: loss 16.129493869905886
Epoch 38: loss 16.129457504852958
Epoch 39: loss 16.129175635006117
Epoch 40: loss 16.129424247534377
Epoch 41: loss 16.129253660077634
Epoch 42: loss 16.12928940316905
Epoch 43: loss 16.12909007176109
Epoch 44: loss 16.129457872846853
Epoch 45: loss 16.129453551250954
Epoch 46: loss 16.129323054396586
Epoch 47: loss 16.129153986599135
Epoch 48: loss 16.129190989162613
Epoch 49: loss 16.12938646959222
-----------Time: 0:02:23.320846, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 20, rmse: 4.01716423034668-------------


Epoch 0: loss 16.12936391312143
Epoch 1: loss 16.129551605556323
Epoch 2: loss 16.129501256735427
Epoch 3: loss 16.129264435560806
Epoch 4: loss 16.129034653953884
Epoch 5: loss 16.129208093104154
Epoch 6: loss 16.12916276143945
Epoch 7: loss 16.12907417546148
Epoch 8: loss 16.129356653793998
Epoch 9: loss 16.129313228441323
Epoch 10: loss 16.129286719405133
Epoch 11: loss 16.12933857648269
Epoch 12: loss 16.129141807556152
Epoch 13: loss 16.12925668799359
Epoch 14: loss 16.12906035444011
Epoch 15: loss 16.129109061282612
Epoch 16: loss 16.129218310895173
Epoch 17: loss 16.12914910212807
Epoch 18: loss 16.12928248592045
Epoch 19: loss 16.129072176891825
Epoch 20: loss 16.129095066112022
Epoch 21: loss 16.12898398378621
Epoch 22: loss 16.129269826930503
Epoch 23: loss 16.129248633592024
Epoch 24: loss 16.129096311071645
Epoch 25: loss 16.128927726330964
Epoch 26: loss 16.129120088660198
Epoch 27: loss 16.12914387350497
Epoch 28: loss 16.12949039728745
Epoch 29: loss 16.129149133226147
Epoch 30: loss 16.129174715539683
Epoch 31: loss 16.12927713808806
Epoch 32: loss 16.129180558868075
Epoch 33: loss 16.12931397375853
Epoch 34: loss 16.12887242151343
Epoch 35: loss 16.12903804882713
Epoch 36: loss 16.12874010334844
Epoch 37: loss 16.128658279128697
Epoch 38: loss 16.12885988898899
Epoch 39: loss 16.128951650080474
Epoch 40: loss 16.129224290018495
Epoch 41: loss 16.129164164999256
Epoch 42: loss 16.129132981922314
Epoch 43: loss 16.129672169685364
Epoch 44: loss 16.12903215366861
Epoch 45: loss 16.12904482613439
Epoch 46: loss 16.12942226866017
Epoch 47: loss 16.12921303562496
Epoch 48: loss 16.12935713270436
Epoch 49: loss 16.129117761487546
-----------Time: 0:02:47.706863, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 50, rmse: 4.017184734344482-------------


Epoch 0: loss 16.129173978515293
Epoch 1: loss 16.129168772697447
Epoch 2: loss 16.129088320939438
Epoch 3: loss 16.12921112102011
Epoch 4: loss 16.12905527923418
Epoch 5: loss 16.12920953501826
Epoch 6: loss 16.129092046488886
Epoch 7: loss 16.1292253618655
Epoch 8: loss 16.129356557389965
Epoch 9: loss 16.128968303099924
Epoch 10: loss 16.129004936632903
Epoch 11: loss 16.129035765191784
Epoch 12: loss 16.129231189644855
Epoch 13: loss 16.128978276252745
Epoch 14: loss 16.129248978780662
Epoch 15: loss 16.12920732394509
Epoch 16: loss 16.129209578555564
Epoch 17: loss 16.129560231125872
Epoch 18: loss 16.12913763212121
Epoch 19: loss 16.128888683733734
Epoch 20: loss 16.128879474556964
Epoch 21: loss 16.129455656590668
Epoch 22: loss 16.129098419521167
Epoch 23: loss 16.129269816564477
Epoch 24: loss 16.12900274525518
Epoch 25: loss 16.128720843273662
Epoch 26: loss 16.129250456975853
Epoch 27: loss 16.129188278447028
Epoch 28: loss 16.129194337388743
Epoch 29: loss 16.12909161629884
Epoch 30: loss 16.12893317160399
Epoch 31: loss 16.128843593597413
Epoch 32: loss 16.128755712509154
Epoch 33: loss 16.129498014242753
Epoch 34: loss 16.129120984284775
Epoch 35: loss 16.129411002864007
Epoch 36: loss 16.129115772247314
Epoch 37: loss 16.129129054235374
Epoch 38: loss 16.129241057064224
Epoch 39: loss 16.129143335508264
Epoch 40: loss 16.129562672324802
Epoch 41: loss 16.12920719333317
Epoch 42: loss 16.128933392400327
Epoch 43: loss 16.129457580524942
Epoch 44: loss 16.129004413148632
Epoch 45: loss 16.12898362926815
Epoch 46: loss 16.12900057979252
Epoch 47: loss 16.129771950970525
Epoch 48: loss 16.129025415752245
Epoch 49: loss 16.128888944957566
-----------Time: 0:03:17.740121, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 100, rmse: 4.017182350158691-------------


Epoch 0: loss 16.128917846472365
Epoch 1: loss 16.129124482818273
Epoch 2: loss 16.12888494885486
Epoch 3: loss 16.1292889709058
Epoch 4: loss 16.129265364356662
Epoch 5: loss 16.129109354641127
Epoch 6: loss 16.129047751426697
Epoch 7: loss 16.128935622132342
Epoch 8: loss 16.12915287328803
Epoch 9: loss 16.12932032605876
Epoch 10: loss 16.12928241957789
Epoch 11: loss 16.12934037913447
Epoch 12: loss 16.1290672675423
Epoch 13: loss 16.12949859266696
Epoch 14: loss 16.129091693007428
Epoch 15: loss 16.129016773597055
Epoch 16: loss 16.129093542306318
Epoch 17: loss 16.129359925311544
Epoch 18: loss 16.12904188943946
Epoch 19: loss 16.12899512415347
Epoch 20: loss 16.12939509516177
Epoch 21: loss 16.129299199062846
Epoch 22: loss 16.129169435086457
Epoch 23: loss 16.12922327103822
Epoch 24: loss 16.129126748831375
Epoch 25: loss 16.12914347855941
Epoch 26: loss 16.12917528670767
Epoch 27: loss 16.128911605088607
Epoch 28: loss 16.129456251600516
Epoch 29: loss 16.12913944410241
Epoch 30: loss 16.12919307688008
Epoch 31: loss 16.129102734897447
Epoch 32: loss 16.129223042985668
Epoch 33: loss 16.129124229887257
Epoch 34: loss 16.12902410445006
Epoch 35: loss 16.129231670628425
Epoch 36: loss 16.128957205233366
Epoch 37: loss 16.129144241498864
Epoch 38: loss 16.1293342994607
Epoch 39: loss 16.12960848600968
Epoch 40: loss 16.129151649060457
Epoch 41: loss 16.129179144942242
Epoch 42: loss 16.1293607711792
Epoch 43: loss 16.129242976852087
Epoch 44: loss 16.12870576174363
Epoch 45: loss 16.128981795518296
Epoch 46: loss 16.12908819447393
Epoch 47: loss 16.12947140050971
Epoch 48: loss 16.129068294815394
Epoch 49: loss 16.128974616009256
-----------Time: 0:03:30.565281, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 150, rmse: 4.017183780670166-------------


Epoch 0: loss 16.12929115710051
Epoch 1: loss 16.12909827750662
Epoch 2: loss 16.12912059141242
Epoch 3: loss 16.129368425452192
Epoch 4: loss 16.129073033125504
Epoch 5: loss 16.12909966448079
Epoch 6: loss 16.129240752303083
Epoch 7: loss 16.129221369909203
Epoch 8: loss 16.129235969419064
Epoch 9: loss 16.12929845581884
Epoch 10: loss 16.129105702690456
Epoch 11: loss 16.128993765167568
Epoch 12: loss 16.129123736464457
Epoch 13: loss 16.1290433894033
Epoch 14: loss 16.12923674998076
Epoch 15: loss 16.12919697450555
Epoch 16: loss 16.129538612780365
Epoch 17: loss 16.129324917171314
Epoch 18: loss 16.129098492083344
Epoch 19: loss 16.129191546854766
Epoch 20: loss 16.129172138545822
Epoch 21: loss 16.12921068046404
Epoch 22: loss 16.129113945753677
Epoch 23: loss 16.12920814493428
Epoch 24: loss 16.128976117009703
Epoch 25: loss 16.129184809975
Epoch 26: loss 16.12902466836183
Epoch 27: loss 16.12947653791179
Epoch 28: loss 16.129434586607893
Epoch 29: loss 16.12929750421773
Epoch 30: loss 16.128812091246896
Epoch 31: loss 16.128995834226192
Epoch 32: loss 16.12898199869239
Epoch 33: loss 16.129493926919025
Epoch 34: loss 16.129349131169526
Epoch 35: loss 16.12922518875288
Epoch 36: loss 16.12917322283206
Epoch 37: loss 16.129108160475027
Epoch 38: loss 16.12875273745993
Epoch 39: loss 16.129016098768815
Epoch 40: loss 16.129264822213546
Epoch 41: loss 16.12868169079656
Epoch 42: loss 16.128869627869648
Epoch 43: loss 16.12938940525055
Epoch 44: loss 16.128941242591196
Epoch 45: loss 16.128980878125066
Epoch 46: loss 16.129692475692085
Epoch 47: loss 16.129085092959198
Epoch 48: loss 16.129185684867526
Epoch 49: loss 16.128883959936058
-----------Time: 0:03:05.983306, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-10, embedding_dim: 200, rmse: 4.017183780670166-------------


Epoch 0: loss 16.12948392474133
Epoch 1: loss 16.129504752159118
Epoch 2: loss 16.129465133210886
Epoch 3: loss 16.12907968085745
Epoch 4: loss 16.129473933966263
Epoch 5: loss 16.12926714212998
Epoch 6: loss 16.129222404438515
Epoch 7: loss 16.129399914326875
Epoch 8: loss 16.129187346541364
Epoch 9: loss 16.128883342120957
Epoch 10: loss 16.129413258511086
Epoch 11: loss 16.129129144419796
Epoch 12: loss 16.12900081406469
Epoch 13: loss 16.129215430176778
Epoch 14: loss 16.12926745829375
Epoch 15: loss 16.129067014611287
Epoch 16: loss 16.129189696519273
Epoch 17: loss 16.128678447267284
Epoch 18: loss 16.129223490797955
Epoch 19: loss 16.129277678157973
Epoch 20: loss 16.129241758844127
Epoch 21: loss 16.128860120151355
Epoch 22: loss 16.1293357807657
Epoch 23: loss 16.128870428126792
Epoch 24: loss 16.12913263777028
Epoch 25: loss 16.129167989025945
Epoch 26: loss 16.129131615680198
Epoch 27: loss 16.12897409045178
Epoch 28: loss 16.129175313659335
Epoch 29: loss 16.129031324386595
Epoch 30: loss 16.128992972166643
Epoch 31: loss 16.128955339348835
Epoch 32: loss 16.128916038637577
Epoch 33: loss 16.129190324700396
Epoch 34: loss 16.12919737048771
Epoch 35: loss 16.129495581336645
Epoch 36: loss 16.128935713353364
Epoch 37: loss 16.12917275325112
Epoch 38: loss 16.129127748116204
Epoch 39: loss 16.129057940192844
Epoch 40: loss 16.129067471752997
Epoch 41: loss 16.128834672596145
Epoch 42: loss 16.12912051677704
Epoch 43: loss 16.129126884626306
Epoch 44: loss 16.128948130814926
Epoch 45: loss 16.12868748644124
Epoch 46: loss 16.128819622164187
Epoch 47: loss 16.128970243619836
Epoch 48: loss 16.12868753205175
Epoch 49: loss 16.12875090992969
-----------Time: 0:01:55.551237, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 20, rmse: 4.017130374908447-------------


Epoch 0: loss 16.129112454082655
Epoch 1: loss 16.12909685321476
Epoch 2: loss 16.129208191581395
Epoch 3: loss 16.129157412570457
Epoch 4: loss 16.129313756072
Epoch 5: loss 16.129547148165496
Epoch 6: loss 16.129161761118016
Epoch 7: loss 16.12896673679352
Epoch 8: loss 16.128972068040266
Epoch 9: loss 16.12909651942875
Epoch 10: loss 16.12893777826558
Epoch 11: loss 16.129344832378884
Epoch 12: loss 16.129255954079007
Epoch 13: loss 16.128711523180424
Epoch 14: loss 16.129333419385162
Epoch 15: loss 16.129317280520564
Epoch 16: loss 16.128885334470997
Epoch 17: loss 16.128894061627594
Epoch 18: loss 16.12895027450893
Epoch 19: loss 16.129225973460986
Epoch 20: loss 16.12947342188462
Epoch 21: loss 16.12869574194369
Epoch 22: loss 16.129071219071097
Epoch 23: loss 16.128969081588412
Epoch 24: loss 16.129146522024403
Epoch 25: loss 16.128861676091734
Epoch 26: loss 16.128732171265977
Epoch 27: loss 16.12883430874866
Epoch 28: loss 16.128612422943114
Epoch 29: loss 16.12873759580695
Epoch 30: loss 16.12912412000739
Epoch 31: loss 16.12873121344525
Epoch 32: loss 16.128898341759392
Epoch 33: loss 16.128701633992403
Epoch 34: loss 16.1284781601118
Epoch 35: loss 16.12895983716716
Epoch 36: loss 16.128905037175056
Epoch 37: loss 16.129082523221555
Epoch 38: loss 16.128591738576475
Epoch 39: loss 16.128751524634982
Epoch 40: loss 16.128588850601858
Epoch 41: loss 16.12869437881138
Epoch 42: loss 16.129298011116358
Epoch 43: loss 16.128790576561638
Epoch 44: loss 16.12910749497621
Epoch 45: loss 16.128473822966868
Epoch 46: loss 16.128914172753042
Epoch 47: loss 16.129272807162742
Epoch 48: loss 16.128818583488464
Epoch 49: loss 16.12872118431589
-----------Time: 0:02:29.532205, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 50, rmse: 4.017129421234131-------------


Epoch 0: loss 16.129491758346557
Epoch 1: loss 16.129035573420317
Epoch 2: loss 16.129141334865405
Epoch 3: loss 16.129151138015416
Epoch 4: loss 16.129081312469815
Epoch 5: loss 16.128963278687518
Epoch 6: loss 16.128931542064834
Epoch 7: loss 16.12909745755403
Epoch 8: loss 16.128756367641948
Epoch 9: loss 16.12888458604398
Epoch 10: loss 16.128882555339647
Epoch 11: loss 16.12911294646885
Epoch 12: loss 16.12867622686469
Epoch 13: loss 16.12920021077861
Epoch 14: loss 16.128955644109976
Epoch 15: loss 16.129302220759186
Epoch 16: loss 16.129257632338483
Epoch 17: loss 16.128756356239318
Epoch 18: loss 16.129278872324072
Epoch 19: loss 16.12901440807011
Epoch 20: loss 16.128877952824467
Epoch 21: loss 16.12876381252123
Epoch 22: loss 16.129123590303504
Epoch 23: loss 16.129093150470567
Epoch 24: loss 16.129206585884095
Epoch 25: loss 16.12893912066584
Epoch 26: loss 16.12893718947535
Epoch 27: loss 16.128904724121092
Epoch 28: loss 16.129100979929387
Epoch 29: loss 16.129313698022262
Epoch 30: loss 16.128540173820827
Epoch 31: loss 16.12862746404565
Epoch 32: loss 16.128642506184786
Epoch 33: loss 16.129185663098873
Epoch 34: loss 16.129021200926406
Epoch 35: loss 16.128576126305955
Epoch 36: loss 16.128776886152185
Epoch 37: loss 16.128794522907423
Epoch 38: loss 16.12871815432673
Epoch 39: loss 16.12887767397839
Epoch 40: loss 16.12881710840308
Epoch 41: loss 16.128893489423007
Epoch 42: loss 16.12910302410955
Epoch 43: loss 16.128860309849614
Epoch 44: loss 16.12891195131385
Epoch 45: loss 16.129130552126014
Epoch 46: loss 16.128700531047322
Epoch 47: loss 16.128933162274567
Epoch 48: loss 16.1285347689753
Epoch 49: loss 16.128717791515847
-----------Time: 0:02:46.775310, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 100, rmse: 4.0171427726745605-------------


Epoch 0: loss 16.12905868240025
Epoch 1: loss 16.129143973018813
Epoch 2: loss 16.12896279770395
Epoch 3: loss 16.129207087599713
Epoch 4: loss 16.129294481484788
Epoch 5: loss 16.12923239728679
Epoch 6: loss 16.129128675875457
Epoch 7: loss 16.12900767326355
Epoch 8: loss 16.12911868095398
Epoch 9: loss 16.129057492380557
Epoch 10: loss 16.12931290709454
Epoch 11: loss 16.128907219223354
Epoch 12: loss 16.12907193225363
Epoch 13: loss 16.12923774097277
Epoch 14: loss 16.12894737098528
Epoch 15: loss 16.12865195688994
Epoch 16: loss 16.12887321555096
Epoch 17: loss 16.12902479275413
Epoch 18: loss 16.12922012909599
Epoch 19: loss 16.129026170398877
Epoch 20: loss 16.12905440226845
Epoch 21: loss 16.12888213655223
Epoch 22: loss 16.128912605410036
Epoch 23: loss 16.129019515410715
Epoch 24: loss 16.129216843066008
Epoch 25: loss 16.129196190834044
Epoch 26: loss 16.128826202516972
Epoch 27: loss 16.12922023690265
Epoch 28: loss 16.12869438192119
Epoch 29: loss 16.128927785417307
Epoch 30: loss 16.129001846520797
Epoch 31: loss 16.128902567987858
Epoch 32: loss 16.12874121458634
Epoch 33: loss 16.128976866473323
Epoch 34: loss 16.129048851261967
Epoch 35: loss 16.12852412306744
Epoch 36: loss 16.128638697707135
Epoch 37: loss 16.128937521188156
Epoch 38: loss 16.12922127868818
Epoch 39: loss 16.12903139902198
Epoch 40: loss 16.129212546348572
Epoch 41: loss 16.129162368567094
Epoch 42: loss 16.12903362357098
Epoch 43: loss 16.128846926274505
Epoch 44: loss 16.128689974287283
Epoch 45: loss 16.128968099925828
Epoch 46: loss 16.12834533090177
Epoch 47: loss 16.128554762964665
Epoch 48: loss 16.128585171699523
Epoch 49: loss 16.129222148397695
-----------Time: 0:03:21.719475, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 150, rmse: 4.017143249511719-------------


Epoch 0: loss 16.129360929779384
Epoch 1: loss 16.1290829544482
Epoch 2: loss 16.12902941185495
Epoch 3: loss 16.129307691947275
Epoch 4: loss 16.129182990737583
Epoch 5: loss 16.129241038405375
Epoch 6: loss 16.129330526227537
Epoch 7: loss 16.129088297097578
Epoch 8: loss 16.129195244415946
Epoch 9: loss 16.12911886961564
Epoch 10: loss 16.129079550245535
Epoch 11: loss 16.128956279547317
Epoch 12: loss 16.12937842141027
Epoch 13: loss 16.12890927998916
Epoch 14: loss 16.128932217929673
Epoch 15: loss 16.1288002356239
Epoch 16: loss 16.12928468351779
Epoch 17: loss 16.129167998355367
Epoch 18: loss 16.129144991999087
Epoch 19: loss 16.128976962877356
Epoch 20: loss 16.12907282165859
Epoch 21: loss 16.129297490741894
Epoch 22: loss 16.12892679027889
Epoch 23: loss 16.12878284454346
Epoch 24: loss 16.128874328862064
Epoch 25: loss 16.129365134239197
Epoch 26: loss 16.129059801930968
Epoch 27: loss 16.12890713940496
Epoch 28: loss 16.128974707230277
Epoch 29: loss 16.1290762020194
Epoch 30: loss 16.128955137211342
Epoch 31: loss 16.128948443868886
Epoch 32: loss 16.129002038292263
Epoch 33: loss 16.128992165689883
Epoch 34: loss 16.128845995405445
Epoch 35: loss 16.128814413236537
Epoch 36: loss 16.12905648480291
Epoch 37: loss 16.128886285035506
Epoch 38: loss 16.128723907470704
Epoch 39: loss 16.12897901846015
Epoch 40: loss 16.128687721750012
Epoch 41: loss 16.12865303081015
Epoch 42: loss 16.128380176295405
Epoch 43: loss 16.128809967248337
Epoch 44: loss 16.129095878808396
Epoch 45: loss 16.12892018712085
Epoch 46: loss 16.128680171137272
Epoch 47: loss 16.12890588822572
Epoch 48: loss 16.128705055817314
Epoch 49: loss 16.128735583761465
-----------Time: 0:04:09.443685, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-09, embedding_dim: 200, rmse: 4.017139911651611-------------


Epoch 0: loss 16.12876920181772
Epoch 1: loss 16.129132211726645
Epoch 2: loss 16.12869802661564
Epoch 3: loss 16.128719683315442
Epoch 4: loss 16.128836263781007
Epoch 5: loss 16.128793201239212
Epoch 6: loss 16.1284005134002
Epoch 7: loss 16.128151132749476
Epoch 8: loss 16.128682670385942
Epoch 9: loss 16.128466680775517
Epoch 10: loss 16.12823184573132
Epoch 11: loss 16.128352290651073
Epoch 12: loss 16.128118508795033
Epoch 13: loss 16.128033369520438
Epoch 14: loss 16.128099209329356
Epoch 15: loss 16.127693836585337
Epoch 16: loss 16.128062509453816
Epoch 17: loss 16.128009028020113
Epoch 18: loss 16.127558071716972
Epoch 19: loss 16.12786793605141
Epoch 20: loss 16.12775737306346
Epoch 21: loss 16.127223601548568
Epoch 22: loss 16.127608326207035
Epoch 23: loss 16.12726230310357
Epoch 24: loss 16.12721845274386
Epoch 25: loss 16.127275545700737
Epoch 26: loss 16.126894389028134
Epoch 27: loss 16.126963176934616
Epoch 28: loss 16.126942209575486
Epoch 29: loss 16.126849675178526
Epoch 30: loss 16.12689309431159
Epoch 31: loss 16.12673056540282
Epoch 32: loss 16.126215340780174
Epoch 33: loss 16.12649799015211
Epoch 34: loss 16.126662271955738
Epoch 35: loss 16.126499804206517
Epoch 36: loss 16.126267753476682
Epoch 37: loss 16.12623559495677
Epoch 38: loss 16.126002102312835
Epoch 39: loss 16.125954464207524
Epoch 40: loss 16.126069040920424
Epoch 41: loss 16.126067346075306
Epoch 42: loss 16.12581284771795
Epoch 43: loss 16.125728415406268
Epoch 44: loss 16.12537019563758
Epoch 45: loss 16.125502247395723
Epoch 46: loss 16.125505216225335
Epoch 47: loss 16.125201629555743
Epoch 48: loss 16.125605247331702
Epoch 49: loss 16.12532649351203
-----------Time: 0:02:53.686741, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 20, rmse: 4.0167012214660645-------------


Epoch 0: loss 16.12903489133586
Epoch 1: loss 16.12913703192835
Epoch 2: loss 16.12903601190318
Epoch 3: loss 16.128916789137797
Epoch 4: loss 16.128813506209333
Epoch 5: loss 16.12904533095982
Epoch 6: loss 16.128617042043935
Epoch 7: loss 16.128785705566408
Epoch 8: loss 16.12814248852108
Epoch 9: loss 16.12805297582046
Epoch 10: loss 16.128306217815567
Epoch 11: loss 16.12806531035382
Epoch 12: loss 16.128470780538475
Epoch 13: loss 16.128005031917407
Epoch 14: loss 16.128179129310276
Epoch 15: loss 16.128028148153554
Epoch 16: loss 16.128125546289528
Epoch 17: loss 16.127781657550646
Epoch 18: loss 16.127658417950506
Epoch 19: loss 16.128061643890714
Epoch 20: loss 16.127579132370325
Epoch 21: loss 16.127707752974136
Epoch 22: loss 16.12732276190882
Epoch 23: loss 16.127228482909825
Epoch 24: loss 16.127184166078983
Epoch 25: loss 16.12731117580248
Epoch 26: loss 16.127059884693313
Epoch 27: loss 16.126886946222058
Epoch 28: loss 16.12717040932697
Epoch 29: loss 16.12703028347181
Epoch 30: loss 16.12727147081624
Epoch 31: loss 16.126860327306
Epoch 32: loss 16.126902521174888
Epoch 33: loss 16.12686450170434
Epoch 34: loss 16.126556153919385
Epoch 35: loss 16.126426398235818
Epoch 36: loss 16.126412821852643
Epoch 37: loss 16.126210623202116
Epoch 38: loss 16.12610490322113
Epoch 39: loss 16.126162190022676
Epoch 40: loss 16.126291751861572
Epoch 41: loss 16.12586735435154
Epoch 42: loss 16.125648659208547
Epoch 43: loss 16.12608308999435
Epoch 44: loss 16.12611997127533
Epoch 45: loss 16.125464666408043
Epoch 46: loss 16.125777045540186
Epoch 47: loss 16.125568029154902
Epoch 48: loss 16.12505054473877
Epoch 49: loss 16.12511355980583
-----------Time: 0:02:07.775883, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 50, rmse: 4.016716957092285-------------


Epoch 0: loss 16.128881727094235
Epoch 1: loss 16.12923026499541
Epoch 2: loss 16.129397100987642
Epoch 3: loss 16.128751010480133
Epoch 4: loss 16.12889384601427
Epoch 5: loss 16.12885683723118
Epoch 6: loss 16.129154747465382
Epoch 7: loss 16.12887543802676
Epoch 8: loss 16.128760476734328
Epoch 9: loss 16.128356750115103
Epoch 10: loss 16.128275072056315
Epoch 11: loss 16.12844595701798
Epoch 12: loss 16.128059026469355
Epoch 13: loss 16.127951756767605
Epoch 14: loss 16.12808182032212
Epoch 15: loss 16.128185240082118
Epoch 16: loss 16.128171904190726
Epoch 17: loss 16.127727957393812
Epoch 18: loss 16.12803656951241
Epoch 19: loss 16.127495274336443
Epoch 20: loss 16.127557310850722
Epoch 21: loss 16.12763915061951
Epoch 22: loss 16.127419731927954
Epoch 23: loss 16.127042432453322
Epoch 24: loss 16.12749389358189
Epoch 25: loss 16.127489132466522
Epoch 26: loss 16.12739469072093
Epoch 27: loss 16.12700087816819
Epoch 28: loss 16.126786795906398
Epoch 29: loss 16.126764884202377
Epoch 30: loss 16.126855422102885
Epoch 31: loss 16.12648546592049
Epoch 32: loss 16.126693757720616
Epoch 33: loss 16.126667440455893
Epoch 34: loss 16.126535622969918
Epoch 35: loss 16.12625757093015
Epoch 36: loss 16.126202594715615
Epoch 37: loss 16.12622089696967
Epoch 38: loss 16.126263797801474
Epoch 39: loss 16.126185855658157
Epoch 40: loss 16.12593099448992
Epoch 41: loss 16.12602368334065
Epoch 42: loss 16.125649977766948
Epoch 43: loss 16.12557181171749
Epoch 44: loss 16.126137920047928
Epoch 45: loss 16.125418770831565
Epoch 46: loss 16.125442661409792
Epoch 47: loss 16.125474241505497
Epoch 48: loss 16.124926717385
Epoch 49: loss 16.125127314484637
-----------Time: 0:02:32.604755, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 100, rmse: 4.016717910766602-------------


Epoch 0: loss 16.128838665589043
Epoch 1: loss 16.128907164283422
Epoch 2: loss 16.129075120842977
Epoch 3: loss 16.12871481957643
Epoch 4: loss 16.129361229357514
Epoch 5: loss 16.12876387368078
Epoch 6: loss 16.128797648264015
Epoch 7: loss 16.128376546113387
Epoch 8: loss 16.12863839087279
Epoch 9: loss 16.128486800193787
Epoch 10: loss 16.128359048262887
Epoch 11: loss 16.128194074008775
Epoch 12: loss 16.127863725371984
Epoch 13: loss 16.12814058013584
Epoch 14: loss 16.128146092788032
Epoch 15: loss 16.127959361283676
Epoch 16: loss 16.12774329185486
Epoch 17: loss 16.1280041653177
Epoch 18: loss 16.12771138937577
Epoch 19: loss 16.127380928785904
Epoch 20: loss 16.12777373894401
Epoch 21: loss 16.127724397700767
Epoch 22: loss 16.127390464492468
Epoch 23: loss 16.12755221180294
Epoch 24: loss 16.127170951470085
Epoch 25: loss 16.127268085272416
Epoch 26: loss 16.127185221340344
Epoch 27: loss 16.1267848616061
Epoch 28: loss 16.126864541095237
Epoch 29: loss 16.12713602003844
Epoch 30: loss 16.126739847141764
Epoch 31: loss 16.12661218021227
Epoch 32: loss 16.12634392924931
Epoch 33: loss 16.12679083450981
Epoch 34: loss 16.126558815914652
Epoch 35: loss 16.126487575406614
Epoch 36: loss 16.12659767503324
Epoch 37: loss 16.12637060828831
Epoch 38: loss 16.12626157636228
Epoch 39: loss 16.126129516311313
Epoch 40: loss 16.126242699830428
Epoch 41: loss 16.126042320417323
Epoch 42: loss 16.12564396754555
Epoch 43: loss 16.12550647362419
Epoch 44: loss 16.125717806816102
Epoch 45: loss 16.125384154527083
Epoch 46: loss 16.125481140095253
Epoch 47: loss 16.125468196039616
Epoch 48: loss 16.125442934036254
Epoch 49: loss 16.125416647869606
-----------Time: 0:03:01.338109, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 150, rmse: 4.016716957092285-------------


Epoch 0: loss 16.129144611565962
Epoch 1: loss 16.12905083635579
Epoch 2: loss 16.12866569498311
Epoch 3: loss 16.129011320031207
Epoch 4: loss 16.128841209411622
Epoch 5: loss 16.128790867846945
Epoch 6: loss 16.128510410889334
Epoch 7: loss 16.128647813589676
Epoch 8: loss 16.128617082471433
Epoch 9: loss 16.128630236957385
Epoch 10: loss 16.128492714011152
Epoch 11: loss 16.127993117208067
Epoch 12: loss 16.128455444004224
Epoch 13: loss 16.127814036866894
Epoch 14: loss 16.128005937908007
Epoch 15: loss 16.127758130819903
Epoch 16: loss 16.12810388337011
Epoch 17: loss 16.12814958199211
Epoch 18: loss 16.128032838779948
Epoch 19: loss 16.128093882229017
Epoch 20: loss 16.12764353648476
Epoch 21: loss 16.127714099054753
Epoch 22: loss 16.127443539577982
Epoch 23: loss 16.127352995457855
Epoch 24: loss 16.127670221743376
Epoch 25: loss 16.127166220416193
Epoch 26: loss 16.127240336459614
Epoch 27: loss 16.12692392598028
Epoch 28: loss 16.127340586289115
Epoch 29: loss 16.12702515747236
Epoch 30: loss 16.126898609036985
Epoch 31: loss 16.12674358409384
Epoch 32: loss 16.126428298328232
Epoch 33: loss 16.126741214420484
Epoch 34: loss 16.126313569234764
Epoch 35: loss 16.12622945619666
Epoch 36: loss 16.126614511531333
Epoch 37: loss 16.126290352448173
Epoch 38: loss 16.126413490461267
Epoch 39: loss 16.126466901405998
Epoch 40: loss 16.126150463974994
Epoch 41: loss 16.125924557188284
Epoch 42: loss 16.12577829464622
Epoch 43: loss 16.12591124928516
Epoch 44: loss 16.12595386401467
Epoch 45: loss 16.12554460919422
Epoch 46: loss 16.125456136205923
Epoch 47: loss 16.125295935506404
Epoch 48: loss 16.125116748395172
Epoch 49: loss 16.125022971111797
-----------Time: 0:03:34.442575, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-08, embedding_dim: 200, rmse: 4.0167155265808105-------------


Epoch 0: loss 16.128619738247085
Epoch 1: loss 16.127933474208998
Epoch 2: loss 16.127418863255045
Epoch 3: loss 16.126982834028162
Epoch 4: loss 16.125824592424475
Epoch 5: loss 16.125057877664982
Epoch 6: loss 16.124447301159734
Epoch 7: loss 16.1234257677327
Epoch 8: loss 16.12255512216817
Epoch 9: loss 16.12221948478533
Epoch 10: loss 16.121097215362216
Epoch 11: loss 16.12095358993696
Epoch 12: loss 16.119793770624245
Epoch 13: loss 16.118891298252603
Epoch 14: loss 16.11835636159648
Epoch 15: loss 16.11721608431443
Epoch 16: loss 16.116996294519176
Epoch 17: loss 16.11601824760437
Epoch 18: loss 16.114875745773315
Epoch 19: loss 16.114141906862674
Epoch 20: loss 16.113417434692384
Epoch 21: loss 16.112852738214574
Epoch 22: loss 16.111988615989684
Epoch 23: loss 16.111491071659586
Epoch 24: loss 16.110699656735296
Epoch 25: loss 16.109677514822586
Epoch 26: loss 16.108776589061904
Epoch 27: loss 16.108375937005746
Epoch 28: loss 16.107627913226253
Epoch 29: loss 16.106763931979305
Epoch 30: loss 16.10600863850635
Epoch 31: loss 16.10538695273192
Epoch 32: loss 16.10445503566576
Epoch 33: loss 16.10396879652272
Epoch 34: loss 16.103060987721317
Epoch 35: loss 16.10237716799197
Epoch 36: loss 16.101327434830043
Epoch 37: loss 16.100706906940626
Epoch 38: loss 16.100180249628814
Epoch 39: loss 16.09935440187869
Epoch 40: loss 16.098374239258145
Epoch 41: loss 16.09777665241905
Epoch 42: loss 16.096771377065906
Epoch 43: loss 16.09665629449098
Epoch 44: loss 16.095334055112755
Epoch 45: loss 16.09487809512926
Epoch 46: loss 16.093883013725282
Epoch 47: loss 16.09368516880533
Epoch 48: loss 16.09233983495961
Epoch 49: loss 16.091764962154887
-----------Time: 0:02:26.565568, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 20, rmse: 4.012462615966797-------------


Epoch 0: loss 16.128618662253672
Epoch 1: loss 16.1280293599419
Epoch 2: loss 16.127172328078228
Epoch 3: loss 16.12683963050013
Epoch 4: loss 16.12584902099941
Epoch 5: loss 16.124863526095513
Epoch 6: loss 16.124472274987593
Epoch 7: loss 16.123165406351504
Epoch 8: loss 16.12268000167349
Epoch 9: loss 16.12236705655637
Epoch 10: loss 16.121108034382697
Epoch 11: loss 16.12043766768082
Epoch 12: loss 16.120279272742895
Epoch 13: loss 16.11873689527097
Epoch 14: loss 16.118310029610342
Epoch 15: loss 16.117113553959392
Epoch 16: loss 16.116623096880705
Epoch 17: loss 16.115964521532472
Epoch 18: loss 16.11465971366219
Epoch 19: loss 16.11453840110613
Epoch 20: loss 16.113425188479216
Epoch 21: loss 16.112762859593268
Epoch 22: loss 16.112027506206346
Epoch 23: loss 16.11120516839235
Epoch 24: loss 16.110244932382003
Epoch 25: loss 16.109705751875172
Epoch 26: loss 16.108612157987512
Epoch 27: loss 16.108672803381214
Epoch 28: loss 16.107764768600465
Epoch 29: loss 16.106684377919073
Epoch 30: loss 16.105761457526164
Epoch 31: loss 16.105109227221945
Epoch 32: loss 16.104636372690617
Epoch 33: loss 16.103695541879404
Epoch 34: loss 16.102959377869315
Epoch 35: loss 16.102268521682078
Epoch 36: loss 16.101122476743615
Epoch 37: loss 16.10047745704651
Epoch 38: loss 16.09997793591541
Epoch 39: loss 16.09900345698647
Epoch 40: loss 16.098057320843573
Epoch 41: loss 16.097615456581117
Epoch 42: loss 16.096790817509525
Epoch 43: loss 16.096159657188085
Epoch 44: loss 16.095283152746116
Epoch 45: loss 16.093912802571836
Epoch 46: loss 16.09358801427095
Epoch 47: loss 16.092515050846597
Epoch 48: loss 16.091982617585554
Epoch 49: loss 16.091471707302592
-----------Time: 0:02:47.557721, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 50, rmse: 4.012448787689209-------------


Epoch 0: loss 16.128411786452585
Epoch 1: loss 16.127986160568568
Epoch 2: loss 16.12702124948087
Epoch 3: loss 16.12654939630757
Epoch 4: loss 16.125555222967396
Epoch 5: loss 16.124694858426633
Epoch 6: loss 16.124143632598546
Epoch 7: loss 16.123499009920202
Epoch 8: loss 16.12257996953052
Epoch 9: loss 16.122024215822634
Epoch 10: loss 16.12134617826213
Epoch 11: loss 16.120294200855753
Epoch 12: loss 16.119517307696135
Epoch 13: loss 16.118960219880808
Epoch 14: loss 16.11841537123141
Epoch 15: loss 16.117282921334972
Epoch 16: loss 16.116542748782944
Epoch 17: loss 16.11588611084482
Epoch 18: loss 16.114975444130277
Epoch 19: loss 16.11456339566604
Epoch 20: loss 16.113419297467107
Epoch 21: loss 16.11316178570623
Epoch 22: loss 16.11206007729406
Epoch 23: loss 16.11128442287445
Epoch 24: loss 16.110571752423827
Epoch 25: loss 16.109696791483007
Epoch 26: loss 16.10864598647408
Epoch 27: loss 16.10821116074272
Epoch 28: loss 16.10730246979257
Epoch 29: loss 16.106506934373275
Epoch 30: loss 16.10577820383984
Epoch 31: loss 16.105297187100287
Epoch 32: loss 16.104341919525808
Epoch 33: loss 16.103658784990724
Epoch 34: loss 16.102940795732582
Epoch 35: loss 16.10208676897961
Epoch 36: loss 16.101446042890135
Epoch 37: loss 16.100659514510113
Epoch 38: loss 16.099874532741048
Epoch 39: loss 16.098669014806333
Epoch 40: loss 16.098258665333624
Epoch 41: loss 16.09733045619467
Epoch 42: loss 16.096858773024184
Epoch 43: loss 16.095892548561096
Epoch 44: loss 16.094617260020712
Epoch 45: loss 16.094322852466416
Epoch 46: loss 16.09362092225448
Epoch 47: loss 16.092755801781365
Epoch 48: loss 16.092158541472063
Epoch 49: loss 16.09109038684679
-----------Time: 0:03:22.416324, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 100, rmse: 4.012456893920898-------------


Epoch 0: loss 16.128977848135907
Epoch 1: loss 16.127773099360258
Epoch 2: loss 16.12753603147424
Epoch 3: loss 16.12638911993607
Epoch 4: loss 16.126052458389946
Epoch 5: loss 16.124802723138227
Epoch 6: loss 16.123957490921022
Epoch 7: loss 16.12350820976755
Epoch 8: loss 16.12249842001044
Epoch 9: loss 16.121890319948612
Epoch 10: loss 16.121136725467185
Epoch 11: loss 16.12037362534067
Epoch 12: loss 16.11967716528022
Epoch 13: loss 16.118884529238162
Epoch 14: loss 16.117798591696697
Epoch 15: loss 16.11723600263181
Epoch 16: loss 16.116621000870413
Epoch 17: loss 16.11559424193009
Epoch 18: loss 16.115162885707356
Epoch 19: loss 16.114483861301256
Epoch 20: loss 16.113312045387598
Epoch 21: loss 16.112580846703572
Epoch 22: loss 16.111727248067442
Epoch 23: loss 16.111144260738207
Epoch 24: loss 16.110616718167844
Epoch 25: loss 16.1095853390901
Epoch 26: loss 16.10893678665161
Epoch 27: loss 16.108228373527528
Epoch 28: loss 16.107476467671603
Epoch 29: loss 16.10657867245052
Epoch 30: loss 16.105534410476686
Epoch 31: loss 16.105330521127453
Epoch 32: loss 16.104567691554195
Epoch 33: loss 16.103479178055473
Epoch 34: loss 16.103005939981212
Epoch 35: loss 16.102325995072075
Epoch 36: loss 16.101568847117218
Epoch 37: loss 16.10085841987444
Epoch 38: loss 16.09975883235102
Epoch 39: loss 16.099117053073385
Epoch 40: loss 16.09835238871367
Epoch 41: loss 16.097659005289493
Epoch 42: loss 16.096660178640615
Epoch 43: loss 16.096072790933693
Epoch 44: loss 16.0950939261395
Epoch 45: loss 16.094283318519594
Epoch 46: loss 16.09354938942453
Epoch 47: loss 16.092751904155897
Epoch 48: loss 16.09188704179681
Epoch 49: loss 16.091410632755444
-----------Time: 0:03:15.860856, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 150, rmse: 4.012456893920898-------------


Epoch 0: loss 16.12870412909466
Epoch 1: loss 16.128018136646435
Epoch 2: loss 16.127106767115386
Epoch 3: loss 16.126418781280517
Epoch 4: loss 16.125509210254837
Epoch 5: loss 16.124917183751645
Epoch 6: loss 16.124014473998027
Epoch 7: loss 16.12341408625893
Epoch 8: loss 16.12299118352973
Epoch 9: loss 16.121897745132447
Epoch 10: loss 16.120939437202786
Epoch 11: loss 16.120469595038372
Epoch 12: loss 16.119759282858475
Epoch 13: loss 16.118864517626555
Epoch 14: loss 16.117986941337584
Epoch 15: loss 16.11738847338635
Epoch 16: loss 16.11683387445367
Epoch 17: loss 16.115586918333303
Epoch 18: loss 16.115197866895926
Epoch 19: loss 16.113966234870578
Epoch 20: loss 16.113507365143818
Epoch 21: loss 16.1131152920101
Epoch 22: loss 16.11202085432799
Epoch 23: loss 16.111061938949252
Epoch 24: loss 16.110784275635428
Epoch 25: loss 16.10958235989446
Epoch 26: loss 16.108505495734835
Epoch 27: loss 16.10828628436379
Epoch 28: loss 16.10752892183221
Epoch 29: loss 16.10671118653339
Epoch 30: loss 16.10588944580244
Epoch 31: loss 16.104827583354453
Epoch 32: loss 16.104362194434458
Epoch 33: loss 16.10362443405649
Epoch 34: loss 16.10229075576948
Epoch 35: loss 16.102267735937367
Epoch 36: loss 16.100949131924175
Epoch 37: loss 16.10067879739015
Epoch 38: loss 16.099706713013028
Epoch 39: loss 16.0989420020062
Epoch 40: loss 16.09790330555128
Epoch 41: loss 16.097313025723334
Epoch 42: loss 16.097085093415302
Epoch 43: loss 16.09558222397514
Epoch 44: loss 16.09492974695952
Epoch 45: loss 16.094621460334114
Epoch 46: loss 16.093834748475448
Epoch 47: loss 16.0927344902702
Epoch 48: loss 16.091991579014323
Epoch 49: loss 16.091390821208126
-----------Time: 0:03:06.781381, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-07, embedding_dim: 200, rmse: 4.012455463409424-------------


Epoch 0: loss 16.125679308435192
Epoch 1: loss 16.117748896971992
Epoch 2: loss 16.110733351500137
Epoch 3: loss 16.10257915413898
Epoch 4: loss 16.094635968622953
Epoch 5: loss 16.087485688665637
Epoch 6: loss 16.08003664949666
Epoch 7: loss 16.072259901917498
Epoch 8: loss 16.06444906151813
Epoch 9: loss 16.05702968991321
Epoch 10: loss 16.049282825511433
Epoch 11: loss 16.041733063822207
Epoch 12: loss 16.0339054107666
Epoch 13: loss 16.026891494833905
Epoch 14: loss 16.01878867563994
Epoch 15: loss 16.011369688614554
Epoch 16: loss 16.00363277663355
Epoch 17: loss 15.995844477155934
Epoch 18: loss 15.988201936431553
Epoch 19: loss 15.980833084686942
Epoch 20: loss 15.973169243854025
Epoch 21: loss 15.965266873525536
Epoch 22: loss 15.957876956981162
Epoch 23: loss 15.950110687380251
Epoch 24: loss 15.94267940002939
Epoch 25: loss 15.93517182080642
Epoch 26: loss 15.927642681287683
Epoch 27: loss 15.919809824487437
Epoch 28: loss 15.912394375386446
Epoch 29: loss 15.904940705714019
Epoch 30: loss 15.897319758456685
Epoch 31: loss 15.88975911140442
Epoch 32: loss 15.882128641916358
Epoch 33: loss 15.874396805141282
Epoch 34: loss 15.866902940169625
Epoch 35: loss 15.859436341990595
Epoch 36: loss 15.851895990579024
Epoch 37: loss 15.844198475713315
Epoch 38: loss 15.836759895863741
Epoch 39: loss 15.829150648739027
Epoch 40: loss 15.821693064855493
Epoch 41: loss 15.814221284700476
Epoch 42: loss 15.80655886919602
Epoch 43: loss 15.798906780325849
Epoch 44: loss 15.791517764589061
Epoch 45: loss 15.784025187077729
Epoch 46: loss 15.776429317308509
Epoch 47: loss 15.768775991771532
Epoch 48: loss 15.76111664461053
Epoch 49: loss 15.753536010825115
-----------Time: 0:01:56.680538, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 20, rmse: 3.969942331314087-------------


Epoch 0: loss 16.1253184536229
Epoch 1: loss 16.117864488518755
Epoch 2: loss 16.1102009607398
Epoch 3: loss 16.10263364315033
Epoch 4: loss 16.094739146854568
Epoch 5: loss 16.087122065088025
Epoch 6: loss 16.07971354360166
Epoch 7: loss 16.071817106785982
Epoch 8: loss 16.064237566616224
Epoch 9: loss 16.056742675408074
Epoch 10: loss 16.049025407044784
Epoch 11: loss 16.04123305756113
Epoch 12: loss 16.033938515704612
Epoch 13: loss 16.02629698670429
Epoch 14: loss 16.01850550071053
Epoch 15: loss 16.01104687400486
Epoch 16: loss 16.003103354702827
Epoch 17: loss 15.995474845430126
Epoch 18: loss 15.988193192689316
Epoch 19: loss 15.980554163974265
Epoch 20: loss 15.972662271624026
Epoch 21: loss 15.965365394302037
Epoch 22: loss 15.957752549129983
Epoch 23: loss 15.950178122520446
Epoch 24: loss 15.942220233834307
Epoch 25: loss 15.934768170895785
Epoch 26: loss 15.92720871904622
Epoch 27: loss 15.919835926138836
Epoch 28: loss 15.9121004415595
Epoch 29: loss 15.905041955864947
Epoch 30: loss 15.896478731735893
Epoch 31: loss 15.888938595937645
Epoch 32: loss 15.881547708096711
Epoch 33: loss 15.873968733911928
Epoch 34: loss 15.866753523246102
Epoch 35: loss 15.858879438690517
Epoch 36: loss 15.851302753324093
Epoch 37: loss 15.843759068198827
Epoch 38: loss 15.836093308614648
Epoch 39: loss 15.82862036642821
Epoch 40: loss 15.820975090109783
Epoch 41: loss 15.813561598114346
Epoch 42: loss 15.805681514739991
Epoch 43: loss 15.798483246305715
Epoch 44: loss 15.790685136421867
Epoch 45: loss 15.78323452576347
Epoch 46: loss 15.775433400402898
Epoch 47: loss 15.768091533495033
Epoch 48: loss 15.760557349868442
Epoch 49: loss 15.75282148692919
-----------Time: 0:02:27.060302, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 50, rmse: 3.9698269367218018-------------


Epoch 0: loss 16.125371075713115
Epoch 1: loss 16.118043734716334
Epoch 2: loss 16.109863241859106
Epoch 3: loss 16.102371522654657
Epoch 4: loss 16.095144490573716
Epoch 5: loss 16.087467754405477
Epoch 6: loss 16.079624355357627
Epoch 7: loss 16.07203298755314
Epoch 8: loss 16.06409642903701
Epoch 9: loss 16.056564398433853
Epoch 10: loss 16.04897618604743
Epoch 11: loss 16.041147974263065
Epoch 12: loss 16.033923659117324
Epoch 13: loss 16.026333986157955
Epoch 14: loss 16.018214551262233
Epoch 15: loss 16.011216463213383
Epoch 16: loss 16.0036672146424
Epoch 17: loss 15.995925022208173
Epoch 18: loss 15.987968726780103
Epoch 19: loss 15.980384466959082
Epoch 20: loss 15.972914572384047
Epoch 21: loss 15.964544926518979
Epoch 22: loss 15.957414151274639
Epoch 23: loss 15.949944261882615
Epoch 24: loss 15.942240364655204
Epoch 25: loss 15.934930346323096
Epoch 26: loss 15.927061943385912
Epoch 27: loss 15.919474550952081
Epoch 28: loss 15.911691724735757
Epoch 29: loss 15.903903112204178
Epoch 30: loss 15.896271768860196
Epoch 31: loss 15.888978064578513
Epoch 32: loss 15.881130242347718
Epoch 33: loss 15.873672243823176
Epoch 34: loss 15.865909544281337
Epoch 35: loss 15.858192068597544
Epoch 36: loss 15.850275122601053
Epoch 37: loss 15.842689774347388
Epoch 38: loss 15.835133622003639
Epoch 39: loss 15.827109361731488
Epoch 40: loss 15.819301067227903
Epoch 41: loss 15.81171485548434
Epoch 42: loss 15.803458240757818
Epoch 43: loss 15.79580934876981
Epoch 44: loss 15.787871701821038
Epoch 45: loss 15.780077517550925
Epoch 46: loss 15.772226513987002
Epoch 47: loss 15.763947489987249
Epoch 48: loss 15.755797698186791
Epoch 49: loss 15.747842470459316
-----------Time: 0:02:47.471603, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 100, rmse: 3.9691741466522217-------------


Epoch 0: loss 16.1252908271292
Epoch 1: loss 16.11785688192948
Epoch 2: loss 16.11025533468827
Epoch 3: loss 16.102526429425115
Epoch 4: loss 16.0948588848114
Epoch 5: loss 16.086790503626286
Epoch 6: loss 16.07978451355644
Epoch 7: loss 16.071810738936716
Epoch 8: loss 16.06467097738515
Epoch 9: loss 16.056606226382048
Epoch 10: loss 16.04896704632303
Epoch 11: loss 16.041938582710596
Epoch 12: loss 16.033814864573273
Epoch 13: loss 16.02616583575373
Epoch 14: loss 16.018330224700595
Epoch 15: loss 16.01075217205545
Epoch 16: loss 16.002906485225843
Epoch 17: loss 15.99592223789381
Epoch 18: loss 15.987522291100543
Epoch 19: loss 15.980633276441823
Epoch 20: loss 15.97276353628739
Epoch 21: loss 15.965122076739435
Epoch 22: loss 15.957132508443749
Epoch 23: loss 15.949503849900287
Epoch 24: loss 15.941395642446436
Epoch 25: loss 15.934016042170317
Epoch 26: loss 15.926605524187503
Epoch 27: loss 15.918349004828412
Epoch 28: loss 15.91053849717845
Epoch 29: loss 15.902398372733074
Epoch 30: loss 15.894629126009734
Epoch 31: loss 15.886491105867469
Epoch 32: loss 15.878092988677647
Epoch 33: loss 15.870280729169432
Epoch 34: loss 15.862026323442874
Epoch 35: loss 15.85383876717609
Epoch 36: loss 15.844921899878461
Epoch 37: loss 15.836376112440359
Epoch 38: loss 15.827697072858395
Epoch 39: loss 15.818641527839329
Epoch 40: loss 15.80957022542539
Epoch 41: loss 15.800340323862821
Epoch 42: loss 15.791130118784697
Epoch 43: loss 15.781817789699721
Epoch 44: loss 15.772180779083916
Epoch 45: loss 15.761266145498857
Epoch 46: loss 15.751368756916213
Epoch 47: loss 15.740888751071433
Epoch 48: loss 15.730433251546776
Epoch 49: loss 15.718928343316783
-----------Time: 0:03:23.807552, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 150, rmse: 3.9653942584991455-------------


Epoch 0: loss 16.12543299301811
Epoch 1: loss 16.117638734112614
Epoch 2: loss 16.110014725768046
Epoch 3: loss 16.10261680872544
Epoch 4: loss 16.094512691705123
Epoch 5: loss 16.086819370933203
Epoch 6: loss 16.079607074157053
Epoch 7: loss 16.072082575507785
Epoch 8: loss 16.06421567875406
Epoch 9: loss 16.056648412994715
Epoch 10: loss 16.048965257147085
Epoch 11: loss 16.041183887357295
Epoch 12: loss 16.034083510481793
Epoch 13: loss 16.026225728574005
Epoch 14: loss 16.01851514007734
Epoch 15: loss 16.01075764324354
Epoch 16: loss 16.002582156139873
Epoch 17: loss 15.995182394981384
Epoch 18: loss 15.98767886472785
Epoch 19: loss 15.97940869746001
Epoch 20: loss 15.972048574945202
Epoch 21: loss 15.963868243797966
Epoch 22: loss 15.955724016479824
Epoch 23: loss 15.94781857262487
Epoch 24: loss 15.939715222690417
Epoch 25: loss 15.931479868681535
Epoch 26: loss 15.923152504796567
Epoch 27: loss 15.914761772363082
Epoch 28: loss 15.905838273919146
Epoch 29: loss 15.896825358142024
Epoch 30: loss 15.887921926249629
Epoch 31: loss 15.878582550131757
Epoch 32: loss 15.86890149012856
Epoch 33: loss 15.858909914804542
Epoch 34: loss 15.848681248789248
Epoch 35: loss 15.838334326122117
Epoch 36: loss 15.827687027143396
Epoch 37: loss 15.816522924796395
Epoch 38: loss 15.805290105031885
Epoch 39: loss 15.7930443794831
Epoch 40: loss 15.780116290631502
Epoch 41: loss 15.767383521536122
Epoch 42: loss 15.75410842584527
Epoch 43: loss 15.74011253895967
Epoch 44: loss 15.726142759945082
Epoch 45: loss 15.711333490454633
Epoch 46: loss 15.696108996349832
Epoch 47: loss 15.680115475861923
Epoch 48: loss 15.66401570154273
Epoch 49: loss 15.647048275367073
-----------Time: 0:04:06.369368, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-06, embedding_dim: 200, rmse: 3.9559154510498047-------------


Epoch 0: loss 16.091333931425343
Epoch 1: loss 16.01519027378248
Epoch 2: loss 15.939784573472064
Epoch 3: loss 15.863268959003946
Epoch 4: loss 15.788235667477483
Epoch 5: loss 15.71274250590283
Epoch 6: loss 15.637825443433679
Epoch 7: loss 15.562732763912367
Epoch 8: loss 15.488050976006882
Epoch 9: loss 15.413378865822501
Epoch 10: loss 15.339057531564132
Epoch 11: loss 15.26494458861973
Epoch 12: loss 15.190851089228754
Epoch 13: loss 15.116859291947407
Epoch 14: loss 15.043198841551076
Epoch 15: loss 14.969132599623308
Epoch 16: loss 14.895904008201931
Epoch 17: loss 14.821983858813411
Epoch 18: loss 14.748050171396006
Epoch 19: loss 14.674481225013732
Epoch 20: loss 14.600794727905937
Epoch 21: loss 14.526822276737379
Epoch 22: loss 14.452556273211604
Epoch 23: loss 14.377656141571377
Epoch 24: loss 14.302683285008307
Epoch 25: loss 14.22676417309305
Epoch 26: loss 14.150751024743785
Epoch 27: loss 14.073936376364335
Epoch 28: loss 13.995783348705457
Epoch 29: loss 13.917060374177021
Epoch 30: loss 13.837226416753685
Epoch 31: loss 13.756083303949108
Epoch 32: loss 13.673654830974082
Epoch 33: loss 13.589528937961743
Epoch 34: loss 13.503867194963538
Epoch 35: loss 13.41628625911215
Epoch 36: loss 13.327008963667828
Epoch 37: loss 13.234966120512588
Epoch 38: loss 13.141586967136549
Epoch 39: loss 13.04509643368099
Epoch 40: loss 12.94648949685304
Epoch 41: loss 12.845505487400553
Epoch 42: loss 12.74118131554645
Epoch 43: loss 12.634144751921944
Epoch 44: loss 12.52429510510486
Epoch 45: loss 12.411188352626302
Epoch 46: loss 12.295037368069524
Epoch 47: loss 12.175515260903731
Epoch 48: loss 12.053081823431928
Epoch 49: loss 11.927108789526898
-----------Time: 0:02:51.039371, Loss: regression, n_iter: 50, l2: 1e-06, batch_size: 1024, learning_rate: 1e-05, embedding_dim: 20, rmse: 3.4482691287994385-------------


Epoch 0: loss 16.091714224608047
Epoch 1: loss 16.014911401790123
Epoch 2: loss 15.939120671023494
Epoch 3: loss 15.863510065493376
Epoch 4: loss 15.787748992961387
Epoch 5: loss 15.71172431966533
Epoch 6: loss 15.635806501430014
Epoch 7: loss 15.558760110191677
Epoch 8: loss 15.480867367205413
Epoch 9: loss 15.401469538522804
Epoch 10: loss 15.318849790614584
Epoch 11: loss 15.232349622767904
Epoch 12: loss 15.139990772371707
Epoch 13: loss 15.040907609981039
Epoch 14: loss 14.934205063529637
Epoch 15: loss 14.817926300090292
Epoch 16: loss 14.691196519395579
Epoch 17: loss 14.553633856773377
Epoch 18: loss 14.404817644409512
Epoch 19: loss 14.243935842099397
Epoch 20: loss 14.07118252982264
Epoch 21: loss 13.886685312312583
Epoch 22: loss 13.689926423197207
Epoch 23: loss 13.480863060121951